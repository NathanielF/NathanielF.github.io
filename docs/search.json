[
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "import pymc as pm\nimport bambi as bmb\nimport pandas as pd\nimport arviz as az\nfrom bambi.plots import plot_cap\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import stats\n\n:"
  },
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#modelling-improvement-as-lift-across-pooled-experiments",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#modelling-improvement-as-lift-across-pooled-experiments",
    "title": "Examined Algorithms",
    "section": "Modelling Improvement as Lift across Pooled Experiments",
    "text": "Modelling Improvement as Lift across Pooled Experiments\nIt’s useful fact that the Lift measurement of success can be more nicely modelled under a log transform. In this analysis we’ll follow the example in Demetri’s blog: https://dpananos.github.io/posts/2022-07-20-pooling-experiments/ and demonstrate how we can pool information across seperate experiments. In particular we’ll see why this type of modelling is apt for planning expected amount of cumulative gains over successive experiments. First observe how the Lift measurement can can be transformed to facilitate modelling.\n\nnp.random.seed(11)\nfig, axs = plt.subplots(2, 2, figsize=(20, 8))\naxs = axs.flatten()\ncounts_success_control = np.random.normal(10, 2, 100)\ncounts_success_treatment = np.random.normal(8, 2, 100)\naxs[0].hist(counts_success_control, alpha=.3, label='control', edgecolor='black')\naxs[0].hist(counts_success_treatment, alpha=0.2, label='treatment', edgecolor='black')\naxs[0].legend()\naxs[0].set_title(\"Successes in Treatment and Control for 100 Experiments\")\nrr = counts_success_treatment / counts_success_control\naxs[1].hist(rr, alpha=0.3, label='RR/Lift', color='y', edgecolor='black')\naxs[1].legend()\naxs[1].set_title(\"RR/Lift\")\naxs[2].hist(np.log(rr), label='logged Lift', alpha=0.4, color='green', edgecolor='black')\naxs[2].set_title(\"Logged RR/Lift\")\naxs[3].hist(np.exp(np.log(rr)), label='exponentiated Logged Lift', alpha=0.4, color='purple', edgecolor='black')\naxs[3].set_title(\" Exp Logged RR/Lift\")\n\nText(0.5, 1.0, ' Exp Logged RR/Lift')"
  },
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#example-experiments-with-count-successes",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#example-experiments-with-count-successes",
    "title": "Examined Algorithms",
    "section": "Example Experiment(s) with Count Successes",
    "text": "Example Experiment(s) with Count Successes\nThe original data was modelled in the Stan probabilistic programming language. We’ll use this opportunity to translate the code into a pymc implementation.\nIn the scenario we have 12 seperate experiments with 50,000 units on either arm of the experiment. The were desigined to detect conversion in the arm of each trial. Only four of the experiments were successful in the sense that they showed a positive lift distinguishable from statistical noise under a 5% p-value threshold. Management wishes to achieve a total Lift of 2x over the next year. We want to determine how plausible that goal is given our track record so far.\n\ndf_pooling = pd.read_csv('pooling_data.csv')\ndf_pooling\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      n_per_group\n      y_txt\n      y_control\n      estimated_relative_lift\n      pvals\n      experiment\n      estimated_sd_relative_lift\n      estimated_log_relative_lift\n    \n  \n  \n    \n      0\n      1\n      50000\n      551\n      492\n      1.119919\n      0.035510\n      1\n      0.061704\n      0.113256\n    \n    \n      1\n      2\n      50000\n      510\n      490\n      1.040816\n      0.272968\n      2\n      0.062941\n      0.040005\n    \n    \n      2\n      3\n      50000\n      548\n      509\n      1.076621\n      0.119989\n      3\n      0.061233\n      0.073827\n    \n    \n      3\n      4\n      50000\n      537\n      511\n      1.050881\n      0.218777\n      4\n      0.061475\n      0.049629\n    \n    \n      4\n      5\n      50000\n      558\n      508\n      1.098425\n      0.065669\n      5\n      0.060997\n      0.093878\n    \n    \n      5\n      6\n      50000\n      542\n      489\n      1.108384\n      0.051774\n      6\n      0.062048\n      0.102904\n    \n    \n      6\n      7\n      50000\n      533\n      495\n      1.076768\n      0.123029\n      7\n      0.062100\n      0.073964\n    \n    \n      7\n      8\n      50000\n      544\n      468\n      1.162393\n      0.008903\n      8\n      0.062729\n      0.150481\n    \n    \n      8\n      9\n      50000\n      519\n      521\n      0.996161\n      0.512433\n      9\n      0.061694\n      -0.003846\n    \n    \n      9\n      10\n      50000\n      532\n      469\n      1.134328\n      0.024447\n      10\n      0.063023\n      0.126041\n    \n    \n      10\n      11\n      50000\n      555\n      487\n      1.139630\n      0.018467\n      11\n      0.061767\n      0.130704\n    \n    \n      11\n      12\n      50000\n      525\n      497\n      1.056338\n      0.197962\n      12\n      0.062264\n      0.054808\n    \n  \n\n\n\n\n\nfig, axs = plt.subplots(2, 2, figsize=(20, 10))\naxs = axs.flatten()\naxs[0].hist(df_pooling['y_control'], alpha=.3, label='control', edgecolor='black')\naxs[0].hist(df_pooling['y_txt'], alpha=0.2, label='treatment', edgecolor='black')\naxs[0].legend()\naxs[0].set_title(\"Successes in Treatment and Control for 100 Experiments\")\nrr = df_pooling['y_txt'] / df_pooling['y_control']\naxs[1].hist(rr, alpha=0.3, label='RR/Lift', color='y', edgecolor='black')\naxs[1].legend()\naxs[1].set_title(\"RR/Lift\")\naxs[2].hist(np.log(rr), label='logged Lift', alpha=0.4, color='green', edgecolor='black')\naxs[2].set_title(\"Logged RR/Lift\")\naxs[3].hist(np.exp(np.log(rr)), label='exponentiated Logged Lift', alpha=0.4, color='purple', edgecolor='black')\naxs[3].set_title(\" Exp Logged RR/Lift\")\n\nText(0.5, 1.0, ' Exp Logged RR/Lift')"
  },
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#the-model",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#the-model",
    "title": "Examined Algorithms",
    "section": "The Model",
    "text": "The Model\nWe want to pool the information across our 12 experiments and to do so we model them hierarchically as a draws from an overarching normal distribution. The assumptions means we have to set priors on the shape of our parameters. We allow the hierarchical Normal distribution be configured with a centre of mass drawn from the StudentT distribution ensuring that we can have heavy tails in the distribution to account for outlier experiments with massive returns. The code and structure of the model are displayed below:\n\nwith pm.Model() as model:\n     pass\n\nmodel.add_coord('exp_id', list(range(12)), mutable=True)\n\nwith model:\n    exp_id = pm.MutableData(\"exp\", list(range(12)))\n    # Priors for the Hierarchical Log Lift Distribution\n    mu_metric = pm.StudentT('mu_metric', mu=0, sigma=2.5, nu=3)\n    sig_ex = pm.HalfCauchy('sig_ex', 0.01)\n\n    # Priors for the Individual effects for each experiment\n    z_ex = pm.Normal('z_ex', 0, 1, dims='exp_id')\n    \n    # Convenience wrappers for inputting fresh data\n    est_lift_sd = pm.MutableData('est_lift_sd', df_pooling['estimated_sd_relative_lift'], dims='exp_id')\n    est_log_lift = pm.MutableData('est_log_lift', df_pooling['estimated_log_relative_lift'], dims='exp_id')\n\n    ## pooling the indivdual experiemnts, ensuring shrinkage to the overall mean\n    true_log_rr = pm.Deterministic('true_log_rr', mu_metric + z_ex[exp_id]*sig_ex, dims='exp_id')\n\n    ## Likelihood model for Logged Lift using observed values\n    estimated_log_relative_lift = pm.Normal(\"estimated_log_relative_lift\", mu=true_log_rr[exp_id], sigma=est_lift_sd[exp_id], \n                                            observed=est_log_lift, dims=\"exp_id\")\n                \n    estimated_relative_lift = pm.Deterministic('estimated_relative_lift', pm.math.exp(estimated_log_relative_lift[exp_id]), dims='exp_id')\n    \n\n\npm.model_to_graphviz(model)"
  },
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#the-estimation-step",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#the-estimation-step",
    "title": "Examined Algorithms",
    "section": "The Estimation Step",
    "text": "The Estimation Step\nIn the Bayesian workflow we sample both the priors, the prior predictive and posterior_predictive distributions. These allow us to assess model fit and the degree to which our model captures the observed data.\n\nwith model:\n    idata = pm.sample()\n    idata.extend(pm.sample_prior_predictive(samples=50, random_seed=1))\n    idata.extend(pm.sample_posterior_predictive(idata, var_names=[\"estimated_log_relative_lift\", \"mu_metric\", \"sig_ex\"]))\n\nAuto-assigning NUTS sampler...\nINFO:pymc:Auto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nINFO:pymc:Initializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nINFO:pymc:Multiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu_metric, sig_ex, z_ex]\nINFO:pymc:NUTS: [mu_metric, sig_ex, z_ex]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:07<00:00 Sampling 4 chains, 21 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 26 seconds.\nINFO:pymc:Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 26 seconds.\nThere was 1 divergence after tuning. Increase `target_accept` or reparameterize.\nERROR:pymc:There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\nThere were 6 divergences after tuning. Increase `target_accept` or reparameterize.\nERROR:pymc:There were 6 divergences after tuning. Increase `target_accept` or reparameterize.\nThere were 11 divergences after tuning. Increase `target_accept` or reparameterize.\nERROR:pymc:There were 11 divergences after tuning. Increase `target_accept` or reparameterize.\nThere were 3 divergences after tuning. Increase `target_accept` or reparameterize.\nERROR:pymc:There were 3 divergences after tuning. Increase `target_accept` or reparameterize.\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:01<00:00]"
  },
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#plotting-and-convergence-checks",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#plotting-and-convergence-checks",
    "title": "Examined Algorithms",
    "section": "Plotting and Convergence checks",
    "text": "Plotting and Convergence checks\n\naz.plot_trace(idata, var_names=['mu_metric', 'z_ex', 'true_log_rr', 'sig_ex'], figsize=(20, 8));\n\n\n\n\n\naz.plot_ppc(idata, figsize=(20, 7), kind='scatter');\n\n\n\n\n\naz.summary(idata)\n\n/Users/nathanielforde/Documents/Gitlab/async_research_club/.venv/lib/python3.9/site-packages/arviz/stats/diagnostics.py:586: RuntimeWarning: invalid value encountered in double_scalars\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      mu_metric\n      0.083\n      0.018\n      0.050\n      0.118\n      0.000\n      0.000\n      4133.0\n      2445.0\n      1.0\n    \n    \n      z_ex[0]\n      0.041\n      0.987\n      -1.774\n      1.968\n      0.014\n      0.017\n      4873.0\n      2808.0\n      1.0\n    \n    \n      z_ex[1]\n      -0.088\n      0.995\n      -2.017\n      1.718\n      0.014\n      0.017\n      5093.0\n      2906.0\n      1.0\n    \n    \n      z_ex[2]\n      -0.037\n      0.971\n      -1.795\n      1.796\n      0.014\n      0.016\n      4757.0\n      3008.0\n      1.0\n    \n    \n      z_ex[3]\n      -0.079\n      1.011\n      -1.973\n      1.788\n      0.014\n      0.017\n      4925.0\n      2573.0\n      1.0\n    \n    \n      z_ex[4]\n      0.046\n      0.974\n      -1.860\n      1.815\n      0.013\n      0.016\n      5477.0\n      2826.0\n      1.0\n    \n    \n      z_ex[5]\n      0.041\n      0.994\n      -1.815\n      1.892\n      0.015\n      0.019\n      4581.0\n      2276.0\n      1.0\n    \n    \n      z_ex[6]\n      -0.003\n      0.965\n      -1.727\n      1.879\n      0.015\n      0.016\n      4196.0\n      2759.0\n      1.0\n    \n    \n      z_ex[7]\n      0.165\n      0.987\n      -1.713\n      1.959\n      0.015\n      0.017\n      4426.0\n      2569.0\n      1.0\n    \n    \n      z_ex[8]\n      -0.183\n      0.992\n      -2.046\n      1.649\n      0.014\n      0.016\n      4770.0\n      2848.0\n      1.0\n    \n    \n      z_ex[9]\n      0.077\n      0.998\n      -1.664\n      2.037\n      0.014\n      0.017\n      4990.0\n      2714.0\n      1.0\n    \n    \n      z_ex[10]\n      0.115\n      0.998\n      -1.752\n      1.988\n      0.014\n      0.018\n      5001.0\n      2612.0\n      1.0\n    \n    \n      z_ex[11]\n      -0.082\n      0.997\n      -1.909\n      1.840\n      0.015\n      0.018\n      4634.0\n      2564.0\n      1.0\n    \n    \n      sig_ex\n      0.010\n      0.010\n      0.000\n      0.027\n      0.000\n      0.000\n      2690.0\n      1933.0\n      1.0\n    \n    \n      true_log_rr[0]\n      0.084\n      0.021\n      0.046\n      0.126\n      0.000\n      0.000\n      4049.0\n      2457.0\n      1.0\n    \n    \n      true_log_rr[1]\n      0.082\n      0.021\n      0.042\n      0.121\n      0.000\n      0.000\n      3788.0\n      2655.0\n      1.0\n    \n    \n      true_log_rr[2]\n      0.083\n      0.022\n      0.042\n      0.123\n      0.000\n      0.000\n      4097.0\n      2861.0\n      1.0\n    \n    \n      true_log_rr[3]\n      0.082\n      0.022\n      0.040\n      0.121\n      0.000\n      0.000\n      3632.0\n      2416.0\n      1.0\n    \n    \n      true_log_rr[4]\n      0.084\n      0.021\n      0.044\n      0.123\n      0.000\n      0.000\n      4032.0\n      2419.0\n      1.0\n    \n    \n      true_log_rr[5]\n      0.084\n      0.022\n      0.042\n      0.124\n      0.000\n      0.000\n      4236.0\n      2668.0\n      1.0\n    \n    \n      true_log_rr[6]\n      0.083\n      0.021\n      0.045\n      0.123\n      0.000\n      0.000\n      3702.0\n      2731.0\n      1.0\n    \n    \n      true_log_rr[7]\n      0.086\n      0.022\n      0.043\n      0.126\n      0.000\n      0.000\n      4035.0\n      2712.0\n      1.0\n    \n    \n      true_log_rr[8]\n      0.080\n      0.022\n      0.040\n      0.121\n      0.000\n      0.000\n      3619.0\n      2592.0\n      1.0\n    \n    \n      true_log_rr[9]\n      0.085\n      0.022\n      0.042\n      0.123\n      0.000\n      0.000\n      3831.0\n      2801.0\n      1.0\n    \n    \n      true_log_rr[10]\n      0.085\n      0.022\n      0.045\n      0.124\n      0.000\n      0.000\n      3814.0\n      2450.0\n      1.0\n    \n    \n      true_log_rr[11]\n      0.082\n      0.022\n      0.043\n      0.124\n      0.000\n      0.000\n      3962.0\n      2829.0\n      1.0\n    \n    \n      estimated_relative_lift[0]\n      1.120\n      0.000\n      1.120\n      1.120\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[1]\n      1.041\n      0.000\n      1.041\n      1.041\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[2]\n      1.077\n      0.000\n      1.077\n      1.077\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[3]\n      1.051\n      0.000\n      1.051\n      1.051\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[4]\n      1.098\n      0.000\n      1.098\n      1.098\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[5]\n      1.108\n      0.000\n      1.108\n      1.108\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[6]\n      1.077\n      0.000\n      1.077\n      1.077\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[7]\n      1.162\n      0.000\n      1.162\n      1.162\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[8]\n      0.996\n      0.000\n      0.996\n      0.996\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[9]\n      1.134\n      0.000\n      1.134\n      1.134\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[10]\n      1.140\n      0.000\n      1.140\n      1.140\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[11]\n      1.056\n      0.000\n      1.056\n      1.056\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n  \n\n\n\n\n\nSimulate Draws from the Posterior and Calculate Lift\nThe Stan implementation has the functionality to enable the calculation of generated quantities on the fly within a model run. We need to replicate that functionality outside of our model making use of the estimated posterior distributions on the model parameters. We calculate the effect size engendered by an observed difference in proportions of conversion across the experiments, then calculate whether the simulated was large enough that we had the power to detect it against a baseline of 1% conversion. The quantities are used to define the amount of detected lift in our posterior distribution. This in turn can be used to project the amount of cumulative lift we will see over 12 experiments.\n\nidata.stack(sample=[\"chain\", \"draw\"], inplace=True)\n\n\ngenerated_quanties = []\nfor i in range(12):\n    generated_data = pd.DataFrame({'mu_metric': idata['posterior']['mu_metric'].values,\n        'sig_ex': idata['posterior']['sig_ex'].values\n        }\n    )\n    generated_data['log_rr_over_the_year'] = generated_data.apply(lambda x: np.random.normal(x['mu_metric'], x['sig_ex'], 1)[0], axis=1)\n    generated_data['rr_over_the_year'] = np.exp(generated_data['log_rr_over_the_year'])\n    ## Calculate effect size for proportions against base 0.01\n    generated_data['es'] = generated_data.apply(lambda x: 2*np.arcsin(np.sqrt(x['rr_over_the_year'] * 0.01)) -  2*np.arcsin(np.sqrt(0.01)), axis=1)\n    ## Calculate power based on difference from baseline with known sample size\n    generated_data['power'] = generated_data.apply(lambda x: 1 - stats.norm.cdf( 1.644854 - x['es'] * np.sqrt(50_000/2), 0, 1), axis=1)\n    ## Weight lift by our ability to detect given power in experiment\n    generated_data['detected_lift'] = generated_data['power']* np.log(generated_data['rr_over_the_year'])\n    generated_data['experiment'] = i\n    generated_data['draw'] = generated_data.index\n    generated_quanties.append(generated_data)\n\nforecast_df = pd.concat(generated_quanties)\nforecast_df\n\n\n\n\n\n  \n    \n      \n      mu_metric\n      sig_ex\n      log_rr_over_the_year\n      rr_over_the_year\n      es\n      power\n      detected_lift\n      experiment\n      draw\n    \n  \n  \n    \n      0\n      0.053614\n      0.003751\n      0.054232\n      1.055729\n      0.005526\n      0.220312\n      0.011948\n      0\n      0\n    \n    \n      1\n      0.089769\n      0.006848\n      0.078595\n      1.081766\n      0.008058\n      0.355404\n      0.027933\n      0\n      1\n    \n    \n      2\n      0.082105\n      0.008848\n      0.075538\n      1.078464\n      0.007739\n      0.336777\n      0.025440\n      0\n      2\n    \n    \n      3\n      0.076757\n      0.031000\n      0.098424\n      1.103431\n      0.010142\n      0.483548\n      0.047593\n      0\n      3\n    \n    \n      4\n      0.076757\n      0.031000\n      0.081759\n      1.085194\n      0.008389\n      0.375086\n      0.030667\n      0\n      4\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3995\n      0.102766\n      0.012876\n      0.118111\n      1.125369\n      0.012232\n      0.613782\n      0.072494\n      11\n      3995\n    \n    \n      3996\n      0.066032\n      0.000387\n      0.065700\n      1.067907\n      0.006714\n      0.279849\n      0.018386\n      11\n      3996\n    \n    \n      3997\n      0.062600\n      0.007694\n      0.067979\n      1.070343\n      0.006951\n      0.292589\n      0.019890\n      11\n      3997\n    \n    \n      3998\n      0.090900\n      0.005162\n      0.103968\n      1.109565\n      0.010729\n      0.520525\n      0.054118\n      11\n      3998\n    \n    \n      3999\n      0.064391\n      0.010588\n      0.065984\n      1.068209\n      0.006743\n      0.281419\n      0.018569\n      11\n      3999\n    \n  \n\n48000 rows × 9 columns\n\n\n\n\n\nGenerate Cumulative Lift Curves for N-Experiments\nWe can now line up the draws for each of our experiments and calculate the cumulative lift by taking the cumulative sum and then exponentiating to return us to the raw Lift scale.\n\ndraws_per_experiment = forecast_df.pivot('experiment', 'draw', 'detected_lift')\n## Probability of Independent events sum on the log scale\ndraws_per_experiment = np.exp(draws_per_experiment.cumsum()).T\ndraws_per_experiment\n\n\n\n\n\n  \n    \n      experiment\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n    \n    \n      draw\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      1.108264\n      1.184080\n      1.270116\n      1.344759\n      1.382252\n      1.493910\n      1.574159\n      1.636984\n      1.770258\n      1.896812\n      2.051490\n      2.125217\n    \n    \n      1\n      1.111839\n      1.173511\n      1.255733\n      1.316744\n      1.336851\n      1.431039\n      1.513762\n      1.624544\n      1.733171\n      1.984078\n      2.259019\n      2.370773\n    \n    \n      2\n      1.019156\n      1.058173\n      1.080910\n      1.120521\n      1.163374\n      1.190512\n      1.231332\n      1.257548\n      1.287433\n      1.320996\n      1.347873\n      1.399662\n    \n    \n      3\n      1.027886\n      1.062590\n      1.088567\n      1.124628\n      1.149337\n      1.189305\n      1.218267\n      1.258150\n      1.290688\n      1.328989\n      1.368187\n      1.412470\n    \n    \n      4\n      1.022567\n      1.037980\n      1.046979\n      1.064810\n      1.079649\n      1.086973\n      1.094045\n      1.097738\n      1.104774\n      1.108017\n      1.125943\n      1.132601\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3995\n      1.022991\n      1.064212\n      1.091823\n      1.118289\n      1.166716\n      1.182341\n      1.208105\n      1.252252\n      1.310564\n      1.319768\n      1.351317\n      1.375962\n    \n    \n      3996\n      1.039176\n      1.084013\n      1.129208\n      1.178795\n      1.230675\n      1.283502\n      1.337933\n      1.395778\n      1.451701\n      1.512018\n      1.576763\n      1.643312\n    \n    \n      3997\n      1.022346\n      1.057272\n      1.118954\n      1.183094\n      1.205393\n      1.225715\n      1.343847\n      1.407721\n      1.506250\n      1.565272\n      1.670593\n      1.755775\n    \n    \n      3998\n      1.037186\n      1.077082\n      1.122035\n      1.166914\n      1.206876\n      1.252481\n      1.300103\n      1.352597\n      1.397989\n      1.453273\n      1.510253\n      1.565866\n    \n    \n      3999\n      1.026173\n      1.057943\n      1.072373\n      1.120320\n      1.155082\n      1.198584\n      1.220144\n      1.246629\n      1.275155\n      1.311698\n      1.351460\n      1.378449\n    \n  \n\n4000 rows × 12 columns"
  },
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#plotting-predicted-trajectories",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#plotting-predicted-trajectories",
    "title": "Examined Algorithms",
    "section": "Plotting Predicted Trajectories",
    "text": "Plotting Predicted Trajectories\nNote how there are cumulative lift curves that go up and down due to the possibility of failed experiments. We plot here a a sample of 100 possible curves along with the key quantiles.\n\nfig, ax = plt.subplots(figsize=(20, 10))\nax.plot(draws_per_experiment.sample(100).T, color='grey', alpha=0.2);\nax.plot(draws_per_experiment.mean(), color='slateblue', label='Expected', linewidth=5)\nax.plot(draws_per_experiment.quantile(0.75), color='slateblue', label='p75', alpha=0.5, linewidth=3)\nax.plot(draws_per_experiment.quantile(0.25), color='slateblue', label='p25', alpha=0.5, linewidth=3)\nax.plot(draws_per_experiment.quantile(0.99), color='slateblue', label='p99', alpha=0.5, linewidth=3)\nax.set_title(\"Credible Cumulative Lift Curves\", fontsize=20)\nax.set_ylabel(\"Lift\")\nax.set_xlabel(\"Number of Experiments\")\nax.legend()\n\n<matplotlib.legend.Legend at 0x180b0c520>\n\n\n\n\n\nWe can now also ask how credible a target lift of 2x over the course of 12 experiments really is?\n\nfig, ax = plt.subplots(figsize=(20, 6))\nN, bins, patches = ax.hist(draws_per_experiment[11], color='slateblue', edgecolor='grey', alpha=0.3, bins=30);\nax.axvline(2)\nfor i in range(9, len(patches)):\n    patches[i].set_facecolor('red')\nax.set_title(\"Proportion of Credible Curves which achieve a cumulative Lift more than 2\", fontsize=20)\nax.set_xlabel(\"Lift\")\n\nText(0.5, 0, 'Lift')"
  },
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#generate-new-views-with-new-experimental-data",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#generate-new-views-with-new-experimental-data",
    "title": "Examined Algorithms",
    "section": "Generate new views with new Experimental data",
    "text": "Generate new views with new Experimental data\nWe can make use of PYMC’s mutable data input to feed in new experimental data and try and predict implied effects using the posterior predictive distribution. We will continue to assume that we have 50,000 observations per experiment on each arm. But now let’s assume we have 20 experiments, with a similar pattern of Lift observed on each of the experiments.\n\nN = 20\ncontrol = np.random.gumbel(2.2, 0.07, N)\ntreatment = np.random.gumbel(2.0, 0.05, N)\nsd = np.random.normal(0.1, 0.01, N)\n\nfig, axs = plt.subplots(2, 2, figsize=(20, 8))\naxs = axs.flatten()\naxs[0].hist(control, alpha=.3, label='control', edgecolor='black')\naxs[0].hist(treatment, alpha=0.2, label='treatment', edgecolor='black')\naxs[0].legend()\naxs[0].set_title(\"Successes in Treatment and Control for 100 Experiments\")\nrr = treatment / control\nlog_rr = np.log(rr)\naxs[1].hist(rr, alpha=0.3, label='RR/Lift', color='y', edgecolor='black')\naxs[1].legend()\naxs[1].set_title(\"RR/Lift\")\naxs[2].hist(log_rr, label='logged Lift', alpha=0.4, color='green', edgecolor='black')\naxs[2].set_title(\"Logged RR/Lift\")\naxs[3].hist(np.exp(log_rr), label='exponentiated Logged Lift', alpha=0.4, color='purple', edgecolor='black')\naxs[3].set_title(\" Exp Logged RR/Lift\")\n\nText(0.5, 1.0, ' Exp Logged RR/Lift')\n\n\n\n\n\nHere we can pass in the new data to our old model and re-generate the posterior predictive distribution.\n\n# Do the posterior predictions\ncoords = {'exp_id': list(range(N))}\nwith model:\n    pm.set_data({\"est_log_lift\": log_rr, 'exp':list(range(N)), 'est_lift_sd': sd}, coords=coords)\n    ppc = pm.sample_posterior_predictive(idata, var_names=[\"estimated_log_relative_lift\"])\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00<00:00]\n    \n    \n\n\n\naz.plot_ppc(ppc, figsize=(20, 10), kind='scatter');\n\n\n\n\n\nppc.stack(sample=[\"chain\", \"draw\"], inplace=True)\npredicted = ppc['posterior_predictive']['estimated_log_relative_lift'].to_dataframe().reset_index()\npredicted\n\n\n\n\n\n  \n    \n      \n      exp_id\n      chain\n      draw\n      estimated_log_relative_lift\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      0.129563\n    \n    \n      1\n      0\n      0\n      1\n      0.160167\n    \n    \n      2\n      0\n      0\n      2\n      0.015636\n    \n    \n      3\n      0\n      0\n      3\n      0.131316\n    \n    \n      4\n      0\n      0\n      4\n      0.155715\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      79995\n      19\n      3\n      995\n      0.047107\n    \n    \n      79996\n      19\n      3\n      996\n      0.152401\n    \n    \n      79997\n      19\n      3\n      997\n      0.257834\n    \n    \n      79998\n      19\n      3\n      998\n      0.037314\n    \n    \n      79999\n      19\n      3\n      999\n      -0.037456\n    \n  \n\n80000 rows × 4 columns\n\n\n\n\npredicted['rr_over_the_year'] = np.exp(predicted['estimated_log_relative_lift'])\n    ## Calculate effect size for proportions against base 0.01\npredicted['es'] = predicted.apply(lambda x: 2*np.arcsin(np.sqrt(x['rr_over_the_year'] * 0.01)) -  2*np.arcsin(np.sqrt(0.01)), axis=1)\n    ## Calculate power based on difference from baseline with known sample size\npredicted['power'] = predicted.apply(lambda x: 1 - stats.norm.cdf( 1.644854 - x['es'] * np.sqrt(50_000/2), 0, 1), axis=1)\n    ## Weight lift by our ability to detect given power in experiment\npredicted['detected_lift'] = predicted['power']* np.log(predicted['rr_over_the_year'])\npredicted\n\n\n\n\n\n  \n    \n      \n      exp_id\n      chain\n      draw\n      estimated_log_relative_lift\n      rr_over_the_year\n      es\n      power\n      detected_lift\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      0.129563\n      1.138330\n      0.013457\n      0.685424\n      0.088805\n    \n    \n      1\n      0\n      0\n      1\n      0.160167\n      1.173706\n      0.016767\n      0.842840\n      0.134995\n    \n    \n      2\n      0\n      0\n      2\n      0.015636\n      1.015759\n      0.001578\n      0.081447\n      0.001273\n    \n    \n      3\n      0\n      0\n      3\n      0.131316\n      1.140329\n      0.013646\n      0.695916\n      0.091385\n    \n    \n      4\n      0\n      0\n      4\n      0.155715\n      1.168493\n      0.016282\n      0.823705\n      0.128263\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      79995\n      19\n      3\n      995\n      0.047107\n      1.048235\n      0.004791\n      0.187461\n      0.008831\n    \n    \n      79996\n      19\n      3\n      996\n      0.152401\n      1.164627\n      0.015922\n      0.808573\n      0.123227\n    \n    \n      79997\n      19\n      3\n      997\n      0.257834\n      1.294124\n      0.027678\n      0.996847\n      0.257021\n    \n    \n      79998\n      19\n      3\n      998\n      0.037314\n      1.038019\n      0.003786\n      0.147719\n      0.005512\n    \n    \n      79999\n      19\n      3\n      999\n      -0.037456\n      0.963237\n      -0.003729\n      0.012726\n      -0.000477\n    \n  \n\n80000 rows × 8 columns\n\n\n\n\npredicted_curves  = predicted.pivot(['chain', 'draw'], 'exp_id', 'detected_lift')\npredicted_curves\n\n\n\n\n\n  \n    \n      \n      exp_id\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n    \n    \n      chain\n      draw\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      0.088805\n      0.001197\n      -0.000484\n      0.171321\n      0.307768\n      -0.000114\n      -0.000358\n      -0.000050\n      0.119042\n      0.035483\n      0.008818\n      0.017530\n      0.126463\n      0.044351\n      0.021847\n      0.077193\n      0.055479\n      8.898377e-03\n      0.010240\n      0.001336\n    \n    \n      1\n      0.134995\n      -0.000085\n      0.169952\n      -0.000517\n      0.005521\n      0.006376\n      -0.000489\n      0.134169\n      0.245929\n      0.038492\n      -0.000515\n      0.002976\n      0.209037\n      0.062656\n      0.002869\n      0.139614\n      0.254202\n      4.575356e-02\n      0.286258\n      0.206261\n    \n    \n      2\n      0.001273\n      0.191886\n      0.001264\n      0.090177\n      -0.000060\n      0.330653\n      0.202727\n      -0.000368\n      0.015053\n      0.305181\n      -0.000259\n      0.049178\n      -0.000262\n      0.015266\n      0.090864\n      0.060138\n      0.008056\n      1.787840e-01\n      0.003004\n      -0.000283\n    \n    \n      3\n      0.091385\n      -0.000165\n      0.175993\n      0.210623\n      -0.000398\n      0.000092\n      0.008886\n      0.169622\n      0.016613\n      -0.000175\n      -0.000333\n      -0.000151\n      0.069115\n      0.003197\n      0.009931\n      0.008144\n      0.069742\n      5.125832e-04\n      0.223892\n      -0.000056\n    \n    \n      4\n      0.128263\n      -0.000264\n      0.029506\n      0.074044\n      -0.000028\n      0.013448\n      0.034857\n      0.181062\n      0.059433\n      0.119991\n      0.022186\n      0.181830\n      0.078825\n      0.042061\n      0.229243\n      0.173755\n      0.131305\n      -3.022104e-08\n      0.019529\n      0.023409\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3\n      995\n      0.010141\n      0.046869\n      0.261628\n      0.187072\n      0.207434\n      0.041217\n      0.029719\n      0.098765\n      -0.000002\n      0.050363\n      -0.000449\n      0.215972\n      0.084759\n      0.035756\n      0.187955\n      0.000416\n      0.005068\n      -3.511800e-04\n      0.208030\n      0.008831\n    \n    \n      996\n      0.020431\n      0.113550\n      -0.000424\n      0.009520\n      0.283799\n      0.010694\n      -0.000157\n      0.030434\n      0.104618\n      0.032070\n      0.117800\n      0.152587\n      -0.000257\n      -0.000003\n      -0.000008\n      0.029872\n      0.224894\n      2.599086e-01\n      0.092621\n      0.123227\n    \n    \n      997\n      0.082644\n      0.004597\n      0.028806\n      0.001567\n      0.009130\n      0.000728\n      0.010956\n      0.001111\n      0.212021\n      0.240963\n      0.067241\n      0.182531\n      0.301767\n      0.221201\n      0.000654\n      0.001957\n      0.137934\n      2.372076e-01\n      0.014318\n      0.257021\n    \n    \n      998\n      -0.000116\n      0.085200\n      -0.000517\n      0.005099\n      0.029492\n      0.040047\n      -0.000026\n      0.002956\n      0.104428\n      0.082584\n      0.000486\n      0.001877\n      0.032360\n      0.001402\n      0.003078\n      0.001904\n      -0.000390\n      -1.231683e-04\n      -0.000350\n      0.005512\n    \n    \n      999\n      0.002641\n      0.138605\n      0.201605\n      0.103506\n      0.180542\n      0.034928\n      0.012054\n      -0.000369\n      0.047570\n      0.086689\n      0.257165\n      0.102930\n      0.221825\n      0.178257\n      0.010981\n      0.324702\n      0.091735\n      2.999001e-02\n      0.200721\n      -0.000477\n    \n  \n\n4000 rows × 20 columns\n\n\n\n\nfig, axs = plt.subplots(1, 2, figsize=(20, 6))\nnp.random.seed(19)\naxs = axs.flatten()\nN, bins, patches = axs[0].hist(np.exp(predicted_curves.cumsum(axis=1))[11], color='slateblue', edgecolor='grey', alpha=0.3, bins=40);\naxs[0].axvline(2)\nfor i in range(1, len(patches)):\n    patches[i].set_facecolor('red')\nax.set_title(\"Proportion of Credible Curves which achieve a cumulative Lift more than 2\", fontsize=20)\nax.set_xlabel(\"Lift\")\naxs[0].set_title(\"Proportion of Curves which achieve 2x Lift after 20 experiments\")\naxs[0].legend()\naxs[1].plot(np.exp(predicted_curves.cumsum(axis=1)).sample(100).T, color='grey');\naxs[1].plot(np.exp(predicted_curves.cumsum(axis=1)).mean(), color='red', label='Expected Growth curve');\naxs[1].set_title(\"Sample Set of Possible Growth Curves\")\naxs[1].set_ylim(0, 10)\n\nWARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n(0.0, 10.0)"
  },
  {
    "objectID": "posts/post-with-code/DiD-Card-Kreuger/DID.html",
    "href": "posts/post-with-code/DiD-Card-Kreuger/DID.html",
    "title": "Differences in Differences",
    "section": "",
    "text": "There is a voluminous literature about how to measure causal impact using the differences-in-differences technique. It includes multiple controversies over how the technique can be applied to panel data with the problems of time-varying heterogeneity. These are fraught and varied issues, and we’ll elide them for now.\nInstead we’ll look at one of the earliest analyses in the literature, popularised by Card and Krueger in 1994. This is a justly famous paper both for the clear illustration of the technique and the conclusions drawn in their own right. It is not the earliest application of the technique. Arguably John Snow’s cholera experiment in London demonstrates the exact same reasoning. But we want we to show how this quasi-experimental design is often employed today in a regression setting and how regression modelling is used to check the robustness of the DiD design."
  },
  {
    "objectID": "posts/post-with-code/DiD-Card-Kreuger/DID.html#the-data",
    "href": "posts/post-with-code/DiD-Card-Kreuger/DID.html#the-data",
    "title": "Differences in Differences",
    "section": "The Data",
    "text": "The Data\nThe data from the Card and Krueger study reports measures of employement by location in bordering states of New Jersey and Pennsylvania afer April 1st, 1992 when New Jersey’s minimum wage rose from $4.25 to $5.05 per hour. Our interest is in the effect of the different state policies relating to minimum wage. There is a juncture in time after which New Jersey enacts a minimum wage policy which is anticipated to have a meaningful negative impact on employment figures. Differences in Differences is a quasi-experimental design intended to capture this effect.\n\n# Load the Data from the minumum wage study\ndf_ck = pd.read_csv('CK1994.txt', sep='\\t')\n\n## Calculate the price of an average meal.\ndf_ck['price'] = df_ck[['pricesoda', 'pricefry', 'priceentree']].sum(axis=1)\n\n# Count of employees\ndf_ck['employees'] = df_ck.apply(lambda x: x['empft'] + x['nmgrs'] + 0.5*x['emppt'], axis=1)\n\n# Interaction of state and time for use in OLS\ndf_ck['treatment'] = df_ck['state']*df_ck['time']\n\n# Group by Region\ndf_ck['group'] = np.where(df_ck['southj'], 'NJ south',\n                    np.where(df_ck['centralj'], 'NJ central', \n                      np.where(df_ck['northj'], 'NJ North', \n                        np.where(df_ck['pa1'], 'PA 1', \n                          np.where(df_ck['pa2'], 'PA 2', np.nan)))))\n\n\ndf_ck.head()\n\n\n\n\n\n  \n    \n      \n      store\n      chain\n      co_owned\n      state\n      southj\n      centralj\n      northj\n      pa1\n      pa2\n      shore\n      ...\n      pricesoda\n      pricefry\n      priceentree\n      nregisters\n      nregisters11\n      time\n      price\n      employees\n      treatment\n      group\n    \n  \n  \n    \n      0\n      46\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      1.03\n      1.03\n      0.52\n      3.0\n      3.0\n      0\n      2.58\n      40.50\n      0\n      PA 1\n    \n    \n      1\n      49\n      2\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      1.01\n      0.90\n      2.35\n      4.0\n      3.0\n      0\n      4.26\n      13.75\n      0\n      PA 1\n    \n    \n      2\n      506\n      2\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      0.95\n      0.74\n      2.33\n      3.0\n      3.0\n      0\n      4.02\n      8.50\n      0\n      PA 1\n    \n    \n      3\n      56\n      4\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      0.87\n      0.82\n      1.79\n      2.0\n      2.0\n      0\n      3.48\n      34.00\n      0\n      PA 1\n    \n    \n      4\n      61\n      4\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      0.87\n      0.77\n      1.65\n      2.0\n      2.0\n      0\n      3.29\n      24.00\n      0\n      PA 1\n    \n  \n\n5 rows × 30 columns\n\n\n\nThe crucial results reported in the paper show a surprising subversion of expecation. The idea is that the neigbouring states should have comparable working conditions and incentives to employment up until the initiative of the policy change. Hence, it is argued that the difference between the states before and after the change can be a gauge of the causal impact of that policy. The data they looked at surveyed 410 fast-food restaurants in New Jersey and eastern Pennsylvania before and after the rise in minimum wage for New Jersey.\n\npivot = (df_ck[['state', 'time', 'employees']].dropna()\n  .groupby(['state', 'time'])[['employees']].mean()\n  .reset_index()\n  .pivot(index='time', columns='state', values='employees')\n  )\npivot = pivot.append(pivot.iloc[1] - pivot.iloc[0], ignore_index=True)\npivot.columns = ['PA', 'NJ']\npivot['Diff'] = pivot['NJ'] - pivot['PA']\npivot.index = ['Before', 'After', 'Diff']\npivot\n\n/var/folders/__/ng_3_9pn1f11ftyml_qr69vh0000gn/T/ipykernel_83904/4208568777.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  pivot = pivot.append(pivot.iloc[1] - pivot.iloc[0], ignore_index=True)\n\n\n\n\n\n\n  \n    \n      \n      PA\n      NJ\n      Diff\n    \n  \n  \n    \n      Before\n      23.331169\n      20.439408\n      -2.891761\n    \n    \n      After\n      21.165584\n      21.027429\n      -0.138155\n    \n    \n      Diff\n      -2.165584\n      0.588021\n      2.753606\n    \n  \n\n\n\n\nThe result here is by traditional economic logic surprising. It’s surprising in that they “find no indication that the rise in the minimum wage reduced employment.” When we might anticipate a negative reaction from their employers. The above table reports the raw differences in average employment per restaurant. That’s it. That’s the quasi-experiemntal design that launched a thousand imitations. Visually the change is striking.\n\nfig, ax = plt.subplots()\nax.plot(['Before', 'After'], [pivot.iloc[0]['PA'],  pivot.iloc[1]['PA']], '-o', label='Pennsylvania')\nax.plot(['Before', 'After'], [pivot.iloc[0]['NJ'],  pivot.iloc[1]['NJ']],'-o', label='New Jersey')\nax.plot(['Before', 'After'], [pivot.iloc[0]['NJ'],  pivot.iloc[2]['PA'] +pivot.iloc[0]['NJ']], '--', color='darkorange', label='New Jersey Counterfactual')\nax.set_title(\"Visualising the Counterfactual\")\nax.plot((1, 1), (18, 21), '--', color='grey', label='treatment effect')\nax.legend()\n\n<matplotlib.legend.Legend at 0x16ad790c0>"
  },
  {
    "objectID": "posts/post-with-code/DiD-Card-Kreuger/DID.html#a-persuasive-design",
    "href": "posts/post-with-code/DiD-Card-Kreuger/DID.html#a-persuasive-design",
    "title": "Differences in Differences",
    "section": "A Persuasive Design",
    "text": "A Persuasive Design\nIt’s not just a simple table. The paper was compelling precisely because the design was persuasive, and the Pennsylvania’s future is a plausible representation of New Jersey’s counterfactual future. The anticipated trajectory of the two states was plausible before the intervention, so the argument goes that the observed difference is plausible estimate of the causal effect.\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 7))\naxs = axs.flatten()\nbefore = df_ck[df_ck['time'] == 0]\nafter = df_ck[df_ck['time'] == 1]\naxs[0].hist(before[before['state'] == 0]['wage_st'], alpha=0.4, bins=20, density=True, ec='black', label='Pennsylvania Before')\naxs[0].hist(before[before['state'] == 1]['wage_st'], alpha=0.4, bins=20, density=True, ec='black', label='New Jersey Before')\naxs[0].set_xlabel(\"Wage per Hour in $\")\naxs[0].legend()\naxs[1].hist(after[after['state'] == 0]['wage_st'], alpha=0.4, bins=15, density=True, ec='black', label='Pennsylvania After')\naxs[1].hist(after[after['state'] == 1]['wage_st'], alpha=0.4, bins=15, density=True, ec='black', label='New Jersey After')\naxs[1].set_xlabel(\"Wage per Hour in $\")\naxs[1].legend()\naxs[1].set_title(\"Wage Distribution After\")\naxs[0].set_title(\"Wage Distribution Before\");\n\n\n\n\nAnd the corressponding view for the employment figures shows that the difference between before and after periods, for both states are centered around zero.\n\nfig, axs = plt.subplots(1, 3, figsize=(12, 5))\naxs = axs.flatten()\nbefore = df_ck[df_ck['time'] == 0]\nafter = df_ck[df_ck['time'] == 1]\naxs[0].hist(before[before['state'] == 0]['employees'], alpha=0.4, bins=20, density=True, ec='black', label='Pennsylvania Before')\naxs[0].hist(before[before['state'] == 1]['employees'], alpha=0.4, bins=20, density=True, ec='black', label='New Jersey Before')\naxs[0].set_xlabel(\"Employees\")\naxs[0].legend()\naxs[2].hist(after[after['state'] == 0]['employees'], alpha=0.4, bins=15, density=True, ec='black', label='Pennsylvania After')\n\ndiff_p = after[after['state'] == 0][['store', 'employees']].merge(before[before['state'] == 0][['store', 'employees']], left_on='store', right_on='store')\n\ndiff_nj = after[after['state'] == 1][['store', 'employees']].merge(before[before['state'] == 1][['store', 'employees']], left_on='store', right_on='store')\n\naxs[1].hist(diff_p['employees_x'] - diff_p['employees_y'], alpha=0.4, bins=15, density=True, ec='black', label='Pennsylvania Diff')\naxs[1].hist(diff_nj['employees_x'] - diff_nj['employees_y'], alpha=0.4, bins=15, density=True, ec='black', label='NJ Diff')\naxs[1].set_xlabel(\"Before - After\")\naxs[1].legend()\n\naxs[2].hist(after[after['state'] == 1]['employees'], alpha=0.4, bins=15, density=True, ec='black', label='New Jersey After')\naxs[2].set_xlabel(\"Employees\")\naxs[2].legend()\naxs[2].set_title(\"Employed Distribution After\")\naxs[1].set_title(\"Difference of Employed Distribution\")\naxs[0].set_title(\"Employed Distribution Before\");"
  },
  {
    "objectID": "posts/post-with-code/DiD-Card-Kreuger/DID.html#robustness-to-controls",
    "href": "posts/post-with-code/DiD-Card-Kreuger/DID.html#robustness-to-controls",
    "title": "Differences in Differences",
    "section": "Robustness to Controls",
    "text": "Robustness to Controls\nThe robustness of the effect might, in principle, be moderated or refined by other factors. So it’s worth exploring the parameter fits for a variety of models. First we recover the simple differences-in-differences control using regression, and then for other subsequent moodels we add controls for the the location, food chain and whether the restaurant is co-owned. All models recover effectively the same estimate for the interaction term of state over time, which is just our differences-in-differences estimate as seen in the above table.\n\ntemp = df_ck[['employees', 'northj', 'centralj', 'pa1', 'pa2', 'time', 'treatment', 'chain', 'state', 'co_owned']].dropna()\ntemp[['chain_1', 'chain_2', 'chain_3', 'chain_4']] = pd.get_dummies(temp['chain'])\nmodel_0 = smf.ols(formula='employees ~ state + time + treatment', data=temp).fit()\nmodel_1 = smf.ols(formula='employees ~ state + time + chain_1 + chain_2 + chain_3 + treatment', data=temp).fit()\nmodel_2 = smf.ols(formula='employees ~ centralj + pa1 + pa2 + time + treatment', data=temp).fit()\n\nmodel_3 = smf.ols(formula='employees ~ centralj + pa1 + pa2 + time + chain_1 + chain_2 + chain_3 + treatment', data=temp).fit()\n\nmodel_4 = smf.ols(formula='employees ~ centralj + pa1 + pa2 + time + chain_1 + chain_2 + chain_3 + co_owned + treatment', data=temp).fit()\n\nstargazer = Stargazer([model_0, model_1, model_2, model_3])\nstargazer.render_html()\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:employees\n\n\n\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n(4)\n\n\n\n\n\n\n\n\nIntercept\n\n\n23.331***\n\n\n24.613***\n\n\n20.196***\n\n\n21.960***\n\n\n\n\n\n\n(1.072)\n\n\n(1.194)\n\n\n(0.554)\n\n\n(0.909)\n\n\n\n\ncentralj\n\n\n\n\n\n\n1.304\n\n\n1.458*\n\n\n\n\n\n\n\n\n\n\n(0.949)\n\n\n(0.858)\n\n\n\n\nchain_1\n\n\n\n\n1.118\n\n\n\n\n1.185\n\n\n\n\n\n\n\n\n(0.930)\n\n\n\n\n(0.933)\n\n\n\n\nchain_2\n\n\n\n\n-9.694***\n\n\n\n\n-9.645***\n\n\n\n\n\n\n\n\n(1.051)\n\n\n\n\n(1.054)\n\n\n\n\nchain_3\n\n\n\n\n-1.134\n\n\n\n\n-1.045\n\n\n\n\n\n\n\n\n(1.011)\n\n\n\n\n(1.030)\n\n\n\n\npa1\n\n\n\n\n\n\n3.780**\n\n\n3.122**\n\n\n\n\n\n\n\n\n\n\n(1.464)\n\n\n(1.337)\n\n\n\n\npa2\n\n\n\n\n\n\n2.598*\n\n\n2.160*\n\n\n\n\n\n\n\n\n\n\n(1.391)\n\n\n(1.259)\n\n\n\n\nstate\n\n\n-2.892**\n\n\n-2.321**\n\n\n\n\n\n\n\n\n\n\n(1.194)\n\n\n(1.080)\n\n\n\n\n\n\n\n\ntime\n\n\n-2.166\n\n\n-2.195\n\n\n-2.166\n\n\n-2.196\n\n\n\n\n\n\n(1.516)\n\n\n(1.369)\n\n\n(1.515)\n\n\n(1.368)\n\n\n\n\ntreatment\n\n\n2.754\n\n\n2.814*\n\n\n2.748\n\n\n2.810*\n\n\n\n\n\n\n(1.688)\n\n\n(1.525)\n\n\n(1.688)\n\n\n(1.524)\n\n\n\n\n\n\n\nObservations\n\n\n794\n\n\n794\n\n\n794\n\n\n794\n\n\n\n\nR2\n\n\n0.007\n\n\n0.194\n\n\n0.011\n\n\n0.197\n\n\n\n\nAdjusted R2\n\n\n0.004\n\n\n0.188\n\n\n0.004\n\n\n0.189\n\n\n\n\nResidual Std. Error\n\n\n9.406 (df=790)\n\n\n8.493 (df=787)\n\n\n9.403 (df=788)\n\n\n8.486 (df=785)\n\n\n\n\nF Statistic\n\n\n1.964 (df=3; 790)\n\n\n31.512*** (df=6; 787)\n\n\n1.677 (df=5; 788)\n\n\n24.098*** (df=8; 785)\n\n\n\n\n\n\n\n\nNote:\n\n\n\np<0.1;p<0.05;p<0.01\n\n\n\n\n\n\nThe effect is consisteny across the model specification. This is evidence of a robust effect. What happens if we look at a different outcome variable? Does the change in policy impact the price of the meal by location? We’ll model this now too.\n\ntemp = df_ck[['price', 'northj', 'centralj', 'pa1', 'pa2', 'time', 'treatment', 'chain', 'state', 'co_owned']].dropna()\ntemp[['chain_1', 'chain_2', 'chain_3', 'chain_4']] = pd.get_dummies(temp['chain'])\nmodel_0 = smf.ols(formula='price ~ state + time + treatment', data=temp).fit()\nmodel_1 = smf.ols(formula='price ~ state + time + chain_1 + chain_2 + chain_3 + treatment', data=temp).fit()\nmodel_2 = smf.ols(formula='price ~ centralj + pa1 + pa2 + time + treatment', data=temp).fit()\n\nmodel_3 = smf.ols(formula='price ~ centralj + pa1 + pa2 + time + chain_1 + chain_2 + chain_3 + treatment', data=temp).fit()\n\nmodel_4 = smf.ols(formula='price ~ centralj + pa1 + pa2 + time + chain_1 + chain_2 + chain_3 + co_owned + treatment', data=temp).fit()\n\nstargazer = Stargazer([model_0, model_1, model_2, model_3])\nstargazer.render_html()\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:price\n\n\n\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n(4)\n\n\n\n\n\n\n\n\nIntercept\n\n\n2.955***\n\n\n2.876***\n\n\n3.243***\n\n\n3.097***\n\n\n\n\n\n\n(0.098)\n\n\n(0.094)\n\n\n(0.051)\n\n\n(0.071)\n\n\n\n\ncentralj\n\n\n\n\n\n\n0.027\n\n\n0.013\n\n\n\n\n\n\n\n\n\n\n(0.086)\n\n\n(0.067)\n\n\n\n\nchain_1\n\n\n\n\n-0.239***\n\n\n\n\n-0.241***\n\n\n\n\n\n\n\n\n(0.072)\n\n\n\n\n(0.072)\n\n\n\n\nchain_2\n\n\n\n\n1.223***\n\n\n\n\n1.221***\n\n\n\n\n\n\n\n\n(0.082)\n\n\n\n\n(0.083)\n\n\n\n\nchain_3\n\n\n\n\n-0.007\n\n\n\n\n-0.014\n\n\n\n\n\n\n\n\n(0.079)\n\n\n\n\n(0.080)\n\n\n\n\npa1\n\n\n\n\n\n\n-0.305**\n\n\n-0.242**\n\n\n\n\n\n\n\n\n\n\n(0.134)\n\n\n(0.105)\n\n\n\n\npa2\n\n\n\n\n\n\n-0.274**\n\n\n-0.198**\n\n\n\n\n\n\n\n\n\n\n(0.127)\n\n\n(0.100)\n\n\n\n\nstate\n\n\n0.293***\n\n\n0.220***\n\n\n\n\n\n\n\n\n\n\n(0.109)\n\n\n(0.085)\n\n\n\n\n\n\n\n\ntime\n\n\n-0.084\n\n\n-0.084\n\n\n-0.084\n\n\n-0.084\n\n\n\n\n\n\n(0.138)\n\n\n(0.108)\n\n\n(0.138)\n\n\n(0.108)\n\n\n\n\ntreatment\n\n\n0.041\n\n\n0.041\n\n\n0.041\n\n\n0.041\n\n\n\n\n\n\n(0.154)\n\n\n(0.120)\n\n\n(0.154)\n\n\n(0.120)\n\n\n\n\n\n\n\nObservations\n\n\n820\n\n\n820\n\n\n820\n\n\n820\n\n\n\n\nR2\n\n\n0.021\n\n\n0.406\n\n\n0.021\n\n\n0.406\n\n\n\n\nAdjusted R2\n\n\n0.017\n\n\n0.402\n\n\n0.015\n\n\n0.401\n\n\n\n\nResidual Std. Error\n\n\n0.869 (df=816)\n\n\n0.678 (df=813)\n\n\n0.870 (df=814)\n\n\n0.679 (df=811)\n\n\n\n\nF Statistic\n\n\n5.798*** (df=3; 816)\n\n\n92.713*** (df=6; 813)\n\n\n3.501*** (df=5; 814)\n\n\n69.405*** (df=8; 811)\n\n\n\n\n\n\n\n\nNote:\n\n\n\np<0.1;p<0.05;p<0.01"
  },
  {
    "objectID": "posts/post-with-code/DiD-Card-Kreuger/DID.html#impact-on-consumers",
    "href": "posts/post-with-code/DiD-Card-Kreuger/DID.html#impact-on-consumers",
    "title": "Differences in Differences",
    "section": "Impact on Consumers?",
    "text": "Impact on Consumers?\nThe effects on price of a meal is much more stable and seemingly not impacted in the same degree as we saw in employment numbers. In fact the effects seem close to negligible. This contextual information suggests that the increased wages have not lead (in the same timeframe) to extra costs for the consumer.\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 7))\naxs = axs.flatten()\nbefore = df_ck[df_ck['time'] == 0]\nafter = df_ck[df_ck['time'] == 1]\naxs[0].hist(before[before['state'] == 0]['price'], alpha=0.4, bins=20, density=True, ec='black', label='Pennsylvania Before')\naxs[0].hist(before[before['state'] == 1]['price'], alpha=0.4, bins=20, density=True, ec='black', label='New Jersey Before')\naxs[0].set_xlabel(\"Price per Meal in $\")\naxs[0].legend()\naxs[1].hist(after[after['state'] == 0]['price'], alpha=0.4, bins=15, density=True, ec='black', label='Pennsylvania After')\naxs[1].hist(after[after['state'] == 1]['price'], alpha=0.4, bins=15, density=True, ec='black', label='New Jersey After')\naxs[1].set_xlabel(\"Price per Meal in $\")\naxs[1].legend()\naxs[1].set_title(\"Price Distribution After\")\naxs[0].set_title(\"Price Distribution Before\");\n\n\n\n\nIt is this combination of details that made the Card and Kreuger study surprising and impactful. There is a clear quasi-experimental design, a compelling narrative and a counter-intuitive conclusion. We won’t speak the wider discussion of minimum wages in economics, our focus here is on the methodology. The methodology almost seems too simple, too straightforward. Much of the plausibility of the inferences gets bundled into the contrast between the treatment group and our pseudo-control. So far we’ve avoided precise mathematical statement of the DiD estimator, but being more precise allows us to say something about when this estimation technique can go wrong."
  },
  {
    "objectID": "posts/post-with-code/DiD-Card-Kreuger/DID.html#when-did-goes-wrong",
    "href": "posts/post-with-code/DiD-Card-Kreuger/DID.html#when-did-goes-wrong",
    "title": "Differences in Differences",
    "section": "When DiD goes Wrong",
    "text": "When DiD goes Wrong\nFollowing Scott Cunningham’s presentation we can view the simple 2x2 DiD estimate as follows\n\\[\\widehat{\\delta}^{2\\times 2}_{tC} = \\bigg(E\\big[Y_t  | Post\\big] - E\\big[Y_t  | Pre\\big]\\bigg)- \\bigg(E\\big[Y_C  | Post\\big] - E\\big[Y_C  | Pre\\big]\\bigg)\\]\nwhich is equivalent to:\n\\[ \\begin{align}\n&\\widehat{\\delta}^{2\\times 2}_{tC} = \\underbrace{E\\big[Y^1_t  | Post\\big] - E\\big[Y^0_t | Post\\big]}_{\\text{ATT}} \\\\\n&+\\Big[\\underbrace{E\\big[Y^0_t | Post\\big] - E\\big[Y^0_t | Pre\\big] \\Big] - \\Big[E\\big[Y^0_C  | Post\\big] - E\\big[Y_C^0 | Pre\\big] }_{\\text{Non-parallel trends bias in $2\\times 2$ case}} \\Big]\n\\end{align}\n\\]\nwhich implies that we can extract that the average treatment effect on the treated (ATT) can be estimated just so long as the bias due to the parallel trends zeros out. In our applied case this amounts to the following\n\\[\n\\begin{align}\n&\\widehat{\\delta}^{2\\times 2}_{NJ,PA} = \\underbrace{E\\big[Y^1_{NJ} | Post\\big] - E\\big[Y^0_{NJ} | Post\\big]}_{\\text{ATT}} \\\\\n&+ \\Big[\\underbrace{E\\big[Y^0_{NJ} | Post\\big] - E\\big[Y^0_{NJ} | Pre\\big] \\Big]-\\Big[E\\big[Y^0_{PA} | Post\\big] - E\\big[Y_{PA}^0 | Pre\\big] }_{\\text{Non-parallel trends bias}} \\Big]\n\\end{align}\n\\]\nwhich is to say that the causal impact discovered by a DiD design is valid just when for both states there would have been a parrallel trend, under the counterfactual, where the treatment had not been applied to New Jersey, i.e. had the treatment not been applied then the differences between the states would have remained stable.\n\nTesting the Parallel Trends Assumption\nWe’ll examine a larger data set discussed by Scott Cunningham in his Causal Inference: The Mixtape. The point is to see how to measure the similarity of the trends prior to an intervention. The rhetorical move in a differences in differences design is heavily reliant on how compelling you can make the counterfactual claim. The data we look at focuses on the staggered rollout of the Roe v Wade decision across states on the effects of gonnoherea in the repeal v non-appeal states after the beginning of the rollout. In our data set we’ll see the differential rates of gonnoherea by demographic information.\n\nabortion = pd.read_csv('abortion_mixtape.csv')\nabortion = abortion[~pd.isnull(abortion.lnr)]\nabortion = abortion[abortion.bm15==1]\n\nfig, ax = plt.subplots(figsize=(10, 6))\nfor i in abortion['fip'].unique():\n  temp = abortion[abortion['fip'] == i]\n  temp.set_index('year', inplace=True)\n  ax.plot(temp['lnr'], color='grey', alpha=0.1)\nax.set_title(\"Trajectories by Individual State\")\nax.plot(abortion.pivot('year', 'fip', 'lnr').mean(axis=1), color='darkblue',  label='Overall Average')\nax.plot(abortion[abortion['repeal'] == 0].pivot('year', 'fip', 'lnr').mean(axis=1), label='Average in Non Repeal States', color='slateblue')\nax.plot(abortion[abortion['repeal'] == 1].pivot('year', 'fip', 'lnr').mean(axis=1), label='Average in Repeal States', color='royalblue')\nax.axvline(1992, color='black', linestyle='--')\nax.legend();\n\n/var/folders/__/ng_3_9pn1f11ftyml_qr69vh0000gn/T/ipykernel_83904/3799677275.py:11: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n  ax.plot(abortion.pivot('year', 'fip', 'lnr').mean(axis=1), color='darkblue',  label='Overall Average')\n/var/folders/__/ng_3_9pn1f11ftyml_qr69vh0000gn/T/ipykernel_83904/3799677275.py:12: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n  ax.plot(abortion[abortion['repeal'] == 0].pivot('year', 'fip', 'lnr').mean(axis=1), label='Average in Non Repeal States', color='slateblue')\n/var/folders/__/ng_3_9pn1f11ftyml_qr69vh0000gn/T/ipykernel_83904/3799677275.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n  ax.plot(abortion[abortion['repeal'] == 1].pivot('year', 'fip', 'lnr').mean(axis=1), label='Average in Repeal States', color='royalblue')\n\n\n\n\n\nScott wants to test a prediction of the literature that this intervention should result in a parabolic curve, where the original stark difference between the repeal and Non-repeal states converges towards zero after the 1992 decision.\n\nIn 1986, only one cohort (the 1971 cohort) was treated and only in the repeal states. Therefore, we should see small declines in gonorrhea incidence among 15-year-olds in 1986 relative to Roe states. In 1987, two cohorts in our data are treated in the repeal states relative to Roe, so we should see larger effects in absolute value than we saw in 1986. But from 1988 to 1991, we should at most see only three net treated cohorts in the repeal states because starting in 1988, the Roe state cohorts enter and begin erasing those differences. Starting in 1992, the effects should get smaller in absolute value until 1992, beyond which there should be no difference between repeal and Roe states.\n\nImplying that the tratment effect should result in a growth trajectory with a distinctive shape. We can evaluate the staggered roll out of the by policy by using a complicated regression model which accounts for the delayed effects and treatment effects measured at successive points in time."
  },
  {
    "objectID": "posts/post-with-code/DiD-Card-Kreuger/DID.html#fitting-regressions-to-evaluate-event-studies",
    "href": "posts/post-with-code/DiD-Card-Kreuger/DID.html#fitting-regressions-to-evaluate-event-studies",
    "title": "Differences in Differences",
    "section": "Fitting Regressions to Evaluate Event Studies",
    "text": "Fitting Regressions to Evaluate Event Studies\nWe want now to measure the efficacy of the treatment as a function of each time point while controlling for other factors. This can be achieved using a regression model specification for the DiD model which measures the impact at each time-point. The point to keep in mind is that this process measures the same thing as the simpler DiD estimate above, but here our interaction of group and time effects occurs across multiple time points. This is a significant step-up in complexity of the regression modelling common in econometric approaches. Much of the sophistication in econometric-style causal inference stems from a thorough understanding of OLS estimation strategies and creative uses of the properties of these estimators.\nWe will specify three distinct model specifications and assess the inferences available on each of the core parameters. Each model will contain the interaction effect terms of time (year) and treatment (repeal) and we will use the individual estimates to plot the graduated effect of the policy program. We will include progressively fewer control variables to show how different model specifications change the story.\nWe use a weighted least squares approach with clusters by location.\n\nformula = (\n    \"lnr ~ C(repeal)*C(year) + C(fip)\"\n    \" + acc + ir + pi + alcohol + crack + poverty + income + ur\"\n)\n\nformula_1 = (\n    \"lnr ~ C(repeal)*C(year) + C(fip)\"\n    \" + acc + ir + pi + alcohol + crack\"\n)\n\nformula_2 = (\n    \"lnr ~ C(repeal)*C(year) + C(fip)\"\n)\n\nreg = (\n    smf\n    .wls(formula, data=abortion, weights=abortion.totpop.values)\n    .fit(\n        cov_type='cluster', \n        cov_kwds={'groups': abortion.fip.values}, \n        method='pinv'\n        )\n)\n\nreg1 = (\n    smf\n    .wls(formula_1, data=abortion, weights=abortion.totpop.values)\n    .fit(\n        cov_type='cluster', \n        cov_kwds={'groups': abortion.fip.values}, \n        method='pinv'\n        )\n)\n\nreg2 = (\n    smf\n    .wls(formula_2, data=abortion, weights=abortion.totpop.values)\n    .fit(\n        cov_type='cluster', \n        cov_kwds={'groups': abortion.fip.values}, \n        method='pinv'\n        )\n)\n\n\nstargazer = Stargazer([reg, reg1, reg2])\nstargazer\n\n/Users/nathanielforde/Documents/Github/NathanielF.github.io/.venv/lib/python3.10/site-packages/statsmodels/base/model.py:1871: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 89, but rank is 27\n  warnings.warn('covariance of constraints does not have full '\n/Users/nathanielforde/Documents/Github/NathanielF.github.io/.venv/lib/python3.10/site-packages/statsmodels/base/model.py:1871: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 86, but rank is 24\n  warnings.warn('covariance of constraints does not have full '\n/Users/nathanielforde/Documents/Github/NathanielF.github.io/.venv/lib/python3.10/site-packages/statsmodels/base/model.py:1871: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 81, but rank is 19\n  warnings.warn('covariance of constraints does not have full '\n\n\n\nDependent variable:lnr(1)(2)(3)C(fip)[T.10.0]-1.604**-0.974**0.266***(0.763)(0.414)(0.003)C(fip)[T.11.0]-3.159*-2.242*0.728***(1.893)(1.197)(0.008)C(fip)[T.12.0]-1.692**-1.326***-0.055***(0.738)(0.484)(0.006)C(fip)[T.13.0]-1.036***-0.762***-0.133***(0.376)(0.240)(0.004)C(fip)[T.15.0]-0.837***-0.703***-0.690***(0.095)(0.099)(0.051)C(fip)[T.16.0]-1.909***-1.843***-1.242***(0.301)(0.240)(0.015)C(fip)[T.17.0]-1.398**-0.861***-0.024***(0.664)(0.284)(0.001)C(fip)[T.18.0]-0.3210.0010.296***(0.253)(0.113)(0.000)C(fip)[T.19.0]-0.1980.1340.365***(0.235)(0.109)(0.002)C(fip)[T.2.0]-0.730**-0.619***-0.116*(0.324)(0.134)(0.064)C(fip)[T.20.0]-0.2890.1010.272***(0.264)(0.122)(0.001)C(fip)[T.21.0]0.124**0.0830.175***(0.059)(0.071)(0.024)C(fip)[T.22.0]-0.921***-1.066***-0.287***(0.351)(0.278)(0.001)C(fip)[T.23.0]-2.020***-1.849***-1.582***(0.179)(0.105)(0.020)C(fip)[T.24.0]-1.934**-1.160***-0.282***(0.763)(0.363)(0.001)C(fip)[T.25.0]-3.023***-2.207***-1.139***(0.856)(0.402)(0.001)C(fip)[T.26.0]-1.046**-0.667***-0.042***(0.498)(0.216)(0.003)C(fip)[T.27.0]-0.979*-0.362*0.328***(0.577)(0.208)(0.007)C(fip)[T.28.0]-0.189-0.523***-0.305***(0.141)(0.064)(0.000)C(fip)[T.29.0]-0.504-0.1880.433***(0.401)(0.228)(0.001)C(fip)[T.30.0]-1.435***-1.489***-1.020***(0.193)(0.140)(0.024)C(fip)[T.31.0]-0.781**-0.317*0.201***(0.393)(0.183)(0.009)C(fip)[T.32.0]-3.108**-2.545***0.284***(1.457)(0.884)(0.006)C(fip)[T.33.0]-4.846***-4.115***-1.731***(1.289)(0.675)(0.021)C(fip)[T.34.0]-2.783***-1.839***-0.774***(0.980)(0.455)(0.003)C(fip)[T.35.0]-1.520***-1.720***-0.716***(0.400)(0.344)(0.010)C(fip)[T.36.0]-0.898***-0.695***-0.651***(0.204)(0.167)(0.030)C(fip)[T.37.0]-0.2610.0170.228***(0.184)(0.079)(0.001)C(fip)[T.38.0]-2.156***-2.006***-1.705***(0.165)(0.128)(0.017)C(fip)[T.39.0]-0.756**-0.416**-0.007***(0.351)(0.170)(0.000)C(fip)[T.4.0]-1.637**-1.482***-0.113***(0.664)(0.492)(0.007)C(fip)[T.40.0]0.317***0.359***0.332***(0.083)(0.085)(0.001)C(fip)[T.41.0]-1.489***-1.237***-0.314***(0.497)(0.338)(0.004)C(fip)[T.42.0]-1.155**-0.742***-0.146***(0.467)(0.278)(0.001)C(fip)[T.44.0]-2.176***-1.728***-0.477***(0.736)(0.493)(0.002)C(fip)[T.45.0]-0.972***-0.904***-0.254***(0.266)(0.213)(0.000)C(fip)[T.46.0]-2.104***-1.855***-0.739***(0.621)(0.507)(0.006)C(fip)[T.47.0]0.1190.263***0.414***(0.140)(0.086)(0.001)C(fip)[T.48.0]-0.908**-0.727***0.058***(0.455)(0.282)(0.002)C(fip)[T.49.0]-1.366***-1.279***-1.359***(0.204)(0.182)(0.006)C(fip)[T.5.0]0.314***0.196***0.128***(0.099)(0.036)(0.001)C(fip)[T.50.0]-1.995***-1.678***-1.062***(0.324)(0.163)(0.012)C(fip)[T.51.0]-1.354***-0.767***-0.282***(0.473)(0.190)(0.005)C(fip)[T.53.0]0.362***0.344***0.387***(0.082)(0.085)(0.095)C(fip)[T.54.0]-0.307**-0.572***-0.647***(0.132)(0.063)(0.001)C(fip)[T.55.0]-1.471**-1.020**0.335***(0.746)(0.444)(0.005)C(fip)[T.56.0]-3.130***-2.785***-1.483***(0.780)(0.524)(0.002)C(fip)[T.6.0]-0.0220.0050.377***(0.227)(0.166)(0.060)C(fip)[T.8.0]-1.661***-1.095***-0.224***(0.621)(0.263)(0.003)C(fip)[T.9.0]-2.726**-1.550***-0.504***(1.085)(0.417)(0.001)C(repeal)[T.1.0]-2.124***-1.669***-0.693**(0.599)(0.331)(0.300)C(repeal)[T.1.0]:C(year)[T.1986.0]-0.302***-0.240***-0.215**(0.105)(0.087)(0.097)C(repeal)[T.1.0]:C(year)[T.1987.0]-0.570***-0.472***-0.405**(0.207)(0.182)(0.170)C(repeal)[T.1.0]:C(year)[T.1988.0]-0.687***-0.567***-0.487***(0.250)(0.204)(0.155)C(repeal)[T.1.0]:C(year)[T.1989.0]-0.688***-0.582***-0.518***(0.193)(0.171)(0.077)C(repeal)[T.1.0]:C(year)[T.1990.0]-0.447-0.331-0.310(0.273)(0.310)(0.282)C(repeal)[T.1.0]:C(year)[T.1991.0]-0.361**-0.315-0.376(0.179)(0.233)(0.273)C(repeal)[T.1.0]:C(year)[T.1992.0]-0.344-0.348-0.406(0.238)(0.307)(0.299)C(repeal)[T.1.0]:C(year)[T.1993.0]-0.238-0.307-0.468(0.215)(0.318)(0.369)C(repeal)[T.1.0]:C(year)[T.1994.0]-0.038-0.141-0.341(0.306)(0.417)(0.536)C(repeal)[T.1.0]:C(year)[T.1995.0]0.1770.108-0.091(0.353)(0.453)(0.610)C(repeal)[T.1.0]:C(year)[T.1996.0]0.0980.044-0.221(0.415)(0.521)(0.674)C(repeal)[T.1.0]:C(year)[T.1997.0]0.2950.255-0.080(0.415)(0.508)(0.659)C(repeal)[T.1.0]:C(year)[T.1998.0]0.1760.174-0.289(0.506)(0.588)(0.746)C(repeal)[T.1.0]:C(year)[T.1999.0]0.1780.237-0.294(0.502)(0.578)(0.724)C(repeal)[T.1.0]:C(year)[T.2000.0]0.1270.249-0.298(0.510)(0.575)(0.704)C(year)[T.1986.0]-0.0220.0250.071(0.064)(0.054)(0.044)C(year)[T.1987.0]-0.154-0.0120.047(0.107)(0.082)(0.063)C(year)[T.1988.0]-0.2160.0450.123*(0.170)(0.116)(0.075)C(year)[T.1989.0]-0.2500.1290.214***(0.239)(0.131)(0.073)C(year)[T.1990.0]-0.3730.0770.211***(0.315)(0.138)(0.067)C(year)[T.1991.0]-0.2530.1900.223***(0.318)(0.138)(0.058)C(year)[T.1992.0]-0.568-0.067-0.009(0.389)(0.153)(0.060)C(year)[T.1993.0]-0.834**-0.250-0.212***(0.413)(0.166)(0.068)C(year)[T.1994.0]-1.028**-0.325*-0.289***(0.473)(0.189)(0.077)C(year)[T.1995.0]-1.453***-0.644***-0.627***(0.532)(0.235)(0.150)C(year)[T.1996.0]-1.835***-0.922***-0.885***(0.589)(0.207)(0.098)C(year)[T.1997.0]-2.021***-0.988***-0.977***(0.643)(0.203)(0.100)C(year)[T.1998.0]-2.109***-0.931***-0.915***(0.731)(0.182)(0.084)C(year)[T.1999.0]-2.218***-0.940***-0.927***(0.789)(0.165)(0.080)C(year)[T.2000.0]-2.405***-0.953***-0.992***(0.884)(0.141)(0.089)Intercept5.954***6.376***8.425***(1.607)(0.597)(0.058)acc0.0030.003(0.002)(0.002)alcohol1.020**1.009***(0.433)(0.299)crack0.0420.061(0.037)(0.043)income0.000(0.000)ir0.001*0.001*(0.000)(0.000)pi-0.029-0.045(0.071)(0.079)poverty-0.013(0.014)ur-0.042(0.034)Observations755755755R20.8930.8860.866Adjusted R20.8780.8720.850Residual Std. Error62.969 (df=666)64.710 (df=669)70.011 (df=674)F Statistic623.790*** (df=88; 666)548.554*** (df=85; 669)239.057*** (df=80; 674)Note:\n \n  *p<0.1;\n  **p<0.05;\n  ***p<0.01\n \n\n\nUsing these regressions we can plot the interaction effect of treatment and time. If the DiD design is to be plausible we should see a change in the effect at the point of the treatment date. Note how we extract the confidence interval around the parameter estimates for the interaction of the treatment and time effects. We then use this to plot the graduated impact of the treatment.\n\ndef make_regression_plot(reg):\n  abortion_plot = pd.DataFrame(\n      {\n          'sd': reg.bse['C(repeal)[T.1.0]:C(year)[T.1986.0]':'C(repeal)[T.1.0]:C(year)[T.2000.0]'],\n          'mean': reg.params['C(repeal)[T.1.0]:C(year)[T.1986.0]':'C(repeal)[T.1.0]:C(year)[T.2000.0]'],\n          'year': np.arange(1986, 2001)\n      })\n  abortion_plot['lb'] = abortion_plot['mean'] - abortion_plot['sd']*1.96\n  abortion_plot['ub'] = abortion_plot['mean'] + abortion_plot['sd']*1.96\n  abortion_plot.set_index('year', inplace=True)\n  return abortion_plot\n\n\nabortion_plot = make_regression_plot(reg)\nabortion_plot1 = make_regression_plot(reg1)\nabortion_plot2 = make_regression_plot(reg2)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(abortion_plot['mean'], 'o', color='blue', label='Full Regression')[0]\nax.vlines(x=abortion_plot.index, ymin=abortion_plot['lb'], ymax=abortion_plot['ub'], color='blue')\nax.plot(abortion_plot1['mean'], 'o', color='red', label=\"Reduced Regression\")[0]\nax.vlines(x=abortion_plot1.index, ymin=abortion_plot1['lb'], ymax=abortion_plot1['ub'], color='red', alpha=0.5)\n\nax.plot(abortion_plot2['mean'], 'o', color='green', label=\"No Controls Regression\")[0]\nax.vlines(x=abortion_plot2.index, ymin=abortion_plot2['lb'], ymax=abortion_plot2['ub'], color='green', alpha=0.5)\nax.legend()\nax.set_ylabel(\"Interaction Coefficient\")\nax.set_title(\"Event Studies with Multiple Regression Specifications\")\nax.axvline(1992, color='black', linestyle='--')\nax.axhline(0);\n\n\n\n\nWe can see here how the temporal points before the introduction of Roe show an effect on the interaction coefficient. Which rises towards 0 after Roe just so long as we control for the independent effects of extra covariates. This suggests some evidence for the contention that the change in legislation will alter the parabolic trajectory as anticipated by theory."
  },
  {
    "objectID": "posts/post-with-code/DiD-Card-Kreuger/DID.html#fitting-a-placebo-model-for-critique",
    "href": "posts/post-with-code/DiD-Card-Kreuger/DID.html#fitting-a-placebo-model-for-critique",
    "title": "Differences in Differences",
    "section": "Fitting a Placebo Model for Critique",
    "text": "Fitting a Placebo Model for Critique\nWe can further test this design if we model the effects of time on a placebo group at the same time. Here we’ll take as a placebo group a cohort of a greater age, where the effect of Roe is less likely to have an impact on the rates of gonorrhea in the cohort. In this way we have a kind of pseudo placebo group for our treatment. I’ve used similar approaches in work to analyse speed improvements made to in-app pages that were exposed to a certain treatment effects compared against pseudo treatment groups that we deemed less likely to be impacted by the implemented change.\n\nabortion = pd.read_csv('abortion_mixtape.csv')\nabortion = abortion[~pd.isnull(abortion.lnr)]\nabortion['yr'] = 0\nabortion.loc[(abortion.younger==1) & (abortion.repeal==1), 'yr'] = 1\n\nabortion['wm'] = 0\nabortion.loc[(abortion.wht==1) & (abortion.male==1), 'wm'] = 1\n\nabortion['wf'] = 0\nabortion.loc[(abortion.wht==1) & (abortion.male==0), 'wf'] = 1\n\nabortion['bm'] = 0\nabortion.loc[(abortion.wht==0) & (abortion.male==1), 'bm'] = 1\n\nabortion['bf'] = 0\nabortion.loc[(abortion.wht==0) & (abortion.male==0), 'bf'] = 1\n\n\nabortion_filt = abortion[(abortion.bm==1) & (abortion.age.isin([15,25]))]\nabortion_filt\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      fip\n      age\n      race\n      year\n      sex\n      totcase\n      totpop\n      rate\n      totrate\n      ...\n      pi\n      wm15\n      wf15\n      bm15\n      bf15\n      yr\n      wm\n      wf\n      bm\n      bf\n    \n  \n  \n    \n      13\n      13\n      1.0\n      25.0\n      2.0\n      1985.0\n      1\n      3969.0\n      84353\n      5682.9\n      4705.2\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      22\n      22\n      1.0\n      15.0\n      2.0\n      1985.0\n      1\n      5683.0\n      106187\n      4153.4\n      5351.9\n      ...\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      26\n      26\n      1.0\n      15.0\n      2.0\n      1986.0\n      1\n      5344.0\n      106831\n      3628.9\n      5002.3\n      ...\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      38\n      38\n      1.0\n      25.0\n      2.0\n      1986.0\n      1\n      3162.0\n      84593\n      4509.4\n      3737.9\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      50\n      50\n      1.0\n      25.0\n      2.0\n      1987.0\n      1\n      2650.0\n      84003\n      3754.1\n      3154.6\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      19460\n      19460\n      56.0\n      15.0\n      2.0\n      1995.0\n      1\n      2.0\n      379\n      471.7\n      527.7\n      ...\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      19475\n      19475\n      56.0\n      15.0\n      2.0\n      1996.0\n      1\n      2.0\n      388\n      460.8\n      515.5\n      ...\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      19488\n      19488\n      56.0\n      15.0\n      2.0\n      1997.0\n      1\n      2.0\n      355\n      947.9\n      563.4\n      ...\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      19524\n      19524\n      56.0\n      15.0\n      2.0\n      1998.0\n      1\n      1.0\n      408\n      409.8\n      245.1\n      ...\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      19561\n      19561\n      56.0\n      15.0\n      2.0\n      2000.0\n      1\n      2.0\n      420\n      414.9\n      476.2\n      ...\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0\n      0\n      0\n      1\n      0\n    \n  \n\n1506 rows × 45 columns\n\n\n\nThe benefit to allowing for a placebo factor is to test how the effect should remain pretty static in the group for which the treatment is unlikey to have an effect. This can strengthen or weaken the plausibility of the claim that the parrallel trends assumption holds. In a regression context we want to interact the effect of year, repeal and younger.\n\npivot = abortion_filt.groupby(['repeal', 'younger', 'year'])['lnr'].mean().reset_index()\npivot = pivot.pivot(['year', 'younger'], 'repeal', 'lnr')\npivot = pd.concat([pivot.T, pivot.T.diff()]).dropna()\ndisplay(pivot)\nfig, axs = plt.subplots(1, 2, figsize=(10, 6))\naxs = axs.flatten()\ndiff = pivot.iloc[2].reset_index().pivot('year', 'younger', 1.0)\ndiff.plot(kind='barh', ax=axs[0])\ndiff['diff'] = diff[0] - diff[1]\ndiff[['diff']].plot(kind='barh', ax=axs[1])\naxs[0].set_title(\"Diff by Treatment by Year and Placebo\")\naxs[1].set_title(\"Diff in Diff by Treatment by Year\")\n\n/var/folders/__/ng_3_9pn1f11ftyml_qr69vh0000gn/T/ipykernel_83904/824042692.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n  pivot = pivot.pivot(['year', 'younger'], 'repeal', 'lnr')\n\n\n\n\n\n\n  \n    \n      year\n      1985.0\n      1986.0\n      1987.0\n      1988.0\n      1989.0\n      ...\n      1996.0\n      1997.0\n      1998.0\n      1999.0\n      2000.0\n    \n    \n      younger\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      1.0\n      ...\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      1.0\n    \n    \n      repeal\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0.0\n      8.455438\n      8.253097\n      8.295281\n      8.327786\n      8.188281\n      8.238825\n      8.145482\n      8.255206\n      8.118427\n      8.518338\n      ...\n      7.276521\n      7.332863\n      7.328013\n      7.180292\n      7.449080\n      7.275626\n      7.350274\n      7.167805\n      7.393483\n      7.147023\n    \n    \n      1.0\n      8.050839\n      7.467163\n      7.967474\n      7.256647\n      7.550540\n      7.187125\n      7.362501\n      7.188717\n      7.232254\n      7.326252\n      ...\n      6.458796\n      6.449874\n      6.302575\n      6.413239\n      6.547882\n      6.307862\n      6.400939\n      6.102952\n      6.477026\n      5.838150\n    \n    \n      1.0\n      -0.404599\n      -0.785934\n      -0.327807\n      -1.071139\n      -0.637740\n      -1.051699\n      -0.782981\n      -1.066489\n      -0.886173\n      -1.192086\n      ...\n      -0.817726\n      -0.882989\n      -1.025438\n      -0.767054\n      -0.901198\n      -0.967764\n      -0.949336\n      -1.064853\n      -0.916457\n      -1.308873\n    \n  \n\n3 rows × 32 columns\n\n\n\n/var/folders/__/ng_3_9pn1f11ftyml_qr69vh0000gn/T/ipykernel_83904/824042692.py:7: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n  diff = pivot.iloc[2].reset_index().pivot('year', 'younger', 1.0)\n\n\nText(0.5, 1.0, 'Diff in Diff by Treatment by Year')\n\n\n\n\n\nFrom the simple contrast above, evidence of a treatment effect seems mixed with both positive and negative effects centered on zero in the post treatment years. This type of placebo design can be specified using a complex regression equation. The equation relies on interaction terms as before with the simpler DiD design, but now we also include interactions for the placebo group. This is occasionally called a triple differenced design.\n\nreg = (\n    smf\n    .wls(\"\"\"lnr ~ C(repeal)*C(year) + C(younger)*C(repeal) + C(younger)*C(year) + \nC(yr)*C(year) + C(fip)*t + acc + ir + pi + alcohol + crack + poverty + income + ur\"\"\", \n        data=abortion_filt, weights=abortion_filt.totpop.values)\n    .fit(\n        cov_type='cluster', \n        cov_kwds={'groups': abortion_filt.fip.values}, \n        method='pinv')\n)\n\nabortion_plot = make_regression_plot(reg)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(abortion_plot['mean'], 'o', color='blue')[0]\nax.vlines(x=abortion_plot.index, ymin=abortion_plot['lb'], ymax=abortion_plot['ub'], color='blue')\nax.axhline(0)\nax.set_title(\"Event Study with DDD design \\n adding interactions with younger cohort\");\n\n\n\n\nWe can alternatively test the diff in diff model seperately to show how there is a less pronounced effect for these two groups with the older group providing evidence against the hypothesis of a parabolic like treatment effect. To reiterate that last point we can estimate the model seperately on different age cohorts and judge whether the treatment effect had a similar graduated effect in both cohorts. The older group appear to have less of a sustained treatment effect and counts as evidence against the theory of a graduated parabolic change due to the Roe decision.\n\nabortion = pd.read_csv('abortion_mixtape.csv')\nabortion = abortion[~pd.isnull(abortion.lnr)]\n\nabortion_filt = abortion[(abortion.race == 2) & (abortion.sex == 1) & (abortion.age == 15)]\n\nregdd = (\n    smf\n    .wls(\"\"\"lnr ~ C(repeal)*C(year) + C(fip) + acc + ir + pi + alcohol+ crack + poverty+ income+ ur\"\"\", \n        data=abortion_filt, weights=abortion_filt.totpop.values)\n    .fit(\n        cov_type='cluster', \n        cov_kwds={'groups': abortion_filt.fip.values}, \n        method='pinv')\n)\n\nabortion_filt = abortion[(abortion.race == 2) & (abortion.sex == 1) & (abortion.age == 25)]\n\nregdd1 = (\n    smf\n    .wls(\"\"\"lnr ~ C(repeal)*C(year) + C(fip) + acc + ir + pi + alcohol+ crack + poverty+ income+ ur\"\"\", \n        data=abortion_filt, weights=abortion_filt.totpop.values)\n    .fit(\n        cov_type='cluster', \n        cov_kwds={'groups': abortion_filt.fip.values}, \n        method='pinv')\n)\n\nabortion_plot = make_regression_plot(regdd)\nabortion_plot1 = make_regression_plot(regdd1)\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(abortion_plot['mean'], 'o', color='blue', label='15 years')[0]\nax.vlines(x=abortion_plot.index, ymin=abortion_plot['lb'], ymax=abortion_plot['ub'], color='blue')\nax.plot(abortion_plot1['mean'], 'o', color='red', label='25 years')[0]\nax.vlines(x=abortion_plot1.index, ymin=abortion_plot1['lb'], ymax=abortion_plot1['ub'], color='red')\nax.axhline(0)\nax.axvline(1992, color='black', linestyle='--')\nax.legend()\nax.set_title(\"Event Study with Diff in Diff design\");"
  },
  {
    "objectID": "posts/post-with-code/Bayesian_AB_tests/bayesian_ab_tests.html",
    "href": "posts/post-with-code/Bayesian_AB_tests/bayesian_ab_tests.html",
    "title": "Bayesian Estimation and the Histogram Approximation",
    "section": "",
    "text": "Imagine you’re pretty confident of your recent bet. All the other gamblers at the table have taken the other side of the bet. Just for fun, assume all other gamblers in the casino have bet against you. The game is in the first quarter and things are looking good, but how do you evaluate now whether you should hedge your bets before final quarter is called? This kind of scenario is typical in an imbalanced A/B test where we haven’t randomised the samples, and we’re letting it ride based entirely on vibes. Maybe you’ve released a change to production but only exposed two clients to the new changes. How do you evaluate the impact of your change?"
  },
  {
    "objectID": "posts/post-with-code/Bayesian_AB_tests/bayesian_ab_tests.html#mixture-models-and-histogram-trick",
    "href": "posts/post-with-code/Bayesian_AB_tests/bayesian_ab_tests.html#mixture-models-and-histogram-trick",
    "title": "Bayesian Estimation and the Histogram Approximation",
    "section": "Mixture Models and Histogram Trick",
    "text": "Mixture Models and Histogram Trick\nLets make the issues a little more concrete. Imagine you’re trying to evaluate a bi-modal distribution of loading times on a website after a new feature is turned on. This distribution has two peaks: (1) represents healthy load times and the other (2) represents unhealthy load times during periods of high activity. Here we can see the distributions of load times on each arm of the experiment. On the treatment arm we have 2000 records and on the control arm we have 7,000,000.\n\n\n\nbi_modal\n\n\nWe have a very imbalanced data on each arm. However because we have a large stock of historic data regarding the loading times at peak and off peak, we’re prepared to ascribe a set of priors to estimate the impact of our new feature on the treamtment arm, can we do the same for the control arm?\n\nWhat kind of Model?\nOften we observe a data generating process which is likely the combination of a number of distinct processes. The overall distribution is known as a mixture distribution and there are some tools in the Bayesian workflow to help identify the manner in which these distinct processes are combined and the relative weight we should ascribe to each.\nBayesian modelling in general requires extensive MCMC simulation and as the data scales up there is a proportional impact on the fitting time of the model. This becomes infeasible with large data such as website performance data measured in the nanosecond. However there is a neat treat to approximate learning the Bayesian posterior from massive data - as discussed here:\n\n\nWant to run Bayesian A/B tests at serious scale? Check out how @ferrine96 and I did this for one of @pymc_labs' clients.https://t.co/cJJPH7X0UZ pic.twitter.com/BnLNtENzam\n\n— Dr Benjamin Vincent (@inferencelab) August 12, 2022\n\n\nInstead of learning from the raw data, we learn from a histogram approximation which buckets our data into n-quantiles (however many required) and weights the observations to be drawn from that quantile in proportion to the number of observations seen in the raw data within that range.\n\n\nTesting the Histogram Trick on a Mixture Model\nDrum up some fake data\n\nsample = pd.Series(np.random.normal(0, .8, 6000))\nsample = pd.concat([sample, pd.Series(np.random.gumbel(2, 1.2, 4000))])\n\nsample1 = pd.Series(np.random.normal(0.3, .8, 6000))\nsample1 = pd.concat([sample, pd.Series(np.random.gumbel(1.8, 1.2, 4000))])\n\nfig, ax = plt.subplots()\nax.hist(sample, alpha=0.9, edgecolor='black', color='slateblue', label='Treatment')\nax.hist(sample1, alpha=0.25, edgecolor='black', color='skyblue', label='Control')\nax.legend()\n\n\n\nfake\n\n\nFirst we’ll estimate the model using the standard PYMC mcmc sampling method and then we’ll show how to do the same using the histogram trick. After seeing that the histogram trick preserves sound inference we’ll apply it to our big data problem and estimate our bi-modal loading time distribution.\n\n\nThe Standard Bayesian Approach\nWe fit a Normal and Gumbel mixture distribution for our fake data using the standard method\n\nwith pm.Model() as model_mix:\n    # First set of priors for our treatment group\n    w = pm.Dirichlet('w', a=np.array([1, 1]))  # 2 mixture weights\n    mu = pm.Normal(\"mu\", 0, 1)\n    mu1 = pm.Normal(\"mu1\", 2, 1)\n    sd = pm.HalfNormal('sd', 1)\n    sd1 = pm.HalfNormal('sd1', 1)\n\n    components = [\n        pm.Normal.dist(mu=mu, sigma=sd),\n        pm.Gumbel.dist(mu=mu1, beta=sd1),\n    ]\n\n    # Second set of priors for our Control group\n    w1 = pm.Dirichlet('w1', a=np.array([1, 1]))  # 2 mixture weights\n    m = pm.Normal(\"m\", 0, 1)\n    m1 = pm.Normal(\"m1\", 2, 1)\n    s = pm.HalfNormal('s', 1)\n    s1 = pm.HalfNormal('s1', 1)\n\n    components1 = [\n        pm.Normal.dist(mu=m, sigma=s),\n        pm.Gumbel.dist(mu=m1, beta=s1),\n    ]\n\n    # Likelihood for our Treatment group\n    mix = pm.Mixture('mix', w=w, comp_dists=components,\n        observed=np.array(sample))\n\n    # Likelihood for our Control group\n    mix1 = pm.Mixture('mix1', w=w1, comp_dists=components1,\n        observed=np.array(sample1))\n\n    idata_mix = pm.sample(draws=1000, cores=1)\n    idata_mix.extend(pm.sample_posterior_predictive(idata_mix))\n\n\nThe Histogram Trick\nNow when we wish to fit the same model using the Histogram trick, the code is similar but we change the likelihoods:\n\n    pot = pmx.distributions.histogram_approximation(\n        \"pot\", pm.Mixture.dist(w=w, comp_dists=components),\n        observed=np.array(sample), n_quantiles=1000)\n\n    pot = pmx.distributions.histogram_approximation(\n        \"pot1\", pm.Mixture.dist(w=w1, comp_dists=components1),\n        observed=np.array(sample1), n_quantiles=1000)\n\n    idata_mix_approx = pm.sample(draws=5000, cores=1)\n\nUnder the hood the histogram trick is a pm.Potential, and as such the implementation cannot make use of the standard posterior predictive sampling. But we can do a manual work-around to achieve the same results, by sampling from the posterior draws on the parameters which determine the shape of our model. Even better it fits very fast!!\n## Extract uncertainty in parameters from our posterior\ngen_df = pd.DataFrame({'mu': idata_mix_approx.stack(sample=[\"chain\", \"draw\"]).posterior['mu'], \n                'sd': idata_mix_approx.stack(sample=[\"chain\", \"draw\"]).posterior['sd'], \n                 'mu1': idata_mix_approx.stack(sample=[\"chain\", \"draw\"]).posterior['mu1'], \n                  'sd1': idata_mix_approx.stack(sample=[\"chain\", \"draw\"]).posterior['sd1'],\n                  'm': idata_mix_approx.stack(sample=[\"chain\", \"draw\"]).posterior['m'], \n                's': idata_mix_approx.stack(sample=[\"chain\", \"draw\"]).posterior['s'], \n                 'm1': idata_mix_approx.stack(sample=[\"chain\", \"draw\"]).posterior['m1'], \n                  's1': idata_mix_approx.stack(sample=[\"chain\", \"draw\"]).posterior['s1']\n                  })\n\n## Generate samples from each mixture component for treatment and control groups\ngen_df['y_T'] = gen_df.apply(lambda x: np.random.normal(x['mu'], x['sd'], 1)[0], axis=1)\ngen_df['y1_T'] = gen_df.apply(lambda x: np.random.gumbel(x['mu1'], x['sd1'], 1)[0], axis=1)\ngen_df['y_C'] = gen_df.apply(lambda x: np.random.normal(x['m'], x['s'], 1)[0], axis=1)\ngen_df['y1_C'] = gen_df.apply(lambda x: np.random.gumbel(x['m1'], x['s1'], 1)[0], axis=1)\n\n## Extract expected weights from each component\nweights_T = idata_mix_approx.stack(sample=['chain', 'draw']).posterior['w'].mean(axis=1).to_dataframe()\nweights_C = idata_mix_approx.stack(sample=['chain', 'draw']).posterior['w1'].mean(axis=1).to_dataframe()\n\n## Combine to recover a predictive distribution for our data\nmix_posterior_y_T = pd.concat([gen_df['y_T'].sample(frac=weights_T['w'][0]), \n                           gen_df['y1_T'].sample(frac=weights_T['w'][1])])\n\nmix_posterior_y_C = pd.concat([gen_df['y_C'].sample(frac=weights_C['w1'][0]), \n                           gen_df['y1_C'].sample(frac=weights_C['w1'][1])])\n\n\nPlotting the Fits\nWith these quantities in place we can plot the predictions of both models against the distribution of the raw data.\n\nfig, axs = plt.subplots(3, 1, figsize=(10, 18))\naxs = axs.flatten()\naxs[0].hist(sample, density=True, alpha=0.4, label='Treatment Data', color='slateblue', bins=50, edgecolor='black')\naxs[0].hist(mix_posterior_y_T, density=True, label='Predicted Approx', histtype='step', linewidth=4, color='cyan', bins=50)\naxs[0].hist(idata_mix.stack(sample=['chain', 'draw']).posterior_predictive['mix'].to_dataframe()['mix'], density=True, label='Predicted Mixture', histtype='step', linewidth=4, color='black', bins=50)\naxs[0].legend()\naxs[0].set_title(\"Mixture Model Posterior Predictive Fit:  \\n Approx and Full Bayesian for Treatment\", fontsize=15)\naxs[1].hist(sample1, density=True, alpha=0.4, label='Control Data',edgecolor='black', \ncolor='skyblue', bins=50)\naxs[1].hist(mix_posterior_y_C, density=True, label='Predicted Approx', histtype='step', linewidth=4, color='cyan', bins=50)\naxs[1].hist(idata_mix.stack(sample=['chain', 'draw']).posterior_predictive['mix1'].to_dataframe()['mix1'], density=True, label='Predicted Mixture', histtype='step', linewidth=4, color='black', bins=50)\naxs[1].legend()\naxs[1].set_title(\"Mixture Model Posterior Predictive Fit:  \\n Approx and Full Bayesian for Control\", fontsize=15)\n\nxs, ys, patches = axs[2].hist(idata_mix.stack(sample=['chain', 'draw']).posterior_predictive['mix'].to_dataframe()['mix'], density=True, label='Predicted Treatment Mixture', histtype='step', linewidth=4, color='black', bins=50)\naxs[2].axvspan(2.5, 10, clip_path=patches[0], color='slateblue', label='Proportion > 2.5 Treatment')\n\nxs1, ys1, patches1 =axs[2].hist(idata_mix.stack(sample=['chain', 'draw']).posterior_predictive['mix1'].to_dataframe()['mix1'], density=True, label='Predicted Control Mixture', histtype='step', linewidth=4, color='grey', bins=50)\naxs[2].axvspan(2.5, 10, clip_path=patches1[0], color='red', alpha=0.25, label='Proportion > 2.5 Control')\naxs[2].set_title(\"Posterior Predictive Distribution \\n Treatment V Control\", fontsize=15)\naxs[2].legend();\n\n\n\nfake\n\n\nHere we can see how the histogram trick properly captures the shape of the distribution as well as the model estimated using the standard likelihood."
  },
  {
    "objectID": "posts/post-with-code/Bayesian_AB_tests/bayesian_ab_tests.html#do-we-push-to-production",
    "href": "posts/post-with-code/Bayesian_AB_tests/bayesian_ab_tests.html#do-we-push-to-production",
    "title": "Bayesian Estimation and the Histogram Approximation",
    "section": "Do we Push to Production?",
    "text": "Do we Push to Production?\nHaving established that the method works well, we can apply a similar model (not shown) to our load times data and recover results for our A/B test. We can see here that the mixture model recovers a nice fit to the observed mixture distribution. Better it ran really, really fast, even with over 7,000,000 records.\n\n\n\nfake\n\n\nWe can also now quantify what proportion of the mixture distribution has shifted from the Gumbel component to the Lognormal component of the distribution, and hence how healthy our loading time distribution is predicted to be if we apply the treatment. We can formulate more precise measures of effect using the estimated mixture posterior distributions, but these cursory plots are at least directionally positive.\n\n\n\nfake\n\n\nIt looks like we should make our change and hold our nerve! If we need to make a call now the data we have suggests we should!"
  },
  {
    "objectID": "posts/post-with-code/Bayesian_AB_tests/bayesian_ab_tests.html#conclusion",
    "href": "posts/post-with-code/Bayesian_AB_tests/bayesian_ab_tests.html#conclusion",
    "title": "Bayesian Estimation and the Histogram Approximation",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve seen here a concrete example of some innovative tooling that the PYMC team is developing to help scale Bayesian inference in industry. I hope i’ve given a clean example of you might use this form of inference to interrogate questions of improvemnt in imbalanced A/B style experiemnts. Feedback or pushback welcome!?"
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html",
    "href": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html",
    "title": "Factor Analysis and Construct Validity in Psychology",
    "section": "",
    "text": "In a prescient paper in the 1970s Paul Meehl wrote about the lack of cumulative success in the psychologocial sciences, and how this should be attributed to poor methodology rather than the sheer difficulty of the subject. He elaborates an impressive list of problems for modeling any psychological process. Common themes criss-cross the list and interact with one another in ways which could make you despair for the discipline. So it is, I think, somewhat surprising that Meehl locates the main problem not in the subject, but in the method of significance testing.\nWe’ll narrow our focus shortly, but first consider the breadth of the issues."
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#meehls-problems-plaguing-psychology",
    "href": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#meehls-problems-plaguing-psychology",
    "title": "Factor Analysis and Construct Validity in Psychology",
    "section": "Meehl’s Problems Plaguing Psychology",
    "text": "Meehl’s Problems Plaguing Psychology\n\n\n\n\n\n\n\nProblem\nDescription\n\n\n\n\nResponse-Classification Problem\nDifficulty attributing mental process to observed behaviour\n\n\nSituation-Taxonomy Problem\nDifficulty isolating the stimulus from rough description of environment\n\n\nUnit of Measurement\nChoice of scale e.g. ratio or interval, continuous or discrete\n\n\nIndividual Differences\nCommon psychological dispositions arise from idiosyncratic mixture of influences\n\n\nPolygenetic Hereditry\nCommon psychological dispositions have complex causal roots\n\n\nDivergent Causality\nVery sensitive to initial conditions, slight differences at source result in large differences in outcomes\n\n\nIdiographic Problem\nOften relates to the specific discovery of facts rather than generalisable laws\n\n\nUnknown Critical Events\nPaucity of medical history or local context\n\n\nNuisance Variables\nDifficulty deciphering wealth of related variables\n\n\nFeedback Loops\nInterventions lead to changing behaviour, disrupting the study\n\n\nAutoCatalytic Processes\nIndividual influences on their own psychological disposition during tests\n\n\nRandom Walk\nDifferences in dispositional response often due to random flux, rather than different causal influence\n\n\nSheer Number of Variables\nCumulative influence of small random-drift can have decisive impact on the psychological disposition\n\n\nImportance of Cultural Factors\nWeight of an individual variable may vary with cultural context\n\n\nContext-Dependent Stochastilogicals\nAny derived rule of behaviour is likely only probabilisitic and subject to contextual variation.\n\n\nOpen Concepts\nLatent psychological factors under study “evolve” as we include/remove indicative measures\n\n\nIntentionality, Purpose and Meaning\nPurpose drives behaviour and changes in behaviour\n\n\nRule Governance\nPeople follow rules influencing their behaviour\n\n\nUniquely Human Events & Powers\nThere are some behaviours which have no animal/ape analogies to compare. Limiting data\n\n\nEthical Constraints on Research\nConstraints on some decisive testing methodologies due to ethical abuses.\n\n\n\nWe’re going to focus on two of the problems that most clearly relate the issue of significance testing: open concepts, context-dependent stochastilogicals. These issues are tightly coupled with the ability to falsify psychological hypotheses since they both serve as reasons to doubt contrary evidence and therefore deny us a decisive rejection of our hypotheses even when they conflict with observable data. This difficulty directly undermines the paradigm of null-hypothesis testing in psychology, since they imply that we are incapable of placing the null under severe scrutiny."
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#the-recipe-open-concepts-and-context-sensitive-stocastologicals",
    "href": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#the-recipe-open-concepts-and-context-sensitive-stocastologicals",
    "title": "Factor Analysis and Construct Validity in Psychology",
    "section": "The Recipe: Open Concepts and Context Sensitive Stocastologicals",
    "text": "The Recipe: Open Concepts and Context Sensitive Stocastologicals\nThe ugly word “stochastological” is to be contrasted with the, perhaps more familar but also pompous, notion of a nomological inference - one which is valid by appeal to a wholly general law.\n\\[ \\text{(Nomological): } \\forall x(Fx \\rightarrow Gx) \\]\nMeehl argues that the idiosyncratic and individual specific patterns of causation in psychology short-circuit any appeals to over-arching rules. Even when we can consistently measure a disposition common across individuals, the nature of the observed patterns support probabilistic inference not law-like generalisations. Even then the underlying probabilities are liable to change with the context. You might be prone to defer to authority in 9 of 10 cases, but always exhibit knee jerk refusal when prompted by a political opponent.\n\\[ \\text{(Stochastic): }  \\underbrace{P(Gx | Fx) \\geq 0.90}_{context = c} \\]\nCombine this issue with contingencies of measurement and the dynamic nature of the scales, and we have a recipe for undermining any null-hypothsis significance test. Consider iteritive model building which tests for predictive aspects of some latent factor, say risk-aversion, on performance at a related task. Say the factor was initially derived from a set of observational measures:\n\\[ \\underbrace{ feature4,  \\overbrace{ \\text{ feature5, } \\overbrace{feature1 , feature2, feature3 }^{\\text{initial feature measures}}}^{\\text{final feature measures}}}_{\\text{Intuitive Concept}} \\twoheadrightarrow \\text{Construct} \\]\nNow each iteration was designed to test the imagined psychological constructand the features are observational traits associated with risk-taking. Each additional feature seemed reasonable at time. We may even have gotten better predictive accuracy. But the model build and the changing measurements raises question over what we’re constructing and whether it truly “captures” the pre-theoretical psychological concept.\nNow we have a measure of risk-aversion and we try to render an intuitive hypothesis mathematically precise with respect to our construct. This is itself an art, but for the moment assume we can state \\((H)\\). Since the hypothesis test is supposed to infer the falsity of the assumptions from evidence contrary to the main hypothesis i.e. if when assuming the hypothesis and our auxilary commitments (regarding measurement and context) we find evidence inconsistent with our expectations, then we should reject the hypothesis! In practice the hypothesis is ussually far more entrenched in the minds of the experimenters than the extensive auxillary commitments, so we normally seek the source of predictive error in mistakes made than with the key hypothesis.\n\\[ (H \\wedge A) \\rightarrow O , \\neg O \\vdash \\neg (H \\wedge A) \\text{ ....so not A}\\]\nIn this way we preserve the hypothesis and remove it from exposure to a strict test of its accuracy. This experiment is really only validating the consistency of the data with a nebulous range of auxilary commitments, and as such allows the experimenter to move the goal-posts almost at whim.\nMeehl concedes that there may be some justification for this procedure if we would grant that \\((H)\\) has in some sense a greater “verisimilitude” than the auxilary commitments. Psychology differs from other disciplines in that the objects of measurement and the manner of measurement are not tightly bound. The truth “content” of claims about measurement of length, for instance, stand or fall with claims about the measurement instrument - they are so intimately connected that the measurement apparatus is almost definitional. You might object that appeals to versimilitude in some way puts the cart before the horse. We’re designing an experiement to test \\((H)\\), how can we presume it’s truth in testing!? This is fair but only points to difficulty of making \\((H)\\) precise and the distance between mental phenomena and our measurement of it"
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#open-concepts-and-factor-analysis",
    "href": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#open-concepts-and-factor-analysis",
    "title": "Factor Analysis and Construct Validity in Psychology",
    "section": "Open Concepts and Factor Analysis",
    "text": "Open Concepts and Factor Analysis\nIf we’re lucky we have a clear idea of how to measure the psychological phenomenon of interest and you can devise a survey to capture the details. If instead you just have some data and you think there might be a latent factor driving the observations, then the the factor analysis effectively tries to group the observed variable by their correlations. The underlying statistical model for \\(m\\) factors states:\n\nAssume the model \\[\\mathbf{X} = \\mathbf{\\mu}^{p \\times 1}  + \\mathbf{L}^{p \\times m} \\mathbf{F}^{m \\times 1} +\\epsilon^{p \\times 1} \\]\nwhere \\(\\mathbf{X}\\) is our data matrix recording the observational data while \\(\\mu\\) is the mean vector with entries for each column in \\(\\mathbf{X}\\). So our observational data is deemed a function modifying the multivariate mean as a linear function of latent factors \\(\\mathbf{F}\\) with factor loadings \\(l_{i, j} \\in \\mathbf{L}\\), for each of the observed variables \\(i\\) and for each \\(j\\) of the \\(m\\) factors.\nThe model assumes \\(\\mathbf{F}\\) and \\(\\epsilon\\) are independent and \\(E(\\mathbf{F}) = \\mathbf{0}, Cov(\\mathbf{F}) = \\mathbf{I}\\) In addition note that \\(E(\\epsilon) = \\mathbf{0}\\) and \\(Cov(\\epsilon) = \\Psi\\) where \\(\\Psi\\) is a diagonal matrix.\nThen we can show that \\[Cov(\\mathbf{X}) = \\mathbf{LL}^{'} + \\Psi\\]\n\nThat’s a bit abstract, but the point is just that each factor is a linear construct of the observed data, and we can choose how many constructs to build. An important consequence of this fact is that we can express the variance of each observed feature in terms of the loadings and a random variance, so if we can explain a high portion of their variance in a low number of factors we can be reasonably sure that the dimensional reduction remains representative of the diversity in the original data set.\n\\[ Var(X_i) = \\underbrace{l_{i, j}^{2} + l_{i, 2}^{2} ... l_{i, m}^{2}}\\_{communalities} + \\psi_{i}\\]\n\nProof. \\[ (\\mathbf{X} - \\mu) = (\\mathbf{L}\\mathbf{F} + \\epsilon)\\] \\[ \\Rightarrow (\\mathbf{X} - \\mu)(\\mathbf{X} - \\mu)^{'} = (\\mathbf{L}\\mathbf{F} + \\epsilon)(\\mathbf{L}\\mathbf{F} + \\epsilon)^{'}\\] \\[ = (\\mathbf{L}\\mathbf{F} + \\epsilon)((\\mathbf{L}\\mathbf{F})^{'} + \\epsilon^{'}) \\] \\[ = \\mathbf{L}\\mathbf{F}(\\mathbf{L}\\mathbf{F})^{'} + \\epsilon(\\mathbf{L}\\mathbf{F})^{'} + \\mathbf{L}\\mathbf{F}\\epsilon^{'} + \\epsilon\\epsilon^{'}\\] \\[ \\Rightarrow E((\\mathbf{X} - \\mu)(\\mathbf{X} - \\mu)^{'}) = E( (\\mathbf{L}\\mathbf{F} + \\epsilon)(\\mathbf{L}\\mathbf{F} + \\epsilon)^{'})\\] \\[ \\Rightarrow Cov(\\mathbf{X}) = \\mathbf{L}E(\\mathbf{F}\\mathbf{F}^{'})\\mathbf{L}^{'} + E(\\epsilon\\mathbf{F}^{'})\\mathbf{L}^{'} + \\mathbf{L}E(\\mathbf{F}\\epsilon^{'}) + E(\\epsilon\\epsilon^{'}) \\] \\[ = \\mathbf{L}\\mathbf{L}^{'} + \\Psi \\]\n\n\nEstimating the latent factor values\nVarious techniques can be applied to estimate the loadings \\(\\mathbf{L}\\) based on a strategic decomposition of sample covariance matrix to derive the principal factors. The typical technique is to use the eigenvalue decomposition, but a maximum likelihood method is also feasible. It is another question altogeher for how to estimate the factor scores \\(f_{j} \\in \\mathbf{F}\\) from our derived factor loadings.\nThis is a two step, where we base an estimate on a set of prior estimates. One or more latent factors are assumed, observable features postulated to be related are grouped and from these we derive a recipe for constructing the latent factor as a composite of the observed features. From this recipe, we can then by a process of optimisation derive estimates for the values of the latent feature. We won’t dwell on the details here, but I want to stress the level of abstraction! When it comes to test the hypotheses about the underlying psychological phenomenon, this method has certain mathematical appeal but it is not above question. It is these kind of contingencies: the bespoke assumptions of the model, but correlation and covariance relationship between the observed features and richness of the data required to discover the expected values of \\(\\mathbf{L}, \\mathbf{f}_j\\) respectively, coupled with the difficulty of interpreting the factors in light of the original psychological concept, that undermine cumulative progress in psychology."
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#weighing-the-hypothesis",
    "href": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#weighing-the-hypothesis",
    "title": "Factor Analysis and Construct Validity in Psychology",
    "section": "Weighing the Hypothesis",
    "text": "Weighing the Hypothesis\nThese observations suggest that the notion construct validity is not easily resolved and as such our initial hypotheses are plagued by innumerable auxilary commitments. It is with these considerations in mind that Paul Meehl can write:\n\n“[T]he almost universal reliance on merely refutng the null hypothesis as the standard method for corroborating substantive theories in the soft areas is a terrible mistake, is basically unsound, poor scientific strategy and one of the worst things that ever happened in theory of psychology” - Theoretical Risks and Tabular Asteriks, Sir Karl, Sir Ronald and the Slow Progress of Soft Psychology\n\nThe accumulation of auxilary commitments makes the cumulative confirmation of substantive psychological theory proportionaly unlikely. At best we may get lucky in some cases, but the character of the object under study is so dynamic that simple significance tests are next to useless. The “distance” between the latent factor and our measurement of it supply an almost endless set of auxilary commitments which can come under pressure when evaluating a given hypothesis.\nBut there is a tension since difficulty of measurement does not necessarily undermine the theory. In particular, there are theory’s which have a high degree of intuitive plausibility (“verisimilitude”) but escape our ability to properly measure. Any measurement construct is at best an attempted proxy. There are some historic measures of construct validity such as Cronbach’s alpha which at least test for a directional consistency in the observed features."
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#factor-reliability-an-example-in-code.",
    "href": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#factor-reliability-an-example-in-code.",
    "title": "Factor Analysis and Construct Validity in Psychology",
    "section": "Factor Reliability: An Example in Code.",
    "text": "Factor Reliability: An Example in Code."
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html",
    "href": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "import pandas as pd\nfrom matplotlib import pyplot as plt\nimport nltk\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom factor_analyzer import FactorAnalyzer\nfrom sklearn.decomposition import FactorAnalysis\nimport random\nimport seaborn as sns\nrandom.seed(30)"
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#create-the-fake-customer-purchase-data",
    "href": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#create-the-fake-customer-purchase-data",
    "title": "Examined Algorithms",
    "section": "Create the Fake Customer Purchase data",
    "text": "Create the Fake Customer Purchase data\nWe create two fake data sets with a discernible structures, but we don’t want the models to work too easily so we sample again from these data pairing both sets as if we have a customer and their products.\n\nX, y = make_blobs(n_samples=10000, \n                  centers=5, \n                  n_features=6,\n                 random_state=0\n                 )\n\nprods = ['product_desc_' + str(x) for x in range(0, 6)]\ndf_products = pd.DataFrame(data=X, columns=prods)\ndf_products['Product_Class'] = y\ndf_products['Product_ID'] = df_products.index\n\nX, y = make_blobs(n_samples=1000, \n                  centers=10,\n                  n_features=8,\n                  random_state=0)\ncusts = ['customer_desc_' + str(x) for x in range(0, 8)]\ndf_customer = pd.DataFrame(data=X,\n                           columns=custs)\n\ndf_customer['Customer_Class'] = y\ndf_customer['Customer_ID'] = df_customer.index\n\n# Randomly Select some of the customers to pair with random purchases\nday1 = pd.DataFrame(zip(\n        [random.randint(0, 1000) for x in range(0, 500)],\n        [random.randint(0, 10000) for x in range(0, 500)]\n        ), \n        columns=['Customer_ID', 'Product_ID']\n                   )\n\nday2 = pd.DataFrame(zip(\n        [random.randint(0, 1000) for x in range(0, 500)],\n        [random.randint(0, 10000) for x in range(0, 500)]\n        ), \n        columns=['Customer_ID', 'Product_ID'])\n\npurchases = pd.concat([day1, day2],\n                      axis=0, \n                      ignore_index=True)\npurchases.head()\n\n\n\n\n\n  \n    \n      \n      Customer_ID\n      Product_ID\n    \n  \n  \n    \n      0\n      552\n      773\n    \n    \n      1\n      827\n      8608\n    \n    \n      2\n      296\n      518\n    \n    \n      3\n      625\n      5394\n    \n    \n      4\n      30\n      8543\n    \n  \n\n\n\n\n\ndf_purchases = None\nfor purchase in range(0, len(purchases)):\n    cust_id = purchases['Customer_ID'][purchase]\n    prod_id = purchases['Product_ID'][purchase]\n    cust = df_customer[df_customer['Customer_ID'] == \n                       cust_id]\n    cust.reset_index(inplace=True, drop=True)\n    prod = df_products[df_products['Product_ID'] == \n                       prod_id]\n    prod.reset_index(inplace=True, drop=True)\n    temp = pd.concat([prod, cust], axis=1)\n    if df_purchases is None:\n        df_purchases = pd.concat([prod, cust], axis=1)\n    else:\n        df_purchases = df_purchases.append(\n            pd.concat([prod, cust], axis=1)\n        )\ndf_purchases.reset_index(inplace=True, drop=True)\ndf_purchases\n\n\n\n\n\n  \n    \n      \n      product_desc_0\n      product_desc_1\n      product_desc_2\n      product_desc_3\n      product_desc_4\n      product_desc_5\n      Product_Class\n      Product_ID\n      customer_desc_0\n      customer_desc_1\n      customer_desc_2\n      customer_desc_3\n      customer_desc_4\n      customer_desc_5\n      customer_desc_6\n      customer_desc_7\n      Customer_Class\n      Customer_ID\n    \n  \n  \n    \n      0\n      -1.233276\n      8.747108\n      9.165746\n      -0.007541\n      4.652380\n      1.913970\n      1\n      773\n      -6.374916\n      -2.257586\n      7.688019\n      -7.992522\n      7.593467\n      -9.193683\n      10.376847\n      -0.388405\n      8.0\n      552.0\n    \n    \n      1\n      -0.170403\n      8.219852\n      9.710727\n      -1.550177\n      5.811414\n      1.492459\n      1\n      8608\n      9.294317\n      -2.231715\n      6.061894\n      -0.438840\n      1.246116\n      8.820684\n      -9.950039\n      -7.391761\n      1.0\n      827.0\n    \n    \n      2\n      -2.822567\n      8.036298\n      10.929105\n      -0.935760\n      6.077699\n      1.265250\n      1\n      518\n      -8.793580\n      4.805865\n      6.167272\n      5.770659\n      7.451190\n      4.144325\n      1.196351\n      5.414350\n      2.0\n      296.0\n    \n    \n      3\n      1.019780\n      6.766354\n      -9.152836\n      -8.611808\n      -8.749707\n      6.637419\n      2\n      5394\n      -7.047638\n      -2.131113\n      5.768863\n      -8.094387\n      6.223832\n      -8.834962\n      9.629384\n      -1.935227\n      8.0\n      625.0\n    \n    \n      4\n      4.321656\n      7.430975\n      8.847703\n      5.921750\n      -0.868445\n      5.483210\n      3\n      8543\n      -0.169121\n      1.346768\n      -10.211423\n      1.709859\n      1.655568\n      1.891809\n      7.856076\n      4.333816\n      4.0\n      30.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      2.151425\n      3.179085\n      2.337353\n      0.559543\n      -1.629433\n      2.493002\n      0\n      8383\n      -4.926378\n      -1.474424\n      2.848533\n      -10.733950\n      4.237231\n      5.048239\n      -5.263423\n      -7.005510\n      5.0\n      272.0\n    \n    \n      996\n      0.475735\n      4.378010\n      2.597665\n      1.569648\n      -1.131376\n      3.110801\n      0\n      6141\n      7.821350\n      -1.948967\n      6.039587\n      1.739432\n      2.351800\n      8.325224\n      -10.263796\n      -7.450850\n      1.0\n      616.0\n    \n    \n      997\n      4.621406\n      7.791088\n      8.592775\n      6.423716\n      0.915721\n      4.727899\n      3\n      406\n      -9.236352\n      2.760513\n      -7.290240\n      9.169618\n      -0.188279\n      -2.443252\n      -4.153840\n      6.140598\n      3.0\n      266.0\n    \n    \n      998\n      -1.720297\n      8.174031\n      9.746428\n      -1.849028\n      5.046019\n      0.951072\n      1\n      9737\n      -1.411479\n      0.830250\n      5.167232\n      -9.188249\n      3.176105\n      4.570698\n      -6.300042\n      -7.561958\n      5.0\n      263.0\n    \n    \n      999\n      2.804878\n      4.882689\n      -0.262516\n      0.311810\n      -1.867281\n      4.639076\n      0\n      3486\n      -6.696620\n      1.945182\n      -7.520248\n      8.545794\n      3.743541\n      -3.216961\n      -4.505348\n      3.400242\n      3.0\n      589.0\n    \n  \n\n1000 rows × 18 columns"
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#preliminary-plotting-do-the-factors-seperate-the-structure",
    "href": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#preliminary-plotting-do-the-factors-seperate-the-structure",
    "title": "Examined Algorithms",
    "section": "Preliminary Plotting: Do the Factors seperate the structure?",
    "text": "Preliminary Plotting: Do the Factors seperate the structure?\nA worthwhile plot to apply with any dimensional reduction technique (such as PCA or factor analysis) is to check if and how the data seperates when plotted on the reduced plane. In lieu of knowledge of the data we can always compare this representation to the output of a clustering algorithm.\n\ncust_desc = [x for x in df_purchases.columns if \n             'customer_desc' in x]\nX = df_customer[cust_desc]\nkmeans = KMeans(init='k-means++',\n                n_clusters=3, \n                n_init=30\n               )\nkmeans.fit(X)\nclusters = kmeans.predict(X)\nX['cluster'] = clusters\n\nsklearn_fa = FactorAnalysis(n_components=2, \n                            rotation='varimax'\n                           )\nY_fa = pd.DataFrame(sklearn_fa.fit_transform(X[cust_desc]))\nX = pd.concat([X, Y_fa], axis=1)\nX[[0, 1, 'cluster']].plot.scatter(x=0,\n                      y=1,\n                      c='cluster',\n                      colormap='viridis')\nplt.title(\"Factor Analysis Representation - Coloured by inferred Clusters\")\nplt.ylabel(\"Factor 1\")\nplt.xlabel(\"Factor 2\")\nplt.style.use('default')\nplt.show()\n\n/Users/nathanielforde/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  if __name__ == '__main__':\n\n\n\n\n\nWe can see here that the factors do a pretty good job of seperating the classes (0, 1), but mix up (2, 1). In addition we can see that there are 7 distinct clusters on the factor analysis representation which suggests that our choice three clustering classes is too low."
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#choosing-the-number-of-factors",
    "href": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#choosing-the-number-of-factors",
    "title": "Examined Algorithms",
    "section": "Choosing the number of Factors",
    "text": "Choosing the number of Factors\nOne suggestive way in which to determine the number of factors which we should extract is to build the skree plot of the eigenvalues and select the number of features where there is a higher relative eigenvalues.\n\nimport numpy as np\nfeature_names = ['customer_desc_' + str(x) for x in range(0, 8)]\n\nfa = FactorAnalyzer(n_factors=3)\nfa.fit(X[feature_names], 10)\nev, v = fa.get_eigenvalues()\nplt.plot(range(1,X[feature_names].shape[1]+1),ev)\nplt.show()\n\n\n\n\nOn the basis of this plot we should probably choose no more than two factors at most but we’ll continue with 3 for purposes of illustration. The factor loadings are linear functions of the observed features and so we may interpret the newly created factors by observing which of the observed features play a greater role in their composition.\n\nfa_loading_matrix = pd.DataFrame(fa.loadings_, \n                                 columns=['FA{}'.format(i) for \n                                          i in range(1, 3+1)], \n                              index=feature_names)\nfa_loading_matrix['Highest_loading'] = fa_loading_matrix.idxmax(axis=1)\nfa_loading_matrix = fa_loading_matrix.sort_values('Highest_loading')\nfa_loading_matrix\n\n\n\n\n\n  \n    \n      \n      FA1\n      FA2\n      FA3\n      Highest_loading\n    \n  \n  \n    \n      customer_desc_1\n      0.727724\n      0.024771\n      0.091443\n      FA1\n    \n    \n      customer_desc_3\n      0.826998\n      0.068252\n      0.014691\n      FA1\n    \n    \n      customer_desc_5\n      0.637687\n      -0.003234\n      0.489801\n      FA1\n    \n    \n      customer_desc_7\n      0.787048\n      0.081877\n      -0.414776\n      FA1\n    \n    \n      customer_desc_4\n      0.015911\n      1.001115\n      0.175406\n      FA2\n    \n    \n      customer_desc_6\n      -0.172587\n      0.077989\n      -0.684129\n      FA2\n    \n    \n      customer_desc_0\n      -0.229668\n      -0.628541\n      0.355323\n      FA3\n    \n    \n      customer_desc_2\n      -0.304156\n      0.217303\n      0.440891\n      FA3\n    \n  \n\n\n\n\nWe can see here that there is probably only one sensible factor to be derived from our dataset.\n\nimport seaborn as sns\n\nplt.figure(figsize=(25,5))\n\n# plot the heatmap for correlation matrix\nax = sns.heatmap(fa_loading_matrix.drop('Highest_loading', axis=1).T, \n                vmin=-1, vmax=1, center=0,\n                cmap=sns.diverging_palette(220, 20, n=200),\n                square=True, annot=True, fmt='.2f')\n\nax.set_yticklabels(\n    ax.get_yticklabels(),\n    rotation=0);\n\n\n\n\n\ncommunalities = pd.DataFrame(fa.get_communalities(), \n                             index=list(feature_names))\nfeatures_comm = list(communalities[communalities[0] > 0.33].index)\nprint('Total variables/features with communalities >0.33: {}'.format(len(features_comm)))\ncommunalities\n\nTotal variables/features with communalities >0.33: 8\n\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      customer_desc_0\n      0.574065\n    \n    \n      customer_desc_1\n      0.538558\n    \n    \n      customer_desc_2\n      0.334116\n    \n    \n      customer_desc_3\n      0.688800\n    \n    \n      customer_desc_4\n      1.033252\n    \n    \n      customer_desc_5\n      0.646560\n    \n    \n      customer_desc_6\n      0.503901\n    \n    \n      customer_desc_7\n      0.798188"
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#testing-the-validity-of-a-hypothetical-factor",
    "href": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#testing-the-validity-of-a-hypothetical-factor",
    "title": "Examined Algorithms",
    "section": "Testing the Validity of a Hypothetical Factor",
    "text": "Testing the Validity of a Hypothetical Factor\nCronbach’s Alpha is a statistical measure of the reliability of the factor analysis derived from a test of the covariances matrix of features which make up the proposed latent factor. A cronbach alpha closer to 1 is desired.\n\\(\\alpha = \\frac{K}{K-1}\\left(1-\\frac{\\sum \\sigma^2_{x_i}}{\\sigma^2_T}\\right)\\)\nwhere\n\\(\\sigma^2_T = \\sum \\sigma^2_{x_i} + 2 \\sum_{i < j}^K {\\rm cov}(x_i,x_j)\\)\na combination of the observational measure of variance and inter-metric covariances for each observational variable. This ties this measure to the Factor analysis model since the covariances can be re-expressed in terms of the factor loadings.\n\\(\\sigma^2_T = \\sum \\sigma^2_{x_i} + 2 \\sum_{i < j}^K (l_{i, 1} + \\epsilon_{i})(l_{j, 1} + \\epsilon_{j})\\)\nwhich means that if the factors loadings are fairly high relative the the random components of the variance then we’ll get a ratio that come close to one. Conversely low loadings will ensure that the denominator drags the ratio down.\n\nimport numpy as np\n\ndef CronbachAlpha(observed_measures):\n    observed_measures = np.asarray(observed_measures)\n    sample_vars = observed_measures.var(axis=1, ddof=1)\n    total_scores = observed_measures.sum(axis=0)\n    nitems = len(observed_measures)\n\n    return nitems / (nitems-1.) * (1 - sample_vars.sum() / total_scores.var(ddof=1))\n\n\n#Collate the observed features\n\nfactor1 = [X['customer_desc_1'], X['customer_desc_3'], \n             X['customer_desc_5'], X['customer_desc_7']]\n\nfactor2 = [X['customer_desc_4'], X['customer_desc_0']]\n\nfactor3 = [X['customer_desc_5'], X['customer_desc_6'], \n             X['customer_desc_2']]\n#Get cronbach alpha\nfactor1_alpha = CronbachAlpha(factor1)\n#factor2_alpha = CronbachAlpha(factor2)\n#factor3_alpha = CronbachAlpha(factor3)\nprint(factor1_alpha, \n      factor2_alpha, \n      factor3_alpha\n     )\n\n0.7889764629492505 -3.382959230225132 -0.8158399732248119\n\n\nWhich shows as expected that only one of the proposed factors is sensible."
  },
  {
    "objectID": "posts/post-with-code/occams_razor/wheat_from_chaff.html",
    "href": "posts/post-with-code/occams_razor/wheat_from_chaff.html",
    "title": "Wheat from Chaff: Maximum Entropy and Information",
    "section": "",
    "text": "There is a rumour of ignorant fish; so involved in the ocean they fail to notice the water around them, what it is and what it could be. There is another cliche about the internet and its denizens drowning in information. There is something to both these images, but the second is too charitable. It suggests a light consistency to the viscous texture of the flow, instead of the congealed sludge that drips down our throat choking off the air. The oozing density of the swamp presents a range of difficulties for even reasonably self aware fish. Each has to struggle to intepret the evolving environment.\nThe problem, familiar now, is one of information retrieval, filtering and ranking. For much of these tasks we delegate and trust others to siphon the sludge and recommend items of value, but we each face unique challenges. Sinkholes tailored to our interests obscure facts in favour of maximising our propensity to purchase. How then do we clamber out of this rats nest?"
  },
  {
    "objectID": "posts/post-with-code/occams_razor/wheat_from_chaff.html#simple-models-are-false.-complex-ones-too.",
    "href": "posts/post-with-code/occams_razor/wheat_from_chaff.html#simple-models-are-false.-complex-ones-too.",
    "title": "Wheat from Chaff: Maximum Entropy and Information",
    "section": "Simple Models are False. Complex ones too.",
    "text": "Simple Models are False. Complex ones too.\nWe each start from different bases and face different difficulties acquiring new knowledge. One way to conceive of the range of barriers is to think of reasoning from different probabilistic priors. This language is ubiquitous today, but stems from a historic dispute in probability theory. Traditional expositions of probability focused on a host of well understood parametric models abstracted from observations of frequency. Watch enough card games and you come to appreciate the regularity of the hands, their odds and value. The thought is that some underlying process drives what we see and this process is well approximated by our expectations. Or if we can’t yet tease out the pattern, further observations will help us converge on the reality. This view of learning is apt (indeed, reliable) in certain special cases where the observable phenomena adheres to consistent patterns over frequent observation. But probability theory, properly construed, is a much more general study.\nSo too is learning properly done. There is no uniform or inevitable convergence of opinion to fact. Instead we progress in fits and starts, become diverted, digress and procrastinate. We may be held back or belabor certain points obvious to others. Only in rare cases is knowledge amenable to easy summary rules. These rules, cited often to the point of cliche, are celebrated for their simplicity and generality, but the tendency is misleading. Like the host of simple parametric probability models these rules have wide applicability but they are the exception. Most knowledge is messier, complex and cobbled together from scraps of metaphor and analogy. The cumulative accretion of these clues allow us to grapple the world with specific hooks. Simple rules, too simple, are mostly just ignorance uplifted.\nPeter Godfrey Smith explains how both the drive to simplicity and need for complexity oscillates in explanations of evolutionary mechanisms.\n\n“Standard recipes for change by natural selection are products of trade-offs, often unacknowledged, between the desire to capture all genuine cases of natural selection in a summary description and the desire for describe a simple and causally transparent machine. The two motives corresspond to two different kinds of understanding that we can seek in any theoretical investigation…Understanding is achieved via similarity relations between the simple cases we have picked apart in detail and the cloud of more complicated ones.” - pg4 Darwinian Populations and Natural Selection by Peter Godfey-Smith\n\nA similar tension is nicely brought out in the “dispute” between objective Bayesians and their subjecitivist cousins. Both propose approaches to learning as methods for updating our view of the relevant probabilities. Starting with the following rule:\n\\[ \\underbrace{p(T | D)}_{posterior} = \\frac{\\overbrace{p(T)}^{prior}\\overbrace{p(D | T)}^{likelihood}}{\\sum_{i}^{N} p(D | T_{i})} \\]\nWe update our theory \\(T\\) based on data \\(D\\) by computing the posterior in light of our prior specification for the probability of \\(T\\). The aforementioned dispute revolves around how we ought to specify \\(p(T)\\). How much structure is implicit in the specification. If our problem is allocating attention, our theory comes to something like which voice should I listen to, given their record against the data, broad reputation and reliability. But how much detail is too much? We’d become lost with exhaustive precision, but it’s unclear how simple is simple enough."
  },
  {
    "objectID": "posts/post-with-code/occams_razor/wheat_from_chaff.html#maximise-entropy-choice-under-minimal-information",
    "href": "posts/post-with-code/occams_razor/wheat_from_chaff.html#maximise-entropy-choice-under-minimal-information",
    "title": "Wheat from Chaff: Maximum Entropy and Information",
    "section": "Maximise Entropy: Choice under minimal information",
    "text": "Maximise Entropy: Choice under minimal information\nHow then do we choose what to learn, where to spend our time or resources? One school of thought emphasises an agnostic attitude. Treat all options equally unless you know better. In the limiting case we array all possible choices before you and if they’re equally appealing, you’re indifferent and ought to pick one indiscriminately. This ties very neatly to a characterisation of how you should frame your beliefs under ignorance and seek their improvement. Simultaneously the method gives rise to a natural measure of information content as deviations from a state of no information.\nSpread out \\(m\\) options very thinly and we weigh their probability with a multinomial distribution where the probabilities \\(p\\_{1} =... = p\\_{m}\\) are flat.\n\\[ \\frac{N!}{n_{1}!n\\_{2}!... n_{m}!}p_{1}^{n_{1}}p_{2}^{n_{2}}... p_{m}^{n_{m}} \\text{ where } \\sum_{1}^{m} n = N \\]\nwhich is maximised when \\(W = \\frac{N!}{n\\_{1}!n\\_{2}!... n\\_{m}!}\\) or when a monotonic transformation reaches its peak. Choosing the following form we get:\n\\[ \\frac{1}{N}log W = \\frac{1}{N}log\\frac{N!}{n_{1}!n\\_{2}!... n_{m}!}\\] \\[ \\sim \\frac{1}{N}log\\frac{N!}{Np_{1}!Np_{2}!... Np_{m}!}  \\text{ by assumption} \\] \\[ =  \\frac{1}{N}\\Big(log(N!) - \\sum_{1}^{m}log(Np_{i}!) \\Big) \\]\nThen taking the limit as \\(N\\) goes to infinity, we get by Stirling’s approximation:\n\\[ \\lim_{0 \\rightarrow \\infty}  \\frac{1}{N}log W = \\frac{1}{N} \\Big(Nlog(N) - \\sum\\_{1}^{m}Np_{i}log(Np_{i}) \\Big) \\]\n\\[ = \\Big(log(N) - \\sum_{1}^{m}p_{i}log(Np_{i}) \\Big) \\] \\[ = \\Big(log(N) - \\sum_{1}^{m}p_{i}(log(N) + log(p_{i})) \\Big) \\] \\[ = log(N) - (\\sum_{1}^{m}p_{i}log(N) + \\sum_{1}^{m}p_{i}log(p_{i})) \\] \\[ = \\Big(1 - \\sum_{1}^{m}p_{i}\\Big)log(N) - \\sum_{1}^{m}p_{i}log(p_{i}) \\] \\[ = - \\sum_{1}^{m}p_{i}log(p_{i}) \\]\nThis last quantity is the entropy of the distribution also known as the expected value of the information content of the random variable \\((X=x)\\) with respect to a probabiliy distribution \\(p\\). Since we have:\n\\[ -log(p_{i})  =_{def} I(x_{i})_{p} \\]\nensuring that the information in perfectly probable events is measured as -log(1) = -0 and impossible events convey the maximum amount of information. Everything in between finds its place on the spectrum of more or less surprising. Entropy, then, is a measure of information content over a random variable.\n\nIn making inferences on the basis of partial information we must use that probability distribution which has maximum entropy subject to what is known. This is the only unbiased assignment we can make; to use any other would amount to arbitrary assumption of information which by hypothesis we do not have…The maximum entropy distribution may be asserted for the positive reason that is uniquely determined as the one that is maximally non-commital with regard to the missing information. - pg 623, Information Theory and Statistical Mechanics, Jaynes (1957)\n\nClaude’s Shannon’s work on information theory and and E.T. Jaynes synthesis of it with probability theory shows that information is not incidental to probability, but arises as naturally as other measures: mean, median and variance of probability distributions.\n\ndef entropy(x):\n    y = []\n    for i in x:\n        if i == 0:\n            y.append(0)\n        else: \n            y.append(i*np.log(i))\n    h = -sum(y)\n    return h\n\nN = 100\nd = {'A':np.concatenate([np.random.randint(0, N, 50),np.random.randint(0, 4, 50)]),\n     'B':np.concatenate([np.random.power(3, 50),np.random.poisson(30, 50)]), \n     'C':np.random.normal(100, 5, N), \n     'D':np.random.poisson(N, N), \n     'E':np.random.uniform(0, N, N), \n     'F':np.random.poisson(100, N)}\np = pd.DataFrame(data=d)\np_norm = p/p.sum(0)\nH = p_norm.apply(entropy, axis=0)\np.columns = ['Entropy:' + str(np.round(h, 3)) for h in H]\np.hist(figsize=(10, 10))\nplt.suptitle(\"Entropy: Histograms across various Distributions\")\n\n\n\nVarious Measures of Entropy\n\n\nFrom the Bayesian perspective on learning, this characterisation suggests an approach to selecting appropriately informative priors. When we’re unsure or ignorant then we can choose prior distributions that maximise the entropy and thereby reflect our ignorance, since our credences should not exceed our evidence. This is not the same as treating all options equally. Maximal Entropy distributions are a much wider class of distribution e.g. if you’re confident of the mean and variance of a distribution but nothing else, then the Gaussian distribution is the maximum entropy distribution characterising your knowledge. The maximum entropy rule asks us to choose our priors based on the minimal information which describes the situtation. It can be justified as an abundance of caution where we commit to the least risky description of the data. But the paradigm goes wrong when choice requires discernment over informative options and outright misinformaiton. Against a flood of misinformation, of which we have little or no information to identify the bad actors the agnostic choice seems uncomfortably generous."
  },
  {
    "objectID": "posts/post-with-code/occams_razor/wheat_from_chaff.html#informative-or-weakly-informative-priors",
    "href": "posts/post-with-code/occams_razor/wheat_from_chaff.html#informative-or-weakly-informative-priors",
    "title": "Wheat from Chaff: Maximum Entropy and Information",
    "section": "Informative or Weakly Informative Priors",
    "text": "Informative or Weakly Informative Priors\nOne famous characterisation of bullshit due to G.A. Cohen is describes it as “unclarifiable unclarity”. Bad actors online flood the space with this kind of unverifiable garbage. That undoubtedly contributes to the noise, but the more fundamental issue is just the volume of actual information we need to filter. With this in mind the agnostic approach is too risk-averse, creating needless work.\nN = 10\ntrue_a, true_b, predictor = 0.5, 3.0, np.random.normal(loc=2, scale=6, size=N)\ntrue_mu = true_a + true_b * predictor\ntrue_sd = 2.0\n\noutcome = np.random.normal(loc=true_mu, scale=true_sd, size=N)\n\npredictor_scaled = standardize(predictor)\noutcome_scaled = standardize(outcome)\nprior_spec = {'a':[0, 10], 'b'[0, 10]}\n\nwith pm.Model() as model_1:\n    # flat priors on constant and beta coeffs\n    a = pm.Normal(\"a\", prior_spec['a'][0], prior_spec['a'][1])\n    b = pm.Normal(\"b\", prior_spec['b'][0], prior_spec['a'][1])\n\n    mu = a + b * predictor_scaled\n    # priors sd\n    sd = pm.Exponential(\"sd\", 1.0)\n\n    # likelihood \n    obs = pm.Normal(\"obs\", mu=mu, sigma=sd, observed=outcome_scaled)\n\n    # diagnostic and posterior data to sample from\n    prior_checks = pm.sample_prior_predictive(samples=50, random_seed=RANDOM_SEED)\n    trace_1 = pm.sample(1000, tune=2000, random_seed=RANDOM_SEED)\n    ppc = pm.sample_posterior_predictive(trace_1, var_names=[\"a\", \"b\", \"obs\"], \n        random_seed=RANDOM_SEED)\nWith the range of flat priors the possible space of linear models is wide and implausible. We build 50 linear models, one from each of the sampled values across the prior_checks and plot these against the real values.\n\n\n\nFlat Priors Sample Range of Possible Models\n\n\nBy constraining our priors for the parameters of the linear model we significantly reduce the possible space of models. The choice here is a little artificial, but in a real problem the informative choices can be made based on knowledge of the problem at hand.\nprior_spec = {'a':[0, 0.5], 'b'[0, 3]}\n\n\n\nWeakly Informative Priors Sample Range of Possible Models\n\n\nThe parameter estimates drawn from the posterior distributions show that the data is sufficient to swamp the priors and give good estimates of the true coefficients, even with flat priors.\n\nThis suggests that the process of Bayesian updating makes a good compromise between our priors and the likelihood. The principle of maximum entropy can be extended to the process of Bayesian updating where we view the problem as one of minimising the divergence between our prior and posterior. This measure of relative entropy is also known as KL Divergence.\n\\[ D\\_{KL}(p, q) = \\sum\\_{i}p\\_{i}log(\\dfrac{p\\_{i}}{q\\_{i}}) \\]\nThe better specified our prior the less we have to learn from the data, the simpler our theory. This optimisation problem is a way of automating the principle of Occam’s razor. Bayesian updating is a specific case of minimising the relative entropy between the prior and the posterior subject to the constraints imposed by the observed data."
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html",
    "href": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "import random\nimport statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nfrom scipy.optimize import minimize_scalar, minimize\nfrom IPython.display import Latex\nfrom stargazer.stargazer import Stargazer\nfrom IPython.core.display import HTML\nThis notebook is a python port of some of the code in “Learning Microeconometrics with R” by Christopher P Adams. It corresponds to the blog post: https://nathanielf.github.io//post/mle_utility_and_choice/"
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#the-problem-modelling-discrete-choice-by-latent-utility-metrics",
    "href": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#the-problem-modelling-discrete-choice-by-latent-utility-metrics",
    "title": "Examined Algorithms",
    "section": "The Problem: Modelling Discrete Choice by Latent Utility Metrics",
    "text": "The Problem: Modelling Discrete Choice by Latent Utility Metrics\nSome assumptions about the form of the utility distribution are crucial as our modeling efforts will go wrong if we know nothing about the latent utilities. We assume that the latent utility can be expressed as by the revealed preferences i.e. as the share or proportion of choices made by the customers.\nThe utility is some function of product and consumer’s properties, perhaps mostly driven by price\n\\[ utility = \\mathbf{X'}\\beta + e\\]\nand market share is an expression of that utility \\[ demand_A = utility_{A} > 0 \\]\nIn a choice context we’re trying to determine if the implicit utility measure is sufficient to drive a purchase, and as such OLS models are inappropriate\n\nN = 1000\na = 2\nb = -3\ne = np.random.normal(0, 1, N)\nconsumer_desc = np.random.uniform(3, 1, N)\nconsumer_desc1 = np.random.uniform(2, 5, N)\nutility = 2 + 3*consumer_desc + -4*consumer_desc1 + e\n## Predicting choice over two options\ndemand_A = utility > 0\nX = pd.DataFrame({'product_desc': consumer_desc, 'product_desc1': consumer_desc1})\nX = sm.add_constant(X)\nlm1 = sm.OLS(demand_A,X)\nlm1_results = lm1.fit()\nprint(lm1_results.summary())\nprint(round(lm1_results.params, 5))\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.200\nModel:                            OLS   Adj. R-squared:                  0.198\nMethod:                 Least Squares   F-statistic:                     124.5\nDate:                Sat, 20 Feb 2021   Prob (F-statistic):           5.42e-49\nTime:                        16:01:07   Log-Likelihood:                 58.448\nNo. Observations:                1000   AIC:                            -110.9\nDf Residuals:                     997   BIC:                            -96.17\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconst             0.2095      0.040      5.291      0.000       0.132       0.287\nproduct_desc      0.1120      0.013      8.939      0.000       0.087       0.137\nproduct_desc1    -0.1045      0.008    -12.630      0.000      -0.121      -0.088\n==============================================================================\nOmnibus:                      472.919   Durbin-Watson:                   1.983\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1999.877\nSkew:                           2.298   Prob(JB):                         0.00\nKurtosis:                       8.183   Cond. No.                         23.8\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nconst            0.20953\nproduct_desc     0.11204\nproduct_desc1   -0.10449\ndtype: float64\n\n\nThis stems from the fact that we’re’trying to estimate a conditional probability over a binary choice not a continuous measure. The revealed preference assumption says that we can predict the purchase if the utility of good is positive.\n\\[Pr(demand_A = 1) = utility > 0 \\] \\[= Pr(\\mathbf{X'}\\beta + e > 0) \\] \\[ = Pr(e > - \\mathbf{X'}\\beta ) \\] \\[ = 1 - F(\\mathbf{X'}\\beta ) \\]\nwhere \\(F\\) is the distribution of the unobserved random variable \\(e\\). The challenge is using the correct distribution as this feeds the method of statistical estimation of the parameters \\(\\beta\\)"
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#maximum-likelihood-fits-over-candidate-distributions",
    "href": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#maximum-likelihood-fits-over-candidate-distributions",
    "title": "Examined Algorithms",
    "section": "Maximum Likelihood Fits over Candidate Distributions",
    "text": "Maximum Likelihood Fits over Candidate Distributions\nThere are a number of candidate distributions which might serve to replace \\(F\\) and estimate the share of purchases\n\ndef log_binomial_dist(params, *args):\n    p = params[0]\n    p_hat = args[0]\n    N = args[1]\n    return -((p_hat*N)*np.log(p) + (1-p_hat)*N*np.log(1-(p)))\n\nres = minimize(log_binomial_dist, x0 = [.1], args =(.34, 100), bounds = ((0, .99),))\nprint(res)\n    \n\n\n      fun: 64.10354778811556\n hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n      jac: array([0.])\n  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n     nfev: 16\n      nit: 6\n     njev: 8\n   status: 0\n  success: True\n        x: array([0.33999999])\n\n\n\ndef ll_ols(params, *args):\n    X, y = args[0], args[1]\n    beta = [params[0], params[1], params[2]]\n    mu, sd, = params[3], params[4]\n    z = (y - X.dot(beta)) / sd\n    log_lik = -sum(np.log(stats.norm.pdf(z)) - np.log(sd))\n    return log_lik\n\nx = np.random.normal(5, 2, 1000)\nx1 = np.random.normal(6, 1, 1000)\nx2 = np.random.uniform(2, 7, 1000)\ny = 1 + .3*x + 5*x1 + np.random.normal(0, 1, 1000)\n\nX1 = pd.DataFrame({'consumer_desc': x, 'consumer_desc1': x1})\nX1 = sm.add_constant(X1)\n\nres = minimize(ll_ols, x0 =[2, 1, 4, 2, 1], method = 'Nelder-Mead', args =(X1, y))\nprint(res)\n\n final_simplex: (array([[1.27170093, 0.28846111, 4.97078397, 2.33608166, 0.98080792],\n       [1.27163611, 0.28845751, 4.97079439, 2.33611095, 0.98081281],\n       [1.27179597, 0.28845924, 4.97076792, 2.33606271, 0.98080085],\n       [1.27178217, 0.28845315, 4.97077749, 2.33606068, 0.98081083],\n       [1.2716755 , 0.28844586, 4.97079715, 2.33609652, 0.98080788],\n       [1.27174903, 0.28845715, 4.97077654, 2.33606353, 0.98081551]]), array([1399.55005084, 1399.55005089, 1399.55005094, 1399.55005095,\n       1399.55005095, 1399.55005106]))\n           fun: 1399.5500508414766\n       message: 'Optimization terminated successfully.'\n          nfev: 384\n           nit: 238\n        status: 0\n       success: True\n             x: array([1.27170093, 0.28846111, 4.97078397, 2.33608166, 0.98080792])\n\n\n\ndef log_probit_dist(params, *args):\n    X, y = args[0], args[1]\n    beta = [params[0], params[1], params[2]]\n    mu, sd, = params[3], params[4]\n    Xb = X.dot(beta)\n    q = 2*y-1\n    log_lik = np.log(stats.norm.cdf(q*Xb))\n    return -sum(log_lik)\n\n### Optimise the probit model for determining the parameters required toe estimate the underlying utility\n### True values of the parameters 2, 3, -4\nres = minimize(log_probit_dist, x0 =[0, 0 ,0 , 0, 1], args =(X, demand_A), options={'disp': True})\nprint(res)\n\nOptimization terminated successfully.\n         Current function value: 94.044202\n         Iterations: 17\n         Function evaluations: 108\n         Gradient evaluations: 18\n      fun: 94.04420183563171\n hess_inv: array([[ 0.72799404, -0.03552851, -0.26060106,  0.        ,  0.        ],\n       [-0.03552851,  0.09513159, -0.08023828,  0.        ,  0.        ],\n       [-0.26060106, -0.08023828,  0.18756252,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  1.        ]])\n      jac: array([ 3.81469727e-06, -9.53674316e-06,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00])\n  message: 'Optimization terminated successfully.'\n     nfev: 108\n      nit: 17\n     njev: 18\n   status: 0\n  success: True\n        x: array([ 2.14197608,  2.42768647, -3.51990129,  0.        ,  1.        ])\n\n\nThese estimates are still incorrect but an awful lot closer than the fits achieved by the ols model in the first section. We can validate the above optimisation against the inbuilt model of statsmodels\n\nprobit_mod = sm.Probit(demand_A, X)\nprobit_res = probit_mod.fit()\nprobit_margeff = probit_res.get_margeff()\nprint('Parameters: ', probit_res.params)\nprint('Marginal effects: ')\nprint(probit_margeff.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.094044\n         Iterations 10\nParameters:  const            2.141972\nproduct_desc     2.427687\nproduct_desc1   -3.519900\ndtype: float64\nMarginal effects: \n       Probit Marginal Effects       \n=====================================\nDep. Variable:                      y\nMethod:                          dydx\nAt:                           overall\n=================================================================================\n                   dy/dx    std err          z      P>|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nproduct_desc      0.1285      0.011     12.137      0.000       0.108       0.149\nproduct_desc1    -0.1863      0.015    -12.674      0.000      -0.215      -0.157\n================================================================================="
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#mcfaddans-bart-discrete-choice-model",
    "href": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#mcfaddans-bart-discrete-choice-model",
    "title": "Examined Algorithms",
    "section": "McFaddan’s BART Discrete Choice Model",
    "text": "McFaddan’s BART Discrete Choice Model\nThe idea is a generalisation of the above to estimate the difference in utilities across multiple products.\n\\[ U_{i,j} = \\mathbf{X'}_{i, j}\\beta + v_{i,j} \\]\nwhere the each individual’s \\(i\\) utility for a given good \\(j\\) is expressed as a linear weighted function of the product characteristics \\(\\mathbf{X}\\). Since we need to predict demand based on utility we’re really interested in estimating the differnce in utility\n\\[U_{i,A} > U_{i, B} \\]\njust when\n\\[ \\mathbf{X'}_{i, A}\\beta + v_{i,A} > \\mathbf{X'}_{i, B}\\beta + v_{i,B}\\]\nor\n\\[  v_{i,A} -  v_{i,B} > - (\\mathbf{X'}_{i, A}  - \\mathbf{X'}_{i, B})\\beta \\]\nbut then the probability of demand is just\n\\[Pr(demand_A = 1 | \\mathbf{X'}_{i, A}, \\mathbf{X'}_{i, B}) = Pr\\Bigg( v_{i,A} -  v_{i,B} > - (\\mathbf{X'}_{i, A}  - \\mathbf{X'}_{i, B})\\beta \\Bigg) \\] \\[ = Pr\\Bigg( -v_{i,A} -  v_{i,B} < (\\mathbf{X'}_{i, A}  - \\mathbf{X'}_{i, B})\\beta \\Bigg) \\] \\[ = F\\Bigg( (\\mathbf{X'}_{i, A}  - \\mathbf{X'}_{i, B})\\beta \\Bigg) \\]"
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#an-example-in-two-products-two-models",
    "href": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#an-example-in-two-products-two-models",
    "title": "Examined Algorithms",
    "section": "An Example in Two Products & Two Models",
    "text": "An Example in Two Products & Two Models\n\nN = 100\nX_A = sm.add_constant(np.random.rand(N,2))\nX_B = sm.add_constant(np.random.rand(N, 2))\nbeta = np.array([1, -2, 3])\n\n#probit we only need one normal error term since sums of normals are normal\nv = np.random.normal(0, 1, N)\ny = (X_A.dot(beta) - X_B.dot(beta)) + v > 0\nX_diff = X_A - X_B\nX_diff[:, 0] =  1\nX_diff = pd.DataFrame(X_diff, columns=['const', 'product_desc', 'product_desc1'])\n\nAgain we can try two classification algorithms which attempt to characterise the error terms \\(v_{1}, v_{2}\\) that the McFaddan model assumes\n\nprobit_mod = sm.Probit(y, X_diff)\nprobit_res = probit_mod.fit()\nprobit_margeff = probit_res.get_margeff()\n\nOptimization terminated successfully.\n         Current function value: 0.426413\n         Iterations 6\n\n\n\nv1 = np.random.weibull(1, N)\nv2 = np.random.weibull(1, N)\ny = (X_A.dot(beta) - X_B.dot(beta)) + (v1 - v2) > 0\nlogit_mod = sm.Logit(y, X_diff)\nlogit_res = logit_mod.fit()\nlogit_margeff = logit_res.get_margeff()\n\nOptimization terminated successfully.\n         Current function value: 0.499093\n         Iterations 6\n\n\n\nstargazer = Stargazer([probit_res, logit_res])\nstargazer.custom_columns(['Probit Model', 'Logit Model'], [1, 1])\n#stargazer.add_custom_notes([str(probit_margeff.summary()), str(logit_margeff.summary())])\nHTML(stargazer.render_html())\n\n\nDependent variable:yProbit ModelLogit Model(1)(2)const-0.255-0.049(0.156)(0.246)product_desc-1.491***-2.157***(0.445)(0.690)product_desc12.964***3.833***(0.578)(0.888)Observations100100R2Adjusted R2Residual Std. Error1.000 (df=97)1.000 (df=97)F Statistic (df=2; 97) (df=2; 97)Note:\n \n  *p<0.1;\n  **p<0.05;\n  ***p<0.01"
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#generalising-to-multiple-choices-the-multinomial-distribution",
    "href": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#generalising-to-multiple-choices-the-multinomial-distribution",
    "title": "Examined Algorithms",
    "section": "Generalising to Multiple choices: The Multinomial Distribution",
    "text": "Generalising to Multiple choices: The Multinomial Distribution\n\nnp.random.seed(100)\nN = 1000\nmu = [0,0]\nrho = 0.1\ncov = [[1, rho], [rho, 1]]\n\n# u is N*2\nu = np.random.multivariate_normal(mu, cov, 1000)\nx1 = np.random.uniform(0, 1, size=(N,2)) #np.random.rand(N,2)\nx2 = np.random.uniform(0, 1, size=(N,2)) #np.random.rand(N,2)\n\nU = -1 + -3*x1 + 4*x2 + u\n\ny = np.zeros(shape=(N, 2))\ny[:,0] = ((U[:,0] > 0) & (U[:,0] > U[:,1]))\ny[:,1] = (U[:,1] > 0 & (U[:,1] > U[:,0]))\n\n\nW1 = pd.DataFrame({'x1':x1[:,0], 'x2':x2[:,0]})\nW2 = pd.DataFrame({'x1':x1[:,1], 'x2':x2[:,1]})\n\n\nOptimization terminated successfully.\n         Current function value: 542.609688\n         Iterations: 185\n         Function evaluations: 338\n\n\n final_simplex: (array([[-1.74855767, -5.18018459,  7.07552039],\n       [-1.74856002, -5.18016799,  7.07549515],\n       [-1.74856866, -5.18022325,  7.07555337],\n       [-1.74862589, -5.18016727,  7.07561422]]), array([542.60968847, 542.60968847, 542.60968848, 542.60968848]))\n           fun: 542.6096884689198\n       message: 'Optimization terminated successfully.'\n          nfev: 338\n           nit: 185\n        status: 0\n       success: True\n             x: array([-1.74855767, -5.18018459,  7.07552039])\n\n\n\ny_full = np.ones(shape=(N*2,1))\nclass_1 = np.where(((U[:,0] > 0) & (U[:,0] > U[:,1])), 'class_1', 'class_0')\nclass_2 = np.where((U[:,1] > 0 & (U[:,1] > U[:,0])), 'class_2', 'class_0')\ny_full = np.append(class_1, class_2)\nW_full = sm.add_constant(W1.append(W2)).reset_index(drop=True)\nmn_logit = sm.MNLogit(y_full, W_full)\nmn_logit_res = mn_logit.fit()\nmn_logit_res.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.630391\n         Iterations 7\n\n\n\n\nMNLogit Regression Results\n\n  Dep. Variable:           y          No. Observations:       2000  \n\n\n  Model:                MNLogit       Df Residuals:           1994  \n\n\n  Method:                 MLE         Df Model:                  4  \n\n\n  Date:            Thu, 25 Feb 2021   Pseudo R-squ.:        0.2924  \n\n\n  Time:                21:23:52       Log-Likelihood:       -1260.8 \n\n\n  converged:             True         LL-Null:              -1781.9 \n\n\n  Covariance Type:     nonrobust      LLR p-value:        2.587e-224\n\n\n\n\n  y=class_1    coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  const        -2.6666     0.219   -12.201  0.000    -3.095    -2.238\n\n\n  x1           -4.7724     0.315   -15.143  0.000    -5.390    -4.155\n\n\n  x2            6.2800     0.360    17.461  0.000     5.575     6.985\n\n\n  y=class_2    coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  const        -2.7293     0.212   -12.865  0.000    -3.145    -2.313\n\n\n  x1           -4.4876     0.299   -15.004  0.000    -5.074    -3.901\n\n\n  x2            6.4438     0.348    18.497  0.000     5.761     7.127\n\n\n\n\n\nfrom scipy.special import softmax\n\ndef cdf(W, beta):\n    Wb = np.dot(W, beta)\n    eXB = np.exp(Wb)\n    eXB = eXB /eXB.sum(1)[:, None]\n    return eXB\n\ndef take_log(probs):\n    epsilon = 1e-20 \n    return np.log(probs)\n\ndef calc_ll(logged, d):\n    ll = d * logged\n    return ll\n\ndef ll_mn_logistic(params, *args):\n    y, W, n_params, n_classes = args[0], args[1], args[2], args[3]\n    beta = [params[i] for i in range(0, len(params))]\n    beta = np.array(beta).reshape(n_params, -1, order='F')\n    beta[:,0] = [0 for i in range(0, n_params)]\n    \n    ## onehot_encode\n    d = pd.get_dummies(y, prefix='Flag').to_numpy()\n    \n    probs = cdf(W, beta)\n    logged = take_log(probs)\n    ll = calc_ll(logged, d)\n    \n    return -np.sum(ll)\n\nn_params = 3 \nn_classes = 3\nz = np.random.rand(3,3).flatten()\n#probs = ll_mn_logistic(list(z), *[y_full, W_full, n_params, n_classes])\n\n\nres = minimize(ll_mn_logistic, x0 =z, args =(y_full, W_full, n_params, n_classes), \n             options={'disp': True, 'maxiter':1000})\nres\n\nWarning: Desired error not necessarily achieved due to precision loss.\n         Current function value: 1260.782799\n         Iterations: 21\n         Function evaluations: 470\n         Gradient evaluations: 47\n\n\n      fun: 1260.782798764732\n hess_inv: array([[ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.15840199,  0.05230628,\n        -0.24846581,  0.08569624, -0.08026449, -0.11217585],\n       [ 0.        ,  0.        ,  0.        ,  0.05230628,  0.05960721,\n        -0.11533024,  0.03950015, -0.02890953, -0.06618335],\n       [ 0.        ,  0.        ,  0.        , -0.24846581, -0.11533024,\n         0.42805566, -0.14239161,  0.11728476,  0.20865474],\n       [ 0.        ,  0.        ,  0.        ,  0.08569624,  0.03950015,\n        -0.14239161,  0.0776523 , -0.03556134, -0.10565188],\n       [ 0.        ,  0.        ,  0.        , -0.08026449, -0.02890953,\n         0.11728476, -0.03556134,  0.07906259,  0.01257425],\n       [ 0.        ,  0.        ,  0.        , -0.11217585, -0.06618335,\n         0.20865474, -0.10565188,  0.01257425,  0.18307424]])\n      jac: array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.05175781e-05,\n       1.52587891e-05, 0.00000000e+00, 3.05175781e-05, 3.05175781e-05,\n       3.05175781e-05])\n  message: 'Desired error not necessarily achieved due to precision loss.'\n     nfev: 470\n      nit: 21\n     njev: 47\n   status: 2\n  success: False\n        x: array([ 0.09640129,  0.00798646,  0.70648002, -2.66658562, -4.77244682,\n        6.27997417, -2.72927898, -4.48757328,  6.44379724])\n\n\n\nbart_data = pd.read_csv('../data_files/mcfaddan_bart.csv')\nbart_data.head()\n\n\n\n\n\n  \n    \n      \n      HOUSEID\n      TRAVDAY\n      SAMPSTRAT\n      HOMEOWN\n      HHSIZE\n      HHVEHCNT\n      HHFAMINC\n      PC\n      SPHONE\n      TAB\n      ...\n      SMPLSRCE\n      WTHHFIN\n      HBHUR\n      HTHTNRNT\n      HTPPOPDN\n      HTRESDN\n      HTEEMPDN\n      HBHTNRNT\n      HBPPOPDN\n      HBRESDN\n    \n  \n  \n    \n      0\n      30000007\n      2\n      3\n      1\n      3\n      5\n      7\n      2\n      1\n      2\n      ...\n      2\n      187.314320\n      T\n      50\n      1500\n      750\n      750\n      20\n      750\n      300\n    \n    \n      1\n      30000008\n      5\n      2\n      1\n      2\n      4\n      8\n      1\n      1\n      2\n      ...\n      2\n      69.513032\n      R\n      5\n      300\n      300\n      150\n      5\n      300\n      300\n    \n    \n      2\n      30000012\n      5\n      3\n      1\n      1\n      2\n      10\n      1\n      1\n      3\n      ...\n      2\n      79.419586\n      C\n      80\n      17000\n      17000\n      5000\n      60\n      17000\n      7000\n    \n    \n      3\n      30000019\n      5\n      3\n      1\n      2\n      2\n      3\n      1\n      5\n      5\n      ...\n      2\n      279.143588\n      S\n      40\n      300\n      300\n      150\n      50\n      750\n      300\n    \n    \n      4\n      30000029\n      3\n      3\n      1\n      2\n      2\n      5\n      2\n      5\n      1\n      ...\n      2\n      103.240304\n      S\n      40\n      1500\n      750\n      750\n      40\n      1500\n      750\n    \n  \n\n5 rows × 58 columns\n\n\n\n\nbart_data['CHOICE'] = np.nan\nbart_data['CHOICE'] = np.where(bart_data['CAR']==1, 'car', bart_data['CHOICE'])\nbart_data['CHOICE'] = np.where(bart_data['BUS']==1, 'bus', bart_data['CHOICE'])\nbart_data['CHOICE'] = np.where(bart_data['TRAIN']==1, 'rail', bart_data['CHOICE'])\nbart_data['car1'] = bart_data['CHOICE'] == 'car'\nbart_data['train1'] = bart_data['CHOICE'] == 'rail'\nbart_data['home'] = np.where(bart_data['HOMEOWN'] == 1, 1, np.nan)\nbart_data['home'] = np.where(bart_data['HOMEOWN'] > 1, 0, bart_data['home'])\nbart_data['income'] = np.where(bart_data['HHFAMINC'] > 0, bart_data['HHFAMINC'], np.nan)\nbart_data['density'] = np.where(bart_data['HTPPOPDN']==-9, np.nan, bart_data['HTPPOPDN']/1000)\nbart_data['urban1'] = bart_data['URBAN']==1\ny = bart_data[(bart_data['WRKCOUNT'] > 0) & ((bart_data['MSACAT'] == 1) | (bart_data['MSACAT'] == 2))]\ny['rail'] = y['RAIL'] == 1\ny['row_sum'] = y[['car1','train1','home','HHSIZE','income', 'urban1','density','MSACAT', 'rail']].sum(axis=1) == 0\ny\n\n/Users/nathanielforde/.local/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  del sys.path[0]\n/Users/nathanielforde/.local/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \n\n\n\n\n\n\n  \n    \n      \n      HOUSEID\n      TRAVDAY\n      SAMPSTRAT\n      HOMEOWN\n      HHSIZE\n      HHVEHCNT\n      HHFAMINC\n      PC\n      SPHONE\n      TAB\n      ...\n      HBRESDN\n      CHOICE\n      car1\n      train1\n      home\n      income\n      density\n      urban1\n      rail\n      row_sum\n    \n  \n  \n    \n      1\n      30000008\n      5\n      2\n      1\n      2\n      4\n      8\n      1\n      1\n      2\n      ...\n      300\n      car\n      True\n      False\n      1.0\n      8.0\n      0.3\n      False\n      False\n      False\n    \n    \n      9\n      30000085\n      1\n      2\n      1\n      1\n      2\n      9\n      1\n      1\n      4\n      ...\n      17000\n      nan\n      False\n      False\n      1.0\n      9.0\n      17.0\n      True\n      False\n      False\n    \n    \n      15\n      30000130\n      1\n      1\n      1\n      2\n      1\n      5\n      -9\n      1\n      -9\n      ...\n      17000\n      rail\n      False\n      True\n      1.0\n      5.0\n      30.0\n      True\n      True\n      False\n    \n    \n      17\n      30000144\n      3\n      2\n      2\n      3\n      0\n      5\n      5\n      1\n      1\n      ...\n      3000\n      bus\n      False\n      False\n      0.0\n      5.0\n      3.0\n      True\n      False\n      False\n    \n    \n      18\n      30000145\n      5\n      2\n      2\n      2\n      2\n      7\n      1\n      1\n      2\n      ...\n      3000\n      car\n      True\n      False\n      0.0\n      7.0\n      7.0\n      True\n      False\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      129674\n      40794087\n      2\n      1\n      2\n      1\n      1\n      5\n      2\n      -9\n      -9\n      ...\n      3000\n      car\n      True\n      False\n      0.0\n      5.0\n      7.0\n      True\n      True\n      False\n    \n    \n      129688\n      40794241\n      3\n      2\n      1\n      2\n      2\n      6\n      1\n      1\n      5\n      ...\n      3000\n      car\n      True\n      False\n      1.0\n      6.0\n      7.0\n      True\n      False\n      False\n    \n    \n      129690\n      40794260\n      5\n      2\n      1\n      4\n      1\n      11\n      1\n      1\n      2\n      ...\n      1500\n      car\n      True\n      False\n      1.0\n      11.0\n      3.0\n      True\n      False\n      False\n    \n    \n      129693\n      40794294\n      5\n      2\n      1\n      2\n      2\n      10\n      1\n      1\n      5\n      ...\n      7000\n      car\n      True\n      False\n      1.0\n      10.0\n      7.0\n      True\n      False\n      False\n    \n    \n      129695\n      50515573\n      3\n      1\n      1\n      1\n      0\n      10\n      1\n      1\n      5\n      ...\n      17000\n      nan\n      False\n      False\n      1.0\n      10.0\n      30.0\n      True\n      True\n      False\n    \n  \n\n39057 rows × 67 columns\n\n\n\n\nfeatures = [\"car1\",\"train1\",\"home\",\"HHSIZE\",\"income\",\"urban1\",\"density\", 'rail', 'CHOICE']\ny_focus = y[features]\ny_focus.dropna(inplace=True)\ny_focus = y_focus[y_focus['CHOICE'] != 'nan']\ny_focus[features].groupby('rail').mean().T\n\n/Users/nathanielforde/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  This is separate from the ipykernel package so we can avoid doing imports until\n\n\n\n\n\n\n  \n    \n      rail\n      False\n      True\n    \n  \n  \n    \n      car1\n      0.973861\n      0.877925\n    \n    \n      train1\n      0.010036\n      0.096610\n    \n    \n      home\n      0.749231\n      0.730127\n    \n    \n      HHSIZE\n      2.462420\n      2.487870\n    \n    \n      income\n      7.054552\n      7.518840\n    \n    \n      urban1\n      0.869753\n      0.910272\n    \n    \n      density\n      4.661936\n      7.559076\n    \n  \n\n\n\n\n\ny_focus[features + ['CHOICE']]\n\n\n\n\n\n  \n    \n      \n      home\n      HHSIZE\n      income\n      urban1\n      density\n      CHOICE\n    \n  \n  \n    \n      1\n      1.0\n      2\n      8.0\n      False\n      0.3\n      car\n    \n    \n      15\n      1.0\n      2\n      5.0\n      True\n      30.0\n      rail\n    \n    \n      17\n      0.0\n      3\n      5.0\n      True\n      3.0\n      bus\n    \n    \n      18\n      0.0\n      2\n      7.0\n      True\n      7.0\n      car\n    \n    \n      33\n      1.0\n      3\n      11.0\n      True\n      1.5\n      car\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      129667\n      1.0\n      2\n      8.0\n      False\n      0.3\n      car\n    \n    \n      129674\n      0.0\n      1\n      5.0\n      True\n      7.0\n      car\n    \n    \n      129688\n      1.0\n      2\n      6.0\n      True\n      7.0\n      car\n    \n    \n      129690\n      1.0\n      4\n      11.0\n      True\n      3.0\n      car\n    \n    \n      129693\n      1.0\n      2\n      10.0\n      True\n      7.0\n      car\n    \n  \n\n34043 rows × 6 columns\n\n\n\n\n# Without Rail\ny_focus_nr = y_focus[y_focus['rail'] == 0]\ny_focus_r = y_focus[y_focus['rail'] == 1]\n\n\nlogit_mod_nr = sm.Logit(np.array(y_focus_nr['car1']), \n                       sm.add_constant(y_focus_nr[[\"home\",\"HHSIZE\",\"income\",\"urban1\",\"density\"]]).astype(float))\nlogit_res_nr = logit_mod_nr.fit()\nlogit_margeff_nr = logit_res_nr.get_margeff()\n\n\nlogit_mod_r = sm.Logit(np.array(y_focus_r['car1']), \n                       sm.add_constant(y_focus_r[[\"home\",\"HHSIZE\",\"income\",\"urban1\",\"density\"]]).astype(float))\nlogit_res_r = logit_mod_r.fit()\nlogit_margeff_r = logit_res.get_margeff()\n\nOptimization terminated successfully.\n         Current function value: 0.114130\n         Iterations 9\nOptimization terminated successfully.\n         Current function value: 0.300803\n         Iterations 7\n\n\n\nstargazer = Stargazer([logit_res_nr, logit_res_r])\nstargazer.custom_columns(['Probability of Car - No Rail', 'Probability of Car -Rail'], [1, 1])\n#stargazer.add_custom_notes([str(probit_margeff.summary()), str(logit_margeff.summary())])\nHTML(stargazer.render_html())\n\n\nDependent variable:yProbability of Car - No RailProbability of Car -Rail(1)(2)HHSIZE-0.0180.050*(0.033)(0.026)const3.775***3.506***(0.270)(0.211)density-0.055***-0.109***(0.008)(0.003)home0.633***0.498***(0.095)(0.071)income0.133***-0.068***(0.019)(0.013)urban1-1.160***-0.312*(0.246)(0.187)Observations22,41911,624R2Adjusted R2Residual Std. Error1.000 (df=22413)1.000 (df=11618)F Statistic (df=5; 22413) (df=5; 11618)Note:\n \n  *p<0.1;\n  **p<0.05;\n  ***p<0.01\n \n\n\n\nMN_logit_mod_r = sm.MNLogit(np.array(y_focus_r['CHOICE']), \n                       y_focus_r[[\"home\",\"HHSIZE\",\"income\",\"urban1\",\"density\"]].astype(float))\nMN_logit_res_r = MN_logit_mod_r.fit()\nMN_logit_res_r.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.387108\n         Iterations 8\n\n\n\n\nMNLogit Regression Results\n\n  Dep. Variable:           y          No. Observations:      11624  \n\n\n  Model:                MNLogit       Df Residuals:          11614  \n\n\n  Method:                 MLE         Df Model:                  8  \n\n\n  Date:            Mon, 01 Mar 2021   Pseudo R-squ.:        0.1071  \n\n\n  Time:                23:06:01       Log-Likelihood:       -4499.7 \n\n\n  converged:             True         LL-Null:              -5039.6 \n\n\n  Covariance Type:     nonrobust      LLR p-value:        9.100e-228\n\n\n\n\n   y=car     coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  home        1.0460     0.128     8.159  0.000     0.795     1.297\n\n\n  HHSIZE      0.2411     0.046     5.234  0.000     0.151     0.331\n\n\n  income      0.2235     0.021    10.829  0.000     0.183     0.264\n\n\n  urban1      1.7447     0.150    11.619  0.000     1.450     2.039\n\n\n  density    -0.0847     0.006   -13.677  0.000    -0.097    -0.073\n\n\n  y=rail     coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  home        0.3757     0.140     2.689  0.007     0.102     0.650\n\n\n  HHSIZE     -0.0495     0.051    -0.973  0.331    -0.149     0.050\n\n\n  income      0.1658     0.022     7.466  0.000     0.122     0.209\n\n\n  urban1     -0.0949     0.167    -0.569  0.569    -0.422     0.232\n\n\n  density     0.0202     0.007     3.107  0.002     0.007     0.033\n\n\n\n\n\nn_params = 5 \nn_classes = 3\nz = np.random.rand(n_params,n_classes).flatten()\n\nres = minimize(ll_mn_logistic, x0 =z, args =(y_focus_r['CHOICE'], \n                                             y_focus_r[[\"home\",\"HHSIZE\",\"income\",\"urban1\",\"density\"]].astype(float), n_params, n_classes), \n             options={'disp': True, 'maxiter':1000})\nres\n\nWarning: Desired error not necessarily achieved due to precision loss.\n         Current function value: 4499.748263\n         Iterations: 33\n         Function evaluations: 896\n         Gradient evaluations: 56\n\n\n      fun: 4499.748263266686\n hess_inv: array([[ 1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  2.49790461e-02,\n         1.22239651e-02, -1.08077892e-03, -4.52948507e-02,\n         9.24499328e-04,  2.18160612e-02,  1.53707043e-02,\n        -2.13296546e-03, -3.42635201e-02,  5.36330909e-04],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  1.22239651e-02,\n         8.71917753e-03, -1.06847160e-03, -2.23203076e-02,\n         3.46191120e-04,  1.19746027e-02,  1.05025079e-02,\n        -1.74296384e-03, -1.65180529e-02,  1.47031357e-04],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00, -1.08077892e-03,\n        -1.06847160e-03,  4.43252403e-04,  1.14187551e-03,\n        -5.55280050e-05, -8.98456837e-04, -1.17483721e-03,\n         4.60854131e-04,  6.94275713e-04, -3.43435221e-05],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00, -4.52948507e-02,\n        -2.23203076e-02,  1.14187551e-03,  8.77146917e-02,\n        -1.70194689e-03, -4.37327155e-02, -2.84783896e-02,\n         3.45189389e-03,  6.85283602e-02, -1.05179786e-03],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  9.24499328e-04,\n         3.46191120e-04, -5.55280050e-05, -1.70194689e-03,\n         6.40830138e-05,  8.65122620e-04,  4.47609224e-04,\n        -8.69531233e-05, -1.35865208e-03,  4.78235451e-05],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  2.18160612e-02,\n         1.19746027e-02, -8.98456837e-04, -4.37327155e-02,\n         8.65122620e-04,  2.42731374e-02,  1.45500604e-02,\n        -2.32279638e-03, -3.32914355e-02,  5.72986223e-04],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  1.53707043e-02,\n         1.05025079e-02, -1.17483721e-03, -2.84783896e-02,\n         4.47609224e-04,  1.45500604e-02,  1.36673618e-02,\n        -2.18820823e-03, -2.15013079e-02,  1.87660290e-04],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00, -2.13296546e-03,\n        -1.74296384e-03,  4.60854131e-04,  3.45189389e-03,\n        -8.69531233e-05, -2.32279638e-03, -2.18820823e-03,\n         7.07813187e-04,  1.90500574e-03, -5.20898364e-05],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00, -3.42635201e-02,\n        -1.65180529e-02,  6.94275713e-04,  6.85283602e-02,\n        -1.35865208e-03, -3.32914355e-02, -2.15013079e-02,\n         1.90500574e-03,  6.33830924e-02, -1.09402475e-03],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  5.36330909e-04,\n         1.47031357e-04, -3.43435221e-05, -1.05179786e-03,\n         4.78235451e-05,  5.72986223e-04,  1.87660290e-04,\n        -5.20898364e-05, -1.09402475e-03,  5.33476436e-05]])\n      jac: array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  6.10351562e-05, -6.10351562e-05, -6.10351562e-05,\n        0.00000000e+00, -1.83105469e-04,  6.10351562e-05,  2.44140625e-04,\n        3.66210938e-04,  6.10351562e-05,  4.88281250e-04])\n  message: 'Desired error not necessarily achieved due to precision loss.'\n     nfev: 896\n      nit: 33\n     njev: 56\n   status: 2\n  success: False\n        x: array([ 0.98480009,  0.00651008,  0.0863602 ,  0.13983792,  0.32455478,\n        1.04604352,  0.24109143,  0.22348356,  1.74469334, -0.08468886,\n        0.3757215 , -0.04946345,  0.16579691, -0.09489807,  0.02024353])\n\n\n\nMN_logit_mod_r = sm.MNLogit(np.array(y_focus_r['CHOICE']), \n                       y_focus_r[[\"home\",\"HHSIZE\",\"income\",\"urban1\",\"density\"]].astype(float))\nMN_logit_res_r = MN_logit_mod_r.fit()\nMN_logit_res_r.summary()\nnr = y_focus_nr[['home', 'HHSIZE', 'income', 'urban1', 'density', 'rail']].astype(float)\nr = y_focus_r[['home', 'HHSIZE', 'income', 'urban1', 'density', 'rail']].astype(float)\nfull = y_focus[['home', 'HHSIZE', 'income', 'urban1', 'density', 'rail']].astype(float)\nnr_nd = nr\nnr_nd['density'] = 0\nnr_d = nr\nnr_d['density'] = r['density'].mean()\nfull_d = full\nfull_d[full_d['rail'] == 0]['density'] = r['density'].mean()\n\n\nres = MN_logit_res_r.predict(full.drop('rail', axis=1)).mean()\n\nres = pd.DataFrame(res).T\nres.columns = ['Bus', 'Car', 'Train']\nres.round(3) * 100\n\n/Users/nathanielforde/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  if __name__ == '__main__':\n\n\n\n\n\n\n  \n    \n      \n      Bus\n      Car\n      Train\n    \n  \n  \n    \n      0\n      2.7\n      88.5\n      8.8"
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/mle_utility_and_choice.html",
    "href": "posts/post-with-code/mle_utility_and_choice/mle_utility_and_choice.html",
    "title": "Trains, Planes …Utility and Maximum Likelihood",
    "section": "",
    "text": "In The Century of Self Adam Curtis’ spiralling survey of the 20th century we’re shown how the Freud’s ideas seeped into public consciousness. How they were taken and deployed by ad-men and marketing gurus to sell our own initimated desires back to ourselves. There is a conceit of individuality and uniqueness - that our mind is our own and the choices we make are ours alone to know and learn from. We plot our own journies and cleave to their contours as if we can see the goal. The main players and places get reshuffled or recycled as we grow, the tale contorts and twists to our constant surprise. How then could we be so utterly predictable?"
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/mle_utility_and_choice.html#latent-utility-and-binary-preference",
    "href": "posts/post-with-code/mle_utility_and_choice/mle_utility_and_choice.html#latent-utility-and-binary-preference",
    "title": "Trains, Planes …Utility and Maximum Likelihood",
    "section": "Latent Utility and Binary Preference",
    "text": "Latent Utility and Binary Preference\nWe’ll start, following the presentation of Christopher Adam’s Learning Microeconometrics, with a simple case of binary choice between goods \\(A, B\\).\nRevealed Preference Axiom: If there are two choices, A and B, and we observe a person choose A, then her utility from A is greater than her utility from B.\nWe assume that the latent utility can be expressed as by the revealed preferences i.e. as the share or proportion of choices made by the customers. Some assumptions about the form of the utility distribution are crucial as our modeling efforts will go wrong if we know nothing about the latent utilities. The utility is some function of product and consumer’s properties, perhaps mostly driven by price\n\\[ utility = \\mathbf{X'}\\beta + v\\]\nand market share is an expression of that utility\n\\[ demand_A = utility_{A} > 0 \\]\nIn a choice context we’re trying to determine if the implicit utility measure is sufficient to drive a purchase, and as such OLS models are inappropriate. This stems from the fact that we’re’trying to estimate a conditional probability over a binary choice not a unbounded continuous measure. The revealed preference assumption says that we can predict the purchase if the utility of the good is positive.\n\\[Pr(demand_A = 1) = utility > 0 \\] \\[= Pr(\\mathbf{X'}\\beta + v > 0) \\] \\[ = Pr(v > - \\mathbf{X'}\\beta ) \\] \\[ = 1 - F(\\mathbf{X'}\\beta ) \\]\nwhere \\(F\\) is the distribution of the unobserved random variable \\(v\\). The challenge is using the correct distribution as this feeds the method of statistical estimation of the parameters \\(\\beta\\)\n\\[U\\_{i,A} > U_{i, B} \\]\njust when\n\\[ \\mathbf{X'}_{i, A}\\beta + v_{i,A} > \\mathbf{X'}_{i, B}\\beta + v_{i,B}\\]\nor\n\\[  v\\_{i,A} -  v\\_{i,B} > - (\\mathbf{X'}_{i, A}  - \\mathbf{X'}_{i, B})\\beta \\]\nbut then the probability of demand is just\n\\[Pr(demand_A = 1 | \\mathbf{X'}_{i, A}, \\mathbf{X'}_{i, B}) \\] \\[ = Pr\\Bigg( v_{i,A} -  v_{i,B} > - (\\mathbf{X'}_{i, A}  - \\mathbf{X'}_{i, B})\\beta \\Bigg) \\] \\[ = Pr\\Bigg( -v_{i,A} -  v_{i,B} < (\\mathbf{X'}_{i, A}  - \\mathbf{X'}_{i, B})\\beta \\Bigg) \\] \\[ = F\\Bigg( (\\mathbf{X'}_{i, A}  - \\mathbf{X'}_{i, B})\\beta \\Bigg) \\]\nThere are a number of candidate distributions that might serve our purposes we’ll look here at the logistic regression and probit binary classification modes."
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/mle_utility_and_choice.html#probit-and-logit-regressions-for-binary-choice",
    "href": "posts/post-with-code/mle_utility_and_choice/mle_utility_and_choice.html#probit-and-logit-regressions-for-binary-choice",
    "title": "Trains, Planes …Utility and Maximum Likelihood",
    "section": "Probit and Logit Regressions for Binary Choice",
    "text": "Probit and Logit Regressions for Binary Choice\nThe probit and logit models are convenient distributions for modeling discrete choice. We’ll initialise some fake data and build two models over the \\(X, y\\) to infer the parameterisations that determined the utility driving of our choices. Note that we specify the distributions of the rror term differently for both models. In the probit case we say that the error term is normally distributed and in the logit case our error term is given a Weibull distribution.\nN = 100\nX_A = sm.add_constant(np.random.rand(N,2))\nX_B = sm.add_constant(np.random.rand(N, 2))\nbeta = np.array([1, -2, 3])\n\n#probit we only need one normal error term since sums of normals are normal\nv = np.random.normal(0, 1, N)\ny = (X_A.dot(beta) - X_B.dot(beta)) + v > 0\nX_diff = X_A - X_B\nX_diff[:, 0] =  1\nX_diff = pd.DataFrame(X_diff, columns=['const', \n'product_desc', 'product_desc1'])\n\n# Fit Probit model\nprobit_mod = sm.Probit(y, X_diff)\nprobit_res = probit_mod.fit()\n\nv1 = np.random.weibull(1, N)\nv2 = np.random.weibull(1, N)\ny = (X_A.dot(beta) - X_B.dot(beta)) + (v1 - v2) > 0\n# Fit logit model\nlogit_mod = sm.Logit(y, X_diff)\nlogit_res = logit_mod.fit()\nThe results are pretty good for both models - slightly better for the probit model in this case as the tails of the logit are wider. Both models discern the directionality and the correct magnitude of the parameters.\n\n\n\nBinary Choice Probit and Logit Models\n\n\n\n\n\n\n\n\n\n\nDependent variable:y\n\n\n\n\n\n\n\n\n\n\nProbit Model\n\n\nLogit Model\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nconst\n\n\n-0.255\n\n\n-0.049\n\n\n\n\n\n\n(0.156)\n\n\n(0.246)\n\n\n\n\nproduct_desc\n\n\n-1.491***\n\n\n-2.157***\n\n\n\n\n\n\n(0.445)\n\n\n(0.690)\n\n\n\n\nproduct_desc1\n\n\n2.964***\n\n\n3.833***\n\n\n\n\n\n\n(0.578)\n\n\n(0.888)\n\n\n\n\n\n\n\n\nObservations\n\n\n100\n\n\n100\n\n\n\n\nR2\n\n\n\n\n\n\n\n\nAdjusted R2\n\n\n\n\n\n\n\n\nResidual Std. Error\n\n\n1.000 (df=97)\n\n\n1.000 (df=97)\n\n\n\n\nF Statistic\n\n\n (df=2; 97)\n\n\n (df=2; 97)\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01"
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/mle_utility_and_choice.html#maximum-likelihood-estimation-and-multiple-choice",
    "href": "posts/post-with-code/mle_utility_and_choice/mle_utility_and_choice.html#maximum-likelihood-estimation-and-multiple-choice",
    "title": "Trains, Planes …Utility and Maximum Likelihood",
    "section": "Maximum Likelihood Estimation and Multiple Choice",
    "text": "Maximum Likelihood Estimation and Multiple Choice\nUnder the hood these models are fit using the technique of maximum likelihood estimation, which searches the parameter space over each distribution to find a setting so that the observed data is most probable. For computational convenience this often means that we minimise the value of negative log likelihood over a variety of parameter settings.\ndef log_probit_dist(params, *args):\n   X, y = args[0], args[1]\n   beta = [params[0], params[1], params[2]]\n   mu, sd, = params[3], params[4]\n   Xb = X.dot(beta)\n   q = 2*y-1\n   log_lik = np.log(stats.norm.cdf(q*Xb))\n   return -sum(log_lik)\n\n### Optimise the probit model for determining the parameters required to\n### estimate the underlying utility\n### True values of the parameters 2, 3, -4\nres = minimize(log_probit_dist, x0 =[0, 0 ,0 , 0, 1], \nargs =(X, demand_A), options={'disp': True})\nBut the situation is slightly more complex when we’re trying to optimise over multiple possible choices. Even with three choices, we have to both measure each latent utility metric as a structural equation and compare the demand for each product \\(T, B, C\\) against a specific reference product. In this case we’ll choose \\(C\\) if the utility exceeds that of \\(B\\) and \\(C\\).\n\\[ U_{i,T} = \\mathbf{X'}\\beta + v_{i,T} \\] \\[ U_{i,B} = \\mathbf{X'}\\beta + v_{i,B} \\] \\[ U_{i,C} = \\mathbf{X'}\\beta + v_{i,C} \\] \\[ Pr(y_C = 1 | \\mathbf{X}_{i, C}, \\mathbf{X}_{i, T}) =\n      Pr\\Big((v_{i, C} - v_{i, B} > - \\mathbf{X'}_{i, C}\\beta )  \\\\\n      \\text{ and } v_{i, C} - v_{i, T}  > - (\\mathbf{X}_{i, C}\\beta - \\mathbf{X}_{i, T})^{'}\\beta \\Big) \\]\nFor some appropriate probability distribution. This is a strong restriction called The Irrelevance of Independent Alternatives, it bakes in the notion that our preferences are consistent and transitive. If we prefer \\(C\\) to \\(B\\) and \\(B\\) to \\(T\\) then we ought to prefer \\(C\\) to \\(T\\) too. The benefit of the assumption is that it allows us to infer a utility ranking metric by computing all the pairwise alternatives to a given product. The mulinomial logit distribution is a convenient measure for discrete choice problems because it allows us to express our preference for each product on a 0-1 scale given by:\n\\[ Pr(y_C = 1 | \\mathbf{X}_{i, C}, ... \\mathbf{X}_{i, j}) = \\dfrac{ exp(\\mathbf{X'}\\beta)_{i, C}}{1 + \\sum_{i, j}^{j=N} exp(\\mathbf{X'}\\beta)_{i, j} } \\]\nwhich can be optimised for the best parameter fits through an maximum likelihood procedure as follows:\nnp.random.seed(100)\nN = 1000\nmu = [0,0]\nrho = 0.1\ncov = [[1, rho], [rho, 1]]\n\n# u is N*2\nu = np.random.multivariate_normal(mu, cov, 1000)\nx1 = np.random.uniform(0, 1, size=(N,2)) #np.random.rand(N,2)\nx2 = np.random.uniform(0, 1, size=(N,2)) #np.random.rand(N,2)\n\nU = -1 + -3*x1 + 4*x2 + u\n\ny = np.zeros(shape=(N, 2))\ny[:,0] = ((U[:,0] > 0) & (U[:,0] > U[:,1]))\ny[:,1] = (U[:,1] > 0 & (U[:,1] > U[:,0]))\n\nW1 = pd.DataFrame({'x1':x1[:,0], 'x2':x2[:,0]})\nW2 = pd.DataFrame({'x1':x1[:,1], 'x2':x2[:,1]})\ny_full = np.ones(shape=(N*2,1))\nclass_1 = np.where(((U[:,0] > 0) & (U[:,0] > U[:,1])), 'class_1', 'class_0')\nclass_2 = np.where((U[:,1] > 0 & (U[:,1] > U[:,0])), 'class_2', 'class_0')\ny_full = np.append(class_1, class_2)\nW_full = sm.add_constant(W1.append(W2)).reset_index(drop=True)\n\n\ndef cdf(W, beta):\n    Wb = np.dot(W, beta)\n    eXB = np.exp(Wb)\n    eXB = eXB /eXB.sum(1)[:, None]\n    return eXB\n\ndef take_log(probs):\n    epsilon = 1e-20 \n    return np.log(probs + epsilon)\n\ndef calc_ll(logged, d):\n    ll = d * logged\n    return ll\n\ndef ll_mn_logistic(params, *args):\n    y, W, n_params, n_classes = args[0], args[1], args[2], args[3]\n    beta = [params[i] for i in range(0, len(params))]\n    beta = np.array(beta).reshape(n_params, -1, order='F')\n    # Ensures fit against a reference class\n    beta[:,0] = [0 for i in range(0, n_params)]\n    \n    ## onehot_encode\n    d = pd.get_dummies(y, prefix='Flag').to_numpy()\n    \n    probs = cdf(W, beta)\n    logged = take_log(probs)\n    ll = calc_ll(logged, d)\n    \n    return -np.sum(ll)\n\nn_params = 3 \nn_classes = 3\nz = np.random.rand(3,3).flatten()\nres = minimize(ll_mn_logistic, x0 =z, args =(y_full, W_full, n_params, n_classes), \n             options={'disp': True, 'maxiter':1000})\nThe expressive power of these kinds of discrete choice model are to be admired. They map onto an innumerable range of practical problems in business, science and politics. So it is all the more important to be aware of their limitations when deploying them at scale. The independence assumption and the linear structure of the latent utility model is not innocent, they’re tantamount to very strong claims about the consistency and preference structure of the population. So long as we’re aware of this, the algorithms can be deployed profitably as sometimes consumers do have rational preferences - the challenge is modelling the considerations that go into their reasoning. It’s utterly useless to articulate preference over goods poorly described."
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/mle_utility_and_choice.html#bart-and-transportation-policy.",
    "href": "posts/post-with-code/mle_utility_and_choice/mle_utility_and_choice.html#bart-and-transportation-policy.",
    "title": "Trains, Planes …Utility and Maximum Likelihood",
    "section": "BART and Transportation policy.",
    "text": "BART and Transportation policy.\nThe BART infrastructure in san Francisco was expensive to implement and such rail networks can have massive impacts on the face of a city, so it is important to evaluate the potential gains to the development. McFadden’s analysis phrased this question a choice over the available modes of transport, and sought to predict the future demand for rail based on other demographic factors. The idea is that house size, ownership, income and proximity to a rail network determine uptake.\n\n\n\n\n\n\n\nhome\n\n\nHHSIZE\n\n\nincome\n\n\nurban1\n\n\ndensity\n\n\nCHOICE\n\n\n\n\n\n\n1\n\n\n1.0\n\n\n2\n\n\n8.0\n\n\nFalse\n\n\n0.3\n\n\ncar\n\n\n\n\n15\n\n\n1.0\n\n\n2\n\n\n5.0\n\n\nTrue\n\n\n30.0\n\n\nrail\n\n\n\n\n17\n\n\n0.0\n\n\n3\n\n\n5.0\n\n\nTrue\n\n\n3.0\n\n\nbus\n\n\n\n\n18\n\n\n0.0\n\n\n2\n\n\n7.0\n\n\nTrue\n\n\n7.0\n\n\ncar\n\n\n\n\n33\n\n\n1.0\n\n\n3\n\n\n11.0\n\n\nTrue\n\n\n1.5\n\n\ncar\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\nIt’s hard to estimate demand for a new product directly, so instead he tried to develop a two step model which estimated the relative impact of the each of the demographic factors on rail use where there already existed some rail infrastructure and then project the demand as if the same conditions held throughout the city. The multinomial model fitted on parts of the city with existing infrastructure gives the following coefficient estimates for the demographic factors relative to a base choice of bus-transport.\nno_rail = y_focus_nr[['home', 'HHSIZE', 'income', 'urban1', 'density', 'rail', 'CHOICE']].astype(float)\nrail = y_focus_r[['home', 'HHSIZE', 'income', 'urban1', 'density', 'rail', 'CHOICE']].astype(float)\nfull = y_focus[['home', 'HHSIZE', 'income', 'urban1', 'density', 'rail', 'CHOICE']].astype(float)\n\nMN_logit_mod_r = sm.MNLogit(np.array(rail['CHOICE']), \n                       rail[[\"home\",\"HHSIZE\",\"income\",\"urban1\",\"density\"]])\nMN_logit_res_r = MN_logit_mod_r.fit()\nMN_logit_res_r.summary()\n\n\n\nRegression Table\n\n\nnr_nd = no_rail\nnr_nd['density'] = 0\nnr_d = no_rail\nnr_d['density'] = r['density'].mean()\nfull_d = full\nfull_d[full_d['rail'] == 0]['density'] = r['density'].mean()\n\n#Predict full city uptake using model trained on rail data\nres = MN_logit_res_r.predict(full.drop('rail', axis=1)).mean()\n\nres = pd.DataFrame(res).T\nres.columns = ['Bus', 'Car', 'Train']\nres.round(3) * 100\n\n\n\n\n\n\n\nBus\n\n\nCar\n\n\nTrain\n\n\n\n\n\n\n0\n\n\n2.7\n\n\n88.5\n\n\n8.8\n\n\n\n\n\nAs it turned out this estimate for rail uptake was almost perfect which raises the question: are we really predictable or was the model really lucky? There is an odd dynamic between economic models and normative behaviour - once one is formulated as an approximate theory of advantageous behaviour in a market, it seeps in the societal consciousness as we try to learn from it. In doing so we conform to a model and make it a better fit to the data. Jevon’s would observe this characteristic as early as 1871\n\nThe laws [of individual economic man’s behaviour] which we are about to trace out are to be conceived as theoretically true of the individual; they can only be practically verified as regards the aggregate transactions, productions and consumptions of a large body of people. But the laws of the aggregate depend of course upon the laws applying to individual cases. - quoted on pg 149 The World in the Model by Mary S. Morgan\n\nMcFadden’s work was exemplary and revolutionary. It showed a clear and principaled method for translating a theory of human motivation into a predictions of market movements. The results were perhaps a little fortunate, but the method is profoundly important. The hardwork and shoe-leather of testing numerous models against observation, trying to infer causal impact of the demographic factors all illustrated the right kind of scientific process - measurement, abstraction and forecast.\nThe discrete choice models have some particular limitations when applied to complex choices over price. Apart from the fact that consumers will often exhibit irrational preferences, price of a product is correlated with the random components of the utility measure. This can prevent proper estimation of the model without making some adjustments. We won’t dwell on the details here but it is enough to note that the urgency of the task is not diminished by the difficulty."
  },
  {
    "objectID": "posts/post-with-code/replication_crisis/replication_and_cross_validation.html",
    "href": "posts/post-with-code/replication_crisis/replication_and_cross_validation.html",
    "title": "Psi, Replication and Bayesian Evidence",
    "section": "",
    "text": "Daryl Bem did not literally explode peoples brains, but his proofs of the possibility shook psychology to the core. In a short and precise paper Feeling the Future Bem set out to show that there was good evidence for the existence of human pre-cognition. Paradoxical as it sounds he succeeded. His research coincided with the burgeoning realisation that there was a replication crisis in psychology, yet he presented his work as invitation to further replication. A tension between careful rigour and fantastical speculation characterised the work, provoking anxiety in others that spurred better explanations and increased rigour in the psychological sciences."
  },
  {
    "objectID": "posts/post-with-code/replication_crisis/replication_and_cross_validation.html#a-flavour-of-the-rigour",
    "href": "posts/post-with-code/replication_crisis/replication_and_cross_validation.html#a-flavour-of-the-rigour",
    "title": "Psi, Replication and Bayesian Evidence",
    "section": "A Flavour of the Rigour",
    "text": "A Flavour of the Rigour\nWe won’t delve into each of the experiments, but it’s worth recalling something of the attention to detail and Bem’s effort to avoid easy refutation. Fundamentally his methods mirror the best practice of the day. He chooses an experimental paradigm of accepted research and adapts or invert it to test for the presence of psi, the catch all term denoting foresight and pre-cognitive ability. The most evocative of these experiments asked 100 participants to pick amongst two curtains where a picture will appear behind one of the two. The choice of curtain and the content of the picture was assigned in a random fashion, although a proportion of the pictures were selected to match the particpant’s sexual orientation. The theory was that some adaptive trait of the human organism would be primed to notice sexually stimulating possibilities in the near future. Success was measured against the benchmark of 50% accuracy.\nThe randomisation algorithm relied on a combination of methods using both software and a physical ramdomisation device to mitigate the risk that participants could either (i) intuit some pattern in the order of selection or (ii) manipulate the selection by psychokinesis. The randomising devices were then also run through the experiment to ensure that each guess achieved no greater accuracy than the benchmark 50%. Finally, each individual was measured on a scale of extraversion (a suspected correlate of psi) and tested. The results were tabulated and evaluated using standard parametric and non-parametric measures of statistical significance. The results showed a (technically) significant divergence from the null-hypothesis (53% accuracy) on the erotic stimuli, suggesting some evidence of pre-cognitive ability. The effect was pronounced in the extraverted participants who achieved 57% accurate predictions of where the image would be displayed. This was one of nine experiments he ran, of which eight returned suggestive evidence for psi\n\nDid anything go wrong?\nWhat if anything was Bem’s mistake? One diagnosis of the problem rests on the inherent weakness of hypothesis testing as a confirmatory device. The null-hypothesis mechanism aims to reject theories if the data is substantially incompatible with expectation. So phrased the burden of proof is massive. Even granting Bem his results, the nine experiments have to be weighed against the cumulative evidence of experience. With this constraint the evidence seems slight and is (at best) an invitation to replication. If we’re less generous we can question fine-graining of the results. Notice how specific the nature of the effect in particular group of people for a particular variety of stimuli. We can fairly wonder if the results hold up to replication but also how the results were achieved. What part of the data was obtained in process of exploration versus experiement? This is commonly called the file-drawer problem - so named for the speculative number of unsuccessful analyses that were filed away instead of published.\nThe emphasis on hypothesis testing and significant results leads to problems clearly outlined by  Meehl , but there are more particular problems with Bem’s analysis nicely brought out by a Bayesian perspective on the data."
  },
  {
    "objectID": "posts/post-with-code/replication_crisis/replication_and_cross_validation.html#bayesian-logic-of-evidence",
    "href": "posts/post-with-code/replication_crisis/replication_and_cross_validation.html#bayesian-logic-of-evidence",
    "title": "Psi, Replication and Bayesian Evidence",
    "section": "Bayesian Logic of Evidence",
    "text": "Bayesian Logic of Evidence\nFollowing the presentation of Lee & Wagenmakers in their Bayesian Cognitive Modeling we consider some problems with Bem’s methods.\n\nEarly Stopping\nThe first suggestion is that the results may be due to a somewhat haphazard approach to ending an experiment. They posit that there is suspicious relationship between the sample size and observed effect size. The evaluation of the hypothesis is Bayesian - they seek to assess the probability of a correlation given the observed values i.e. Bem’s results:\n\\[ \\overbrace{p(C | O)}^{posterior} = \\dfrac{\\overbrace{p( C )}^{prior}\\overbrace{p(O | C)}^{likelihood}}{\\underbrace{\\int_{i}^{N} p(O |C_{i})}_{evidence}} \\]\nThe observed values are suggestive of a relationship:\n\n\n\nNegative Relationship\n\n\nOn the hypothesis that there is some artifact of Bem’s process either deliberate or unintentional that influences the data, we want to see what the data suggests is the most likely value for their correlation. A strong anti-correlation is suggestive that the sample sizes are picked when the results support Bem’s conclusion.\nThe Bayesian paradigm of reasoning is especially nice for this kind of problem as it forces you to express your prior credence in the question at hand. In this case we want to say what our beliefs about the correlation between effect size and sample size would be were we to condition on Bem’s observations. To incorporate our prior beliefs we build a generative model of our observed values as draws from a bivariate normal distribution with inverse gamma distributions over the variance terms and the correlation term \\(r\\) enters the model as:\n\\[ MvNormal([\\mu_1, \\mu_2],  \\begin{bmatrix} \\sigma_1^{2} , & r\\sigma_1\\sigma_2  \\newline r\\sigma_1\\sigma_2 & ,  \\sigma_2^{2}  \\end{bmatrix}^{-1}) = MvNormal( \\overrightarrow{\\mu}, \\Sigma^{-1} ) \\]\nWe’ll set a fairly open ended flat prior allowing any possible value for correlation. Updating on the observed results will generate a view of the likely value. First look at the code, then we’ll unpack it a little.\n\n\n\nGenerative Model\n\n\nThis can be implemented efficiently using the PYMC3 package in python. The posterior distribution is constructed by looping over samples from the joint distribution of this system.\nwith pm.Model() as model1:\n    # r∼Uniform(−1,1) flat prior for correlation value\n    r = pm.Uniform(\"r\", lower=-1, upper=1)\n\n    # μ1,μ2∼Gaussian(0,.001)  priors for the bivaraite gaussian means\n    mu = pm.Normal(\"mu\", mu=0, tau=0.001, shape=n2)\n\n    # σ1,σ2∼InvSqrtGamma(.001,.001) prior for the bivariate sigma terms\n    lambda1 = pm.Gamma(\"lambda1\", alpha=0.001, beta=0.001)\n    lambda2 = pm.Gamma(\"lambda2\", alpha=0.001, beta=0.001)\n    sigma1 = pm.Deterministic(\"sigma1\", 1 / np.sqrt(lambda1))\n    sigma2 = pm.Deterministic(\"sigma2\", 1 / np.sqrt(lambda2))\n\n    cov = pm.Deterministic(\n        \"cov\",\n        tt.stacklists(\n            [[lambda1 ** -1, r * sigma1 * sigma2], [r * sigma1 * sigma2, lambda2 ** -1]]\n        ),\n    )\n\n    tau1 = pm.Deterministic(\"tau1\", tt.nlinalg.matrix_inverse(cov))\n\n    # The liklihood term - distribution of parameter conditional on the data\n    yd = pm.MvNormal(\"yd\", mu=mu, tau=tau1, observed=y)\n\n    trace1 = pm.sample()\nThere is quite alot in that. The key points are to note that we are estimating the joint distribution of multiple parameters (\\(\\overrightarrow{\\mu}, \\Sigma\\)) at once:\n\\[ p(\\overrightarrow{\\mu}, \\Sigma | y) = \\underbrace{p(\\overrightarrow{\\mu}) | \\Sigma, y)}_{\\text{conditional mean}}\\cdot p(\\Sigma | y) = \\overbrace{p(\\Sigma |\\overrightarrow{\\mu}, y)}^{\\text{conditional cov}} \\cdot p(\\overrightarrow{\\mu} | y) \\]\nIn the case of estimating a normal distribution with unknown mean and unknown variance both conditional quantities need to be estimated, so it helps that the inverse Gamma distribution is conjugate prior distribution for \\(\\Sigma\\). A conjugate prior relationship makes it easier to compute a posterior for particular families of probability distributions.\n\\[ \\text{Conjugate Relation : } prior_{fam}^{f(x)} \\mapsto_{likelihood}  posterior_{fam}^{f(x + n)} \\]\nThe bounded gamma variable is technically “improper” in that it can exceed 1, but taking the inverse squareroot transform is common practice to specify a conjugate prior for \\(\\Sigma\\) in the multivariate normal distribution. The conjugate prior for \\(\\overrightarrow{\\mu}\\) is similarly a normally distributed variable.\n\\[ p(\\overrightarrow{\\mu} | y, \\Sigma) \\sim MvNormal(\\overrightarrow{\\mu}_{0}, \\Sigma_{0}) \\] \\[ p(\\Sigma | y , \\overrightarrow{\\mu}) \\sim InvSqrtGamma(\\alpha, \\beta) \\]\nUsing this fact we can iteratively sample from both the conditional distributions of both parameters based on the observations \\(y\\) and feed the sample values through the likelihood for function for the overall multivariate normal expressed as:\n\\[ p (y | \\overrightarrow{\\mu}, \\Sigma) = (2 \\pi)^{\\frac{-2}{2}} \\vert \\Sigma \\vert^{\\frac{-1}{2}} exp\\Big(-(y - \\overrightarrow{\\mu})^{'} \\Sigma^{-1}(y - \\overrightarrow{\\mu})/2 \\Big) \\]\nwhich in turn is used to calculate the posterior values of the joint parameters conditional on the data. This method is called Gibbs Sampling and constructs the posterior distribution from our data. The implementation of this process in Pymc3 allows us to recover the marginal distribution of the correlation coefficient \\(r\\) from the range of samples.\n# Plot the marginal distribution of the correlation term\naz.plot_trace(trace1, var_names=[\"r\"])\n\n\n\nCorrelation Plot\n\n\nThe results decisively show that the data implies a strong relationship between effect size and sample size, that merits some suspicion.\n\n\nEvidence for Psi\nIt’s one thing to question the methodological missteps of the paper, but it’s quite another thing to try and evaluate the basic claim of Bem’s paper. How does his evidence contribute to our belief in the occurence of psi. Even though he arrays a list of 9 experiments with suggestive evidence. It’s possible that each represents a statistical fluke - a better test for the existence of psi would involve measuring consistent performance over a number of trials. This is exactly what Wagenmaker’s research attempted when trying to replicate Bem’s results. He repeated Bem’s original experiments and another analogous task measuring performance on both both tasks. The darker the square, the more participants in the cross-section.\n\n\n\nConsistent performance\n\n\nThis plot does not suggest any strong relation between performance over the tests. Since we want to evaluate the correlation of performance on both tests, we would like to evaluate if the data suggests a positive correlation in the posterior distribution. Performance on the test is simply the count of correct answers for each participant on each experiment. We’ll model this as a Binomial distribution. As before we treat our observed guesses as driven by draws from a bivariate normal distribution which is “filtered” through a probit transform. A probit transform like the logistic transform converts a continuous variable into a symmetrical bi-furcated distribution which allows us to model our choices.\ndef phi(x):\n    # probit transform\n    return 0.5 + 0.5 * pm.math.erf(x / pm.math.sqrt(2))\n\n\nwith pm.Model() as model2:\n    # r∼Uniform(−1,1)\n    r = pm.Uniform(\"r\", lower=0, upper=1)\n\n    # μ1,μ2∼Gaussian(0,.001)\n    mu = pm.Normal(\"mu\", mu=0, tau=0.001, shape=n2)\n\n    # σ1,σ2∼InvSqrtGamma(.001,.001)\n    lambda1 = pm.Gamma(\"lambda1\", alpha=0.001, beta=0.001, testval=100)\n    lambda2 = pm.Gamma(\"lambda2\", alpha=0.001, beta=0.001, testval=100)\n    sigma1 = pm.Deterministic(\"sigma1\", 1 / np.sqrt(lambda1))\n    sigma2 = pm.Deterministic(\"sigma2\", 1 / np.sqrt(lambda2))\n\n    cov = pm.Deterministic(\n        \"cov\",\n        tt.stacklists(\n            [[lambda1 ** -1, r * sigma1 * sigma2], [r * sigma1 * sigma2, lambda2 ** -1]]\n        ),\n    )\n\n    tau1 = pm.Deterministic(\"tau1\", tt.nlinalg.matrix_inverse(cov))\n\n    thetai = pm.MvNormal(\"thetai\", mu=mu, tau=tau1, shape=(n, n2))\n    theta = phi(thetai)\n    #likelihood\n    kij = pm.Binomial(\"kij\", p=theta, n=Nt, observed=xobs)\n\n    trace2 = pm.sample()\nThis looks like an analogous model, and in some respects it is, but note what the multivariate gaussian is modelling. In this model it’s an expression of the latent mental process that drives the performance each task. This flexibility is a virtue which emphasises the versatility of thinking through the underlying probabilistic phenomena. We have a suite of models apt to describe a range of processes if we’re creative enough to use them. More, we’ve been able to use these models to incorporate our information about the phenomena - the structure of the relationship and the admissable range of values.\n\n“As soon as we look at the nature of inference at this many-moves-ahead level of perception, our attitude toward probability theory and the proper way to use it in science becomes almost diametrically opposite to that expounded in most current textbooks. We need have no fear of making shaky calculations on inadequate knowledge; for if our predictions are indeed wrong, then we shall have an opportunity to improve that knowledge, an opportunity that would have been lost had we been too timid to make the calculations. Instead of fearing wrong predictions, we look eagerly for them; it is only when predictions based on our present knowledge fail that probability theory leads us to fundamental new knowledge.” - E.T. Jaynes Bayesian Methods\n\nBased on these results we should predict poor future performance since, as expected, the correlation between the performance on both tasks is very slight.\naz.plot_trace(trace2, var_names=[\"r\"])\n\n\n\nPosterior for relation between performance on both tests\n\n\nDefinitely not promising evidence of aptitude in the studied group.\n\n\nUpdating Belief: Always a Choice not a Necessity\nBut notice the way we’ve used the same updating mechanism in both cases but to different ends. In the first case we’ve updated on a flat prior and reasoned to the conclusion as a kind of reductio ad absurdum. We intuitively reject the hypothesis that effect size should be anti-correlated with sample size and when Bem’s results suggests the opposite, we take it as evidence against his theory. In the second case, more exploratory, we genuinely don’t know how the performance over the two tests should be related but for Bem’s results to stand they need to be positive. Conditioning on the experimental observations we learn what the expected correlation is to be and deny the presence of psi. Will Bem update his beliefs in the same manner? As with any argument we can always question the premises, but the Bayesian methodology has a simple consistency going for it. It’s an explicit and appealing form of analysis, which even if wrong is interesting for how it enables us to learn from the error."
  },
  {
    "objectID": "posts/post-with-code/replication_crisis/replication_and_cross_validation.html#conclusion",
    "href": "posts/post-with-code/replication_crisis/replication_and_cross_validation.html#conclusion",
    "title": "Psi, Replication and Bayesian Evidence",
    "section": "Conclusion",
    "text": "Conclusion\nBem’s paper is an unsettling reminder that rigour comes in many forms and even witha good faith effort exactitude is not always enough to prevent error. In this case the Bayesian perspective provides a solid critique and proves itself a valuable lens with which to evaluate reliability of performance. More generally the explicit nature of Bayesian modelling and the careful manner in which you specify your prior expectations is appealing. Allowing for a more nuanced weighing of evidence than available through a simplistic binary of hypothesis testing."
  },
  {
    "objectID": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html",
    "href": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "import random\nimport statsmodels.api as sm\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html#simultaneity-bias-in-an-ols-regression",
    "href": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html#simultaneity-bias-in-an-ols-regression",
    "title": "Examined Algorithms",
    "section": "Simultaneity Bias in an OLS regression",
    "text": "Simultaneity Bias in an OLS regression\nThe coefficients in an ordinary least squares (OLS) regression are imprecisely estimated when there is a relationship between the feature variables and the error terms. This is true even when the functional form is properly specified! In our case we can see that the values for the price feature are too high due to the simultaneity bias.\n\nA Supply and Demand Example\n\nN = 1000\nnp.random.seed(0)\nmu, sigma = 0, 7\ne_1 = np.random.normal(mu, sigma, N)\nW = np.random.normal(100, 20, N)\nH = np.random.uniform(0, 10, N)\n\n# True Values\na0, a1, a2, a3 = 2, 3, 5, 10\n\nmu, sigma = 0, 6\ne_2 = np.random.normal(mu, sigma, N)\nF = np.random.uniform(6, 30, N)\nO = np.random.normal(7, 4, N)\n\n#True Values\nb0, b1, b2, b3 = 4, 6, 2, 7\n\nP = np.random.normal(5, 2, N) + .2*(e_1+e_2)\n\ns = a0 + a1*P + a2*W + a3*H + e_1\nd = b0 + b1*P +b2*F + b3*O + e_2\n\n# True coeffs             3       5       10\nX_supply = pd.DataFrame({'P': P , 'W':W, 'H':H})\nX_supply = sm.add_constant(X_supply)\nmodel = sm.OLS(s, X_supply)\nresults = model.fit()\nresults.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:            y          R-squared:             0.997 \n\n\n  Model:                   OLS         Adj. R-squared:        0.997 \n\n\n  Method:             Least Squares    F-statistic:        9.625e+04\n\n\n  Date:             Thu, 11 Feb 2021   Prob (F-statistic):    0.00  \n\n\n  Time:                 22:42:02       Log-Likelihood:      -3203.0 \n\n\n  No. Observations:        1000        AIC:                   6414. \n\n\n  Df Residuals:             996        BIC:                   6434. \n\n\n  Df Model:                   3                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n           coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  const    -3.3455     1.102    -3.037  0.002    -5.508    -1.184\n\n\n  P         4.2631     0.068    62.574  0.000     4.129     4.397\n\n\n  W         4.9892     0.010   512.078  0.000     4.970     5.008\n\n\n  H         9.9924     0.066   152.473  0.000     9.864    10.121\n\n\n\n\n  Omnibus:        0.846   Durbin-Watson:         2.049\n\n\n  Prob(Omnibus):  0.655   Jarque-Bera (JB):      0.738\n\n\n  Skew:          -0.058   Prob(JB):              0.691\n\n\n  Kurtosis:       3.066   Cond. No.               598.\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe price coefficient estimates are statistically significant and the model has a high R squared figure but fundamentally incorrect due to the simultaneity bias."
  },
  {
    "objectID": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html#detour-a-simpler-example",
    "href": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html#detour-a-simpler-example",
    "title": "Examined Algorithms",
    "section": "Detour: A simpler Example",
    "text": "Detour: A simpler Example\nWe’ll explain some the details with a simpler example first, and then proceed to show how to estimate the supply and demand system using instrumental variable regression.\n\nnp.random.seed(1235)\nz = np.random.uniform(0, 1, 1000)\ne_3 = np.random.normal(0, 3, 1000)\ne_1 = np.random.normal(0, 1, 1000)\nx = -1 + 5*z + 2*(e_1) + e_3\ny = 2 + 3*x + e_3\n\nX = pd.DataFrame({'X': x})\nX = sm.add_constant(X)\n\nZ = pd.DataFrame({'Z': z})\nZ = sm.add_constant(Z)\n\nOn the OLS linear model \\[ Y = \\beta X + \\epsilon \\] the beta coefficients can be estimated as follows: \\[ \\hat{\\beta} = (X^{'}X)^{-1}X^{'}y \\] under a number of conditions, but crucially we require that: \\[ E(X'\\epsilon) = 0 \\]\n\nbeta_hat_OLS = linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\nbeta_hat_OLS\n#True coefficient values 2,  3\n\narray([0.99580253, 3.60164346])\n\n\nThe failure of this assumption leads to skewed coefficient estimates. On the IV variable regression model we allow that the last assumption fails, so that: \\[ Y = \\beta X + \\psi \\] and we allow that \\[ E(X'\\psi) \\neq 0 \\] and choose an instrument Z as a proxy for X so specifically that \\[ E(Z'\\psi) = 0\\] then then estimator can be stated: \\[ Z'Y = Z'X\\beta + Z'\\psi \\Rightarrow \\hat\\beta =  [X'Z(Z'Z)^{-1}Z'X]^{-1}X'Z(Z'Z)^{-1}Z'y \\]\n\ndef iv_estimate(Z, X, y):\n    return linalg.inv(X.T.dot(Z).dot(linalg.inv(Z.T.dot(Z)).dot(Z.T.dot(X)))).dot(X.T.dot(Z).dot(linalg.inv(\n            Z.T.dot(Z)).dot(Z.T.dot(y))))\n\n\niv_estimate(Z, X, y)\n#True coefficient values 2,  3\n\narray([2.22432093, 2.84420838])\n\n\nwhich (a) gives better estimates of the coefficients and (b) can, thankfully, be simplified when we have as many instruments as variables that need to be “instrumented” to: \\[ (Z'X)^{-1}Z'y\\]. A good instrument needs to be carefully chosen so as to correlate with \\(X\\) with but not be influenced by the error terms in \\(y\\).\n\nlinalg.inv(Z.T.dot(X)).dot(Z.T).dot(y)\n\narray([2.22432093, 2.84420838])\n\n\n\ndef bootstrap_iv_estimator(reps, y, X_in, Z_in=None):\n    np.random.seed(100)\n    bs_mat = np.zeros(shape=(reps, X_in.shape[1]))\n    N = len(y)\n    if Z_in is None:\n        Z_in = X_in\n    for i in range(0, reps):\n        index_bs = np.random.randint(N, size=N)\n        y_bs = y[index_bs]\n        X_bs = X_in.iloc[index_bs,]\n        Z_bs = Z_in.iloc[index_bs,]\n        bs_mat[i,] = linalg.inv(Z_bs.T.dot(X_bs)).dot(Z_bs.T).dot(y_bs)\n    bs_mat = pd.DataFrame(bs_mat, columns = ['const', 'X'])\n    summary = pd.concat([bs_mat.mean(), bs_mat.std(), bs_mat.quantile(0.05), bs_mat.quantile(0.95)], axis=1)\n    summary.columns = ['coefs', 'std', '0.05', '0.95']\n    return summary\n\n\nestimates = bootstrap_iv_estimator(100, y, X) #OLS\nestimates\n\n\n\n\n\n  \n    \n      \n      coefs\n      std\n      0.05\n      0.95\n    \n  \n  \n    \n      const\n      0.996938\n      0.068499\n      0.892036\n      1.113305\n    \n    \n      X\n      3.599339\n      0.014494\n      3.576504\n      3.622485\n    \n  \n\n\n\n\n\nestimates = bootstrap_iv_estimator(100, y, X, Z) #IV\nestimates\n\n\n\n\n\n  \n    \n      \n      coefs\n      std\n      0.05\n      0.95\n    \n  \n  \n    \n      const\n      2.220228\n      0.169027\n      2.001413\n      2.465313\n    \n    \n      X\n      2.852359\n      0.080436\n      2.732629\n      2.967042"
  },
  {
    "objectID": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html#returning-to-our-model-of-supply-and-demand",
    "href": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html#returning-to-our-model-of-supply-and-demand",
    "title": "Examined Algorithms",
    "section": "Returning to our model of Supply and Demand",
    "text": "Returning to our model of Supply and Demand\n\nN = 1000\nnp.random.seed(0)\nmu, sigma = 0, 7\ne_1 = np.random.normal(mu, sigma, N)\nW = np.random.normal(100, 20, N)\nH = np.random.uniform(0, 10, N)\n\n# True Values\na0, a1, a2, a3 = 2, 3, 5, 10\n\nmu, sigma = 0, 6\ne_2 = np.random.normal(mu, sigma, N)\nF = np.random.uniform(6, 30, N)\nO = np.random.normal(7, 4, N)\n\n#True Values\nb0, b1, b2, b3 = 4, 6, 2, 7\n\nP = np.random.normal(5, 2, N) + .2*(e_1+e_2)\n\ns = a0 + a1*P + a2*W + a3*H + e_1\nd = b0 + b1*P +b2*F + b3*O + e_2\n\nX_supply = pd.DataFrame({'P': P, 'W':W, 'H': H})\nX_supply = sm.add_constant(X_supply)\n\nZ_supply = pd.DataFrame({'F': F, 'O': O, 'W':W, 'H': H})\nZ_supply = sm.add_constant(Z_supply)\n\niv_estimate(Z_supply, X_supply, s)\n#True coefficient values 2, 3, 5, 10\n\narray([1.33465635, 3.32553682, 4.98887685, 9.97598787])"
  },
  {
    "objectID": "posts/post-with-code/bayesian_model_comparison/bayes_decision.html",
    "href": "posts/post-with-code/bayesian_model_comparison/bayes_decision.html",
    "title": "Bayesian Decisions & Model Comparison",
    "section": "",
    "text": "Most days we face an infinite horizon of future decisions without being overwhelmed. A sequence of such decisions can compound, or reduce that certainty. But we tend to focus on the exaggerated drama of the momentous choice. Even when change is gradual and cumulative, history tells of a tale of pivot points, epic junctures and transformative events. Two paths diverge in a woods and we choose one. An array of possibilities suddenly open before us, or gets collapsed into a cul de sac. We weigh the options differently according to our needs and desires and pursue strategies to maximise our goals.\nIncorporating uncertainty Leonard J. Savage famously showed that the optimal weights on each desire are proportional to their probabilities if we seek to maximise our expected gains. In particular, he showed that if we reason with probabilities our outcomes will the dominate all other methods for mitigating uncertainty. This alone should serve to motivate probabilisitic modelling, but we want to show concretely how this translates to making decisions under uncertainty. We’ll first show how such considerations underwrite a slew of familiar decision rules, and then how these methods can be applied to problem of reserving enough premium to cover costs accruing against insurance contracts. In both cases a concern for accuracy is implict."
  },
  {
    "objectID": "posts/post-with-code/bayesian_model_comparison/bayes_decision.html#loss-functions-and-ratio-tests",
    "href": "posts/post-with-code/bayesian_model_comparison/bayes_decision.html#loss-functions-and-ratio-tests",
    "title": "Bayesian Decisions & Model Comparison",
    "section": "Loss functions and Ratio Tests",
    "text": "Loss functions and Ratio Tests\nImagine we frame the cost or loss of incorrectly choosing amongst our theories under uncertainty.Then the question becomes one of expected loss. Define \\(l(T, T\\_1)\\) as the loss incurred when choosing \\(T\\) when \\(T\\_1\\) is true. Then the expected loss in light of the data is calculated as:\n\\[ (1):  E(l(T_{\\beta} , T) | D) =\nl(T_{\\beta} , T_{\\beta})p(T_{\\beta} | D) + l(T_{\\beta} , T_{\\beta^{1}})p(T_{\\beta^{1}} | D) \\]\n\\[ (2):  E(l(T_{\\beta^{1}} , T) | D) =\nl(T_{\\beta^{1}} , T_{\\beta})p(T_{\\beta} | D) + l(T_{\\beta^{1}} , T_{\\beta^{1}})p(T_{\\beta^{1}} | D) \\]\nWe want to choose the theory with least expected loss. Assume (1) \\(\\geq\\) (2), then\n\\[ l(T_{\\beta} , T_{\\beta})p(T_{\\beta} | D) + l(T_{\\beta} , T_{\\beta^{1}})p(T_{\\beta^{1}} | D) \\geq l(T_{\\beta^{1}} , T_{\\beta})p(T_{\\beta} | D) + l(T_{\\beta^{1}} , T_{\\beta^{1}})p(T_{\\beta^{1}} | D) \\]\nmanipulating:\n\\[ l(T_{\\beta} , T_{\\beta^{1}})p(T_{\\beta^{1}} | D) \\geq l(T_{\\beta^{1}} , T_{\\beta})p(T_{\\beta} | D) -  l(T_{\\beta} , T_{\\beta})p(T_{\\beta} | D) + l(T_{\\beta^{1}} , T_{\\beta^{1}})p(T_{\\beta^{1}} | D) \\]\ngrouping:\n\\[ l(T_{\\beta} , T_{\\beta^{1}})p(T_{\\beta^{1}} | D) \\geq p(T_{\\beta} | D)(l(T_{\\beta^{1}} , T_{\\beta}) -  l(T_{\\beta} , T_{\\beta})) + l(T_{\\beta^{1}} , T_{\\beta^{1}})p(T_{\\beta^{1}} | D) \\]\nrearraging:\n\\[ (l(T_{\\beta} , T_{\\beta^{1}}) - l(T_{\\beta^{1}} , T_{\\beta^{1}}))p(T_{\\beta^{1}} | D)  \\geq p(T_{\\beta} | D)(l(T_{\\beta^{1}} , T_{\\beta}) -  l(T_{\\beta} , T_{\\beta})) \\]\n\\[ p(T_{\\beta^{1}} | D)  \\geq p(T_{\\beta} | D)\\frac{(l(T_{\\beta^{1}} , T_{\\beta}) -  l(T_{\\beta} , T_{\\beta}))}{(l(T_{\\beta} , T_{\\beta^{1}}) - l(T_{\\beta^{1}} , T_{\\beta^{1}}))} \\]\n\\[ \\frac{p(T_{\\beta^{1}} | D)}{p(T_{\\beta} | D)}  \\geq \\frac{(l(T_{\\beta^{1}} , T_{\\beta}) -  l(T_{\\beta} , T_{\\beta}))}{(l(T_{\\beta} , T_{\\beta^{1}}) - l(T_{\\beta^{1}} , T_{\\beta^{1}}))} \\]\nwhich by Bayes Rule:\n\\[ \\frac{\\frac{p(D | T_{\\beta^{1}})p(T_{\\beta^{1}})}{p(D)}}{\\frac{p(D | T_{\\beta})p(T_{\\beta})}{p(D)}} \\geq \\frac{(l(T_{\\beta^{1}} , T_{\\beta}) -  l(T_{\\beta} , T_{\\beta}))}{(l(T_{\\beta} , T_{\\beta^{1}}) - l(T_{\\beta^{1}} , T_{\\beta^{1}}))} \\]\nwhich cancels to become:\n\\[ \\frac{p(D | T_{\\beta^{1}})p(T_{\\beta^{1}})}{p(D | T_{\\beta})p(T_{\\beta})} \\geq \\frac{(l(T_{\\beta^{1}} , T_{\\beta}) -  l(T_{\\beta} , T_{\\beta}))}{(l(T_{\\beta} , T_{\\beta^{1}}) - l(T_{\\beta^{1}} , T_{\\beta^{1}}))} \\]\n\\[ \\Rightarrow \\frac{p(D | T_{\\beta^{1}})}{p(D | T_{\\beta})} \\geq \\frac{p(T_{\\beta})(l(T_{\\beta^{1}} , T_{\\beta}) -  l(T_{\\beta} , T_{\\beta}))}{p(T_{\\beta^{1}})(l(T_{\\beta} , T_{\\beta^{1}}) - l(T_{\\beta^{1}} , T_{\\beta^{1}}))} \\]\nwhich shows how each expected loss calculation between two theories reduces to a likelihood ratio test, where we have a specific threshold for accepted preference. The preferred theory dominates the other just when the ratio of their likelihood exceeds that threshold. Therefore our estimate of the correct theory \\(\\hat{\\mathbf{T}}\\) is characterised by this comparison.\n\\[ \\hat{\\mathbf{T}} = \\mathbb{1} \\Bigg[  \\frac{p( D | \\mathbf{T}_{\\beta^{1}})}{p(D | \\mathbf{T}_{\\beta})} \\geq \\eta \\Bigg] \\]\nHardt and Recht show a number of the very familiar procedures reduce to likelihood ratio tests with more or less constraints on threshold priors or loss functions. For instance, the (MAP) maximum a posteriori decision rule emerges naturally as a likelihood ratio test when we set the \\(l(T, T) = l(\\neg T, \\neg T) = 0\\) and \\(l(T, \\neg T) = l(\\neg T, T) = 1\\) This results in the following decision rule:\n\\[ \\hat{\\mathbf{T}} = \\mathbb{1} \\Bigg[  \\frac{p( D | \\mathbf{T}_{\\beta^{1}})}{p(D | \\mathbf{T}_{\\beta})} \\geq \\frac{p(\\mathbf{T_{\\beta}})}{p(\\mathbf{T_{\\beta^{1}}})} \\Bigg] \\]\nThis is a particularly natural decision rule since it relies in how much we have learned about the likely theories (after conditioning on the data) relative to our prior commitments.\n\nOptimising Success and and Failure Rates\nIn the binary classification case we can compare two hypotheses directly while aiming to maximise some measure of accuracy with a high probability. This requires that we express our decision rule as some selective mapping over our data points in light of the theories.\n\\[ \\hat{\\mathbf{d}}: \\{ \\mathbf{T_{\\beta}}, \\mathbf{T\\_{\\beta^{1}}} \\} \\times D  \\mapsto \\{ False, True \\} \\]\nWith this set up the Neyman Pearson lemma states that any attempt to optimise our rule so that we maximise the True positive rate (TPR) while we minimise our False positive rate (FPR) subject to a constraint is equivalent to a likelihood ratio test.\n\\[ \\text{ maximise TPR: } p(\\hat{\\mathbf{d}} \\text{ is True} | \\mathbf{T_\\beta}) \\] \\[ \\text{ subject to FPR: } p(\\hat{\\mathbf{d}} \\text{ is True} | \\mathbf{T_\\beta^{1}}) \\leq \\alpha \\]\nThis follows because the optimisation task does not require specific information about the prior probabilities. The TPR and FPR terms likelihood terms, so we can construct a loss function such that the decision rule is equivalent to a simple likelihood ratio test.\nLemma 1. Neyman-Pearson Lemma The optimal probabilistic decision rule that maximizes TPR with an upper bound on FPR is a deterministic likelihood ratio test.\nThe proof is short and elaborated in Patterns, Predictions and Actions, but the key point is that a likelihood ratio test can be constructed to mirror this optimisation constraint with appropriate modifications to our priors over both theories.\n\nProof. Let \\(R\\) be a decision rule such that\n\\[ \\Bigg[  \\frac{p( \\hat{\\mathbf{d}} | \\mathbf{T}_{\\beta^{1}})}{p(\\hat{\\mathbf{d}} | \\mathbf{T}_{\\beta})} \\geq \\eta \\Bigg] \\text{ where FPR = } \\alpha \\] Then let \\(\\tau\\) be the TPR of R. Keeping the likelihood terms untouched we’re free to manipulate the priors so that\n\\[ \\frac{p(\\mathbf{T_{\\beta}})}{p(\\mathbf{T_{\\beta^{1}}})} = \\frac{1}{1 + \\eta} / \\frac{\\eta}{1 + \\eta} = \\frac{1+\\eta}{(1+\\eta)\\eta} = \\eta\\]\nwhich is just the likelihood ratio test MAP rule above. Then let \\(tpr\\) and \\(fpr\\) be the true postive rate and false postive rate of an alternative rule \\(R^{1}\\) with \\(fpr \\leq \\alpha\\) implying $ p({}) fpr p({}) $. Adding to both sides:\n\\[ (1) \\text{    } p(\\mathbf{T}_{\\beta}) fpr + p(\\mathbf{T}_{\\beta^{1}})(1- tpr) \\leq p(\\mathbf{T}_{\\beta}) \\alpha  +  p(\\mathbf{T}_{\\beta^{1}})(1- tpr)  \\]\nAssume \\(fpr < \\alpha\\) then unpacking a bit we see by complementarity that:\n\\[ \\frac{fp}{neg}  <  \\frac{FP}{neg} \\text{ and since } (1-tpr) = \\frac{fn}{pos} , (1-\\tau) = \\frac{FN}{pos} \\text{ implies } (1-\\tau) < (1 - tpr) \\]\n\\[ \\Rightarrow  (2) \\text{   } p(\\mathbf{T}_{\\beta}) \\alpha  +  p(\\mathbf{T}_{\\beta^{1}})(1- \\tau) \\leq p(\\mathbf{T}_{\\beta})fpr + p(\\mathbf{T}_{\\beta^{1}})(1- tpr) \\]\nAlternatively \\(fpr = \\alpha\\) and (2) folllows quickly. In either case (1) & (2) imply:\n\\[ (3) p(\\mathbf{T}_{\\beta}) \\alpha  +  p(\\mathbf{T}_{\\beta^{1}})(1- \\tau) \\leq \\text{    } p(\\mathbf{T}_{\\beta}) fpr + p(\\mathbf{T}_{\\beta^{1}})(1- tpr) \\leq p(\\mathbf{T}_{\\beta}) \\alpha  +  p(\\mathbf{T}_{\\beta^{1}})(1- tpr)  \\]\nwhich implies that \\(tpr \\leq \\tau\\) in all cases.\nWhich is enough to complete the proof since \\(R^{1}\\) was arbitrary, we have shown that \\(R\\) maximises TPR with a fixed FPR.\n\nSo, in practice, optimisation amounts to the modification of these priors. Our priors are adjusted as hyper-parameters as we tune them to achieve optimal classification performance and generalisability."
  },
  {
    "objectID": "posts/post-with-code/bayesian_model_comparison/bayes_decision.html#example-expected-loss-curves",
    "href": "posts/post-with-code/bayesian_model_comparison/bayes_decision.html#example-expected-loss-curves",
    "title": "Bayesian Decisions & Model Comparison",
    "section": "Example: Expected Loss Curves",
    "text": "Example: Expected Loss Curves\nImagine an insurer seeks to reserve enough of their premium to cover their expected losses. Then month on month we need to estimate the approriate loss ratio and project those losses over the ensuing months. This problem has a natural expression in the Bayesian setting. As can be seen in Stan language case study here\nWe’ll write up the model in python’s PYMC3 framework and walk through the code. The main point to see here is that we are trying to estimate the losses accruing the a given cohort of insurance policies. So we are estimating in turn both the loss ratio and growth curves. We’ll canvas two options for the growth curves, one based on the logistic curve and another based on the Weibull function. The empirical loss curves show a particular growth curve which rises and plateaus as we step through time.\n\n\n\nLoss Ratios\n\n\nThe modelling challenge is to determine both the probable loss ratios for a given year’s premium and the manner in which the most probable curvature for the accruing losses. The range of probable realisations are, in the Bayesian setting, constrained by the distributions used for the prior probabilities. In our model we need specifications for the priors governing the loss ratios for each year, and the growth factors for each subsequent year. These are combined to calculate the ultimate losses as a growing proportion of the annual premium. We use lognormal priors for to loosely constrain our prior distributions above zero without a fixed upper bound. The growth factors are modeled using either a logistic function over theta and omega at each subsequent year \\(t\\).\n\\[t^{\\omega} /  (t^{\\omega} + \\theta^{\\omega})\\]\nor\n\\[ 1 - e^{-(t/ \\theta)^{\\omega}} \\]\nWe set priors for these time-based parameters too, so that by conditioning on the observed data we may learn and update our beliefs about the likely curvature of the growth as we step through time.\n\n\n\nModel Structure\n\n\nAs we can see here the model has a hierarchical structure, where we have specified a series of priors for each of the features we wish to model. The main call outs are that we model observed cumulative losses ‘loss’ as the likelihood based on the prior parameters of an expected loss ratio ‘LR’, against an input dollar premium, and a latent growth factor ‘gf’ which increases over time. These are combined in the final loss calculation:\n\\[ (premium * LR_{j})*gf_{i} \\]\nThe benefits of using a hierarchical structure is that we can capture the variance of the loss ratios across the years, and this uncertainty propagates through our estimates of the growth parameters fleshing out the range of plausible loss curves. The code to put this altogether is below.\n\nparams = {'mu_LR': [0, 0.5], 'sd_LR': [0, 0.5], 'loss_sd': [0, .7], 'omega': [0, .5], 'theta': [0, .5], \n         'tune': 2000, 'target_accept':.9}\n\ndef make_model(model_data, params,  growth_function ='logistic'):   \n    with pm.Model(coords=coords) as basic_model:\n\n        # Priors for unknown model parameters\n        mu_LR = pm.Normal('mu_LR', params['mu_LR'][0],  params['mu_LR'][1]);\n        sd_LR = pm.Lognormal('sd_LR', params['sd_LR'][0], params['sd_LR'][1]);\n\n        LR = pm.Lognormal('LR', mu_LR, sd_LR, dims='cohort')\n\n        loss_sd = pm.Lognormal('loss_sd', params['loss_sd'][0], params['loss_sd'][1]);\n\n        ## Parameters for the growth factor\n        omega = pm.Lognormal('omega', params['omega'][0], params['omega'][1]);\n        theta = pm.Lognormal('theta', params['theta'][0], params['theta'][1]);\n        t = pm.Data(\"t\", t_values, dims='t_values')\n        if growth_function == 'logistic':\n            gf = pm.Deterministic('gf', (t**omega /  (t**omega + theta**omega)), dims='t_values')\n        else:\n            gf = pm.Deterministic('gf', 1-(pm.math.exp(-(t/theta)**omega)), dims='t_values')\n        ## Premium\n        prem = pm.Data(\"premium\", premium, dims='cohort')\n\n        t_indx = pm.Data(\"t_idx\", t_idx, dims='obs')\n        cohort_idx = pm.Data('c_idx', cohort_id, dims='obs')\n        lm = pm.Deterministic('lm', LR[cohort_idx] * prem[cohort_idx] *gf[t_indx], dims=('obs'))\n\n        # Likelihood (sampling distribution) of observations\n        loss = pm.Normal('loss', lm, (loss_sd * prem[cohort_idx]), observed=loss_real, dims='obs')\n\n        prior_checks = pm.sample_prior_predictive(samples=100, random_seed=100)\n        idata = az.from_pymc3(prior=prior_checks)\n\n        trace = pm.sample(return_inferencedata=True, tune=params['tune'], init=\"adapt_diag\", \n                          target_accept=params['target_accept'])\n        idata.extend(trace)\n        ppc = pm.sample_posterior_predictive(\n            trace, var_names=[\"loss\", \"LR\", \"lm\"], random_seed=100)\n        ppc = az.from_pymc3(posterior_predictive=ppc)\n        idata.extend(ppc)\n        \n    return basic_model, idata\n\nRunning these models generates enough sample data to plot some posterior predictive checks and test that our model fits well with the observed data. But crucially we also see a range of plausible curves. These are the probable range of alternatives which can be used to hedge against a range of losses rather than merely expected losses.\n\n\n\nPosterior Predictive Checks\n\n\nFundamentally we want to model the shape of the curve and our question remains which candidate model is better? Which should we choose to reserve enough premium against future costs?\n\nModel Comparison\nWe’ve seen above how likelihood ratio tests may be used to assess how each model fits the observed data. But it is quite another to determine how the model generalises outside of sample. There are a number complexities to computing model comparisons for Bayesian models, but the state of the art relies on information theoretic measures such as AIC, BIC and WAIC or Leave one out cross-validation methods. Information Criteria mostly rely on some comparison of likelihood as described above, but with some additional controls for the complexity of each model. The cross validation methods asseses predictive fit by successively partitioning the data into training and test sets where the accuracy of the fit is assessed by predicting on the test set. The details of the computation are a little complex but Aki Vehtari has a nice tutorial here. The output of the LOO test ranks each model according to their relative accuracy.\n\n\n\n\n\n\n\nrank\n\n\nloo\n\n\np_loo\n\n\nd_loo\n\n\nweight\n\n\nse\n\n\ndse\n\n\nwarning\n\n\nloo_scale\n\n\n\n\n\n\nWeibull Growth Model\n\n\n0\n\n\n-387.915\n\n\n10.6734\n\n\n0\n\n\n0.811983\n\n\n11.1849\n\n\n0\n\n\nTrue\n\n\nlog\n\n\n\n\nLogistic Growth Model\n\n\n1\n\n\n-391.599\n\n\n10.2344\n\n\n3.68346\n\n\n0.188017\n\n\n11.874\n\n\n3.81681\n\n\nTrue\n\n\nlog\n\n\n\n\n\n\n\n\nModel Comparison\n\n\nIn our case we can see that the Weibull growth model is deemed slightly more accurate than the logistic model. Allowing us to lean on criteria of predictive accuracy in choosing our model specification."
  },
  {
    "objectID": "posts/post-with-code/bayesian_model_comparison/bayes_decision.html#conclusion-confidence-in-point-estimates-are-unearned.",
    "href": "posts/post-with-code/bayesian_model_comparison/bayes_decision.html#conclusion-confidence-in-point-estimates-are-unearned.",
    "title": "Bayesian Decisions & Model Comparison",
    "section": "Conclusion: Confidence in Point Estimates are Unearned.",
    "text": "Conclusion: Confidence in Point Estimates are Unearned.\nThis discussion serves to show the wide applicability of Bayesian decision theory and its implicit role in many of the typical frequentist decision rules and model comparison tests. The role and flexibility of the priors in those decision rules suggests that it’s better to be aware of the specification of the prior rather than assume their use is innocent. More positively the Bayesian approach to decision making benefits from our ability to sample directly from the posterior and evaluate probable loss across an entire range of the probability distribution. The gains of this perspective are a clarity and confidence that cannot come from decisions made on the comparison of simple point estimates.\nMore concretely, we’ve seen an explicit model of loss-curve generation where we’ve specified the priors and constraints as appropriate and varied the parameters to test and evaluate competing theories in an transparent probabilisitc framework. This is not a black box model or an obscure algorithm, but a chain of reasoning embedded in code. Any decision worth making is worth the effort required to trace it’s dimensions thouroughly. The value of the exercise stems as much from complexities and pitfalls discovered in the modelling process as the ultimate the decision it enables. There is then something cheap about an alternative model which fails to capture the richness of the data generating process and settles for a simple point estimate comparison. Perhaps we’re guilty of a sunk-costs fallacy - valuing the model for the effort it took to make rather than it’s genuine usefulness, but at least Bayesian models enforce awareness of choices made and assumptions accepted. The tangled network-graph of influence and dependencies traces out a series of discoveries. Our conclusions rest on our best understanding of the process and our decisions are justified in light of the full range of each candidate probability distribution and our explanations can be traced back through the web of these beliefs."
  },
  {
    "objectID": "posts/post-with-code/bayesian_model_comparison/Hierarchical_Claims.html",
    "href": "posts/post-with-code/bayesian_model_comparison/Hierarchical_Claims.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "import numpy as np \nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport pymc3 as pm\nimport arviz as az\nimport theano.tensor as tt\nRANDOM_SEED = 13\n\n\nusetable = pd.read_csv('../data_files/insurace_res_usedata_tbl.csv')\nprint(usetable.columns)\nn_data = len(usetable)\nn_time = len(usetable['dev_year'].unique())\nn_cohort = len(usetable['acc_year'].unique())\ncohort_id, cohort = pd.factorize(usetable['acc_year'], sort=True)\ncohort_maxtime = usetable.groupby('acc_year')['dev_lag'].max().to_list()\nt_values = list(np.sort(usetable['dev_lag'].unique()).astype(int))\nt_idx, _  = pd.factorize(usetable['dev_lag'], sort=True)\npremium = usetable.groupby('acc_year')['premium'].mean().to_list()\nloss_real = usetable['cum_loss']\ncoords = {\"t_idx\": t_idx, \"cohort\": cohort, \"t_values\": t_values, 'obs': range(n_data),'cohort_id':cohort_id}\n\nIndex(['Unnamed: 0', 'grcode', 'grname', 'acc_year', 'dev_year', 'dev_lag',\n       'premium', 'cum_loss', 'loss_ratio'],\n      dtype='object')\n\n\n\nparams = {'mu_LR': [0, 0.5], 'sd_LR': [0, 0.5], 'loss_sd': [0, .7], 'omega': [0, .5], 'theta': [0, .5], \n         'tune': 2000, 'target_accept':.9}\n\ndef make_model(model_data, params,  growth_function ='logistic'):   \n    with pm.Model(coords=coords) as basic_model:\n\n        # Priors for unknown model parameters\n        mu_LR = pm.Normal('mu_LR', params['mu_LR'][0],  params['mu_LR'][1]);\n        sd_LR = pm.Lognormal('sd_LR', params['sd_LR'][0], params['sd_LR'][1]);\n\n        LR = pm.Lognormal('LR', mu_LR, sd_LR, dims='cohort')\n\n        loss_sd = pm.Lognormal('loss_sd', params['loss_sd'][0], params['loss_sd'][1]);\n\n        ## Parameters for the growth factor\n        omega = pm.Lognormal('omega', params['omega'][0], params['omega'][1]);\n        theta = pm.Lognormal('theta', params['theta'][0], params['theta'][1]);\n        \n        t = pm.Data(\"t\", t_values, dims='t_values')\n        if growth_function == 'logistic':\n            gf = pm.Deterministic('gf', (t**omega /  (t**omega + theta**omega)), dims='t_values')\n        else:\n            gf = pm.Deterministic('gf', 1-(pm.math.exp(-(t/theta)**omega)), dims='t_values')\n        ## Premium\n        prem = pm.Data(\"premium\", premium, dims='cohort')\n\n        t_indx = pm.Data(\"t_idx\", t_idx, dims='obs')\n        cohort_idx = pm.Data('c_idx', cohort_id, dims='obs')\n        obs = pm.Data('obs_idx', range(n_data), dims='obs')\n        lm = pm.Deterministic('lm', LR[cohort_idx] * prem[cohort_idx] *gf[t_indx], dims=('obs'))\n        #max_loss = pm.Deterministic('max_loss', tt.max(lm[t_idx]))\n\n        # Likelihood (sampling distribution) of observations\n        loss = pm.Normal('loss', lm, loss_sd * prem[cohort_idx], observed=loss_real, dims='obs')\n\n        prior_checks = pm.sample_prior_predictive(samples=100, random_seed=100)\n\n        trace = pm.sample(tune=params['tune'], init=\"adapt_diag\", \n                          target_accept=params['target_accept'])\n        ppc = pm.sample_posterior_predictive(trace, var_names=[\"loss\", \"LR\", \"lm\"], random_seed=100)\n\n        idata = az.from_pymc3(prior=prior_checks, posterior_predictive=ppc, trace=trace)\n        \n    return basic_model, idata, trace\n\n\nlogistic_model, logistic_idata, logistic_trace = make_model(usetable, params)\n#weibull_model, weibull_idata, weibull_trace = make_model(usetable, params, 'weibull')\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [theta, omega, loss_sd, LR, sd_LR, mu_LR]\n\n\n\n    \n        \n      \n      100.00% [6000/6000 00:11<00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 2_000 tune and 1_000 draw iterations (4_000 + 2_000 draws total) took 11 seconds.\nThe number of effective samples is smaller than 25% for some parameters.\n\n\n\n    \n        \n      \n      100.00% [2000/2000 00:03<00:00]\n    \n    \n\n\n\naz.hdi(logistic_idata, var_names='lm').to_dataframe()\n\n\n\n\n\n  \n    \n      \n      \n      lm\n    \n    \n      hdi\n      obs\n      \n    \n  \n  \n    \n      lower\n      0\n      144.536642\n    \n    \n      1\n      345.880432\n    \n    \n      2\n      460.309382\n    \n    \n      3\n      522.841780\n    \n    \n      4\n      558.101366\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      higher\n      50\n      26942.142200\n    \n    \n      51\n      35365.145610\n    \n    \n      52\n      11878.581162\n    \n    \n      53\n      27013.067386\n    \n    \n      54\n      15285.784189\n    \n  \n\n110 rows × 1 columns\n\n\n\n\naxes = az.plot_forest(logistic_idata,\n                           kind='ridgeplot',\n                           var_names=['gf'],\n                           combined=True,\n                           ridgeplot_overlap=3,\n                           colors='white',\n                           figsize=(9, 7))\naxes[0].set_title('Loss Ratio Plots')\n\nText(0.5, 1.0, 'Loss Ratio Plots')\n\n\n\n\n\n\ng = pm.model_to_graphviz(logistic_model)\ng\n\n\n\n\n\nlogistic_idata\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:   (chain: 2, cohort: 10, draw: 1000, obs: 55, t_values: 10)\nCoordinates:\n  * chain     (chain) int64 0 1\n  * draw      (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * cohort    (cohort) int64 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\n  * t_values  (t_values) int64 1 2 3 4 5 6 7 8 9 10\n  * obs       (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    mu_LR     (chain, draw) float64 -0.187 -0.09882 ... -0.03967 0.007514\n    sd_LR     (chain, draw) float64 0.6209 0.3373 0.2758 ... 0.2711 0.2706\n    LR        (chain, draw, cohort) float64 0.6702 0.8334 ... 0.8889 0.7807\n    loss_sd   (chain, draw) float64 0.03186 0.02677 0.03494 ... 0.0332 0.02944\n    omega     (chain, draw) float64 2.047 2.063 2.117 ... 1.928 1.942 1.925\n    theta     (chain, draw) float64 1.73 1.75 1.699 1.712 ... 1.81 1.803 1.801\n    gf        (chain, draw, t_values) float64 0.2457 0.5738 ... 0.9568 0.9644\n    lm        (chain, draw, obs) float64 157.6 368.0 ... 2.567e+04 1.084e+04\nAttributes:\n    created_at:                 2021-09-15T19:52:34.368634\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2\n    sampling_time:              11.485157012939453\n    tuning_steps:               2000xarray.DatasetDimensions:chain: 2cohort: 10draw: 1000obs: 55t_values: 10Coordinates: (5)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])t_values(t_values)int641 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (8)mu_LR(chain, draw)float64-0.187 -0.09882 ... 0.007514array([[-0.18696659, -0.0988205 , -0.00810847, ...,  0.05311949,\n         0.20880435, -0.01687328],\n       [-0.11414088, -0.15425932, -0.22073582, ...,  0.0246603 ,\n        -0.03966546,  0.00751418]])sd_LR(chain, draw)float640.6209 0.3373 ... 0.2711 0.2706array([[0.62089146, 0.33727945, 0.27581849, ..., 0.42592517, 0.45086283,\n        0.48777715],\n       [0.32289577, 0.44645476, 0.49395078, ..., 0.32981233, 0.27107624,\n        0.27061255]])LR(chain, draw, cohort)float640.6702 0.8334 ... 0.8889 0.7807array([[[0.67022333, 0.83343362, 1.50476005, ..., 0.84746063,\n         0.75324862, 0.97680847],\n        [0.67852191, 0.85033978, 1.5128585 , ..., 0.8348323 ,\n         0.82923175, 0.78441512],\n        [0.65084057, 0.81301633, 1.50223526, ..., 0.86576549,\n         0.81385395, 1.1107245 ],\n        ...,\n        [0.69134537, 0.87360691, 1.53672948, ..., 0.88517673,\n         0.80569189, 0.80970752],\n        [0.67295471, 0.8505383 , 1.55445959, ..., 0.9003714 ,\n         0.81393564, 0.9427412 ],\n        [0.68806353, 0.85636222, 1.52987837, ..., 0.84520793,\n         0.77860489, 0.83172496]],\n\n       [[0.68118037, 0.84469149, 1.52453007, ..., 0.89498748,\n         0.80324074, 1.07317632],\n        [0.66690657, 0.85809314, 1.53552822, ..., 0.83513642,\n         0.8337994 , 0.70777644],\n        [0.69604056, 0.83549505, 1.52540707, ..., 0.85311843,\n         0.83007865, 0.58221531],\n        ...,\n        [0.66498216, 0.85115548, 1.55624145, ..., 0.84190874,\n         0.84255311, 0.92859606],\n        [0.67333019, 0.82537763, 1.546271  , ..., 0.88271328,\n         0.97106623, 0.93680019],\n        [0.66692557, 0.85914168, 1.55441355, ..., 0.88582965,\n         0.88886908, 0.78072028]]])loss_sd(chain, draw)float640.03186 0.02677 ... 0.0332 0.02944array([[0.03186241, 0.02677491, 0.03494346, ..., 0.02950565, 0.03100382,\n        0.0271407 ],\n       [0.02958521, 0.03073347, 0.03223621, ..., 0.03685306, 0.03319626,\n        0.02944148]])omega(chain, draw)float642.047 2.063 2.117 ... 1.942 1.925array([[2.04684299, 2.06348333, 2.11704502, ..., 2.02350399, 1.90782721,\n        2.02067407],\n       [2.10779663, 1.94099992, 2.01335649, ..., 1.92760786, 1.94155975,\n        1.92504071]])theta(chain, draw)float641.73 1.75 1.699 ... 1.803 1.801array([[1.72969432, 1.75027019, 1.6993544 , ..., 1.82016814, 1.74772066,\n        1.78329374],\n       [1.76581379, 1.75222828, 1.76108597, ..., 1.80953151, 1.80345161,\n        1.80096094]])gf(chain, draw, t_values)float640.2457 0.5738 ... 0.9568 0.9644array([[[0.24572263, 0.57375952, 0.75530888, ..., 0.95830295,\n         0.96694015, 0.97318136],\n        [0.23956308, 0.56837426, 0.75248162, ..., 0.9583462 ,\n         0.96703728, 0.97330626],\n        [0.24553678, 0.58537157, 0.76910278, ..., 0.9637262 ,\n         0.9715035 , 0.9770701 ],\n        ...,\n        [0.22935886, 0.54751893, 0.73323759, ..., 0.95238508,\n         0.96209965, 0.96915217],\n        [0.2563239 , 0.56395803, 0.73707071, ..., 0.94794775,\n         0.95798322, 0.96536932],\n        [0.23705689, 0.55767741, 0.74097726, ..., 0.95404246,\n         0.96342013, 0.97022592]],\n\n       [[0.23173901, 0.56524966, 0.75345663, ..., 0.96024765,\n         0.96871358, 0.9747871 ],\n        [0.25186615, 0.56382847, 0.73956758, ..., 0.95014575,\n         0.95992601, 0.96709377],\n        [0.24242684, 0.56368527, 0.74506788, ..., 0.95466309,\n         0.96388892, 0.97058815],\n        ...,\n        [0.24173286, 0.54807935, 0.72601378, ..., 0.94609631,\n         0.95656922, 0.96426648],\n        [0.24141408, 0.55004295, 0.72870832, ..., 0.94747034,\n         0.95775477, 0.96529882],\n        [0.24369253, 0.55027846, 0.72757083, ..., 0.94636716,\n         0.95677735, 0.96442982]]])lm(chain, draw, obs)float64157.6 368.0 ... 2.567e+04 1.084e+04array([[[  157.60741315,   368.01149881,   484.45793209, ...,\n          9713.72058482, 22681.4259549 , 13676.08451242],\n        [  155.55919791,   369.07124816,   488.62052768, ...,\n         10425.52435542, 24735.02909624, 10707.12910397],\n        [  152.93367252,   364.60127339,   479.03907065, ...,\n         10487.33492388, 25002.31377805, 15539.25209443],\n        ...,\n        [  151.74783701,   362.24811303,   485.12283192, ...,\n          9698.09893366, 23151.02546387, 10581.58893866],\n        [  165.07711826,   363.19893028,   474.68655257, ...,\n         10949.17187914, 24090.11955022, 13768.56860626],\n        [  156.09646279,   367.21763655,   487.91633048, ...,\n          9686.61005323, 22787.79407511, 11234.13198617]],\n\n       [[  151.0682531 ,   368.4803803 ,   491.1705532 , ...,\n          9768.92945857, 23828.02983572, 14170.24714991],\n        [  160.74842881,   359.85201422,   472.01390953, ...,\n         11021.31677114, 24672.35959206, 10157.1789823 ],\n        [  161.48314057,   375.47685914,   496.29778013, ...,\n         10560.92705732, 24556.02304463,  8042.1379405 ],\n        ...,\n        [  153.83587622,   348.79108035,   462.02640537, ...,\n         10688.95096964, 24234.98892586, 12789.97614652],\n        [  155.56168039,   354.43502325,   469.5628732 , ...,\n         12303.07167812, 28031.57876309, 12885.95974292],\n        [  155.53621549,   351.21400257,   464.37046077, ...,\n         11367.94919334, 25669.79609618, 10840.38943469]]])Attributes: (6)created_at :2021-09-15T19:52:34.368634arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2sampling_time :11.485157012939453tuning_steps :2000\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 2, cohort: 10, draw: 1000, obs: 55)\nCoordinates:\n  * chain    (chain) int64 0 1\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\n  * cohort   (cohort) int64 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\nData variables:\n    loss     (chain, draw, obs) float64 104.3 378.5 ... 2.644e+04 9.016e+03\n    LR       (chain, draw, cohort) float64 0.6702 0.8334 1.505 ... 0.8889 0.7807\n    lm       (chain, draw, obs) float64 157.6 368.0 ... 2.567e+04 1.084e+04\nAttributes:\n    created_at:                 2021-09-15T19:52:34.700411\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 2cohort: 10draw: 1000obs: 55Coordinates: (4)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])Data variables: (3)loss(chain, draw, obs)float64104.3 378.5 ... 2.644e+04 9.016e+03array([[[  104.25299878,   378.46062043,   519.61667247, ...,\n          8141.80268618, 21296.98149285, 13873.7213711 ],\n        [  168.57110391,   346.97788603,   520.63643116, ...,\n         10456.73879506, 25180.98640336,  9559.26030343],\n        [  109.58118079,   367.78282075,   464.86966193, ...,\n         11868.35118362, 22050.81611823, 19408.3032024 ],\n        ...,\n        [  106.27795502,   318.52680823,   510.43588738, ...,\n         11021.18578426, 22826.81417456, 12527.08543234],\n        [  171.64762377,   365.27633379,   457.72868241, ...,\n         12032.70208003, 22850.73831518, 12817.78476422],\n        [  138.78409136,   360.61093994,   480.48285626, ...,\n         10137.62921603, 20979.07259156, 13465.6251659 ]],\n\n       [[   97.16103159,   355.99598682,   512.81621348, ...,\n         11204.55137772, 23671.12813507, 12157.79410608],\n        [  131.26783254,   355.3390368 ,   460.1770658 , ...,\n         10682.5367999 , 23307.75834914,  8833.09949403],\n        [  194.26536002,   369.60172602,   492.15711635, ...,\n         10816.10853934, 23849.85945329,  5220.66454384],\n        ...,\n        [  103.44871872,   337.44440949,   461.41297666, ...,\n         10020.64412924, 24559.0585406 , 15059.02844165],\n        [  154.30464618,   327.38538816,   474.78895535, ...,\n         15633.51420392, 24639.37731399, 14192.66494558],\n        [  132.40295851,   395.31181651,   455.54853151, ...,\n         10602.40295703, 26440.182822  ,  9015.82177028]]])LR(chain, draw, cohort)float640.6702 0.8334 ... 0.8889 0.7807array([[[0.67022333, 0.83343362, 1.50476005, ..., 0.84746063,\n         0.75324862, 0.97680847],\n        [0.67852191, 0.85033978, 1.5128585 , ..., 0.8348323 ,\n         0.82923175, 0.78441512],\n        [0.65084057, 0.81301633, 1.50223526, ..., 0.86576549,\n         0.81385395, 1.1107245 ],\n        ...,\n        [0.69134537, 0.87360691, 1.53672948, ..., 0.88517673,\n         0.80569189, 0.80970752],\n        [0.67295471, 0.8505383 , 1.55445959, ..., 0.9003714 ,\n         0.81393564, 0.9427412 ],\n        [0.68806353, 0.85636222, 1.52987837, ..., 0.84520793,\n         0.77860489, 0.83172496]],\n\n       [[0.68118037, 0.84469149, 1.52453007, ..., 0.89498748,\n         0.80324074, 1.07317632],\n        [0.66690657, 0.85809314, 1.53552822, ..., 0.83513642,\n         0.8337994 , 0.70777644],\n        [0.69604056, 0.83549505, 1.52540707, ..., 0.85311843,\n         0.83007865, 0.58221531],\n        ...,\n        [0.66498216, 0.85115548, 1.55624145, ..., 0.84190874,\n         0.84255311, 0.92859606],\n        [0.67333019, 0.82537763, 1.546271  , ..., 0.88271328,\n         0.97106623, 0.93680019],\n        [0.66692557, 0.85914168, 1.55441355, ..., 0.88582965,\n         0.88886908, 0.78072028]]])lm(chain, draw, obs)float64157.6 368.0 ... 2.567e+04 1.084e+04array([[[  157.60741315,   368.01149881,   484.45793209, ...,\n          9713.72058482, 22681.4259549 , 13676.08451242],\n        [  155.55919791,   369.07124816,   488.62052768, ...,\n         10425.52435542, 24735.02909624, 10707.12910397],\n        [  152.93367252,   364.60127339,   479.03907065, ...,\n         10487.33492388, 25002.31377805, 15539.25209443],\n        ...,\n        [  151.74783701,   362.24811303,   485.12283192, ...,\n          9698.09893366, 23151.02546387, 10581.58893866],\n        [  165.07711826,   363.19893028,   474.68655257, ...,\n         10949.17187914, 24090.11955022, 13768.56860626],\n        [  156.09646279,   367.21763655,   487.91633048, ...,\n          9686.61005323, 22787.79407511, 11234.13198617]],\n\n       [[  151.0682531 ,   368.4803803 ,   491.1705532 , ...,\n          9768.92945857, 23828.02983572, 14170.24714991],\n        [  160.74842881,   359.85201422,   472.01390953, ...,\n         11021.31677114, 24672.35959206, 10157.1789823 ],\n        [  161.48314057,   375.47685914,   496.29778013, ...,\n         10560.92705732, 24556.02304463,  8042.1379405 ],\n        ...,\n        [  153.83587622,   348.79108035,   462.02640537, ...,\n         10688.95096964, 24234.98892586, 12789.97614652],\n        [  155.56168039,   354.43502325,   469.5628732 , ...,\n         12303.07167812, 28031.57876309, 12885.95974292],\n        [  155.53621549,   351.21400257,   464.37046077, ...,\n         11367.94919334, 25669.79609618, 10840.38943469]]])Attributes: (4)created_at :2021-09-15T19:52:34.700411arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 2, draw: 1000, obs: 55)\nCoordinates:\n  * chain    (chain) int64 0 1\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    loss     (chain, draw, obs) float64 -4.662 -4.996 -5.873 ... -9.298 -8.718\nAttributes:\n    created_at:                 2021-09-15T19:52:34.697576\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 2draw: 1000obs: 55Coordinates: (3)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (1)loss(chain, draw, obs)float64-4.662 -4.996 ... -9.298 -8.718array([[[ -4.66204085,  -4.99560314,  -5.8731991 , ...,  -9.83460231,\n          -8.44534821,  -8.71364938],\n        [ -4.55001055,  -5.15331207,  -6.69084417, ...,  -9.36860769,\n          -8.58761574,  -8.78868814],\n        [ -4.6063776 ,  -4.87522004,  -5.46053709, ...,  -9.09921933,\n          -8.79322533,  -9.84534805],\n        ...,\n        [ -4.47998193,  -4.79602114,  -6.09651578, ..., -10.0247998 ,\n          -8.28211525,  -8.86372827],\n        [ -4.89349193,  -4.82706018,  -5.39305197, ...,  -8.8306795 ,\n          -8.39185554,  -8.74504146],\n        [ -4.57138313,  -5.04378999,  -6.57694207, ..., -10.27797608,\n          -8.28719365,  -8.49661696]],\n\n       [[ -4.46588533,  -5.04744865,  -6.52047369, ...,  -9.93369734,\n          -8.29693424,  -8.96962124],\n        [ -4.74537964,  -4.71708996,  -5.27260529, ...,  -8.7861692 ,\n          -8.59379556,  -9.13006987],\n        [ -4.7742971 ,  -5.29597798,  -6.5881131 , ...,  -9.0816783 ,\n          -8.56772943, -11.11150275],\n        ...,\n        [ -4.65643627,  -4.58216115,  -4.86888165, ...,  -8.97653424,\n          -8.5695356 ,  -8.59666319],\n        [ -4.62960387,  -4.60504662,  -5.11415089, ...,  -8.396745  ,\n         -11.84581071,  -8.51334596],\n        [ -4.5772725 ,  -4.46633895,  -4.95876778, ...,  -8.58177161,\n          -9.29750017,  -8.71840896]]])Attributes: (4)created_at :2021-09-15T19:52:34.697576arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:           (chain: 2, draw: 1000)\nCoordinates:\n  * chain             (chain) int64 0 1\n  * draw              (draw) int64 0 1 2 3 4 5 6 ... 993 994 995 996 997 998 999\nData variables:\n    energy            (chain, draw) float64 418.4 411.7 410.1 ... 411.3 415.6\n    tree_size         (chain, draw) float64 7.0 7.0 7.0 7.0 ... 7.0 7.0 7.0 7.0\n    step_size         (chain, draw) float64 0.3709 0.3709 ... 0.3642 0.3642\n    mean_tree_accept  (chain, draw) float64 0.9878 1.0 0.6001 ... 0.9535 0.9023\n    step_size_bar     (chain, draw) float64 0.4186 0.4186 ... 0.4023 0.4023\n    max_energy_error  (chain, draw) float64 -0.3033 -0.9073 ... 0.1527 0.2162\n    diverging         (chain, draw) bool False False False ... False False False\n    energy_error      (chain, draw) float64 -0.08659 -0.9073 ... 0.05396 0.01052\n    depth             (chain, draw) int64 3 3 3 3 3 3 3 3 4 ... 4 3 3 4 3 3 3 3\n    lp                (chain, draw) float64 -409.4 -398.4 ... -405.5 -405.0\nAttributes:\n    created_at:                 2021-09-15T19:52:34.379572\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2\n    sampling_time:              11.485157012939453\n    tuning_steps:               2000xarray.DatasetDimensions:chain: 2draw: 1000Coordinates: (2)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (10)energy(chain, draw)float64418.4 411.7 410.1 ... 411.3 415.6array([[418.43883273, 411.65728711, 410.0767806 , ..., 410.51405687,\n        408.58036529, 407.35921943],\n       [412.80822536, 411.47900652, 413.17891412, ..., 411.18710152,\n        411.29444508, 415.58466124]])tree_size(chain, draw)float647.0 7.0 7.0 7.0 ... 7.0 7.0 7.0 7.0array([[ 7.,  7.,  7., ...,  7.,  7.,  7.],\n       [ 7., 15.,  7., ...,  7.,  7.,  7.]])step_size(chain, draw)float640.3709 0.3709 ... 0.3642 0.3642array([[0.37085185, 0.37085185, 0.37085185, ..., 0.37085185, 0.37085185,\n        0.37085185],\n       [0.36415012, 0.36415012, 0.36415012, ..., 0.36415012, 0.36415012,\n        0.36415012]])mean_tree_accept(chain, draw)float640.9878 1.0 0.6001 ... 0.9535 0.9023array([[0.98779061, 1.        , 0.60011115, ..., 0.93739839, 0.98545547,\n        0.9950132 ],\n       [0.43950928, 0.89151176, 0.84991465, ..., 0.90923646, 0.95350635,\n        0.90231548]])step_size_bar(chain, draw)float640.4186 0.4186 ... 0.4023 0.4023array([[0.41864123, 0.41864123, 0.41864123, ..., 0.41864123, 0.41864123,\n        0.41864123],\n       [0.40234341, 0.40234341, 0.40234341, ..., 0.40234341, 0.40234341,\n        0.40234341]])max_energy_error(chain, draw)float64-0.3033 -0.9073 ... 0.1527 0.2162array([[-0.30333558, -0.90728229,  0.69568241, ..., -0.31374491,\n        -0.2200231 , -0.40118408],\n       [ 1.42927712,  0.33138796,  0.36049719, ...,  0.18928538,\n         0.15266867,  0.21622557]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])energy_error(chain, draw)float64-0.08659 -0.9073 ... 0.01052array([[-0.08658624, -0.90728229,  0.52606004, ..., -0.05300835,\n         0.02450066, -0.40118408],\n       [ 0.38322751,  0.00231922,  0.16376489, ...,  0.12195152,\n         0.05395713,  0.01051789]])depth(chain, draw)int643 3 3 3 3 3 3 3 ... 4 3 3 4 3 3 3 3array([[3, 3, 3, ..., 3, 3, 3],\n       [3, 4, 3, ..., 3, 3, 3]])lp(chain, draw)float64-409.4 -398.4 ... -405.5 -405.0array([[-409.43533657, -398.44740163, -404.77855576, ..., -402.57284061,\n        -403.52737457, -400.50675223],\n       [-401.88597959, -402.44777691, -407.25696595, ..., -404.72076691,\n        -405.45093988, -404.98509764]])Attributes: (6)created_at :2021-09-15T19:52:34.379572arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2sampling_time :11.485157012939453tuning_steps :2000\n                      \n                  \n            \n            \n            \n                  \n                  prior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:         (LR_log___dim_0: 10, chain: 1, cohort: 10, draw: 100, obs: 55, t_values: 10)\nCoordinates:\n  * chain           (chain) int64 0\n  * draw            (draw) int64 0 1 2 3 4 5 6 7 8 ... 92 93 94 95 96 97 98 99\n  * LR_log___dim_0  (LR_log___dim_0) int64 0 1 2 3 4 5 6 7 8 9\n  * cohort          (cohort) int64 1988 1989 1990 1991 ... 1994 1995 1996 1997\n  * obs             (obs) int64 0 1 2 3 4 5 6 7 8 ... 46 47 48 49 50 51 52 53 54\n  * t_values        (t_values) int64 1 2 3 4 5 6 7 8 9 10\nData variables:\n    theta           (chain, draw) float64 0.5739 1.253 3.127 ... 1.24 1.031\n    LR_log__        (chain, draw, LR_log___dim_0) float64 -0.6171 ... 1.319\n    mu_LR           (chain, draw) float64 -0.8749 0.1713 ... -0.09251 -1.244\n    loss_sd_log__   (chain, draw) float64 0.1178 -0.06791 ... -0.3026 -1.538\n    theta_log__     (chain, draw) float64 -0.5553 0.2252 1.14 ... 0.2152 0.0308\n    omega_log__     (chain, draw) float64 0.4461 0.07193 ... 0.1054 -0.7762\n    LR              (chain, draw, cohort) float64 0.5395 0.2832 ... 3.739\n    loss_sd         (chain, draw) float64 1.125 0.9343 0.4603 ... 0.7389 0.2147\n    sd_LR_log__     (chain, draw) float64 -0.8523 -0.5681 -1.487 ... -0.3 0.7881\n    omega           (chain, draw) float64 1.562 1.075 1.892 ... 1.111 0.4602\n    sd_LR           (chain, draw) float64 0.4264 0.5666 0.2261 ... 0.7408 2.199\n    lm              (chain, draw, obs) float64 363.6 452.0 ... 1.058e+05\n    gf              (chain, draw, t_values) float64 0.7042 0.8755 ... 0.7399\nAttributes:\n    created_at:                 2021-09-15T19:52:34.707602\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:LR_log___dim_0: 10chain: 1cohort: 10draw: 100obs: 55t_values: 10Coordinates: (6)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 6 ... 94 95 96 97 98 99array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])LR_log___dim_0(LR_log___dim_0)int640 1 2 3 4 5 6 7 8 9array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])t_values(t_values)int641 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])Data variables: (13)theta(chain, draw)float640.5739 1.253 3.127 ... 1.24 1.031array([[0.57389012, 1.25260178, 3.12725504, 2.33099523, 1.5559494 ,\n        0.7639599 , 0.82833537, 0.80557966, 2.20412348, 0.43069581,\n        2.9093212 , 0.99399548, 0.86455246, 1.32475578, 1.44164061,\n        1.10965243, 1.07620636, 1.29017089, 2.16036625, 2.32360565,\n        0.55518025, 0.90958767, 1.01729211, 0.84623772, 0.82608081,\n        1.62644821, 1.1668848 , 1.25656072, 0.83539401, 0.65453681,\n        1.83275323, 0.46182585, 1.40712532, 1.19493509, 0.94248559,\n        0.93866154, 0.83764564, 2.25087097, 1.27622612, 0.57064121,\n        0.52945567, 0.88550147, 0.75327822, 1.37076844, 0.93745489,\n        1.51501646, 0.62072393, 1.25873035, 2.29427686, 0.68248132,\n        1.06593879, 0.66061023, 0.77861607, 1.41031485, 2.15389213,\n        0.5953411 , 0.56525208, 1.340487  , 0.69754309, 1.16981863,\n        1.33030513, 1.57734588, 0.8597188 , 0.35135396, 0.82764502,\n        0.6132911 , 0.7778497 , 1.74519489, 0.71525789, 0.83759542,\n        2.2658658 , 4.32680955, 1.6694292 , 1.51162302, 0.54751728,\n        1.73666316, 1.22802165, 0.55897645, 0.70077877, 0.67783987,\n        0.70416641, 1.29484782, 0.81285727, 0.7551851 , 1.18750981,\n        2.39590503, 1.30989887, 0.78067317, 1.26049998, 1.12601871,\n        0.74383829, 1.38761336, 1.04839036, 1.80706916, 0.89511397,\n        0.67815156, 1.02856998, 1.19869844, 1.24008379, 1.03128319]])LR_log__(chain, draw, LR_log___dim_0)float64-0.6171 -1.262 ... -3.077 1.319array([[[-6.17143188e-01, -1.26166054e+00, -6.22430937e-01,\n         -1.06125666e+00, -8.31483299e-01, -3.16974749e-01,\n         -2.83178673e-01, -1.61489186e+00, -7.51912234e-01,\n         -1.01947626e+00],\n        [ 4.70138943e-01, -4.07337019e-02,  4.41652909e-01,\n         -6.38739983e-01,  4.02340063e-01, -2.97949948e-01,\n          4.11264185e-01,  8.09602667e-02,  8.74517052e-01,\n          1.11387272e+00],\n        [ 4.26835799e-01,  1.36009991e+00,  6.65032809e-01,\n          6.50516967e-01,  5.06113871e-01,  2.54364564e-01,\n          4.35217322e-01,  5.51644954e-01,  7.92007920e-01,\n          3.54152889e-01],\n        [-1.23916176e+00, -3.71620985e-01, -7.31328680e-01,\n         -1.12553935e+00, -3.65059826e-02, -7.39102641e-01,\n          1.01824470e+00,  8.16408078e-01, -2.43349269e+00,\n         -4.80237285e-01],\n        [ 3.45828028e-01,  1.56488574e+00, -6.28437141e-02,\n          2.47892004e-01,  1.50385087e+00,  1.51489840e+00,\n          1.11068047e+00, -6.43011394e-01,  1.83483899e+00,\n          3.31755049e-01],\n...\n        [ 1.24232366e+00, -1.79165022e+00, -1.05836185e+00,\n         -3.09225403e+00,  6.56009661e-02, -7.50671829e-02,\n          5.89357991e-01,  8.63156384e-01, -2.81532734e-01,\n         -1.24847464e+00],\n        [-6.86571297e-01, -3.02133976e-01,  3.05860017e-01,\n          5.61005282e-04, -1.20440890e-01,  1.16642382e+00,\n         -2.26335350e-01, -9.18702845e-01, -8.41696486e-01,\n         -3.58155741e-01],\n        [ 1.09884268e+00,  1.13119969e+00, -6.78816255e-01,\n         -5.85593856e-01,  7.15943176e-01,  2.24503315e+00,\n         -4.51843600e-01,  3.07867675e-01,  9.31380940e-01,\n         -2.27855222e+00],\n        [-3.37456137e-01,  2.42518433e-01, -4.85528021e-01,\n         -3.01018200e-01, -5.69692766e-01, -9.29658170e-01,\n         -9.53172168e-01,  3.29390486e-02, -1.36732900e-01,\n          7.26723678e-01],\n        [-2.69919207e+00, -2.03199061e+00, -1.40009199e+00,\n         -1.02835591e+00,  2.65043778e+00,  3.31279141e-01,\n         -1.79784476e+00,  2.42504669e+00, -3.07654973e+00,\n          1.31886562e+00]]])mu_LR(chain, draw)float64-0.8749 0.1713 ... -0.09251 -1.244array([[-0.87488274,  0.1713402 ,  0.5765179 , -0.12621802,  0.49066039,\n         0.25710942,  0.11058983, -0.53502167, -0.09474792,  0.12750072,\n        -0.22901349,  0.21758174, -0.29179753,  0.40842354,  0.3363604 ,\n        -0.05220557, -0.26564019,  0.51486634, -0.21906781, -0.55915912,\n         0.80949083,  0.77080259, -0.12593957, -0.42121787,  0.09225935,\n         0.4685411 ,  0.36550017,  0.68077806, -0.16311903,  0.02783801,\n         0.1111998 , -0.7216085 , -0.37817615,  0.40822701,  0.37522238,\n        -0.22797346,  0.59481113, -0.84530841, -0.67819952, -0.61621726,\n        -0.27221958, -0.33408587,  0.00365728, -0.30646937,  0.64987404,\n        -0.86654781, -0.49165505,  0.17875388, -0.80678925,  0.73535693,\n        -0.5940088 , -0.2748731 , -0.47002308, -0.41396618,  0.05443173,\n         0.2539048 , -0.43111367,  0.62473487, -0.03980562, -0.44486574,\n        -0.44089919,  0.00931947,  0.11892231,  0.00677427, -0.8177647 ,\n        -0.52210494,  0.30651944,  0.36810261,  0.51346072, -0.71609531,\n        -0.92059415,  0.18304661, -0.16588857, -0.34460899,  1.01730378,\n        -0.27535721,  0.37522667, -0.65349617,  0.29028667, -0.55226155,\n         0.34506074,  0.34344503, -0.78334376,  0.45248706,  0.3894112 ,\n         0.21411644,  0.05443599,  0.01414182, -0.28941291, -0.5997256 ,\n        -0.852976  ,  0.18458198,  0.93828671, -0.18845168,  0.91596804,\n         0.00150872, -0.03801173,  0.0019788 , -0.09250706, -1.24357577]])loss_sd_log__(chain, draw)float640.1178 -0.06791 ... -0.3026 -1.538array([[ 0.11776897, -0.06791383, -0.77586015,  0.81418801, -1.54295884,\n        -0.08635401, -0.3073733 ,  0.44099133, -0.60868549, -0.42565676,\n         0.83953297,  1.90751457,  0.31553409, -0.58763441, -0.02813683,\n         0.61956292,  1.1102456 ,  1.41804546,  1.07121072,  1.2829134 ,\n         0.67823319,  0.11953965,  0.49559607,  0.91112716,  0.26419822,\n        -0.55153079,  0.64974865,  1.42332632,  0.40688404,  1.0925064 ,\n         0.0515981 , -0.0156497 ,  0.47990747, -0.15405117, -1.44822335,\n         0.44352918, -1.63864786, -0.29389226,  0.71212342, -0.27237528,\n        -0.51608351, -0.00461874,  0.19355207, -0.40968742,  1.59689048,\n         0.80746392,  0.90807538, -0.15603457,  0.24094942, -1.39879746,\n         0.38607813, -0.58429016, -0.56743807,  0.07267915, -0.20197823,\n        -0.65064838,  0.31415215, -0.38924771,  0.91696663, -0.48047711,\n        -0.79327309, -0.11503221,  0.74234383, -0.37075465, -0.29457163,\n        -1.81513457,  0.20137675,  0.56713415,  0.07135635, -0.19635674,\n        -0.4044409 , -0.17024778, -1.04476412,  0.16943208, -0.98912851,\n        -0.66173423,  1.065507  ,  1.40047042, -0.45946529, -0.33103924,\n         1.22237942,  1.01016131, -0.64361443,  0.76310702,  0.76479435,\n        -0.22690514,  0.57659883,  1.36853881, -0.45579136,  0.67822079,\n         0.29416748, -1.19540042, -0.00414685, -0.5183479 ,  0.94649573,\n        -0.45149198, -0.10859997, -0.04805518, -0.302624  , -1.53838435]])theta_log__(chain, draw)float64-0.5553 0.2252 ... 0.2152 0.0308array([[-0.55531734,  0.22522281,  1.14015564,  0.84629531,  0.44208591,\n        -0.26923997, -0.18833717, -0.21619318,  0.79032991, -0.84235321,\n         1.06791979, -0.00602262, -0.1455433 ,  0.28122813,  0.36578178,\n         0.10404684,  0.07344223,  0.25477469,  0.77027777,  0.84312014,\n        -0.58846245, -0.09476389,  0.0171443 , -0.16695497, -0.19106268,\n         0.48639862,  0.15433763,  0.2283784 , -0.17985179, -0.42382746,\n         0.60581933, -0.7725674 ,  0.34154884,  0.17809187, -0.05923465,\n        -0.06330032, -0.17716014,  0.81131724,  0.24390738, -0.56099462,\n        -0.63590583, -0.12160116, -0.28332064,  0.31537149, -0.06458664,\n         0.4154263 , -0.47686886,  0.23010355,  0.8304177 , -0.38202012,\n         0.06385591, -0.41459128, -0.25023721,  0.34381298,  0.7672765 ,\n        -0.51862076, -0.57048348,  0.29303298, -0.360191  ,  0.15684872,\n         0.28540834,  0.45574361, -0.15114992, -1.04596112, -0.18917094,\n        -0.48891559, -0.25122197,  0.55686623, -0.33511211, -0.17722009,\n         0.81795694,  1.46483045,  0.51248177,  0.41318392, -0.60236126,\n         0.55196555,  0.20540446, -0.58164794, -0.35556303, -0.3888442 ,\n        -0.35074057,  0.25839318, -0.20719975, -0.2807924 ,  0.17185852,\n         0.87376104,  0.26994994, -0.24759869,  0.23150845,  0.11868815,\n        -0.29593161,  0.32758526,  0.047256  ,  0.59170629, -0.11080422,\n        -0.38838448,  0.02816947,  0.18123634,  0.21517895,  0.03080384]])omega_log__(chain, draw)float640.4461 0.07193 ... 0.1054 -0.7762array([[ 0.44612894,  0.0719305 ,  0.63779866, -0.23449358,  0.23706241,\n         0.33419761, -0.60235794, -0.42556604,  0.44956226,  0.46859577,\n         0.47679333, -1.10255173, -1.1406529 , -0.30581554,  1.40485057,\n         0.08792286, -0.3153778 ,  0.92777277,  0.08014537, -0.71142477,\n        -0.18382993, -0.94314418,  0.37741105,  0.89061229,  0.32631473,\n         0.1187519 ,  0.29710071, -0.56646717,  0.24871394, -0.12211726,\n         0.52034076, -0.13353552, -0.33864184, -0.33580391,  1.31280722,\n        -0.55840032,  0.46812679, -0.28309119, -0.25267346, -0.43320864,\n         0.69851556, -0.43096099, -0.05757817,  0.00526835, -0.25980818,\n        -0.12478563,  1.12182973, -0.2414983 , -1.18449009, -0.46079041,\n         0.75950166,  0.27624921, -0.34634081, -0.53000413,  0.73357778,\n        -0.08395659,  0.37989795, -0.10431087, -0.02411082, -0.0916873 ,\n         0.64151539,  0.34238158,  0.25391359,  0.20807299,  0.07147848,\n        -0.03081378, -0.15740337,  0.33052872, -0.54743424, -0.17367713,\n         0.6791806 , -0.14902945, -0.04004145,  0.24734065,  0.68666321,\n        -0.46438288, -0.5281403 ,  0.10112753, -0.33912545, -0.01100623,\n        -0.95018485, -0.51196821,  0.10753402, -0.2633491 , -0.26221749,\n         0.2735264 ,  0.05936596, -1.15266121,  0.35999557, -0.64225228,\n         0.67848939,  0.24234314, -0.12805771,  0.67731315,  1.03797529,\n         0.3334401 , -0.50685985,  0.09829753,  0.10543659, -0.77619435]])LR(chain, draw, cohort)float640.5395 0.2832 ... 0.04612 3.739array([[[5.39483441e-01, 2.83183398e-01, 5.36638317e-01, 3.46020707e-01,\n         4.35402974e-01, 7.28349146e-01, 7.53385166e-01, 1.98912181e-01,\n         4.71464140e-01, 3.60783849e-01],\n        [1.60021652e+00, 9.60084765e-01, 1.55527582e+00, 5.27957240e-01,\n         1.49531975e+00, 7.42338495e-01, 1.50872389e+00, 1.08432781e+00,\n         2.39771704e+00, 3.04613240e+00],\n        [1.53240102e+00, 3.89658259e+00, 1.94455432e+00, 1.91653136e+00,\n         1.65883222e+00, 1.28964188e+00, 1.54529885e+00, 1.73610649e+00,\n         2.20782512e+00, 1.42497303e+00],\n        [2.89626892e-01, 6.89615568e-01, 4.81269113e-01, 3.24477413e-01,\n         9.64152326e-01, 4.77542250e-01, 2.76833126e+00, 2.26235901e+00,\n         8.77298833e-02, 6.18636581e-01],\n        [1.41315957e+00, 4.78212851e+00, 9.39090229e-01, 1.28132155e+00,\n         4.49898074e+00, 4.54895891e+00, 3.03642390e+00, 5.25706928e-01,\n         6.26412548e+00, 1.39341149e+00],\n        [9.57696979e-01, 1.86763571e+00, 4.13374817e+00, 1.56821742e+00,\n         1.25487332e+00, 2.51191010e+00, 8.10945988e+00, 6.10722752e-01,\n         1.41395916e+00, 9.82281959e+00],\n        [1.71912712e+00, 1.49080130e-01, 4.37317875e+00, 6.93848752e-01,\n         2.87455238e-01, 2.20250288e+00, 8.11983450e-01, 7.26701892e-01,\n...\n         9.69395468e-01, 6.87721577e+01, 1.36484576e-01, 1.13037865e-01,\n         4.30151712e-02, 2.71561166e+02],\n        [3.40901443e+00, 2.08439147e+00, 6.84379805e+00, 9.55929917e+00,\n         1.77361679e+00, 1.82087509e+01, 4.04976821e+00, 1.13211119e+01,\n         1.86859175e+00, 1.89121247e+00],\n        [3.46365246e+00, 1.66684876e-01, 3.47023822e-01, 4.53995073e-02,\n         1.06780054e+00, 9.27681160e-01, 1.80283061e+00, 2.37063152e+00,\n         7.54626213e-01, 2.86942154e-01],\n        [5.03298776e-01, 7.39239018e-01, 1.35779223e+00, 1.00056116e+00,\n         8.86529488e-01, 3.21049079e+00, 7.97450632e-01, 3.99036318e-01,\n         4.30978753e-01, 6.98964209e-01],\n        [3.00069125e+00, 3.09937257e+00, 5.07217053e-01, 5.56775119e-01,\n         2.04611562e+00, 9.44072847e+00, 6.36453703e-01, 1.36052095e+00,\n         2.53801160e+00, 1.02432399e-01],\n        [7.13583274e-01, 1.27445474e+00, 6.15372182e-01, 7.40064303e-01,\n         5.65699214e-01, 3.94688604e-01, 3.85516160e-01, 1.03348754e+00,\n         8.72203161e-01, 2.06829310e+00],\n        [6.72598317e-02, 1.31074344e-01, 2.46574280e-01, 3.57594394e-01,\n         1.41602364e+01, 1.39274851e+00, 1.65655532e-01, 1.13027571e+01,\n         4.61181024e-02, 3.73917734e+00]]])loss_sd(chain, draw)float641.125 0.9343 ... 0.7389 0.2147array([[1.12498418, 0.93434098, 0.46030768, 2.257342  , 0.21374772,\n        0.91726945, 0.73537603, 1.55424723, 0.54406558, 0.65334056,\n        2.31528541, 6.73632528, 1.37099135, 0.55564015, 0.97225532,\n        1.85811573, 3.03510372, 4.12904219, 2.91891135, 3.60713346,\n        1.97039335, 1.12697793, 1.64147638, 2.48712433, 1.30238633,\n        0.5760673 , 1.91505942, 4.15090475, 1.5021299 , 2.98173813,\n        1.05295248, 0.98447212, 1.61592487, 0.85722816, 0.23498741,\n        1.55819668, 0.19424251, 0.7453568 , 2.03831486, 0.76156841,\n        0.59685355, 0.99539191, 1.21355257, 0.66385773, 4.93765477,\n        2.24221434, 2.47954574, 0.85552962, 1.27245668, 0.24689368,\n        1.47119961, 0.55750146, 0.56697613, 1.07538545, 0.81711272,\n        0.5217074 , 1.36909803, 0.67756641, 2.50169031, 0.61848824,\n        0.45236175, 0.89133743, 2.10085379, 0.69021326, 0.74485059,\n        0.162816  , 1.22308549, 1.76320672, 1.07396387, 0.82171904,\n        0.66734982, 0.8434558 , 0.35177479, 1.18463189, 0.37190066,\n        0.51595577, 2.90231009, 4.05710805, 0.63162129, 0.71817699,\n        3.39525688, 2.74604394, 0.52539   , 2.14493023, 2.14855249,\n        0.79699638, 1.77997412, 3.9296046 , 0.63394609, 1.97036892,\n        1.34200864, 0.30258277, 0.99586174, 0.59550356, 2.57666449,\n        0.63667753, 0.89708921, 0.9530812 , 0.73887686, 0.21472775]])sd_LR_log__(chain, draw)float64-0.8523 -0.5681 ... -0.3 0.7881array([[-0.8523256 , -0.5681305 , -1.48665774,  0.01665864, -0.12444433,\n        -0.22508822,  0.0662139 ,  0.01110696,  0.15868399, -0.37620709,\n        -0.6481959 ,  0.04756972, -0.21185755, -0.59299178, -0.182731  ,\n        -0.63551152,  0.79308547,  0.34669533, -0.97904062, -0.06740066,\n        -0.77030801,  1.02335698, -0.69849967, -0.54858599, -0.11935643,\n        -0.71453345,  0.47450239, -0.00969879,  0.44729885,  0.37984656,\n        -0.74886019, -0.59694299,  0.64813129,  0.47613781, -0.60862707,\n        -0.07863258, -0.75379258,  0.05394207,  0.37352783,  0.21483822,\n        -0.70752146, -0.32037996,  0.38981315, -0.21906046,  1.03739658,\n        -0.17164884, -0.30831469,  0.38159182,  0.0964586 , -0.17422947,\n         1.14932697, -0.08260478,  0.23314968,  0.13499362, -0.15991552,\n        -0.5738708 ,  0.85181199, -0.36107539,  0.54684332, -0.11475888,\n        -0.00444933, -0.271599  ,  0.37653109, -0.80471945,  0.97163113,\n        -0.72371806,  0.06512423,  0.47468043, -1.00759436, -0.03977029,\n         0.15052473, -0.84244998,  0.1111954 , -0.34246087, -0.06310059,\n         0.99513682,  0.2614989 , -0.0081727 , -0.20790817, -0.67925147,\n        -0.25721495, -0.10803006,  0.21119011, -0.54702147,  0.61845394,\n        -0.11514234, -0.3522091 , -0.29568756,  0.36849758,  0.21793363,\n         0.88799679,  0.25653719,  0.58526349,  1.03885612, -0.22796101,\n         0.32458646, -0.08739078,  0.50863217, -0.29999152,  0.78808336]])omega(chain, draw)float641.562 1.075 1.892 ... 1.111 0.4602array([[1.56225289, 1.07458066, 1.89231067, 0.79097131, 1.26752023,\n        1.39681914, 0.5475191 , 0.65339983, 1.56762583, 1.59774901,\n        1.61090048, 0.33202277, 0.31961028, 0.73652247, 4.07491779,\n        1.09190388, 0.72951322, 2.52887052, 1.08344456, 0.49094421,\n        0.83207731, 0.38940156, 1.4585037 , 2.43662111, 1.38585147,\n        1.1260905 , 1.34595085, 0.56752688, 1.28237514, 0.88504458,\n        1.68260091, 0.87499639, 0.71273768, 0.71476325, 3.71659238,\n        0.57212355, 1.59699987, 0.75345108, 0.77672147, 0.64842519,\n        2.01076562, 0.64988426, 0.94404809, 1.00528225, 0.7711995 ,\n        0.8826861 , 3.07046718, 0.78545014, 0.30590212, 0.63078487,\n        2.13721089, 1.31817633, 0.7072714 , 0.58860254, 2.08251808,\n        0.91947117, 1.46213538, 0.90094518, 0.97617753, 0.91239041,\n        1.89935696, 1.40829757, 1.28906041, 1.23130304, 1.07409503,\n        0.96965612, 0.85435936, 1.39170375, 0.57843202, 0.84056825,\n        1.97226099, 0.86154374, 0.96074961, 1.28061528, 1.987074  ,\n        0.62852286, 0.58970061, 1.10641774, 0.71239307, 0.98905411,\n        0.38666954, 0.59931484, 1.11352875, 0.76847358, 0.76934368,\n        1.31459206, 1.06116352, 0.31579525, 1.43332306, 0.52610615,\n        1.97089823, 1.27423136, 0.87980261, 1.96858134, 2.82349447,\n        1.39576144, 0.60238419, 1.103291  , 1.11119564, 0.46015387]])sd_LR(chain, draw)float640.4264 0.5666 ... 0.7408 2.199array([[0.42642209, 0.56658368, 0.22612717, 1.01679817, 0.88298741,\n        0.79844578, 1.06845524, 1.01116888, 1.17196753, 0.68646016,\n        0.52298845, 1.04871932, 0.80907994, 0.55267134, 0.8329922 ,\n        0.52966448, 2.21020544, 1.41438574, 0.37567134, 0.93482058,\n        0.46287048, 2.78251998, 0.4973309 , 0.5777662 , 0.88749141,\n        0.4894204 , 1.60721423, 0.99034809, 1.56408166, 1.46206023,\n        0.47290527, 0.55049193, 1.91196459, 1.60984486, 0.54409737,\n        0.92437949, 0.47057846, 1.05542345, 1.45285099, 1.23966133,\n        0.49286427, 0.72587318, 1.47670485, 0.80327315, 2.82186097,\n        0.84227489, 0.73468409, 1.46461414, 1.10126398, 0.8401041 ,\n        3.15606807, 0.92071496, 1.26257045, 1.14452948, 0.85221578,\n        0.56334063, 2.34389012, 0.69692646, 1.72779033, 0.8915811 ,\n        0.99556055, 0.76215982, 1.45722085, 0.44721338, 2.64225081,\n        0.48494584, 1.0672916 , 1.60750041, 0.36509621, 0.96101016,\n        1.16244405, 0.43065414, 1.11761327, 0.7100209 , 0.93884903,\n        2.70509444, 1.29887552, 0.9918606 , 0.81228163, 0.50699635,\n        0.773202  , 0.89760061, 1.23514715, 0.57867084, 1.85605625,\n        0.89123928, 0.70313308, 0.74401985, 1.44556115, 1.24350453,\n        2.43025646, 1.29244683, 1.79546401, 2.82598257, 0.7961553 ,\n        1.38345842, 0.91631895, 1.66301492, 0.7408245 , 2.19917736]])lm(chain, draw, obs)float64363.6 452.0 ... 1.393e+03 1.058e+05array([[[3.63586344e+02, 4.52004030e+02, 4.80050323e+02, ...,\n         1.74248189e+04, 2.16622228e+04, 1.44767740e+04],\n        [6.73495425e+02, 9.54254126e+02, 1.10077873e+03, ...,\n         5.53406168e+04, 7.84103498e+04, 7.63308220e+04],\n        [1.51973891e+02, 4.40389457e+02, 7.04447107e+02, ...,\n         1.20074665e+04, 3.47951982e+04, 8.41392159e+03],\n        ...,\n        [1.29275537e+03, 1.83085465e+03, 2.10618368e+03, ...,\n         5.99623720e+04, 8.49212392e+04, 2.62740876e+03],\n        [3.00821669e+02, 4.30051269e+02, 4.96767448e+02, ...,\n         2.01637950e+04, 2.88259343e+04, 5.19124828e+04],\n        [3.19557383e+01, 3.70507050e+01, 3.99353603e+01, ...,\n         1.20158548e+03, 1.39316416e+03, 1.05770463e+05]]])gf(chain, draw, t_values)float640.7042 0.8755 ... 0.7304 0.7399array([[[0.70423484, 0.87549214, 0.92981535, 0.95405553, 0.9671339 ,\n         0.97507705, 0.98030598, 0.98395463, 0.98661528, 0.98862353],\n        [0.4397886 , 0.62312239, 0.71880211, 0.77689541, 0.81569534,\n         0.84335026, 0.86401169, 0.88000818, 0.89274339, 0.90311221],\n        [0.10362979, 0.30029807, 0.48035688, 0.61438231, 0.70848302,\n         0.77434966, 0.8212351 , 0.85537809, 0.88082887, 0.90022056],\n        [0.33863225, 0.46975303, 0.54972892, 0.60518664, 0.64648496,\n         0.67870678, 0.70469655, 0.7261912 , 0.74432036, 0.75985477],\n        [0.36346583, 0.57889167, 0.69681155, 0.76795562, 0.81451854,\n         0.84693205, 0.87058348, 0.88848725, 0.90244344, 0.91358541],\n        [0.59292719, 0.79319304, 0.87109128, 0.90990553, 0.93240122,\n         0.94679063, 0.95665138, 0.96376016, 0.96908782, 0.97320487],\n        [0.52575673, 0.6183686 , 0.66921337, 0.70310774, 0.7279638 ,\n         0.74727678, 0.76288122, 0.77585146, 0.78686662, 0.79638106],\n        [0.53525654, 0.64431972, 0.70247126, 0.74021092, 0.76725344,\n         0.78784695, 0.80419428, 0.8175696 , 0.82876883, 0.83831844],\n        [0.22462027, 0.46198695, 0.61852007, 0.71793657, 0.78313921,\n         0.82776462, 0.85954517, 0.88296832, 0.90074093, 0.91456169],\n        [0.79345344, 0.92080566, 0.95694122, 0.9723696 , 0.98049424,\n         0.98535136, 0.98851254, 0.99069905, 0.99228223, 0.99347016],\n...\n        [0.64181514, 0.87537833, 0.93982881, 0.96495661, 0.97714108,\n         0.98393041, 0.98809055, 0.99082103, 0.99270871, 0.99406781],\n        [0.39713443, 0.61439167, 0.72759741, 0.79397269, 0.83663171,\n         0.86595956, 0.88716659, 0.90311019, 0.9154717 , 0.92529784],\n        [0.48960751, 0.63836083, 0.71605695, 0.76460411, 0.798092  ,\n         0.82271025, 0.84163459, 0.85667234, 0.86893193, 0.87913289],\n        [0.23779221, 0.54975842, 0.73064268, 0.82695681, 0.88116276,\n         0.91391499, 0.93498184, 0.94924842, 0.95932411, 0.96668948],\n        [0.57758201, 0.90635774, 0.96816329, 0.98561457, 0.99228692,\n         0.99537608, 0.99700294, 0.99794238, 0.99852365, 0.99890312],\n        [0.63229895, 0.81899602, 0.88849829, 0.92251565, 0.94205031,\n         0.95447765, 0.96296374, 0.96906659, 0.97363229, 0.97715586],\n        [0.49575789, 0.5988251 , 0.65584172, 0.69383563, 0.72162392,\n         0.74314104, 0.76046247, 0.7748066 , 0.78694441, 0.79739179],\n        [0.45017679, 0.63755935, 0.7334373 , 0.79076283, 0.82859779,\n         0.85531065, 0.87511355, 0.89034527, 0.90240364, 0.91217334],\n        [0.4405067 , 0.62974341, 0.72743891, 0.78606014, 0.82481181,\n         0.85219095, 0.87249273, 0.88810812, 0.90046845, 0.91048055],\n        [0.49645643, 0.57561057, 0.62042586, 0.65106736, 0.67401798,\n         0.69217567, 0.70707985, 0.71964241, 0.73044606, 0.73988461]]])Attributes: (4)created_at :2021-09-15T19:52:34.707602arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  prior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 1, draw: 100, obs: 55)\nCoordinates:\n  * chain    (chain) int64 0\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 9 ... 90 91 92 93 94 95 96 97 98 99\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    loss     (chain, draw, obs) float64 -2.671e+03 1.343e+03 ... 9.672e+04\nAttributes:\n    created_at:                 2021-09-15T19:52:34.712755\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 1draw: 100obs: 55Coordinates: (3)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 6 ... 94 95 96 97 98 99array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (1)loss(chain, draw, obs)float64-2.671e+03 1.343e+03 ... 9.672e+04array([[[-2.67100326e+03,  1.34255155e+03,  1.40821615e+03, ...,\n          5.44546338e+04, -2.82037218e+04,  2.25204026e+04],\n        [ 6.91120357e+02,  1.25047752e+03,  2.34623536e+03, ...,\n          3.67907448e+04,  1.27985416e+05,  4.26089822e+04],\n        [-1.05994085e+02,  7.82732937e+02,  3.65909060e+02, ...,\n         -1.07724563e+04,  2.61310680e+04,  2.36614675e+04],\n        ...,\n        [ 8.83043472e+02,  2.15344855e+03,  3.03744893e+03, ...,\n          4.12138121e+04, -2.16126888e+04,  5.72089335e+03],\n        [ 1.13652047e+03,  1.26135401e+03,  1.58322237e+03, ...,\n         -4.63703410e+04,  6.47498746e+04,  1.04504263e+05],\n        [ 1.67803663e+02,  2.66545568e+02, -9.96214671e+01, ...,\n         -2.17424973e+04, -9.03439475e+03,  9.67249413e+04]]])Attributes: (4)created_at :2021-09-15T19:52:34.712755arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (obs: 55)\nCoordinates:\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    loss     (obs) int32 133 333 431 570 615 ... 26012 31677 12604 23446 12292\nAttributes:\n    created_at:                 2021-09-15T19:52:34.713725\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:obs: 55Coordinates: (1)obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (1)loss(obs)int32133 333 431 ... 12604 23446 12292array([  133,   333,   431,   570,   615,   615,   615,   614,   614,\n         614,   934,  1746,  2365,  2579,  2763,  2966,  2940,  2978,\n        2978,  2030,  4864,  6880,  8087,  8595,  8743,  8763,  8762,\n        4537, 11527, 15123, 16656, 17321, 18076, 18308,  7564, 16061,\n       22465, 25204, 26517, 27124,  8343, 19900, 26732, 30079, 31249,\n       12565, 26922, 33867, 38338, 13437, 26012, 31677, 12604, 23446,\n       12292], dtype=int32)Attributes: (4)created_at :2021-09-15T19:52:34.713725arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  constant_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:   (cohort: 10, obs: 55, t_values: 10)\nCoordinates:\n  * t_values  (t_values) int64 1 2 3 4 5 6 7 8 9 10\n  * cohort    (cohort) int64 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\n  * obs       (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    t         (t_values) int32 1 2 3 4 5 6 7 8 9 10\n    premium   (cohort) int32 957 3695 6138 17533 ... 46095 51512 52481 56978\n    t_idx     (obs) int32 0 1 2 3 4 5 6 7 8 9 0 1 2 ... 3 4 0 1 2 3 0 1 2 0 1 0\n    c_idx     (obs) int32 0 0 0 0 0 0 0 0 0 0 1 1 1 ... 5 5 6 6 6 6 7 7 7 8 8 9\n    obs_idx   (obs) float64 0.0 1.0 2.0 3.0 4.0 5.0 ... 50.0 51.0 52.0 53.0 54.0\nAttributes:\n    created_at:                 2021-09-15T19:52:34.715851\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:cohort: 10obs: 55t_values: 10Coordinates: (3)t_values(t_values)int641 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (5)t(t_values)int321 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=int32)premium(cohort)int32957 3695 6138 ... 51512 52481 56978array([  957,  3695,  6138, 17533, 29341, 37194, 46095, 51512, 52481,\n       56978], dtype=int32)t_idx(obs)int320 1 2 3 4 5 6 7 ... 2 3 0 1 2 0 1 0array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2,\n       3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3,\n       4, 0, 1, 2, 3, 0, 1, 2, 0, 1, 0], dtype=int32)c_idx(obs)int320 0 0 0 0 0 0 0 ... 6 6 7 7 7 8 8 9array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2,\n       2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5,\n       5, 6, 6, 6, 6, 7, 7, 7, 8, 8, 9], dtype=int32)obs_idx(obs)float640.0 1.0 2.0 3.0 ... 52.0 53.0 54.0array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,\n       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,\n       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51.,\n       52., 53., 54.])Attributes: (4)created_at :2021-09-15T19:52:34.715851arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n              \n            \n            \n\n\n\nusetable[['grcode','grname','acc_year','dev_year','dev_lag','premium','cum_loss','loss_ratio']].head(10)\n\n\n\n\n\n  \n    \n      \n      grcode\n      grname\n      acc_year\n      dev_year\n      dev_lag\n      premium\n      cum_loss\n      loss_ratio\n    \n  \n  \n    \n      0\n      43\n      IDS Property Cas Ins Co\n      1988\n      1988\n      1\n      957\n      133\n      0.138976\n    \n    \n      1\n      43\n      IDS Property Cas Ins Co\n      1988\n      1989\n      2\n      957\n      333\n      0.347962\n    \n    \n      2\n      43\n      IDS Property Cas Ins Co\n      1988\n      1990\n      3\n      957\n      431\n      0.450366\n    \n    \n      3\n      43\n      IDS Property Cas Ins Co\n      1988\n      1991\n      4\n      957\n      570\n      0.595611\n    \n    \n      4\n      43\n      IDS Property Cas Ins Co\n      1988\n      1992\n      5\n      957\n      615\n      0.642633\n    \n    \n      5\n      43\n      IDS Property Cas Ins Co\n      1988\n      1993\n      6\n      957\n      615\n      0.642633\n    \n    \n      6\n      43\n      IDS Property Cas Ins Co\n      1988\n      1994\n      7\n      957\n      615\n      0.642633\n    \n    \n      7\n      43\n      IDS Property Cas Ins Co\n      1988\n      1995\n      8\n      957\n      614\n      0.641588\n    \n    \n      8\n      43\n      IDS Property Cas Ins Co\n      1988\n      1996\n      9\n      957\n      614\n      0.641588\n    \n    \n      9\n      43\n      IDS Property Cas Ins Co\n      1988\n      1997\n      10\n      957\n      614\n      0.641588\n    \n  \n\n\n\n\n\nprediction_coords = {\"obs\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}\nwith logistic_model:\n    pm.set_data({'premium':np.array([800]*10), \n                 'obs_idx':[21, 11, 12, 13, 14, 15, 16, 17, 18, 50]\n                })\n    ppc = pm.sample_posterior_predictive(logistic_trace,  var_names=[\"lm\"])\n    \n\n\n    \n        \n      \n      100.00% [2000/2000 00:01<00:00]\n    \n    \n\n\n\npd.DataFrame(ppc['lm']).T[0:10].plot(legend=False)\n#logistic_idata.constant_data.to_dataframe()\n#usetable\n#cohort_id\n\n<AxesSubplot:>\n\n\n\n\n\n\nyears = usetable['acc_year'].unique()\nfig, axs = plt.subplots((int(len(years)/2)), 2, figsize=(20,10))\naxs = axs.flatten()\nfor ax, year in zip(axs, years):\n    usetable[usetable['acc_year'] == year]['loss_ratio'].plot(ax=ax, title=\"Loss Ratio Curve for: \" + str(year))\n\n\n\n\n\nfrom graphviz import Digraph, Graph\n\n# Packages\np = Digraph()\np.node('Echo', 'Echo')\np.node('Severen', 'Severen')\np.node('Panacea', 'Panacea')\n\n\n## Frameworks\nf = Digraph()\nf.node('ACDC', 'ACDC Framework')\nf.node('Jenkins', 'Jenkins')\n\np.edge('Echo', 'Panacea')\n\np.subgraph(f)\n\nf.edge('Jenkins', 'Echo')\np\n\n\n\n\n\ng = Digraph('G', filename='cluster_edge.gv')\ng.attr(compound='true')\ng.node('e', 'Echo')\ng.node('s', 'Severen')\ng.node('p', 'Panacea')\ng.node('j', 'Jenkins')\ng.node('slack', 'Slack')\n\n\nwith g.subgraph(name='cluster0') as c:\n    c.edges(['jb', 'ac', 'bd', 'cd'])\n\nwith g.subgraph(name='cluster1') as c:\n    c.edges(['ps', 'pe'])\n\n\ng\n\n\n\n\n\nidata\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:   (chain: 2, cohort: 10, draw: 1000, obs: 55, t_values: 10)\nCoordinates:\n  * chain     (chain) int64 0 1\n  * draw      (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * cohort    (cohort) int64 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\n  * t_values  (t_values) int64 1 2 3 4 5 6 7 8 9 10\n  * obs       (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    mu_LR     (chain, draw) float64 -0.187 -0.09882 ... -0.03967 0.007514\n    sd_LR     (chain, draw) float64 0.6209 0.3373 0.2758 ... 0.2711 0.2706\n    LR        (chain, draw, cohort) float64 0.6702 0.8334 ... 0.8889 0.7807\n    loss_sd   (chain, draw) float64 0.03186 0.02677 0.03494 ... 0.0332 0.02944\n    omega     (chain, draw) float64 2.047 2.063 2.117 ... 1.928 1.942 1.925\n    theta     (chain, draw) float64 1.73 1.75 1.699 1.712 ... 1.81 1.803 1.801\n    gf        (chain, draw, t_values) float64 0.2457 0.5738 ... 0.9568 0.9644\n    lm        (chain, draw, obs) float64 157.6 368.0 ... 2.567e+04 1.084e+04\nAttributes:\n    created_at:                 2021-07-10T22:10:08.100990\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2\n    sampling_time:              10.192897081375122\n    tuning_steps:               2000xarray.DatasetDimensions:chain: 2cohort: 10draw: 1000obs: 55t_values: 10Coordinates: (5)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])t_values(t_values)int641 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (8)mu_LR(chain, draw)float64-0.187 -0.09882 ... 0.007514array([[-0.18696659, -0.0988205 , -0.00810847, ...,  0.05311949,\n         0.20880435, -0.01687328],\n       [-0.11414088, -0.15425932, -0.22073582, ...,  0.0246603 ,\n        -0.03966546,  0.00751418]])sd_LR(chain, draw)float640.6209 0.3373 ... 0.2711 0.2706array([[0.62089146, 0.33727945, 0.27581849, ..., 0.42592517, 0.45086283,\n        0.48777715],\n       [0.32289577, 0.44645476, 0.49395078, ..., 0.32981233, 0.27107624,\n        0.27061255]])LR(chain, draw, cohort)float640.6702 0.8334 ... 0.8889 0.7807array([[[0.67022333, 0.83343362, 1.50476005, ..., 0.84746063,\n         0.75324862, 0.97680847],\n        [0.67852191, 0.85033978, 1.5128585 , ..., 0.8348323 ,\n         0.82923175, 0.78441512],\n        [0.65084057, 0.81301633, 1.50223526, ..., 0.86576549,\n         0.81385395, 1.1107245 ],\n        ...,\n        [0.69134537, 0.87360691, 1.53672948, ..., 0.88517673,\n         0.80569189, 0.80970752],\n        [0.67295471, 0.8505383 , 1.55445959, ..., 0.9003714 ,\n         0.81393564, 0.9427412 ],\n        [0.68806353, 0.85636222, 1.52987837, ..., 0.84520793,\n         0.77860489, 0.83172496]],\n\n       [[0.68118037, 0.84469149, 1.52453007, ..., 0.89498748,\n         0.80324074, 1.07317632],\n        [0.66690657, 0.85809314, 1.53552822, ..., 0.83513642,\n         0.8337994 , 0.70777644],\n        [0.69604056, 0.83549505, 1.52540707, ..., 0.85311843,\n         0.83007865, 0.58221531],\n        ...,\n        [0.66498216, 0.85115548, 1.55624145, ..., 0.84190874,\n         0.84255311, 0.92859606],\n        [0.67333019, 0.82537763, 1.546271  , ..., 0.88271328,\n         0.97106623, 0.93680019],\n        [0.66692557, 0.85914168, 1.55441355, ..., 0.88582965,\n         0.88886908, 0.78072028]]])loss_sd(chain, draw)float640.03186 0.02677 ... 0.0332 0.02944array([[0.03186241, 0.02677491, 0.03494346, ..., 0.02950565, 0.03100382,\n        0.0271407 ],\n       [0.02958521, 0.03073347, 0.03223621, ..., 0.03685306, 0.03319626,\n        0.02944148]])omega(chain, draw)float642.047 2.063 2.117 ... 1.942 1.925array([[2.04684299, 2.06348333, 2.11704502, ..., 2.02350399, 1.90782721,\n        2.02067407],\n       [2.10779663, 1.94099992, 2.01335649, ..., 1.92760786, 1.94155975,\n        1.92504071]])theta(chain, draw)float641.73 1.75 1.699 ... 1.803 1.801array([[1.72969432, 1.75027019, 1.6993544 , ..., 1.82016814, 1.74772066,\n        1.78329374],\n       [1.76581379, 1.75222828, 1.76108597, ..., 1.80953151, 1.80345161,\n        1.80096094]])gf(chain, draw, t_values)float640.2457 0.5738 ... 0.9568 0.9644array([[[0.24572263, 0.57375952, 0.75530888, ..., 0.95830295,\n         0.96694015, 0.97318136],\n        [0.23956308, 0.56837426, 0.75248162, ..., 0.9583462 ,\n         0.96703728, 0.97330626],\n        [0.24553678, 0.58537157, 0.76910278, ..., 0.9637262 ,\n         0.9715035 , 0.9770701 ],\n        ...,\n        [0.22935886, 0.54751893, 0.73323759, ..., 0.95238508,\n         0.96209965, 0.96915217],\n        [0.2563239 , 0.56395803, 0.73707071, ..., 0.94794775,\n         0.95798322, 0.96536932],\n        [0.23705689, 0.55767741, 0.74097726, ..., 0.95404246,\n         0.96342013, 0.97022592]],\n\n       [[0.23173901, 0.56524966, 0.75345663, ..., 0.96024765,\n         0.96871358, 0.9747871 ],\n        [0.25186615, 0.56382847, 0.73956758, ..., 0.95014575,\n         0.95992601, 0.96709377],\n        [0.24242684, 0.56368527, 0.74506788, ..., 0.95466309,\n         0.96388892, 0.97058815],\n        ...,\n        [0.24173286, 0.54807935, 0.72601378, ..., 0.94609631,\n         0.95656922, 0.96426648],\n        [0.24141408, 0.55004295, 0.72870832, ..., 0.94747034,\n         0.95775477, 0.96529882],\n        [0.24369253, 0.55027846, 0.72757083, ..., 0.94636716,\n         0.95677735, 0.96442982]]])lm(chain, draw, obs)float64157.6 368.0 ... 2.567e+04 1.084e+04array([[[  157.60741315,   368.01149881,   484.45793209, ...,\n          9713.72058482, 22681.4259549 , 13676.08451242],\n        [  155.55919791,   369.07124816,   488.62052768, ...,\n         10425.52435542, 24735.02909624, 10707.12910397],\n        [  152.93367252,   364.60127339,   479.03907065, ...,\n         10487.33492388, 25002.31377805, 15539.25209443],\n        ...,\n        [  151.74783701,   362.24811303,   485.12283192, ...,\n          9698.09893366, 23151.02546387, 10581.58893866],\n        [  165.07711826,   363.19893028,   474.68655257, ...,\n         10949.17187914, 24090.11955022, 13768.56860626],\n        [  156.09646279,   367.21763655,   487.91633048, ...,\n          9686.61005323, 22787.79407511, 11234.13198617]],\n\n       [[  151.0682531 ,   368.4803803 ,   491.1705532 , ...,\n          9768.92945857, 23828.02983572, 14170.24714991],\n        [  160.74842881,   359.85201422,   472.01390953, ...,\n         11021.31677114, 24672.35959206, 10157.1789823 ],\n        [  161.48314057,   375.47685914,   496.29778013, ...,\n         10560.92705732, 24556.02304463,  8042.1379405 ],\n        ...,\n        [  153.83587622,   348.79108035,   462.02640537, ...,\n         10688.95096964, 24234.98892586, 12789.97614652],\n        [  155.56168039,   354.43502325,   469.5628732 , ...,\n         12303.07167812, 28031.57876309, 12885.95974292],\n        [  155.53621549,   351.21400257,   464.37046077, ...,\n         11367.94919334, 25669.79609618, 10840.38943469]]])Attributes: (6)created_at :2021-07-10T22:10:08.100990arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2sampling_time :10.192897081375122tuning_steps :2000\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 2, cohort: 10, draw: 1000, obs: 55)\nCoordinates:\n  * chain    (chain) int64 0 1\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\n  * cohort   (cohort) int64 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\nData variables:\n    loss     (chain, draw, obs) float64 104.3 378.5 ... 2.644e+04 9.016e+03\n    LR       (chain, draw, cohort) float64 0.6702 0.8334 1.505 ... 0.8889 0.7807\n    lm       (chain, draw, obs) float64 157.6 368.0 ... 2.567e+04 1.084e+04\nAttributes:\n    created_at:                 2021-07-10T22:10:08.573587\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 2cohort: 10draw: 1000obs: 55Coordinates: (4)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])Data variables: (3)loss(chain, draw, obs)float64104.3 378.5 ... 2.644e+04 9.016e+03array([[[  104.25299878,   378.46062043,   519.61667247, ...,\n          8141.80268618, 21296.98149285, 13873.7213711 ],\n        [  168.57110391,   346.97788603,   520.63643116, ...,\n         10456.73879506, 25180.98640336,  9559.26030343],\n        [  109.58118079,   367.78282075,   464.86966193, ...,\n         11868.35118362, 22050.81611823, 19408.3032024 ],\n        ...,\n        [  106.27795502,   318.52680823,   510.43588738, ...,\n         11021.18578426, 22826.81417456, 12527.08543234],\n        [  171.64762377,   365.27633379,   457.72868241, ...,\n         12032.70208003, 22850.73831518, 12817.78476422],\n        [  138.78409136,   360.61093994,   480.48285626, ...,\n         10137.62921603, 20979.07259156, 13465.6251659 ]],\n\n       [[   97.16103159,   355.99598682,   512.81621348, ...,\n         11204.55137772, 23671.12813507, 12157.79410608],\n        [  131.26783254,   355.3390368 ,   460.1770658 , ...,\n         10682.5367999 , 23307.75834914,  8833.09949403],\n        [  194.26536002,   369.60172602,   492.15711635, ...,\n         10816.10853934, 23849.85945329,  5220.66454384],\n        ...,\n        [  103.44871872,   337.44440949,   461.41297666, ...,\n         10020.64412924, 24559.0585406 , 15059.02844165],\n        [  154.30464618,   327.38538816,   474.78895535, ...,\n         15633.51420392, 24639.37731399, 14192.66494558],\n        [  132.40295851,   395.31181651,   455.54853151, ...,\n         10602.40295703, 26440.182822  ,  9015.82177028]]])LR(chain, draw, cohort)float640.6702 0.8334 ... 0.8889 0.7807array([[[0.67022333, 0.83343362, 1.50476005, ..., 0.84746063,\n         0.75324862, 0.97680847],\n        [0.67852191, 0.85033978, 1.5128585 , ..., 0.8348323 ,\n         0.82923175, 0.78441512],\n        [0.65084057, 0.81301633, 1.50223526, ..., 0.86576549,\n         0.81385395, 1.1107245 ],\n        ...,\n        [0.69134537, 0.87360691, 1.53672948, ..., 0.88517673,\n         0.80569189, 0.80970752],\n        [0.67295471, 0.8505383 , 1.55445959, ..., 0.9003714 ,\n         0.81393564, 0.9427412 ],\n        [0.68806353, 0.85636222, 1.52987837, ..., 0.84520793,\n         0.77860489, 0.83172496]],\n\n       [[0.68118037, 0.84469149, 1.52453007, ..., 0.89498748,\n         0.80324074, 1.07317632],\n        [0.66690657, 0.85809314, 1.53552822, ..., 0.83513642,\n         0.8337994 , 0.70777644],\n        [0.69604056, 0.83549505, 1.52540707, ..., 0.85311843,\n         0.83007865, 0.58221531],\n        ...,\n        [0.66498216, 0.85115548, 1.55624145, ..., 0.84190874,\n         0.84255311, 0.92859606],\n        [0.67333019, 0.82537763, 1.546271  , ..., 0.88271328,\n         0.97106623, 0.93680019],\n        [0.66692557, 0.85914168, 1.55441355, ..., 0.88582965,\n         0.88886908, 0.78072028]]])lm(chain, draw, obs)float64157.6 368.0 ... 2.567e+04 1.084e+04array([[[  157.60741315,   368.01149881,   484.45793209, ...,\n          9713.72058482, 22681.4259549 , 13676.08451242],\n        [  155.55919791,   369.07124816,   488.62052768, ...,\n         10425.52435542, 24735.02909624, 10707.12910397],\n        [  152.93367252,   364.60127339,   479.03907065, ...,\n         10487.33492388, 25002.31377805, 15539.25209443],\n        ...,\n        [  151.74783701,   362.24811303,   485.12283192, ...,\n          9698.09893366, 23151.02546387, 10581.58893866],\n        [  165.07711826,   363.19893028,   474.68655257, ...,\n         10949.17187914, 24090.11955022, 13768.56860626],\n        [  156.09646279,   367.21763655,   487.91633048, ...,\n          9686.61005323, 22787.79407511, 11234.13198617]],\n\n       [[  151.0682531 ,   368.4803803 ,   491.1705532 , ...,\n          9768.92945857, 23828.02983572, 14170.24714991],\n        [  160.74842881,   359.85201422,   472.01390953, ...,\n         11021.31677114, 24672.35959206, 10157.1789823 ],\n        [  161.48314057,   375.47685914,   496.29778013, ...,\n         10560.92705732, 24556.02304463,  8042.1379405 ],\n        ...,\n        [  153.83587622,   348.79108035,   462.02640537, ...,\n         10688.95096964, 24234.98892586, 12789.97614652],\n        [  155.56168039,   354.43502325,   469.5628732 , ...,\n         12303.07167812, 28031.57876309, 12885.95974292],\n        [  155.53621549,   351.21400257,   464.37046077, ...,\n         11367.94919334, 25669.79609618, 10840.38943469]]])Attributes: (4)created_at :2021-07-10T22:10:08.573587arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 2, draw: 1000, obs: 55)\nCoordinates:\n  * chain    (chain) int64 0 1\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    loss     (chain, draw, obs) float64 -4.662 -4.996 -5.873 ... -9.298 -8.718\nAttributes:\n    created_at:                 2021-07-10T22:10:08.569795\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 2draw: 1000obs: 55Coordinates: (3)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (1)loss(chain, draw, obs)float64-4.662 -4.996 ... -9.298 -8.718array([[[ -4.66204085,  -4.99560314,  -5.8731991 , ...,  -9.83460231,\n          -8.44534821,  -8.71364938],\n        [ -4.55001055,  -5.15331207,  -6.69084417, ...,  -9.36860769,\n          -8.58761574,  -8.78868814],\n        [ -4.6063776 ,  -4.87522004,  -5.46053709, ...,  -9.09921933,\n          -8.79322533,  -9.84534805],\n        ...,\n        [ -4.47998193,  -4.79602114,  -6.09651578, ..., -10.0247998 ,\n          -8.28211525,  -8.86372827],\n        [ -4.89349193,  -4.82706018,  -5.39305197, ...,  -8.8306795 ,\n          -8.39185554,  -8.74504146],\n        [ -4.57138313,  -5.04378999,  -6.57694207, ..., -10.27797608,\n          -8.28719365,  -8.49661696]],\n\n       [[ -4.46588533,  -5.04744865,  -6.52047369, ...,  -9.93369734,\n          -8.29693424,  -8.96962124],\n        [ -4.74537964,  -4.71708996,  -5.27260529, ...,  -8.7861692 ,\n          -8.59379556,  -9.13006987],\n        [ -4.7742971 ,  -5.29597798,  -6.5881131 , ...,  -9.0816783 ,\n          -8.56772943, -11.11150275],\n        ...,\n        [ -4.65643627,  -4.58216115,  -4.86888165, ...,  -8.97653424,\n          -8.5695356 ,  -8.59666319],\n        [ -4.62960387,  -4.60504662,  -5.11415089, ...,  -8.396745  ,\n         -11.84581071,  -8.51334596],\n        [ -4.5772725 ,  -4.46633895,  -4.95876778, ...,  -8.58177161,\n          -9.29750017,  -8.71840896]]])Attributes: (4)created_at :2021-07-10T22:10:08.569795arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:           (chain: 2, draw: 1000)\nCoordinates:\n  * chain             (chain) int64 0 1\n  * draw              (draw) int64 0 1 2 3 4 5 6 ... 993 994 995 996 997 998 999\nData variables:\n    depth             (chain, draw) int64 3 3 3 3 3 3 3 3 4 ... 4 3 3 4 3 3 3 3\n    step_size         (chain, draw) float64 0.3709 0.3709 ... 0.3642 0.3642\n    max_energy_error  (chain, draw) float64 -0.3033 -0.9073 ... 0.1527 0.2162\n    mean_tree_accept  (chain, draw) float64 0.9878 1.0 0.6001 ... 0.9535 0.9023\n    tree_size         (chain, draw) float64 7.0 7.0 7.0 7.0 ... 7.0 7.0 7.0 7.0\n    step_size_bar     (chain, draw) float64 0.4186 0.4186 ... 0.4023 0.4023\n    diverging         (chain, draw) bool False False False ... False False False\n    energy            (chain, draw) float64 418.4 411.7 410.1 ... 411.3 415.6\n    lp                (chain, draw) float64 -409.4 -398.4 ... -405.5 -405.0\n    energy_error      (chain, draw) float64 -0.08659 -0.9073 ... 0.05396 0.01052\nAttributes:\n    created_at:                 2021-07-10T22:10:08.109313\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2\n    sampling_time:              10.192897081375122\n    tuning_steps:               2000xarray.DatasetDimensions:chain: 2draw: 1000Coordinates: (2)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (10)depth(chain, draw)int643 3 3 3 3 3 3 3 ... 4 3 3 4 3 3 3 3array([[3, 3, 3, ..., 3, 3, 3],\n       [3, 4, 3, ..., 3, 3, 3]])step_size(chain, draw)float640.3709 0.3709 ... 0.3642 0.3642array([[0.37085185, 0.37085185, 0.37085185, ..., 0.37085185, 0.37085185,\n        0.37085185],\n       [0.36415012, 0.36415012, 0.36415012, ..., 0.36415012, 0.36415012,\n        0.36415012]])max_energy_error(chain, draw)float64-0.3033 -0.9073 ... 0.1527 0.2162array([[-0.30333558, -0.90728229,  0.69568241, ..., -0.31374491,\n        -0.2200231 , -0.40118408],\n       [ 1.42927712,  0.33138796,  0.36049719, ...,  0.18928538,\n         0.15266867,  0.21622557]])mean_tree_accept(chain, draw)float640.9878 1.0 0.6001 ... 0.9535 0.9023array([[0.98779061, 1.        , 0.60011115, ..., 0.93739839, 0.98545547,\n        0.9950132 ],\n       [0.43950928, 0.89151176, 0.84991465, ..., 0.90923646, 0.95350635,\n        0.90231548]])tree_size(chain, draw)float647.0 7.0 7.0 7.0 ... 7.0 7.0 7.0 7.0array([[ 7.,  7.,  7., ...,  7.,  7.,  7.],\n       [ 7., 15.,  7., ...,  7.,  7.,  7.]])step_size_bar(chain, draw)float640.4186 0.4186 ... 0.4023 0.4023array([[0.41864123, 0.41864123, 0.41864123, ..., 0.41864123, 0.41864123,\n        0.41864123],\n       [0.40234341, 0.40234341, 0.40234341, ..., 0.40234341, 0.40234341,\n        0.40234341]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])energy(chain, draw)float64418.4 411.7 410.1 ... 411.3 415.6array([[418.43883273, 411.65728711, 410.0767806 , ..., 410.51405687,\n        408.58036529, 407.35921943],\n       [412.80822536, 411.47900652, 413.17891412, ..., 411.18710152,\n        411.29444508, 415.58466124]])lp(chain, draw)float64-409.4 -398.4 ... -405.5 -405.0array([[-409.43533657, -398.44740163, -404.77855576, ..., -402.57284061,\n        -403.52737457, -400.50675223],\n       [-401.88597959, -402.44777691, -407.25696595, ..., -404.72076691,\n        -405.45093988, -404.98509764]])energy_error(chain, draw)float64-0.08659 -0.9073 ... 0.01052array([[-0.08658624, -0.90728229,  0.52606004, ..., -0.05300835,\n         0.02450066, -0.40118408],\n       [ 0.38322751,  0.00231922,  0.16376489, ...,  0.12195152,\n         0.05395713,  0.01051789]])Attributes: (6)created_at :2021-07-10T22:10:08.109313arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2sampling_time :10.192897081375122tuning_steps :2000\n                      \n                  \n            \n            \n            \n                  \n                  prior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:         (LR_log___dim_0: 10, chain: 1, cohort: 10, draw: 100, obs: 55, t_values: 10)\nCoordinates:\n  * chain           (chain) int64 0\n  * draw            (draw) int64 0 1 2 3 4 5 6 7 8 ... 92 93 94 95 96 97 98 99\n  * cohort          (cohort) int64 1988 1989 1990 1991 ... 1994 1995 1996 1997\n  * t_values        (t_values) int64 1 2 3 4 5 6 7 8 9 10\n  * LR_log___dim_0  (LR_log___dim_0) int64 0 1 2 3 4 5 6 7 8 9\n  * obs             (obs) int64 0 1 2 3 4 5 6 7 8 ... 46 47 48 49 50 51 52 53 54\nData variables:\n    LR              (chain, draw, cohort) float64 0.5395 0.2832 ... 3.739\n    omega_log__     (chain, draw) float64 0.4461 0.07193 ... 0.1054 -0.7762\n    sd_LR_log__     (chain, draw) float64 -0.8523 -0.5681 -1.487 ... -0.3 0.7881\n    mu_LR           (chain, draw) float64 -0.8749 0.1713 ... -0.09251 -1.244\n    loss_sd_log__   (chain, draw) float64 0.1178 -0.06791 ... -0.3026 -1.538\n    gf              (chain, draw, t_values) float64 0.7042 0.8755 ... 0.7399\n    theta           (chain, draw) float64 0.5739 1.253 3.127 ... 1.24 1.031\n    sd_LR           (chain, draw) float64 0.4264 0.5666 0.2261 ... 0.7408 2.199\n    loss_sd         (chain, draw) float64 1.125 0.9343 0.4603 ... 0.7389 0.2147\n    LR_log__        (chain, draw, LR_log___dim_0) float64 -0.6171 ... 1.319\n    omega           (chain, draw) float64 1.562 1.075 1.892 ... 1.111 0.4602\n    theta_log__     (chain, draw) float64 -0.5553 0.2252 1.14 ... 0.2152 0.0308\n    lm              (chain, draw, obs) float64 363.6 452.0 ... 1.058e+05\nAttributes:\n    created_at:                 2021-07-10T22:10:08.582306\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:LR_log___dim_0: 10chain: 1cohort: 10draw: 100obs: 55t_values: 10Coordinates: (6)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 6 ... 94 95 96 97 98 99array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])t_values(t_values)int641 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])LR_log___dim_0(LR_log___dim_0)int640 1 2 3 4 5 6 7 8 9array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (13)LR(chain, draw, cohort)float640.5395 0.2832 ... 0.04612 3.739array([[[5.39483441e-01, 2.83183398e-01, 5.36638317e-01, 3.46020707e-01,\n         4.35402974e-01, 7.28349146e-01, 7.53385166e-01, 1.98912181e-01,\n         4.71464140e-01, 3.60783849e-01],\n        [1.60021652e+00, 9.60084765e-01, 1.55527582e+00, 5.27957240e-01,\n         1.49531975e+00, 7.42338495e-01, 1.50872389e+00, 1.08432781e+00,\n         2.39771704e+00, 3.04613240e+00],\n        [1.53240102e+00, 3.89658259e+00, 1.94455432e+00, 1.91653136e+00,\n         1.65883222e+00, 1.28964188e+00, 1.54529885e+00, 1.73610649e+00,\n         2.20782512e+00, 1.42497303e+00],\n        [2.89626892e-01, 6.89615568e-01, 4.81269113e-01, 3.24477413e-01,\n         9.64152326e-01, 4.77542250e-01, 2.76833126e+00, 2.26235901e+00,\n         8.77298833e-02, 6.18636581e-01],\n        [1.41315957e+00, 4.78212851e+00, 9.39090229e-01, 1.28132155e+00,\n         4.49898074e+00, 4.54895891e+00, 3.03642390e+00, 5.25706928e-01,\n         6.26412548e+00, 1.39341149e+00],\n        [9.57696979e-01, 1.86763571e+00, 4.13374817e+00, 1.56821742e+00,\n         1.25487332e+00, 2.51191010e+00, 8.10945988e+00, 6.10722752e-01,\n         1.41395916e+00, 9.82281959e+00],\n        [1.71912712e+00, 1.49080130e-01, 4.37317875e+00, 6.93848752e-01,\n         2.87455238e-01, 2.20250288e+00, 8.11983450e-01, 7.26701892e-01,\n...\n         9.69395468e-01, 6.87721577e+01, 1.36484576e-01, 1.13037865e-01,\n         4.30151712e-02, 2.71561166e+02],\n        [3.40901443e+00, 2.08439147e+00, 6.84379805e+00, 9.55929917e+00,\n         1.77361679e+00, 1.82087509e+01, 4.04976821e+00, 1.13211119e+01,\n         1.86859175e+00, 1.89121247e+00],\n        [3.46365246e+00, 1.66684876e-01, 3.47023822e-01, 4.53995073e-02,\n         1.06780054e+00, 9.27681160e-01, 1.80283061e+00, 2.37063152e+00,\n         7.54626213e-01, 2.86942154e-01],\n        [5.03298776e-01, 7.39239018e-01, 1.35779223e+00, 1.00056116e+00,\n         8.86529488e-01, 3.21049079e+00, 7.97450632e-01, 3.99036318e-01,\n         4.30978753e-01, 6.98964209e-01],\n        [3.00069125e+00, 3.09937257e+00, 5.07217053e-01, 5.56775119e-01,\n         2.04611562e+00, 9.44072847e+00, 6.36453703e-01, 1.36052095e+00,\n         2.53801160e+00, 1.02432399e-01],\n        [7.13583274e-01, 1.27445474e+00, 6.15372182e-01, 7.40064303e-01,\n         5.65699214e-01, 3.94688604e-01, 3.85516160e-01, 1.03348754e+00,\n         8.72203161e-01, 2.06829310e+00],\n        [6.72598317e-02, 1.31074344e-01, 2.46574280e-01, 3.57594394e-01,\n         1.41602364e+01, 1.39274851e+00, 1.65655532e-01, 1.13027571e+01,\n         4.61181024e-02, 3.73917734e+00]]])omega_log__(chain, draw)float640.4461 0.07193 ... 0.1054 -0.7762array([[ 0.44612894,  0.0719305 ,  0.63779866, -0.23449358,  0.23706241,\n         0.33419761, -0.60235794, -0.42556604,  0.44956226,  0.46859577,\n         0.47679333, -1.10255173, -1.1406529 , -0.30581554,  1.40485057,\n         0.08792286, -0.3153778 ,  0.92777277,  0.08014537, -0.71142477,\n        -0.18382993, -0.94314418,  0.37741105,  0.89061229,  0.32631473,\n         0.1187519 ,  0.29710071, -0.56646717,  0.24871394, -0.12211726,\n         0.52034076, -0.13353552, -0.33864184, -0.33580391,  1.31280722,\n        -0.55840032,  0.46812679, -0.28309119, -0.25267346, -0.43320864,\n         0.69851556, -0.43096099, -0.05757817,  0.00526835, -0.25980818,\n        -0.12478563,  1.12182973, -0.2414983 , -1.18449009, -0.46079041,\n         0.75950166,  0.27624921, -0.34634081, -0.53000413,  0.73357778,\n        -0.08395659,  0.37989795, -0.10431087, -0.02411082, -0.0916873 ,\n         0.64151539,  0.34238158,  0.25391359,  0.20807299,  0.07147848,\n        -0.03081378, -0.15740337,  0.33052872, -0.54743424, -0.17367713,\n         0.6791806 , -0.14902945, -0.04004145,  0.24734065,  0.68666321,\n        -0.46438288, -0.5281403 ,  0.10112753, -0.33912545, -0.01100623,\n        -0.95018485, -0.51196821,  0.10753402, -0.2633491 , -0.26221749,\n         0.2735264 ,  0.05936596, -1.15266121,  0.35999557, -0.64225228,\n         0.67848939,  0.24234314, -0.12805771,  0.67731315,  1.03797529,\n         0.3334401 , -0.50685985,  0.09829753,  0.10543659, -0.77619435]])sd_LR_log__(chain, draw)float64-0.8523 -0.5681 ... -0.3 0.7881array([[-0.8523256 , -0.5681305 , -1.48665774,  0.01665864, -0.12444433,\n        -0.22508822,  0.0662139 ,  0.01110696,  0.15868399, -0.37620709,\n        -0.6481959 ,  0.04756972, -0.21185755, -0.59299178, -0.182731  ,\n        -0.63551152,  0.79308547,  0.34669533, -0.97904062, -0.06740066,\n        -0.77030801,  1.02335698, -0.69849967, -0.54858599, -0.11935643,\n        -0.71453345,  0.47450239, -0.00969879,  0.44729885,  0.37984656,\n        -0.74886019, -0.59694299,  0.64813129,  0.47613781, -0.60862707,\n        -0.07863258, -0.75379258,  0.05394207,  0.37352783,  0.21483822,\n        -0.70752146, -0.32037996,  0.38981315, -0.21906046,  1.03739658,\n        -0.17164884, -0.30831469,  0.38159182,  0.0964586 , -0.17422947,\n         1.14932697, -0.08260478,  0.23314968,  0.13499362, -0.15991552,\n        -0.5738708 ,  0.85181199, -0.36107539,  0.54684332, -0.11475888,\n        -0.00444933, -0.271599  ,  0.37653109, -0.80471945,  0.97163113,\n        -0.72371806,  0.06512423,  0.47468043, -1.00759436, -0.03977029,\n         0.15052473, -0.84244998,  0.1111954 , -0.34246087, -0.06310059,\n         0.99513682,  0.2614989 , -0.0081727 , -0.20790817, -0.67925147,\n        -0.25721495, -0.10803006,  0.21119011, -0.54702147,  0.61845394,\n        -0.11514234, -0.3522091 , -0.29568756,  0.36849758,  0.21793363,\n         0.88799679,  0.25653719,  0.58526349,  1.03885612, -0.22796101,\n         0.32458646, -0.08739078,  0.50863217, -0.29999152,  0.78808336]])mu_LR(chain, draw)float64-0.8749 0.1713 ... -0.09251 -1.244array([[-0.87488274,  0.1713402 ,  0.5765179 , -0.12621802,  0.49066039,\n         0.25710942,  0.11058983, -0.53502167, -0.09474792,  0.12750072,\n        -0.22901349,  0.21758174, -0.29179753,  0.40842354,  0.3363604 ,\n        -0.05220557, -0.26564019,  0.51486634, -0.21906781, -0.55915912,\n         0.80949083,  0.77080259, -0.12593957, -0.42121787,  0.09225935,\n         0.4685411 ,  0.36550017,  0.68077806, -0.16311903,  0.02783801,\n         0.1111998 , -0.7216085 , -0.37817615,  0.40822701,  0.37522238,\n        -0.22797346,  0.59481113, -0.84530841, -0.67819952, -0.61621726,\n        -0.27221958, -0.33408587,  0.00365728, -0.30646937,  0.64987404,\n        -0.86654781, -0.49165505,  0.17875388, -0.80678925,  0.73535693,\n        -0.5940088 , -0.2748731 , -0.47002308, -0.41396618,  0.05443173,\n         0.2539048 , -0.43111367,  0.62473487, -0.03980562, -0.44486574,\n        -0.44089919,  0.00931947,  0.11892231,  0.00677427, -0.8177647 ,\n        -0.52210494,  0.30651944,  0.36810261,  0.51346072, -0.71609531,\n        -0.92059415,  0.18304661, -0.16588857, -0.34460899,  1.01730378,\n        -0.27535721,  0.37522667, -0.65349617,  0.29028667, -0.55226155,\n         0.34506074,  0.34344503, -0.78334376,  0.45248706,  0.3894112 ,\n         0.21411644,  0.05443599,  0.01414182, -0.28941291, -0.5997256 ,\n        -0.852976  ,  0.18458198,  0.93828671, -0.18845168,  0.91596804,\n         0.00150872, -0.03801173,  0.0019788 , -0.09250706, -1.24357577]])loss_sd_log__(chain, draw)float640.1178 -0.06791 ... -0.3026 -1.538array([[ 0.11776897, -0.06791383, -0.77586015,  0.81418801, -1.54295884,\n        -0.08635401, -0.3073733 ,  0.44099133, -0.60868549, -0.42565676,\n         0.83953297,  1.90751457,  0.31553409, -0.58763441, -0.02813683,\n         0.61956292,  1.1102456 ,  1.41804546,  1.07121072,  1.2829134 ,\n         0.67823319,  0.11953965,  0.49559607,  0.91112716,  0.26419822,\n        -0.55153079,  0.64974865,  1.42332632,  0.40688404,  1.0925064 ,\n         0.0515981 , -0.0156497 ,  0.47990747, -0.15405117, -1.44822335,\n         0.44352918, -1.63864786, -0.29389226,  0.71212342, -0.27237528,\n        -0.51608351, -0.00461874,  0.19355207, -0.40968742,  1.59689048,\n         0.80746392,  0.90807538, -0.15603457,  0.24094942, -1.39879746,\n         0.38607813, -0.58429016, -0.56743807,  0.07267915, -0.20197823,\n        -0.65064838,  0.31415215, -0.38924771,  0.91696663, -0.48047711,\n        -0.79327309, -0.11503221,  0.74234383, -0.37075465, -0.29457163,\n        -1.81513457,  0.20137675,  0.56713415,  0.07135635, -0.19635674,\n        -0.4044409 , -0.17024778, -1.04476412,  0.16943208, -0.98912851,\n        -0.66173423,  1.065507  ,  1.40047042, -0.45946529, -0.33103924,\n         1.22237942,  1.01016131, -0.64361443,  0.76310702,  0.76479435,\n        -0.22690514,  0.57659883,  1.36853881, -0.45579136,  0.67822079,\n         0.29416748, -1.19540042, -0.00414685, -0.5183479 ,  0.94649573,\n        -0.45149198, -0.10859997, -0.04805518, -0.302624  , -1.53838435]])gf(chain, draw, t_values)float640.7042 0.8755 ... 0.7304 0.7399array([[[0.70423484, 0.87549214, 0.92981535, 0.95405553, 0.9671339 ,\n         0.97507705, 0.98030598, 0.98395463, 0.98661528, 0.98862353],\n        [0.4397886 , 0.62312239, 0.71880211, 0.77689541, 0.81569534,\n         0.84335026, 0.86401169, 0.88000818, 0.89274339, 0.90311221],\n        [0.10362979, 0.30029807, 0.48035688, 0.61438231, 0.70848302,\n         0.77434966, 0.8212351 , 0.85537809, 0.88082887, 0.90022056],\n        [0.33863225, 0.46975303, 0.54972892, 0.60518664, 0.64648496,\n         0.67870678, 0.70469655, 0.7261912 , 0.74432036, 0.75985477],\n        [0.36346583, 0.57889167, 0.69681155, 0.76795562, 0.81451854,\n         0.84693205, 0.87058348, 0.88848725, 0.90244344, 0.91358541],\n        [0.59292719, 0.79319304, 0.87109128, 0.90990553, 0.93240122,\n         0.94679063, 0.95665138, 0.96376016, 0.96908782, 0.97320487],\n        [0.52575673, 0.6183686 , 0.66921337, 0.70310774, 0.7279638 ,\n         0.74727678, 0.76288122, 0.77585146, 0.78686662, 0.79638106],\n        [0.53525654, 0.64431972, 0.70247126, 0.74021092, 0.76725344,\n         0.78784695, 0.80419428, 0.8175696 , 0.82876883, 0.83831844],\n        [0.22462027, 0.46198695, 0.61852007, 0.71793657, 0.78313921,\n         0.82776462, 0.85954517, 0.88296832, 0.90074093, 0.91456169],\n        [0.79345344, 0.92080566, 0.95694122, 0.9723696 , 0.98049424,\n         0.98535136, 0.98851254, 0.99069905, 0.99228223, 0.99347016],\n...\n        [0.64181514, 0.87537833, 0.93982881, 0.96495661, 0.97714108,\n         0.98393041, 0.98809055, 0.99082103, 0.99270871, 0.99406781],\n        [0.39713443, 0.61439167, 0.72759741, 0.79397269, 0.83663171,\n         0.86595956, 0.88716659, 0.90311019, 0.9154717 , 0.92529784],\n        [0.48960751, 0.63836083, 0.71605695, 0.76460411, 0.798092  ,\n         0.82271025, 0.84163459, 0.85667234, 0.86893193, 0.87913289],\n        [0.23779221, 0.54975842, 0.73064268, 0.82695681, 0.88116276,\n         0.91391499, 0.93498184, 0.94924842, 0.95932411, 0.96668948],\n        [0.57758201, 0.90635774, 0.96816329, 0.98561457, 0.99228692,\n         0.99537608, 0.99700294, 0.99794238, 0.99852365, 0.99890312],\n        [0.63229895, 0.81899602, 0.88849829, 0.92251565, 0.94205031,\n         0.95447765, 0.96296374, 0.96906659, 0.97363229, 0.97715586],\n        [0.49575789, 0.5988251 , 0.65584172, 0.69383563, 0.72162392,\n         0.74314104, 0.76046247, 0.7748066 , 0.78694441, 0.79739179],\n        [0.45017679, 0.63755935, 0.7334373 , 0.79076283, 0.82859779,\n         0.85531065, 0.87511355, 0.89034527, 0.90240364, 0.91217334],\n        [0.4405067 , 0.62974341, 0.72743891, 0.78606014, 0.82481181,\n         0.85219095, 0.87249273, 0.88810812, 0.90046845, 0.91048055],\n        [0.49645643, 0.57561057, 0.62042586, 0.65106736, 0.67401798,\n         0.69217567, 0.70707985, 0.71964241, 0.73044606, 0.73988461]]])theta(chain, draw)float640.5739 1.253 3.127 ... 1.24 1.031array([[0.57389012, 1.25260178, 3.12725504, 2.33099523, 1.5559494 ,\n        0.7639599 , 0.82833537, 0.80557966, 2.20412348, 0.43069581,\n        2.9093212 , 0.99399548, 0.86455246, 1.32475578, 1.44164061,\n        1.10965243, 1.07620636, 1.29017089, 2.16036625, 2.32360565,\n        0.55518025, 0.90958767, 1.01729211, 0.84623772, 0.82608081,\n        1.62644821, 1.1668848 , 1.25656072, 0.83539401, 0.65453681,\n        1.83275323, 0.46182585, 1.40712532, 1.19493509, 0.94248559,\n        0.93866154, 0.83764564, 2.25087097, 1.27622612, 0.57064121,\n        0.52945567, 0.88550147, 0.75327822, 1.37076844, 0.93745489,\n        1.51501646, 0.62072393, 1.25873035, 2.29427686, 0.68248132,\n        1.06593879, 0.66061023, 0.77861607, 1.41031485, 2.15389213,\n        0.5953411 , 0.56525208, 1.340487  , 0.69754309, 1.16981863,\n        1.33030513, 1.57734588, 0.8597188 , 0.35135396, 0.82764502,\n        0.6132911 , 0.7778497 , 1.74519489, 0.71525789, 0.83759542,\n        2.2658658 , 4.32680955, 1.6694292 , 1.51162302, 0.54751728,\n        1.73666316, 1.22802165, 0.55897645, 0.70077877, 0.67783987,\n        0.70416641, 1.29484782, 0.81285727, 0.7551851 , 1.18750981,\n        2.39590503, 1.30989887, 0.78067317, 1.26049998, 1.12601871,\n        0.74383829, 1.38761336, 1.04839036, 1.80706916, 0.89511397,\n        0.67815156, 1.02856998, 1.19869844, 1.24008379, 1.03128319]])sd_LR(chain, draw)float640.4264 0.5666 ... 0.7408 2.199array([[0.42642209, 0.56658368, 0.22612717, 1.01679817, 0.88298741,\n        0.79844578, 1.06845524, 1.01116888, 1.17196753, 0.68646016,\n        0.52298845, 1.04871932, 0.80907994, 0.55267134, 0.8329922 ,\n        0.52966448, 2.21020544, 1.41438574, 0.37567134, 0.93482058,\n        0.46287048, 2.78251998, 0.4973309 , 0.5777662 , 0.88749141,\n        0.4894204 , 1.60721423, 0.99034809, 1.56408166, 1.46206023,\n        0.47290527, 0.55049193, 1.91196459, 1.60984486, 0.54409737,\n        0.92437949, 0.47057846, 1.05542345, 1.45285099, 1.23966133,\n        0.49286427, 0.72587318, 1.47670485, 0.80327315, 2.82186097,\n        0.84227489, 0.73468409, 1.46461414, 1.10126398, 0.8401041 ,\n        3.15606807, 0.92071496, 1.26257045, 1.14452948, 0.85221578,\n        0.56334063, 2.34389012, 0.69692646, 1.72779033, 0.8915811 ,\n        0.99556055, 0.76215982, 1.45722085, 0.44721338, 2.64225081,\n        0.48494584, 1.0672916 , 1.60750041, 0.36509621, 0.96101016,\n        1.16244405, 0.43065414, 1.11761327, 0.7100209 , 0.93884903,\n        2.70509444, 1.29887552, 0.9918606 , 0.81228163, 0.50699635,\n        0.773202  , 0.89760061, 1.23514715, 0.57867084, 1.85605625,\n        0.89123928, 0.70313308, 0.74401985, 1.44556115, 1.24350453,\n        2.43025646, 1.29244683, 1.79546401, 2.82598257, 0.7961553 ,\n        1.38345842, 0.91631895, 1.66301492, 0.7408245 , 2.19917736]])loss_sd(chain, draw)float641.125 0.9343 ... 0.7389 0.2147array([[1.12498418, 0.93434098, 0.46030768, 2.257342  , 0.21374772,\n        0.91726945, 0.73537603, 1.55424723, 0.54406558, 0.65334056,\n        2.31528541, 6.73632528, 1.37099135, 0.55564015, 0.97225532,\n        1.85811573, 3.03510372, 4.12904219, 2.91891135, 3.60713346,\n        1.97039335, 1.12697793, 1.64147638, 2.48712433, 1.30238633,\n        0.5760673 , 1.91505942, 4.15090475, 1.5021299 , 2.98173813,\n        1.05295248, 0.98447212, 1.61592487, 0.85722816, 0.23498741,\n        1.55819668, 0.19424251, 0.7453568 , 2.03831486, 0.76156841,\n        0.59685355, 0.99539191, 1.21355257, 0.66385773, 4.93765477,\n        2.24221434, 2.47954574, 0.85552962, 1.27245668, 0.24689368,\n        1.47119961, 0.55750146, 0.56697613, 1.07538545, 0.81711272,\n        0.5217074 , 1.36909803, 0.67756641, 2.50169031, 0.61848824,\n        0.45236175, 0.89133743, 2.10085379, 0.69021326, 0.74485059,\n        0.162816  , 1.22308549, 1.76320672, 1.07396387, 0.82171904,\n        0.66734982, 0.8434558 , 0.35177479, 1.18463189, 0.37190066,\n        0.51595577, 2.90231009, 4.05710805, 0.63162129, 0.71817699,\n        3.39525688, 2.74604394, 0.52539   , 2.14493023, 2.14855249,\n        0.79699638, 1.77997412, 3.9296046 , 0.63394609, 1.97036892,\n        1.34200864, 0.30258277, 0.99586174, 0.59550356, 2.57666449,\n        0.63667753, 0.89708921, 0.9530812 , 0.73887686, 0.21472775]])LR_log__(chain, draw, LR_log___dim_0)float64-0.6171 -1.262 ... -3.077 1.319array([[[-6.17143188e-01, -1.26166054e+00, -6.22430937e-01,\n         -1.06125666e+00, -8.31483299e-01, -3.16974749e-01,\n         -2.83178673e-01, -1.61489186e+00, -7.51912234e-01,\n         -1.01947626e+00],\n        [ 4.70138943e-01, -4.07337019e-02,  4.41652909e-01,\n         -6.38739983e-01,  4.02340063e-01, -2.97949948e-01,\n          4.11264185e-01,  8.09602667e-02,  8.74517052e-01,\n          1.11387272e+00],\n        [ 4.26835799e-01,  1.36009991e+00,  6.65032809e-01,\n          6.50516967e-01,  5.06113871e-01,  2.54364564e-01,\n          4.35217322e-01,  5.51644954e-01,  7.92007920e-01,\n          3.54152889e-01],\n        [-1.23916176e+00, -3.71620985e-01, -7.31328680e-01,\n         -1.12553935e+00, -3.65059826e-02, -7.39102641e-01,\n          1.01824470e+00,  8.16408078e-01, -2.43349269e+00,\n         -4.80237285e-01],\n        [ 3.45828028e-01,  1.56488574e+00, -6.28437141e-02,\n          2.47892004e-01,  1.50385087e+00,  1.51489840e+00,\n          1.11068047e+00, -6.43011394e-01,  1.83483899e+00,\n          3.31755049e-01],\n...\n        [ 1.24232366e+00, -1.79165022e+00, -1.05836185e+00,\n         -3.09225403e+00,  6.56009661e-02, -7.50671829e-02,\n          5.89357991e-01,  8.63156384e-01, -2.81532734e-01,\n         -1.24847464e+00],\n        [-6.86571297e-01, -3.02133976e-01,  3.05860017e-01,\n          5.61005282e-04, -1.20440890e-01,  1.16642382e+00,\n         -2.26335350e-01, -9.18702845e-01, -8.41696486e-01,\n         -3.58155741e-01],\n        [ 1.09884268e+00,  1.13119969e+00, -6.78816255e-01,\n         -5.85593856e-01,  7.15943176e-01,  2.24503315e+00,\n         -4.51843600e-01,  3.07867675e-01,  9.31380940e-01,\n         -2.27855222e+00],\n        [-3.37456137e-01,  2.42518433e-01, -4.85528021e-01,\n         -3.01018200e-01, -5.69692766e-01, -9.29658170e-01,\n         -9.53172168e-01,  3.29390486e-02, -1.36732900e-01,\n          7.26723678e-01],\n        [-2.69919207e+00, -2.03199061e+00, -1.40009199e+00,\n         -1.02835591e+00,  2.65043778e+00,  3.31279141e-01,\n         -1.79784476e+00,  2.42504669e+00, -3.07654973e+00,\n          1.31886562e+00]]])omega(chain, draw)float641.562 1.075 1.892 ... 1.111 0.4602array([[1.56225289, 1.07458066, 1.89231067, 0.79097131, 1.26752023,\n        1.39681914, 0.5475191 , 0.65339983, 1.56762583, 1.59774901,\n        1.61090048, 0.33202277, 0.31961028, 0.73652247, 4.07491779,\n        1.09190388, 0.72951322, 2.52887052, 1.08344456, 0.49094421,\n        0.83207731, 0.38940156, 1.4585037 , 2.43662111, 1.38585147,\n        1.1260905 , 1.34595085, 0.56752688, 1.28237514, 0.88504458,\n        1.68260091, 0.87499639, 0.71273768, 0.71476325, 3.71659238,\n        0.57212355, 1.59699987, 0.75345108, 0.77672147, 0.64842519,\n        2.01076562, 0.64988426, 0.94404809, 1.00528225, 0.7711995 ,\n        0.8826861 , 3.07046718, 0.78545014, 0.30590212, 0.63078487,\n        2.13721089, 1.31817633, 0.7072714 , 0.58860254, 2.08251808,\n        0.91947117, 1.46213538, 0.90094518, 0.97617753, 0.91239041,\n        1.89935696, 1.40829757, 1.28906041, 1.23130304, 1.07409503,\n        0.96965612, 0.85435936, 1.39170375, 0.57843202, 0.84056825,\n        1.97226099, 0.86154374, 0.96074961, 1.28061528, 1.987074  ,\n        0.62852286, 0.58970061, 1.10641774, 0.71239307, 0.98905411,\n        0.38666954, 0.59931484, 1.11352875, 0.76847358, 0.76934368,\n        1.31459206, 1.06116352, 0.31579525, 1.43332306, 0.52610615,\n        1.97089823, 1.27423136, 0.87980261, 1.96858134, 2.82349447,\n        1.39576144, 0.60238419, 1.103291  , 1.11119564, 0.46015387]])theta_log__(chain, draw)float64-0.5553 0.2252 ... 0.2152 0.0308array([[-0.55531734,  0.22522281,  1.14015564,  0.84629531,  0.44208591,\n        -0.26923997, -0.18833717, -0.21619318,  0.79032991, -0.84235321,\n         1.06791979, -0.00602262, -0.1455433 ,  0.28122813,  0.36578178,\n         0.10404684,  0.07344223,  0.25477469,  0.77027777,  0.84312014,\n        -0.58846245, -0.09476389,  0.0171443 , -0.16695497, -0.19106268,\n         0.48639862,  0.15433763,  0.2283784 , -0.17985179, -0.42382746,\n         0.60581933, -0.7725674 ,  0.34154884,  0.17809187, -0.05923465,\n        -0.06330032, -0.17716014,  0.81131724,  0.24390738, -0.56099462,\n        -0.63590583, -0.12160116, -0.28332064,  0.31537149, -0.06458664,\n         0.4154263 , -0.47686886,  0.23010355,  0.8304177 , -0.38202012,\n         0.06385591, -0.41459128, -0.25023721,  0.34381298,  0.7672765 ,\n        -0.51862076, -0.57048348,  0.29303298, -0.360191  ,  0.15684872,\n         0.28540834,  0.45574361, -0.15114992, -1.04596112, -0.18917094,\n        -0.48891559, -0.25122197,  0.55686623, -0.33511211, -0.17722009,\n         0.81795694,  1.46483045,  0.51248177,  0.41318392, -0.60236126,\n         0.55196555,  0.20540446, -0.58164794, -0.35556303, -0.3888442 ,\n        -0.35074057,  0.25839318, -0.20719975, -0.2807924 ,  0.17185852,\n         0.87376104,  0.26994994, -0.24759869,  0.23150845,  0.11868815,\n        -0.29593161,  0.32758526,  0.047256  ,  0.59170629, -0.11080422,\n        -0.38838448,  0.02816947,  0.18123634,  0.21517895,  0.03080384]])lm(chain, draw, obs)float64363.6 452.0 ... 1.393e+03 1.058e+05array([[[3.63586344e+02, 4.52004030e+02, 4.80050323e+02, ...,\n         1.74248189e+04, 2.16622228e+04, 1.44767740e+04],\n        [6.73495425e+02, 9.54254126e+02, 1.10077873e+03, ...,\n         5.53406168e+04, 7.84103498e+04, 7.63308220e+04],\n        [1.51973891e+02, 4.40389457e+02, 7.04447107e+02, ...,\n         1.20074665e+04, 3.47951982e+04, 8.41392159e+03],\n        ...,\n        [1.29275537e+03, 1.83085465e+03, 2.10618368e+03, ...,\n         5.99623720e+04, 8.49212392e+04, 2.62740876e+03],\n        [3.00821669e+02, 4.30051269e+02, 4.96767448e+02, ...,\n         2.01637950e+04, 2.88259343e+04, 5.19124828e+04],\n        [3.19557383e+01, 3.70507050e+01, 3.99353603e+01, ...,\n         1.20158548e+03, 1.39316416e+03, 1.05770463e+05]]])Attributes: (4)created_at :2021-07-10T22:10:08.582306arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  prior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 1, draw: 100, obs: 55)\nCoordinates:\n  * chain    (chain) int64 0\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 9 ... 90 91 92 93 94 95 96 97 98 99\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    loss     (chain, draw, obs) float64 -2.671e+03 1.343e+03 ... 9.672e+04\nAttributes:\n    created_at:                 2021-07-10T22:10:08.587369\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 1draw: 100obs: 55Coordinates: (3)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 6 ... 94 95 96 97 98 99array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (1)loss(chain, draw, obs)float64-2.671e+03 1.343e+03 ... 9.672e+04array([[[-2.67100326e+03,  1.34255155e+03,  1.40821615e+03, ...,\n          5.44546338e+04, -2.82037218e+04,  2.25204026e+04],\n        [ 6.91120357e+02,  1.25047752e+03,  2.34623536e+03, ...,\n          3.67907448e+04,  1.27985416e+05,  4.26089822e+04],\n        [-1.05994085e+02,  7.82732937e+02,  3.65909060e+02, ...,\n         -1.07724563e+04,  2.61310680e+04,  2.36614675e+04],\n        ...,\n        [ 8.83043472e+02,  2.15344855e+03,  3.03744893e+03, ...,\n          4.12138121e+04, -2.16126888e+04,  5.72089335e+03],\n        [ 1.13652047e+03,  1.26135401e+03,  1.58322237e+03, ...,\n         -4.63703410e+04,  6.47498746e+04,  1.04504263e+05],\n        [ 1.67803663e+02,  2.66545568e+02, -9.96214671e+01, ...,\n         -2.17424973e+04, -9.03439475e+03,  9.67249413e+04]]])Attributes: (4)created_at :2021-07-10T22:10:08.587369arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (obs: 55)\nCoordinates:\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    loss     (obs) int32 133 333 431 570 615 ... 26012 31677 12604 23446 12292\nAttributes:\n    created_at:                 2021-07-10T22:10:08.588339\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:obs: 55Coordinates: (1)obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (1)loss(obs)int32133 333 431 ... 12604 23446 12292array([  133,   333,   431,   570,   615,   615,   615,   614,   614,\n         614,   934,  1746,  2365,  2579,  2763,  2966,  2940,  2978,\n        2978,  2030,  4864,  6880,  8087,  8595,  8743,  8763,  8762,\n        4537, 11527, 15123, 16656, 17321, 18076, 18308,  7564, 16061,\n       22465, 25204, 26517, 27124,  8343, 19900, 26732, 30079, 31249,\n       12565, 26922, 33867, 38338, 13437, 26012, 31677, 12604, 23446,\n       12292], dtype=int32)Attributes: (4)created_at :2021-07-10T22:10:08.588339arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  constant_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:   (cohort: 10, obs: 55, t_values: 10)\nCoordinates:\n  * t_values  (t_values) int64 1 2 3 4 5 6 7 8 9 10\n  * cohort    (cohort) int64 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\n  * obs       (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    t         (t_values) int32 1 2 3 4 5 6 7 8 9 10\n    premium   (cohort) int32 957 3695 6138 17533 ... 46095 51512 52481 56978\n    t_idx     (obs) int32 0 1 2 3 4 5 6 7 8 9 0 1 2 ... 3 4 0 1 2 3 0 1 2 0 1 0\n    c_idx     (obs) int32 0 0 0 0 0 0 0 0 0 0 1 1 1 ... 5 5 6 6 6 6 7 7 7 8 8 9\nAttributes:\n    created_at:                 2021-07-10T22:10:08.590166\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:cohort: 10obs: 55t_values: 10Coordinates: (3)t_values(t_values)int641 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (4)t(t_values)int321 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=int32)premium(cohort)int32957 3695 6138 ... 51512 52481 56978array([  957,  3695,  6138, 17533, 29341, 37194, 46095, 51512, 52481,\n       56978], dtype=int32)t_idx(obs)int320 1 2 3 4 5 6 7 ... 2 3 0 1 2 0 1 0array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2,\n       3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3,\n       4, 0, 1, 2, 3, 0, 1, 2, 0, 1, 0], dtype=int32)c_idx(obs)int320 0 0 0 0 0 0 0 ... 6 6 7 7 7 8 8 9array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2,\n       2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5,\n       5, 6, 6, 6, 6, 7, 7, 7, 8, 8, 9], dtype=int32)Attributes: (4)created_at :2021-07-10T22:10:08.590166arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n              \n            \n            \n\n\n\naz.plot_ppc(idata, alpha=0.3, kind=\"cumulative\", figsize=(12, 6), textsize=14)\n\narray([<AxesSubplot:xlabel='loss'>], dtype=object)\n\n\n\n\n\n\nmodel_compare = az.compare(\n    {\n        \"Logistic Growth Model\": logistic_idata,\n        \"Weibull Growth Model\": weibull_idata,\n    }\n)\naz.plot_compare(model_compare, figsize=(12, 4), insample_dev=False)\n\nplt.show()\n\n/Users/nathanielforde/anaconda/envs/examined_algorithms/lib/python3.6/site-packages/arviz/stats/stats.py:151: UserWarning: \nThe scale is now log by default. Use 'scale' argument or 'stats.ic_scale' rcParam if you rely on a specific value.\nA higher log-score (or a lower deviance) indicates a model with better predictive accuracy.\n  \"\\nThe scale is now log by default. Use 'scale' argument or \"\n/Users/nathanielforde/anaconda/envs/examined_algorithms/lib/python3.6/site-packages/arviz/stats/stats.py:683: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  \"Estimated shape parameter of Pareto distribution is greater than 0.7 for \"\n/Users/nathanielforde/anaconda/envs/examined_algorithms/lib/python3.6/site-packages/arviz/stats/stats.py:683: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  \"Estimated shape parameter of Pareto distribution is greater than 0.7 for \"\n\n\n\n\n\n\naz.plot_trace(logistic_idata, var_names=[\"gf\", \"LR\"], circ_var_names=[\"LR\"])\n\nTypeError: plot_trace() got an unexpected keyword argument 'circ_var_names'\n\n\n\nlogistic_model.logp\n\n<pymc3.model.LoosePointFunc at 0x14d286b70>\n\n\n\nlogp = logistic_model.logp\nlnp = np.array([logp(logistic_trace.point(i,chain=c)) for c in logistic_trace.chains for i in range(len(logistic_trace))])\n\n\n\n\n0       0.0\n1       0.0\n2       0.0\n3       0.0\n4       0.0\n       ... \n1995    0.0\n1996    0.0\n1997    0.0\n1998    0.0\n1999    0.0\nLength: 2000, dtype: float64\n\n\n\nlogistic_trace.report.log_marginal_likelihood\n\nAttributeError: 'SamplerReport' object has no attribute 'log_marginal_likelihood'\n\n\n\nmodel_compare\n\n\n\n\n\n  \n    \n      \n      rank\n      loo\n      p_loo\n      d_loo\n      weight\n      se\n      dse\n      warning\n      loo_scale\n    \n  \n  \n    \n      Weibull Growth Model\n      0\n      -387.915\n      10.6734\n      0\n      0.811983\n      11.1849\n      0\n      True\n      log\n    \n    \n      Logistic Growth Model\n      1\n      -391.599\n      10.2344\n      3.68346\n      0.188017\n      11.874\n      3.81681\n      True\n      log"
  },
  {
    "objectID": "posts/post-with-code/spatial_correlation/spatial_correlation.html",
    "href": "posts/post-with-code/spatial_correlation/spatial_correlation.html",
    "title": "Gaussian Processes and Inferred Networks",
    "section": "",
    "text": "Of the varied ways in which you add to the world, you might imagine a modest influence over your social network. Of all those connections their cumulative impact is likely less than you imagine. Maybe you’re a new parent hopeful to leave a positive impression on your children, but conscious of friends ignored. Attention is a resource, so the periphery of our network may suffer neglect or fall out altogether - some individuals are sacrificed, others deliberately culled. Your influence may wax and wane, but you aspire to a level of appropriate influence on family and friends. Your aspirations change over time and this has to corressponding effect of shrinking or shifting your sphere of influence. But this phenomena is not unique to new parents, we all struggle to allot time and attention and the residual impact of our past influence always degrades.\nThese dynamics are so familiar that they’ve become cliche tradition. But despite the tired anecdotes of age and degradation, there is an interesting question as to how you might truly measure the evolution of influence over time. This question can be extended more to broadly to how measure correlations of any measure across a cluster of related individuals. We’ll take up a nicely concrete example of the general phenomena modelling spatial correlation and the transmission of technology across an archipelago. This model has some general characteristics which translate well to discussion of changing social dynamics under the pressures of aspiration. And the process will highlight some of features we need to account for when predicting the transmission of information or influence in the production of tools.\n\nTool usage and Population Size\nThe example (taken from McElreath’s superb Statistical Rethinking) looks at the discovery of historic tools across a number of islands in the pacific and tries to draw out the impact of population on the proliferation of those tools. At first glance there seems to be a straightforward relationship between the number of tools discovered and the relative size of the populations. But we’re also interested in how the discovery rates between cultures can be measured.\n\n\n\nLogpop Tools\n\n\nBefore coming to these details we first want to crudely validate the hypothesis that population size and contact with others does influence tool creation. We can see here how there seems to be a direct linear relationship between the tool counts and the log of the population suggesting innovation is achieved at a point of critical mass. Similarly there is a clear distinction between the tool counts achieved based on contact rates.\n<img src=\"tool_count_contact.png\" alt=\"drawing\" height=\"300\"/>\nWith this in mind, we could choose to model this relationship as a simple poisson regression, but even if this solved a prediction problem it doesn’t help us answer the inference question. Plotting the residuals from the model estimated with statsmodels we find a better fit to the data when we include the contact rate information. But this is a kind of crude observation. Yes, contact rates matter but what can we say about the strength of these relationships.\n\n\nSpatial Correlation and Covariance\nSo we have a reasonable theory that greater population and increased interaction between neighbouring populations is likely to increase tool creation. We want to know how the individual islands directly influenced the tool prediction on their neighbouring islands. To measure this influence we’ll look at geographic distance to determine if and how the proximity of islands shapes the relative proliferation of tools in the historic record. This is the crucial set of measurements. It is our proxy for a series of less quantifiable cross-cultural influences. We know Tonga has the largest population but we don’t know how their influence extends across the network. We don’t know, for example, if technological innovation stems from the population centres such as Tonga or was just most pronounced there in the historic record?\n\n\n\nDistance Plot\n\n\nAs before we want to predict tool use, but now we wish to allow that the tool use on the neighbouring islands is informed by their relative distance from one another. Put another way we might say that the prior covariance structure is a function of observed distance. This suggests a model like the following:\nwith pm.Model() as poisson_GP_model:\n    ## Priors for the covariance matrix\n    etasq = pm.HalfCauchy('etasq', 1)\n    rhosq = pm.HalfCauchy('rhosq', 1)\n    \n    ## Gaussian Process to extract a varying intercept model for each society\n    ## We fit a covariance matrix against the squared distance between islands\n    ## for (a) mathematical convenience and (b) the quickly decreasing influence\n    ## represented by the parabola like shape of the square distance. \n    Kij = etasq*(tt.exp(-rhosq*Dmatsq)+np.diag([.01]*Nsociety))\n\n    ## The prior covariance matrix is used to model a multivariate normal distribution \n    ## distribution resulting in a baseline estimate for each society\n    g = pm.MvNormal('g', mu=np.zeros(Nsociety), cov=Kij, shape=Nsociety)\n    \n    ## The global priors for prediction model\n    a = pm.Normal('a', 0, 10)\n    bp = pm.Normal('bp', 0, 1)\n    ## prediction model with a varying intercept derived from MvNormal\n    eq= pm.math.exp(a + g[df['society_idx'].values] + bp*df['logpop'])\n\n    ## The Poisson likelihood\n    obs = pm.Poisson('totalTools', eq, observed=df['total_tools'])\n\n    idata = pm.sample(1000, tune=1000, return_inferencedata=True)\nWhich sees is a hierarchical model of this format which embeds uncertainty in the estimation of the covariance matrix by using the hierarchical priors etasq and rhosq.\n\n\n\nDistance Plot\n\n\nThe prediction of tools count is based on a poisson distribution with a mean expressed as a kind of fixed-effects model with varying intercepts for each island.\n\\[ totalTools \\sim \\mathbf{Poisson}(exp((a + g_{i}) + bp*Logpop)) \\]\nwhere \\(g_{i}\\) is an island specific estimate of tool production informed by the distance between neighbouring islands. This is the underlying gaussian process assumed to contribute to the tool-creation capacity of each island. in So when we fit this model it returns estimated posterior probability distributions for all the parameters in the model, including the hierarchical parameters for the covariance matrix K. We can therefore “reconstruct” the covariance matrix that is suggested by conditioning on the observed tool counts.\n_, ax = plt.subplots(1, 1, figsize=(8, 5))\nxrange = np.linspace(0, 10, 200)\npost_etasq = idata.posterior['etasq'].to_dataframe()\npost_rhosq = idata.posterior['rhosq'].to_dataframe()\n\n## compute posterior median covariance among societies\n## we compute the median rather than the mean because of the skew in the distribution\n## makes the median a better measure of central tendency.\nKij_post = np.median(post_etasq) * (np.exp(-np.median(post_rhosq) * Dmatsq) + np.diag([.01] * Nsociety))\nsigma_post = np.sqrt(np.diag(Kij_post))\nRho = np.diag(sigma_post**-1).dot(Kij_post.dot(np.diag(sigma_post**-1)))\nRho = pd.DataFrame(Rho, df['culture'], columns=df['culture'])\n\n## Plot Point Estimates\nax.plot(xrange, np.median(post_etasq) * np.exp(-np.median(post_rhosq) * xrange**2), 'k', legend='Median')\n## Plot Draws from Posterior\nax.plot(xrange, (post_etasq.sample(len(xrange)).values * np.exp(-post_rhosq.sample(len(xrange)).values * xrange**2)).T,\n        'k', alpha=.1)\n\nax.set_ylim(0, 1)\nax.set_xlabel('distance')\nax.set_ylabel('covariance')\nax.set_title(r' $\\eta^2 exp(\\rho^2Distance^2)$')\nplt.suptitle(\"Draws from Posterior Distribution\", y=1.05, fontsize=10)\nWhich allows us to recover the steeply degrading slope of influence.\n\n\n\nDistance Plot\n\n\nand establish the probable correlation structure between the islands.\n\n\n\nDistance Plot\n\n\nThese estimates are plausible as they track well with the geographic details. In particular we can see how the distance of Hawaii proves a fairly insurmountable barrier preventing strong correlation. Conversely, we see a strong relationship between the small cluster of islands: Malekula,Tikopia and Santa Cruz. We can further extract the expected growth models for each island.\na = idata.posterior['a'].to_dataframe()\ng = idata.posterior['g'].to_dataframe()\ng = g.reset_index().pivot(['chain', 'draw'], 'g_dim_0', 'g')\nb = idata.posterior['bp'].to_dataframe()\nxrange = np.linspace(0, 15, 100)\nexpected = pd.DataFrame(g.mean(), columns=['island_intercept']).reset_index(drop=True) \nexpected['global_intercept'] = a.mean()[0]\nexpected['global_beta'] = b.mean()[0]\nexpected['island'] = df['culture']\nfor i in range(len(expected)):\n    y  = (expected.iloc[i]['global_intercept'] + expected.iloc[i]['island_intercept']) + expected.iloc[i]['global_beta']*xrange\n    plt.plot(xrange, np.exp(y), label=expected.iloc[i]['island']) \nplt.title(\"Expected Tool Counts against LogPopulation\")\nplt.xlabel(\"Log-population\")\nplt.ylabel(r'$exp(\\alpha + g_{i} + \\beta LogPop)$')\nplt.legend()\n\n\n\nExpected\n\n\nWhich suggests that even though Hawaii has the largest population it suffers from lesser exposures to foreign influence and slower rate of tool innovation than Yap which is positively influenced by the existence of its neigbours.\n\n\nProjects, Aspiration and inferred Influence\nYour role in a network can change gradually or dramatically. In either case it may be hard to know how you will feel about those changes. There is a school of thought that argues certain varieties of event are so transformative (e.g. parenthood, near-death events) that they are impossible to predict how you feel after the event occurs. The notion rests on the idea that individual decisions are weighed against the probability of the outcome and the attendant rewards for each decision, but in the scenario of transformative events we can’t even begin to estimate our subjective utility in the future. An expectant parent will not be able to predict how they will feel as a new parent. Averages from other parents will not capture how you will feel or how your priorities and goals will change. The example above shows how we can infer the latent structure of a social network from observable facts about the output of the group. The hypothesis of a transformative event could be profitably investigated by tracking the evolution of these inferred social network over a number of distinct epochs. Whether these events generate gradual or dramatic change would be quickly reflected in the influence and interaction with their peers. Deliberate cultivation of particular peer groups for a work project or personal goal will be captured just so long as we can measure the output. The focus of a new parent or the introduction of a new child into the peer network may alter the relations throughout the network. These kind of models offer a plausible way to express the inferred network effects."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nChoice Models and Subjective Utility\n\n\nDublin Data Science\n\n\n\n\nbayesian\n\n\ndiscrete choice\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2023\n\n\n14 min\n\n\n\n\n\n\n  \n\n\n\n\nTenuous Relations and Timeseries Analysis\n\n\nBerlin Timeseries\n\n\n\n\nbayesian\n\n\nVAR\n\n\ntimeseries analysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2023\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/tenuous_relations/first_talk.html",
    "href": "talks/tenuous_relations/first_talk.html",
    "title": "Tenuous Relations and Timeseries Analysis",
    "section": "",
    "text": "Hierarchical Bayesian VARs"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Differences in Differences\n\n\n\n\n\n\n\n\n\nFeb 4, 2023\n\n\n\n\n\n\n\n\nBayesian Estimation and the Histogram Approximation\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\n\n\n\n\n\n\nPsi, Replication and Bayesian Evidence\n\n\n\n\n\n\n\n\n\nAug 1, 2021\n\n\n\n\n\n\n\n\nGaussian Processes and Inferred Networks\n\n\nThis post describes the idea of spatial correlation outlined using a gaussian process model\n\n\n\n\n\n\nJun 22, 2021\n\n\n\n\n\n\n\n\nWheat from Chaff: Maximum Entropy and Information\n\n\n\n\n\n\n\n\n\nMar 22, 2021\n\n\n\n\n\n\n\n\nBayesian Decisions & Model Comparison\n\n\n\n\n\n\n\n\n\nMar 22, 2021\n\n\n\n\n\n\n\n\nTrains, Planes …Utility and Maximum Likelihood\n\n\n\n\n\n\n\n\n\nFeb 21, 2021\n\n\n\n\n\n\n\n\nFactor Analysis and Construct Validity in Psychology\n\n\n\n\n\n\n\n\n\nJan 22, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "Welcome to my site!\nYou’ll find a range of writings on data science, philosophy, statistics and inference more generally. I’ll update this site with semi-regular blog entries, and any talks or presentations on these topics.\n\n\nI’m a data scientist specialising in probabilistic modelling for the study of risk and causal inference. I have experience in model development, deployment, multivariate testing and monitoring.\nI’m interested in questions of inference and measurement in the face of natural variation and confounding.\nMy academic background is in mathematical logic and philosophy where I mostly imagined possible worlds and modal logics.\n\nRecent ExperiencePast Experience\n\n\n\nPersonio | Senior Data Scientist | December 2021 - Present\n\nWorking with Product and Engineering to measure risk and quantify impact.\n\nPyMC | Open Source Contributor | November 2021 - Present\n\nHelping to document some of the more esoteric applications of Bayesian modelling in PyMC."
  },
  {
    "objectID": "notes/Statistics/Introduction - Statistics Topics.html",
    "href": "notes/Statistics/Introduction - Statistics Topics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "The idea is that this section of my notes will serve to capture my zettelkasten style notes on topics in Statistics."
  },
  {
    "objectID": "notes/Uncertain Things.html",
    "href": "notes/Uncertain Things.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "This is a space for my notes. They’ll be sporadic, haphazard in their level of detail but I aim for them to be regular and consitently updated log of topics of interest or distraction."
  },
  {
    "objectID": "notes/Logic/Introduction - Logic Topics.html",
    "href": "notes/Logic/Introduction - Logic Topics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "The idea is that this section will serve to capture any and all notes on topics in logic -mathematical, philosophical or any style in between."
  },
  {
    "objectID": "notes/Philosophy/Introduction - Philosophy Topics.html",
    "href": "notes/Philosophy/Introduction - Philosophy Topics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "The idea is that this section will serve to capture the philosophical topics in my Zettlekasten notes."
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.html",
    "href": "oss/pymc/bayesian_var_model.html",
    "title": "Bayesian Vector Autoregressive Models in PyMC",
    "section": "",
    "text": "Bayesian Vector Autoregressive Models\nIn this project I demonstrated how to fit a hierarchical bayesian autoregressive model in PyMC. The work drew on a PyMC labs blogpost showing how to fit a simple VAR model in PyMC. We applied these types of model to econometric timeseries data to analyse the relationships between GDP, investment and consumption for Ireland.\nThe project culminated in a publication to the official PyMC documentation that can be found online here or downloaded as notebook here\n\n\n\nIreland’s GDP v Peers"
  },
  {
    "objectID": "oss/pymc/discrete_choice.html",
    "href": "oss/pymc/discrete_choice.html",
    "title": "Discrete Choice Models in PyMC",
    "section": "",
    "text": "In this project I demonstrated how to fit a discrete choice models using random utility components in PyMC. I applied these types of model to micro-econometric data over product choice to estimate market share and the correlation among good within an market.\nThe project culminated in a publication to the official PyMC documentation that can be found online here or downloaded as notebook here\n\n\n\nDiscrete Choice"
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.myst.html",
    "href": "oss/pymc/bayesian_var_model.myst.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "(Bayesian Vector Autoregressive Models)= # Bayesian Vector Autoregressive Models\n:::{post} November, 2022 :tags: time series, vector autoregressive model, hierarchical model :category: intermediate :author: Nathaniel Forde :::\n```{code-cell} ipython3 import os\nimport arviz as az import matplotlib.pyplot as plt import numpy as np import pandas as pd import pymc as pm import statsmodels.api as sm\nfrom pymc.sampling_jax import sample_blackjax_nuts"
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.myst.html#vectorautoregression-models",
    "href": "oss/pymc/bayesian_var_model.myst.html#vectorautoregression-models",
    "title": "Examined Algorithms",
    "section": "V(ector)A(uto)R(egression) Models",
    "text": "V(ector)A(uto)R(egression) Models\nIn this notebook we will outline an application of the Bayesian Vector Autoregressive Modelling. We will draw on the work in the PYMC Labs blogpost (see {cite:t}vieira2022BVAR). This will be a three part series. In the first we want to show how to fit Bayesian VAR models in PYMC. In the second we will show how to extract extra insight from the fitted model with Impulse Response analysis and make forecasts from the fitted VAR model. In the third and final post we will show in some more detail the benefits of using hierarchical priors with Bayesian VAR models. Specifically, we’ll outline how and why there are actually a range of carefully formulated industry standard priors which work with Bayesian VAR modelling.\nIn this post we will (i) demonstrate the basic pattern on a simple VAR model on fake data and show how the model recovers the true data generating parameters and (ii) we will show an example applied to macro-economic data and compare the results to those achieved on the same data with statsmodels MLE fits and (iii) show an example of estimating a hierarchical bayesian VAR model over a number of countries."
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.myst.html#autoregressive-models-in-general",
    "href": "oss/pymc/bayesian_var_model.myst.html#autoregressive-models-in-general",
    "title": "Examined Algorithms",
    "section": "Autoregressive Models in General",
    "text": "Autoregressive Models in General\nThe idea of a simple autoregressive model is to capture the manner in which past observations of the timeseries are predictive of the current observation. So in traditional fashion, if we model this as a linear phenomena we get simple autoregressive models where the current value is predicted by a weighted linear combination of the past values and an error term.\n\\[ y_t = \\alpha + \\beta_{y0} \\cdot y_{t-1} + \\beta_{y1} \\cdot y_{t-2} ... + \\epsilon \\]\nfor however many lags are deemed appropriate to the predict the current observation.\nA VAR model is kind of generalisation of this framework in that it retains the linear combination approach but allows us to model multiple timeseries at once. So concretely this mean that \\(\\mathbf{y}_{t}\\) as a vector where:\n\\[ \\mathbf{y}_{T} =  \\nu + A_{1}\\mathbf{y}_{T-1} + A_{2}\\mathbf{y}_{T-2} ... A_{p}\\mathbf{y}_{T-p} + \\mathbf{e}_{t}  \\]\nwhere the As are coefficient matrices to be combined with the past values of each individual timeseries. For example consider an economic example where we aim to model the relationship and mutual influence of each variable on themselves and one another.\n\\[ \\begin{bmatrix} gdp \\\\ inv \\\\ con \\end{bmatrix}_{T} = \\nu + A_{1}\\begin{bmatrix} gdp \\\\ inv \\\\ con \\end{bmatrix}_{T-1} +\n    A_{2}\\begin{bmatrix} gdp \\\\ inv \\\\ con \\end{bmatrix}_{T-2} ... A_{p}\\begin{bmatrix} gdp \\\\ inv \\\\ con \\end{bmatrix}_{T-p} + \\mathbf{e}_{t} \\]\nThis structure is compact representation using matrix notation. The thing we are trying to estimate when we fit a VAR model is the A matrices that determine the nature of the linear combination that best fits our timeseries data. Such timeseries models can have an auto-regressive or a moving average representation, and the details matter for some of the implication of a VAR model fit.\nWe’ll see in the next notebook of the series how the moving-average representation of a VAR lends itself to the interpretation of the covariance structure in our model as representing a kind of impulse-response relationship between the component timeseries.\n\nA Concrete Specification with Two lagged Terms\nThe matrix notation is convenient to suggest the broad patterns of the model, but it is useful to see the algebra is a simple case. Consider the case of Ireland’s GDP and consumption described as:\n\\[ gdp_{t} = \\beta_{gdp1} \\cdot gdp_{t-1} + \\beta_{gdp2} \\cdot gdp_{t-2} +  \\beta_{cons1} \\cdot cons_{t-1} + \\beta_{cons2} \\cdot cons_{t-2}  + \\epsilon_{gdp}\\] \\[ cons_{t} = \\beta_{cons1} \\cdot cons_{t-1} + \\beta_{cons2} \\cdot cons_{t-2} +  \\beta_{gdp1} \\cdot gdp_{t-1} + \\beta_{gdp2} \\cdot gdp_{t-2}  + \\epsilon_{cons}\\]\nIn this way we can see that if we can estimate the \\(\\beta\\) terms we have an estimate for the bi-directional effects of each variable on the other. This is a useful feature of the modelling. In what follows i should stress that i’m not an economist and I’m aiming to show only the functionality of these models not give you a decisive opinion about the economic relationships determining Irish GDP figures.\n\n\nCreating some Fake Data\n{code-cell} ipython3 def simulate_var(     intercepts, coefs_yy, coefs_xy, coefs_xx, coefs_yx, noises=(1, 1), *, warmup=100, steps=200 ):     draws_y = np.zeros(warmup + steps)     draws_x = np.zeros(warmup + steps)     draws_y[:2] = intercepts[0]     draws_x[:2] = intercepts[1]     for step in range(2, warmup + steps):         draws_y[step] = (             intercepts[0]             + coefs_yy[0] * draws_y[step - 1]             + coefs_yy[1] * draws_y[step - 2]             + coefs_xy[0] * draws_x[step - 1]             + coefs_xy[1] * draws_x[step - 2]             + rng.normal(0, noises[0])         )         draws_x[step] = (             intercepts[1]             + coefs_xx[0] * draws_x[step - 1]             + coefs_xx[1] * draws_x[step - 2]             + coefs_yx[0] * draws_y[step - 1]             + coefs_yx[1] * draws_y[step - 2]             + rng.normal(0, noises[1])         )     return draws_y[warmup:], draws_x[warmup:]\nFirst we generate some fake data with known parameters.\n```{code-cell} ipython3 var_y, var_x = simulate_var( intercepts=(18, 8), coefs_yy=(-0.8, 0), coefs_xy=(0.9, 0), coefs_xx=(1.3, -0.7), coefs_yx=(-0.1, 0.3), )\ndf = pd.DataFrame({“x”: var_x, “y”: var_y}) df.head()\n\n```{code-cell} ipython3\nfig, axs = plt.subplots(2, 1, figsize=(10, 3))\naxs[0].plot(df[\"x\"], label=\"x\")\naxs[0].set_title(\"Series X\")\naxs[1].plot(df[\"y\"], label=\"y\")\naxs[1].set_title(\"Series Y\");"
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.myst.html#handling-multiple-lags-and-different-dimensions",
    "href": "oss/pymc/bayesian_var_model.myst.html#handling-multiple-lags-and-different-dimensions",
    "title": "Examined Algorithms",
    "section": "Handling Multiple Lags and Different Dimensions",
    "text": "Handling Multiple Lags and Different Dimensions\nWhen Modelling multiple timeseries and accounting for potentially any number lags to incorporate in our model we need to abstract some of the model definition to helper functions. An example will make this a bit clearer.\n```{code-cell} ipython3 ### Define a helper function that will construct our autoregressive step for the marginal contribution of each lagged ### term in each of the respective time series equations def calc_ar_step(lag_coefs, n_eqs, n_lags, df): ars = [] for j in range(n_eqs): ar = pm.math.sum( [ pm.math.sum(lag_coefs[j, i] * df.values[n_lags - (i + 1) : -(i + 1)], axis=-1) for i in range(n_lags) ], axis=0, ) ars.append(ar) beta = pm.math.stack(ars, axis=-1)\nreturn beta\n\nMake the model in such a way that it can handle different specifications of the likelihood term\n\n\nand can be run for simple prior predictive checks. This latter functionality is important for debugging of\n\n\nshape handling issues. Building a VAR model involves quite a few moving parts and it is handy to\n### inspect the shape implied in the prior predictive checks. def make_model(n_lags, n_eqs, df, priors, mv_norm=True, prior_checks=True): coords = { “lags”: np.arange(n_lags) + 1, “equations”: df.columns.tolist(), “cross_vars”: df.columns.tolist(), “time”: [x for x in df.index[n_lags:]], }\nwith pm.Model(coords=coords) as model:\n    lag_coefs = pm.Normal(\n        \"lag_coefs\",\n        mu=priors[\"lag_coefs\"][\"mu\"],\n        sigma=priors[\"lag_coefs\"][\"sigma\"],\n        dims=[\"equations\", \"lags\", \"cross_vars\"],\n    )\n    alpha = pm.Normal(\n        \"alpha\", mu=priors[\"alpha\"][\"mu\"], sigma=priors[\"alpha\"][\"sigma\"], dims=(\"equations\",)\n    )\n    data_obs = pm.Data(\"data_obs\", df.values[n_lags:], dims=[\"time\", \"equations\"], mutable=True)\n\n    betaX = calc_ar_step(lag_coefs, n_eqs, n_lags, df)\n    betaX = pm.Deterministic(\n        \"betaX\",\n        betaX,\n        dims=[\n            \"time\",\n        ],\n    )\n    mean = alpha + betaX\n\n    if mv_norm:\n        n = df.shape[1]\n        ## Under the hood the LKJ prior will retain the correlation matrix too.\n        noise_chol, _, _ = pm.LKJCholeskyCov(\n            \"noise_chol\",\n            eta=priors[\"noise_chol\"][\"eta\"],\n            n=n,\n            sd_dist=pm.HalfNormal.dist(sigma=priors[\"noise_chol\"][\"sigma\"]),\n        )\n        obs = pm.MvNormal(\n            \"obs\", mu=mean, chol=noise_chol, observed=data_obs, dims=[\"time\", \"equations\"]\n        )\n    else:\n        ## This is an alternative likelihood that can recover sensible estimates of the coefficients\n        ## But lacks the multivariate correlation between the timeseries.\n        sigma = pm.HalfNormal(\"noise\", sigma=priors[\"noise\"][\"sigma\"], dims=[\"equations\"])\n        obs = pm.Normal(\n            \"obs\", mu=mean, sigma=sigma, observed=data_obs, dims=[\"time\", \"equations\"]\n        )\n\n    if prior_checks:\n        idata = pm.sample_prior_predictive()\n        return model, idata\n    else:\n        idata = pm.sample_prior_predictive()\n        idata.extend(pm.sample(draws=2000, random_seed=130))\n        pm.sample_posterior_predictive(idata, extend_inferencedata=True, random_seed=rng)\nreturn model, idata\n\nThe model has a deterministic component in the auto-regressive calculation which is required at each timestep, but the key point here is that we model the likelihood of the VAR as a multivariate normal distribution with a particular covariance relationship. The estimation of these covariance relationship gives the main insight in the manner in which our component timeseries relate to one another. \n\nWe will inspect the structure of a VAR with 2 lags and 2 equations\n\n```{code-cell} ipython3\nn_lags = 2\nn_eqs = 2\npriors = {\n    \"lag_coefs\": {\"mu\": 0.3, \"sigma\": 1},\n    \"alpha\": {\"mu\": 15, \"sigma\": 5},\n    \"noise_chol\": {\"eta\": 1, \"sigma\": 1},\n    \"noise\": {\"sigma\": 1},\n}\n\nmodel, idata = make_model(n_lags, n_eqs, df, priors)\npm.model_to_graphviz(model)\nAnother VAR with 3 lags and 2 equations.\n{code-cell} ipython3 n_lags = 3 n_eqs = 2 model, idata = make_model(n_lags, n_eqs, df, priors) for rv, shape in model.eval_rv_shapes().items():     print(f\"{rv:>11}: shape={shape}\") pm.model_to_graphviz(model)\nWe can inspect the correlation matrix between our timeseries which is implied by the prior specification, to see that we have allowed a flat uniform prior over their correlation.\n{code-cell} ipython3 ax = az.plot_posterior(     idata,     var_names=\"noise_chol_corr\",     hdi_prob=\"hide\",     group=\"prior\",     point_estimate=\"mean\",     grid=(2, 2),     kind=\"hist\",     ec=\"black\",     figsize=(10, 4), )\nNow we will fit the VAR with 2 lags and 2 equations\n{code-cell} ipython3 n_lags = 2 n_eqs = 2 model, idata_fake_data = make_model(n_lags, n_eqs, df, priors, prior_checks=False)\nWe’ll now plot some of the results to see that the parameters are being broadly recovered. The alpha parameters match well, but the individual lag coefficients show differences.\n{code-cell} ipython3 az.summary(idata_fake_data, var_names=[\"alpha\", \"lag_coefs\", \"noise_chol_corr\"])\n{code-cell} ipython3 az.plot_posterior(idata_fake_data, var_names=[\"alpha\"], ref_val=[18, 8]);\nNext we’ll plot the posterior predictive distribution to check that the fitted model can capture the patterns in the observed data. This is the primary test of goodness of fit.\n```{code-cell} ipython3 def shade_background(ppc, ax, idx, palette=“cividis”): palette = palette cmap = plt.get_cmap(palette) percs = np.linspace(51, 99, 100) colors = (percs - np.min(percs)) / (np.max(percs) - np.min(percs)) for i, p in enumerate(percs[::-1]): upper = np.percentile( ppc[:, idx, :], p, axis=1, ) lower = np.percentile( ppc[:, idx, :], 100 - p, axis=1, ) color_val = colors[i] ax[idx].fill_between( x=np.arange(ppc.shape[0]), y1=upper.flatten(), y2=lower.flatten(), color=cmap(color_val), alpha=0.1, )\ndef plot_ppc(idata, df, group=“posterior_predictive”): fig, axs = plt.subplots(2, 1, figsize=(25, 15)) df = pd.DataFrame(idata_fake_data[“observed_data”][“obs”].data, columns=[“x”, “y”]) axs = axs.flatten() ppc = az.extract_dataset(idata, group=group, num_samples=100)[“obs”] # Minus the lagged terms and the constant shade_background(ppc, axs, 0, “inferno”) axs[0].plot(np.arange(ppc.shape[0]), ppc[:, 0, :].mean(axis=1), color=“cyan”, label=“Mean”) axs[0].plot(df[“x”], “o”, mfc=“black”, mec=“white”, mew=1, markersize=7, label=“Observed”) axs[0].set_title(“VAR Series 1”) axs[0].legend() shade_background(ppc, axs, 1, “inferno”) axs[1].plot(df[“y”], “o”, mfc=“black”, mec=“white”, mew=1, markersize=7, label=“Observed”) axs[1].plot(np.arange(ppc.shape[0]), ppc[:, 1, :].mean(axis=1), color=“cyan”, label=“Mean”) axs[1].set_title(“VAR Series 2”) axs[1].legend()\nplot_ppc(idata_fake_data, df)\n\nAgain we can check the learned posterior distribution for the correlation parameter.\n\n```{code-cell} ipython3\nax = az.plot_posterior(\n    idata_fake_data,\n    var_names=\"noise_chol_corr\",\n    hdi_prob=\"hide\",\n    point_estimate=\"mean\",\n    grid=(2, 2),\n    kind=\"hist\",\n    ec=\"black\",\n    figsize=(10, 6),\n)"
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.myst.html#applying-the-theory-macro-economic-timeseries",
    "href": "oss/pymc/bayesian_var_model.myst.html#applying-the-theory-macro-economic-timeseries",
    "title": "Examined Algorithms",
    "section": "Applying the Theory: Macro Economic Timeseries",
    "text": "Applying the Theory: Macro Economic Timeseries\nThe data is from the World Bank’s World Development Indicators. In particular, we’re pulling annual values of GDP, consumption, and gross fixed capital formation (investment) for all countries from 1970. Timeseries models in general work best when we have a stable mean throughout the series, so for the estimation procedure we have taken the first difference and the natural log of each of these series.\n```{code-cell} ipython3 try: gdp_hierarchical = pd.read_csv( os.path.join(“..”, “data”, “gdp_data_hierarchical_clean.csv”), index_col=0 ) except FileNotFoundError: gdp_hierarchical = pd.read_csv(pm.get_data(“gdp_data_hierarchical_clean.csv”), …)\ngdp_hierarchical\n\n```{code-cell} ipython3\nfig, axs = plt.subplots(3, 1, figsize=(20, 10))\nfor country in gdp_hierarchical[\"country\"].unique():\n    temp = gdp_hierarchical[gdp_hierarchical[\"country\"] == country].reset_index()\n    axs[0].plot(temp[\"dl_gdp\"], label=f\"{country}\")\n    axs[1].plot(temp[\"dl_cons\"], label=f\"{country}\")\n    axs[2].plot(temp[\"dl_gfcf\"], label=f\"{country}\")\naxs[0].set_title(\"Differenced and Logged GDP\")\naxs[1].set_title(\"Differenced and Logged Consumption\")\naxs[2].set_title(\"Differenced and Logged Investment\")\naxs[0].legend()\naxs[1].legend()\naxs[2].legend()\nplt.suptitle(\"Macroeconomic Timeseries\");"
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.myst.html#irelands-economic-situation",
    "href": "oss/pymc/bayesian_var_model.myst.html#irelands-economic-situation",
    "title": "Examined Algorithms",
    "section": "Ireland’s Economic Situation",
    "text": "Ireland’s Economic Situation\nIreland is somewhat infamous for its GDP numbers that are largely the product of foreign direct investment and inflated beyond expectation in recent years by the investment and taxation deals offered to large multi-nationals. We’ll look here at just the relationship between GDP and consumption. We just want to show the mechanics of the VAR estimation, you shouldn’t read too much into the subsequent analysis.\n{code-cell} ipython3 ireland_df = gdp_hierarchical[gdp_hierarchical[\"country\"] == \"Ireland\"] ireland_df.reset_index(inplace=True, drop=True) ireland_df.head()\n{code-cell} ipython3 n_lags = 2 n_eqs = 2 priors = {     ## Set prior for expected positive relationship between the variables.     \"lag_coefs\": {\"mu\": 0.3, \"sigma\": 1},     \"alpha\": {\"mu\": 0, \"sigma\": 0.1},     \"noise_chol\": {\"eta\": 1, \"sigma\": 1},     \"noise\": {\"sigma\": 1}, } model, idata_ireland = make_model(     n_lags, n_eqs, ireland_df[[\"dl_gdp\", \"dl_cons\"]], priors, prior_checks=False ) idata_ireland\n{code-cell} ipython3 az.plot_trace(idata_ireland, var_names=[\"lag_coefs\", \"alpha\", \"betaX\"], kind=\"rank_vlines\");\n```{code-cell} ipython3 def plot_ppc_macro(idata, df, group=“posterior_predictive”): df = pd.DataFrame(idata[“observed_data”][“obs”].data, columns=[“dl_gdp”, “dl_cons”]) fig, axs = plt.subplots(2, 1, figsize=(20, 10)) axs = axs.flatten() ppc = az.extract_dataset(idata, group=group, num_samples=100)[“obs”]\nshade_background(ppc, axs, 0, \"inferno\")\naxs[0].plot(np.arange(ppc.shape[0]), ppc[:, 0, :].mean(axis=1), color=\"cyan\", label=\"Mean\")\naxs[0].plot(df[\"dl_gdp\"], \"o\", mfc=\"black\", mec=\"white\", mew=1, markersize=7, label=\"Observed\")\naxs[0].set_title(\"Differenced and Logged GDP\")\naxs[0].legend()\nshade_background(ppc, axs, 1, \"inferno\")\naxs[1].plot(df[\"dl_cons\"], \"o\", mfc=\"black\", mec=\"white\", mew=1, markersize=7, label=\"Observed\")\naxs[1].plot(np.arange(ppc.shape[0]), ppc[:, 1, :].mean(axis=1), color=\"cyan\", label=\"Mean\")\naxs[1].set_title(\"Differenced and Logged Consumption\")\naxs[1].legend()\nplot_ppc_macro(idata_ireland, ireland_df)\n\n```{code-cell} ipython3\nax = az.plot_posterior(\n    idata_ireland,\n    var_names=\"noise_chol_corr\",\n    hdi_prob=\"hide\",\n    point_estimate=\"mean\",\n    grid=(2, 2),\n    kind=\"hist\",\n    ec=\"black\",\n    figsize=(10, 6),\n)\n\nComparison with Statsmodels\nIt’s worthwhile comparing these model fits to the one achieved by Statsmodels just to see if we can recover a similar story.\n{code-cell} ipython3 VAR_model = sm.tsa.VAR(ireland_df[[\"dl_gdp\", \"dl_cons\"]]) results = VAR_model.fit(2, trend=\"c\")\n{code-cell} ipython3 results.params\nThe intercept parameters broadly agree with our Bayesian model with some differences in the implied relationships defined by the estimates for the lagged terms.\n{code-cell} ipython3 corr = pd.DataFrame(results.resid_corr, columns=[\"dl_gdp\", \"dl_cons\"]) corr.index = [\"dl_gdp\", \"dl_cons\"] corr\nThe residual correlation estimates reported by statsmodels agree quite closely with the multivariate gaussian correlation between the variables in our Bayesian model.\n{code-cell} ipython3 az.summary(idata_ireland, var_names=[\"alpha\", \"lag_coefs\", \"noise_chol_corr\"])\nWe plot the alpha parameter estimates against the Statsmodels estimates\n{code-cell} ipython3 az.plot_posterior(idata_ireland, var_names=[\"alpha\"], ref_val=[0.034145, 0.006996]);\n{code-cell} ipython3 az.plot_posterior(     idata_ireland,     var_names=[\"lag_coefs\"],     ref_val=[0.330003, -0.053677],     coords={\"equations\": \"dl_cons\", \"lags\": [1, 2], \"cross_vars\": \"dl_gdp\"}, );\nWe can see here again how the Bayesian VAR model recovers much of the same story. Similar magnitudes in the estimates for the alpha terms for both equations and a clear relationship between the first lagged GDP numbers and consumption along with a very similar covariance structure.\n+++"
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.myst.html#adding-a-bayesian-twist-hierarchical-vars",
    "href": "oss/pymc/bayesian_var_model.myst.html#adding-a-bayesian-twist-hierarchical-vars",
    "title": "Examined Algorithms",
    "section": "Adding a Bayesian Twist: Hierarchical VARs",
    "text": "Adding a Bayesian Twist: Hierarchical VARs\nIn addition we can add some hierarchical parameters if we want to model multiple countries and the relationship between these economic metrics at the national level. This is a useful technique in the cases where we have reasonably short timeseries data because it allows us to “borrow” information across the countries to inform the estimates of the key parameters.\n```{code-cell} ipython3 def make_hierarchical_model(n_lags, n_eqs, df, group_field, prior_checks=True): cols = [col for col in df.columns if col != group_field] coords = {“lags”: np.arange(n_lags) + 1, “equations”: cols, “cross_vars”: cols}\ngroups = df[group_field].unique()\n\nwith pm.Model(coords=coords) as model:\n    ## Hierarchical Priors\n    rho = pm.Beta(\"rho\", alpha=2, beta=2)\n    alpha_hat_location = pm.Normal(\"alpha_hat_location\", 0, 0.1)\n    alpha_hat_scale = pm.InverseGamma(\"alpha_hat_scale\", 3, 0.5)\n    beta_hat_location = pm.Normal(\"beta_hat_location\", 0, 0.1)\n    beta_hat_scale = pm.InverseGamma(\"beta_hat_scale\", 3, 0.5)\n    omega_global, _, _ = pm.LKJCholeskyCov(\n        \"omega_global\", n=n_eqs, eta=1.0, sd_dist=pm.Exponential.dist(1)\n    )\n\n    for grp in groups:\n        df_grp = df[df[group_field] == grp][cols]\n        z_scale_beta = pm.InverseGamma(f\"z_scale_beta_{grp}\", 3, 0.5)\n        z_scale_alpha = pm.InverseGamma(f\"z_scale_alpha_{grp}\", 3, 0.5)\n        lag_coefs = pm.Normal(\n            f\"lag_coefs_{grp}\",\n            mu=beta_hat_location,\n            sigma=beta_hat_scale * z_scale_beta,\n            dims=[\"equations\", \"lags\", \"cross_vars\"],\n        )\n        alpha = pm.Normal(\n            f\"alpha_{grp}\",\n            mu=alpha_hat_location,\n            sigma=alpha_hat_scale * z_scale_alpha,\n            dims=(\"equations\",),\n        )\n\n        betaX = calc_ar_step(lag_coefs, n_eqs, n_lags, df_grp)\n        betaX = pm.Deterministic(f\"betaX_{grp}\", betaX)\n        mean = alpha + betaX\n\n        n = df_grp.shape[1]\n        noise_chol, _, _ = pm.LKJCholeskyCov(\n            f\"noise_chol_{grp}\", eta=10, n=n, sd_dist=pm.Exponential.dist(1)\n        )\n        omega = pm.Deterministic(f\"omega_{grp}\", rho * omega_global + (1 - rho) * noise_chol)\n        obs = pm.MvNormal(f\"obs_{grp}\", mu=mean, chol=omega, observed=df_grp.values[n_lags:])\n\n    if prior_checks:\n        idata = pm.sample_prior_predictive()\n        return model, idata\n    else:\n        idata = pm.sample_prior_predictive()\n        idata.extend(sample_blackjax_nuts(2000, random_seed=120))\n        pm.sample_posterior_predictive(idata, extend_inferencedata=True)\nreturn model, idata\n\nThe model design allows for a non-centred parameterisation of the key likeihood for each of the individual country components by allowing the us to shift the country specific estimates away from the hierarchical mean. This is done by `rho * omega_global + (1 - rho) * noise_chol` line. The parameter `rho` determines the share of impact each country's data contributes to the estimation of the covariance relationship among the economic variables. Similar country specific adjustments are made with the `z_alpha_scale` and `z_beta_scale` parameters.\n\n```{code-cell} ipython3\ndf_final = gdp_hierarchical[[\"country\", \"dl_gdp\", \"dl_cons\", \"dl_gfcf\"]]\nmodel_full_test, idata_full_test = make_hierarchical_model(\n    2,\n    3,\n    df_final,\n    \"country\",\n    prior_checks=False,\n)\n{code-cell} ipython3 idata_full_test\n{code-cell} ipython3 az.plot_trace(     idata_full_test,     var_names=[\"rho\", \"alpha_hat_location\", \"beta_hat_location\", \"omega_global\"],     kind=\"rank_vlines\", );\nNext we’ll look at some of the summary statistics and how they vary across the countries.\n```{code-cell} ipython3\n\n```{code-cell} ipython3\naz.summary(\n    idata_full_test,\n    var_names=[\n        \"rho\",\n        \"alpha_hat_location\",\n        \"alpha_hat_scale\",\n        \"beta_hat_location\",\n        \"beta_hat_scale\",\n        \"z_scale_alpha_Ireland\",\n        \"z_scale_alpha_United States\",\n        \"z_scale_beta_Ireland\",\n        \"z_scale_beta_United States\",\n        \"alpha_Ireland\",\n        \"alpha_United States\",\n        \"omega_global_corr\",\n        \"lag_coefs_Ireland\",\n        \"lag_coefs_United States\",\n    ],\n)\n```{code-cell} ipython3 ax = az.plot_forest( idata_full_test, var_names=[ “alpha_Ireland”, “alpha_United States”, “alpha_Australia”, “alpha_Chile”, “alpha_New Zealand”, “alpha_South Africa”, “alpha_Canada”, “alpha_United Kingdom”, ], kind=“ridgeplot”, combined=True, ridgeplot_truncate=False, ridgeplot_quantiles=[0.25, 0.5, 0.75], ridgeplot_overlap=0.7, figsize=(10, 10), )\nax[0].axvline(0, color=“red”) ax[0].set_title(“Intercept Parameters for each country and Economic Measure”);\n\n```{code-cell} ipython3\nax = az.plot_forest(\n    idata_full_test,\n    var_names=[\n        \"lag_coefs_Ireland\",\n        \"lag_coefs_United States\",\n        \"lag_coefs_Australia\",\n        \"lag_coefs_Chile\",\n        \"lag_coefs_New Zealand\",\n        \"lag_coefs_South Africa\",\n        \"lag_coefs_Canada\",\n        \"lag_coefs_United Kingdom\",\n    ],\n    kind=\"ridgeplot\",\n    ridgeplot_truncate=False,\n    figsize=(10, 10),\n    coords={\"equations\": \"dl_cons\", \"lags\": 1, \"cross_vars\": \"dl_gdp\"},\n)\nax[0].axvline(0, color=\"red\")\nax[0].set_title(\"Lag Coefficient for the first lag of GDP on Consumption \\n by Country\");\nNext we’ll examine the correlation between the three variables and see what we’ve learned by including the hierarchical structure.\n{code-cell} ipython3 corr = pd.DataFrame(     az.summary(idata_full_test, var_names=[\"omega_global_corr\"])[\"mean\"].values.reshape(3, 3),     columns=[\"GDP\", \"CONS\", \"GFCF\"], ) corr.index = [\"GDP\", \"CONS\", \"GFCF\"] corr\n{code-cell} ipython3 ax = az.plot_posterior(     idata_full_test,     var_names=\"omega_global_corr\",     hdi_prob=\"hide\",     point_estimate=\"mean\",     grid=(3, 3),     kind=\"hist\",     ec=\"black\",     figsize=(10, 7), ) titles = [     \"GDP/GDP\",     \"GDP/CONS\",     \"GDP/GFCF\",     \"CONS/GDP\",     \"CONS/CONS\",     \"CONS/GFCF\",     \"GFCF/GDP\",     \"GFCF/CONS\",     \"GFCF/GFCF\", ] for ax, t in zip(ax.ravel(), titles):     ax.set_xlim(0.6, 1)     ax.set_title(t, fontsize=10) plt.suptitle(\"The Posterior Correlation Estimates\", fontsize=20);\nWe can see these estimates of the correlations between the 3 economic variables differ markedly from the simple case where we examined Ireland alone. In particular we can see that the correlation between GDF and CONS is now much higher. Which suggests that we have learned something about the relationship between these variables which would not be clear examining the Irish case alone.\nNext we’ll plot the model fits for each country to ensure that the predictive distribution can recover the observed data. It is important for the question of model adequacy that we can recover both the outlier case of Ireland and the more regular countries such as Australia and United States.\n{code-cell} ipython3 az.plot_ppc(idata_full_test);\nAnd to see the development of these model fits over time:\n```{code-cell} ipython3 countries = gdp_hierarchical[“country”].unique()\nfig, axs = plt.subplots(8, 3, figsize=(20, 40)) for ax, country in zip(axs, countries): temp = pd.DataFrame( idata_full_test[“observed_data”][f”obs_{country}”].data, columns=[“dl_gdp”, “dl_cons”, “dl_gfcf”], ) ppc = az.extract_dataset(idata_full_test, group=“posterior_predictive”, num_samples=100)[ f”obs_{country}” ] if country == “Ireland”: color = “viridis” else: color = “inferno” for i in range(3): shade_background(ppc, ax, i, color) ax[0].plot(np.arange(ppc.shape[0]), ppc[:, 0, :].mean(axis=1), color=“cyan”, label=“Mean”) ax[0].plot(temp[“dl_gdp”], “o”, mfc=“black”, mec=“white”, mew=1, markersize=7, label=“Observed”) ax[0].set_title(f”Posterior Predictive GDP: {country}“) ax[0].legend(loc=”lower left”) ax[1].plot( temp[“dl_cons”], “o”, mfc=“black”, mec=“white”, mew=1, markersize=7, label=“Observed” ) ax[1].plot(np.arange(ppc.shape[0]), ppc[:, 1, :].mean(axis=1), color=“cyan”, label=“Mean”) ax[1].set_title(f”Posterior Predictive Consumption: {country}“) ax[1].legend(loc=”lower left”) ax[2].plot( temp[“dl_gfcf”], “o”, mfc=“black”, mec=“white”, mew=1, markersize=7, label=“Observed” ) ax[2].plot(np.arange(ppc.shape[0]), ppc[:, 2, :].mean(axis=1), color=“cyan”, label=“Mean”) ax[2].set_title(f”Posterior Predictive Investment: {country}“) ax[2].legend(loc=”lower left”) plt.suptitle(“Posterior Predictive Checks on Hierarchical VAR”, fontsize=20);\n\nHere we can see that the model appears to have recovered reasonable posterior predictions for the observed data and the volatility of the Irish GDP figures is clear next to the other countries. Whether this is a cautionary tale about data quality or the corruption of metrics we leave to the economists to figure out.\n\n+++\n\n## Conclusion\n\nVAR modelling is a rich an interesting area of research within economics and there are a range of challenges and pitfalls which come with the interpretation and understanding of these models. We hope this example encourages you to continue exploring the potential of this kind of VAR modelling in the Bayesian framework. Whether you're interested in the relationship between grand economic theory or simpler questions about the impact of poor app performance on customer feedback, VAR models give you a powerful tool for interrogating these relationships over time. As we've seen Hierarchical VARs further enables the precise quantification of outliers within a cohort and does not throw away the information because of odd accounting practices engendered by international capitalism. \n\nIn the next post in this series we will spend some time digging into the implied relationships between the timeseries which result from fitting our VAR models.\n\n+++\n\n## References\n\n:::{bibliography}\n:filter: docname in docnames\n:::\n\n+++\n\n## Authors\n* Adapted from the PYMC labs [Blog post](https://www.pymc-labs.io/blog-posts/bayesian-vector-autoregression/) and Jim Savage's discussion [here](https://rpubs.com/jimsavage/hierarchical_var) by [Nathaniel Forde](https://nathanielf.github.io/) in November 2022 ([pymc-examples#456](https://github.com/pymc-devs/pymc-examples/pull/456))\n\n+++\n\n## Watermark\n\n```{code-cell} ipython3\n%load_ext watermark\n%watermark -n -u -v -iv -w -p pytensor,aeppl,xarray\n:::{include} ../page_footer.md :::"
  },
  {
    "objectID": "oss/pymc/missing_info.html",
    "href": "oss/pymc/missing_info.html",
    "title": "Missing Data Imputation in PyMC",
    "section": "",
    "text": "Missing Data Imputation and Employee Survey Data\nIn this project I demonstrate the technique of imputation for missing data using both the standard frequentist approach full information maximum likelihod and a more nuanced Bayesian method of chained equations.\nThe project culminated in a publication to the official PyMC documentation that can be found online here or downloaded as notebook here\nThe notebook demonstates these techniques applied to employee satisfaction data. In particular we show how Bayesian hierarchical methods can be used to help predict missing data values across various teams within an organisation based on the observed values andt the characteristics of the team dynamics which drove the observed data.\n\n\n\nDeviations from the Grand Mean by Team"
  },
  {
    "objectID": "oss/pymc/reliability_stats.html",
    "href": "oss/pymc/reliability_stats.html",
    "title": "Reliability Statistics in PyMC",
    "section": "",
    "text": "Reliability Statistics and Calibrated Prediction\nThis project was inspired by the need to apply survival analysis techniques in software engineering to predict and quantify the failure time distribution of software products. The focus was on parameteric modelling of failure distributions and the notion of calibrated predictions of failure times.\nThe project culminated in a publication to the official PyMC documentation that can be found online here or downloaded as notebook here\n\n\n\nLinearised MLE fits to Failure time data"
  },
  {
    "objectID": "oss/pymc/ordinal_regression.html",
    "href": "oss/pymc/ordinal_regression.html",
    "title": "Ordinal Regression Models in PyMC",
    "section": "",
    "text": "Ordinal Models of Regression\nIn this project I outline the strategy of latent variable ordinal regression Bayesian models to evaluate the categorical choice on Likert like scales. The main theme of the work is to try and articulate the modelling approach to survey data, comparing the risk of model misspecification to the assumed metric based analysis of ordinal response variables.\nThe project culminated in a publication to the official PyMC documentation that can be found online here and downloaded here\nThe notebook demonstates these techniques applied to simulated manager engagement evaluations and applied to rotten tomatoes movie ratings data. In particular we show how a latent variable formulate can help characterise the manner in which different factors such as working from home, salary can influence an individual’s ordinal rating response and why it’s important to understand the sources of variation in such responses.\n\n\n\nLatent and Explicit Ratings"
  },
  {
    "objectID": "oss/pymc/longitudinal_models.html",
    "href": "oss/pymc/longitudinal_models.html",
    "title": "Longitudinal Models in PyMC",
    "section": "",
    "text": "Longitudinal Analysis of Growth Trajectories\nIn this project I outline the strategy of using multi-level or hierarchical Bayesian models to evaluate the growth trajectories of individuals, and estimate the between individual effects. The main theme of the work is to try and disambiguate some of the complexities of mixed level (hierarchical modelling) when applied to longitudinal data.\nThe project culminated in a publication to the official PyMC documentation that can be found online here and downloaded here\nThe notebook demonstates these techniques applied to data of youth alcohol consuption. and behaviourial data. In particular we show how Bayesian hierarchical methods can be used to help characterise the manner in which different factors such can influence an individual’s trajectory and why it’s important to understand the sources of variation in such growth trajectories.\n\n\n\nWithin and Between Individual Trajectories"
  },
  {
    "objectID": "oss/pymc/autoregressive_forecasting.html",
    "href": "oss/pymc/autoregressive_forecasting.html",
    "title": "Autoregressive Forecasting in PyMC",
    "section": "",
    "text": "Autoregressive Forecasting in PyMC\nThis project stemmed from a gap in the PyMC documentation around how to use a fitted auto-regressive model to make forecasts about the future state of the world. In this project I demonstrate how fit and make predictictions with bayesian structural timeseries models.\nThe project culminated in a publication to the official PyMC documentation that can be found online here or downloaded as notebook here\nThe notebook demonstates these techniques applied to a series of fake data building in complexity as we add more structure.\n\n\n\nForecasting"
  },
  {
    "objectID": "opensource.html",
    "href": "opensource.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nMultilevel Regression and Post-Stratification\n\n\n\n\n\n\n\nregression\n\n\npost-stratification\n\n\nsurvey data\n\n\n \n\n\n\n\nSep 21, 2023\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nBayesian IV Regression in CausalPy\n\n\n\n\n\n\n\nregression\n\n\ninstrumental variables\n\n\ncausal inference\n\n\n \n\n\n\n\nAug 15, 2023\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nDiscrete Choice Models in PyMC\n\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\ndiscrete choice\n\n\nsubjective utility\n\n\n \n\n\n\n\nJul 15, 2023\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nOrdinal Regression Models in PyMC\n\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nordinal regression\n\n\nlikert scales\n\n\n \n\n\n\n\nJun 1, 2023\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nLongitudinal Models in PyMC\n\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nlongitudinal models\n\n\n \n\n\n\n\nApr 10, 2023\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nMissing Data Imputation in PyMC\n\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nmissing_data\n\n\nimputation\n\n\n \n\n\n\n\nFeb 10, 2023\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nReliability Statistics in PyMC\n\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\ntime-to-failure\n\n\ncalibration\n\n\n \n\n\n\n\nJan 10, 2023\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nBayesian Vector Autoregressive Models in PyMC\n\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nautoregressive\n\n\nhierarchical_models\n\n\n \n\n\n\n\nDec 15, 2022\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nAutoregressive Forecasting in PyMC\n\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nautoregressive\n\n\n \n\n\n\n\nAug 15, 2022\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nDifferences in Differences\n\n\n\n\n\n\n\ncard-kreuger\n\n\ncausal-inference\n\n\ndiff-in-diff\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2023\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nBayesian Estimation and the Histogram Approximation\n\n\n\n\n\n\n\nprobability\n\n\nexperiments\n\n\nbayesian-AB-tests\n\n\nhistogram-trick\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nPsi, Replication and Bayesian Evidence\n\n\n\n\n\n\n\nreplication_crisis\n\n\nbayesian\n\n\nhypothesis_tests\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2021\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nGaussian Processes and Inferred Networks\n\n\nThis post describes the idea of spatial correlation outlined using a gaussian process model\n\n\n\n\nprobability\n\n\ninformation\n\n\noptimisation\n\n\nspatial correlation\n\n\ngaussian_process\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2021\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nWheat from Chaff: Maximum Entropy and Information\n\n\n\n\n\n\n\nentropy\n\n\ninformation\n\n\njaynes\n\n\npriors\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2021\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nBayesian Decisions & Model Comparison\n\n\n\n\n\n\n\nmodel_evaluation\n\n\nlikelihood_ratio_tests\n\n\nloss_ratios\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2021\n\n\n15 min\n\n\n\n\n\n\n  \n\n\n\n\nTrains, Planes …Utility and Maximum Likelihood\n\n\n\n\n\n\n\nmle\n\n\nrevealed preference\n\n\nlog-likelihood\n\n\nutility\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2021\n\n\n15 min\n\n\n\n\n\n\n  \n\n\n\n\nFactor Analysis and Construct Validity in Psychology\n\n\n\n\n\n\n\nconstruct_validity\n\n\nsignificance_tests\n\n\nfactor analysis\n\n\nreplicability\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2021\n\n\n15 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#getting-up",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#getting-up",
    "title": "Discrete Choice Models in PyMC",
    "section": "Getting up",
    "text": "Getting up\n\nTurn off alarm\nGet out of bed"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#types-of-problem-to-solve",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#types-of-problem-to-solve",
    "title": "Product Choice Models and Subjective Utility",
    "section": "Types of Problem to Solve",
    "text": "Types of Problem to Solve\n\nChoice of Policy\nChoice of Brand\nChoice of School\nChoice of Car"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#choice-a-statistical-model",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#choice-a-statistical-model",
    "title": "Discrete Choice Models in PyMC",
    "section": "Choice: A Statistical Model",
    "text": "Choice: A Statistical Model\nLet there be five goods described by their cost of installation and operation.\n\\[ \\begin{split} \\begin{pmatrix}\nu_{gc}   \\\\\nu_{gr}   \\\\\nu_{ec}   \\\\\nu_{er}   \\\\\nu_{hp}   \\\\\n\\end{pmatrix} =  \\begin{pmatrix}\ngc_{ic} & gc_{oc}  \\\\\ngr_{ic} & gr_{oc}  \\\\\nec_{ic} & ec_{oc}  \\\\\ner_{ic} & er_{oc}  \\\\\nhp_{ic} & hp_{oc}  \\\\\n\\end{pmatrix} \\begin{pmatrix}\n\\beta_{ic}   \\\\\n\\beta_{oc}   \\\\\n\\end{pmatrix}  \\end{split}\n\\]\nWe want to estimate how a linear combination of their attributes determines their utility."
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#choice-a-statistical-model-1",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#choice-a-statistical-model-1",
    "title": "Discrete Choice Models in PyMC",
    "section": "Choice: A Statistical Model",
    "text": "Choice: A Statistical Model\nUtility determines choice probability:\n\\[\\text{softmax}(u)_{j} = \\frac{\\exp(u_{j})}{\\sum_{q=1}^{J}\\exp(u_{q})}\\]\nchoices determine market share where:\n\\[ s_{j}(\\mathbf{\\beta}) = P(u_{j} > u_{k}; ∀k ̸= j) \\]"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#choice-the-data",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#choice-the-data",
    "title": "Choice Models and Subjective Utility",
    "section": "Choice: The Data",
    "text": "Choice: The Data\nGas Central Heating and Electrical Central Heating described by their cost of installation and operation.\n\n\n\nchoice_id\nchosen\nic_gc\noc_gc\n…\noc_ec\n\n\n\n\n1\ngc\n866\n200\n…\n542\n\n\n2\nec\n802\n195\n…\n510\n\n\n3\ner\n759\n203\n…\n495\n\n\n4\ngr\n789\n220\n…\n502"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#a-too-simple-model",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#a-too-simple-model",
    "title": "Discrete Choice Models in PyMC",
    "section": "A Too Simple Model",
    "text": "A Too Simple Model\n\nwith pm.Model(coords=coords) as model_1:\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n\n    ## Construct Utility matrix and Pivot\n    u0 = beta_ic * wide_heating_df[\"ic.ec\"] + beta_oc * wide_heating_df[\"oc.ec\"]\n    u1 = beta_ic * wide_heating_df[\"ic.er\"] + beta_oc * wide_heating_df[\"oc.er\"]\n    u2 = beta_ic * wide_heating_df[\"ic.gc\"] + beta_oc * wide_heating_df[\"oc.gc\"]\n    u3 = beta_ic * wide_heating_df[\"ic.gr\"] + beta_oc * wide_heating_df[\"oc.gr\"]\n    u4 = np.zeros(N)  # Outside Good\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    ## Apply Softmax Transform\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n\n    ## Likelihood\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#choice-who-is-choosing",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#choice-who-is-choosing",
    "title": "Discrete Choice Models in PyMC",
    "section": "Choice: Who is Choosing?",
    "text": "Choice: Who is Choosing?\n\nMutually exclusive discrete choice\nMultiple Agents in distinct choice scenarios"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#choice-estimation",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#choice-estimation",
    "title": "Choice Models and Subjective Utility",
    "section": "Choice: Estimation",
    "text": "Choice: Estimation\nThe model is traditionally estimated with maximum likelihood caclulations.\n\\[  L(\\color{blue}{\\beta}) = \\prod s_{j}(\\mathbf{\\color{blue}{\\beta}}) \\]\nor taking the log:\n\\[  l(\\color{blue}{\\beta}) = \\sum log(s_{j}(\\mathbf{\\color{blue}{\\beta}})) \\] \\[ \\text{ We find: } \\underset{\\color{blue}{\\beta}}{\\mathrm{argmax}} \\text{ } l(\\color{blue}{\\beta}) \\]\nResults are often brittle!"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#choice-a-naive-model",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#choice-a-naive-model",
    "title": "Choice Models and Subjective Utility",
    "section": "Choice: A Naive Model",
    "text": "Choice: A Naive Model\nUnderspecified Utilities\nLet there be five goods described by their cost of installation and operation.\n\\[ \\begin{split} \\overbrace{\\begin{pmatrix}\n\\color{green}{u_{gc}}   \\\\\n\\color{green}{u_{gr}}   \\\\\n\\color{green}{u_{ec}}   \\\\\n\\color{green}{u_{er}}   \\\\\n\\color{green}{u_{hp}}   \\\\\n\\end{pmatrix}}^{utility} =  \\begin{pmatrix}\ngc_{ic} & gc_{oc}  \\\\\ngr_{ic} & gr_{oc}  \\\\\nec_{ic} & ec_{oc}  \\\\\ner_{ic} & er_{oc}  \\\\\nhp_{ic} & hp_{oc}  \\\\\n\\end{pmatrix} \\overbrace{\\begin{pmatrix}\n\\color{blue}{\\beta_{ic}}   \\\\\n\\color{blue}{\\beta_{oc}}   \\\\\n\\end{pmatrix}}^{parameters}  \\end{split}\n\\]"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#choice-a-naive-model-1",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#choice-a-naive-model-1",
    "title": "Choice Models and Subjective Utility",
    "section": "Choice: A Naive Model",
    "text": "Choice: A Naive Model\nThe utility calculation is fundamentally comparative. \\[ \\begin{split} \\begin{pmatrix}\n\\color{green}{u_{gc}}   \\\\\n\\color{green}{u_{gr}}   \\\\\n\\color{green}{u_{ec}}   \\\\\n\\color{green}{u_{er}}   \\\\\n\\color{red}{\\overbrace{0}^{\\text{outside good}}}   \\\\\n\\end{pmatrix} =  \\begin{pmatrix}\ngc_{ic} & gc_{oc}  \\\\\ngr_{ic} & gr_{oc}  \\\\\nec_{ic} & ec_{oc}  \\\\\ner_{ic} & er_{oc}  \\\\\n\\color{red}{0} & \\color{red}{0} \\\\\n\\end{pmatrix} \\begin{pmatrix}\n\\color{blue}{\\beta_{ic}}   \\\\\n\\color{blue}{\\beta_{oc}}   \\\\\n\\end{pmatrix}  \\end{split}\n\\]\nWe zero out one category in the data set to represent the “outside good” for comparison. Similar to dummy variables in Regression, this is required for the model to be identified."
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#the-naive-model-in-code",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#the-naive-model-in-code",
    "title": "Choice Models and Subjective Utility",
    "section": "The Naive Model in Code",
    "text": "The Naive Model in Code\nwith pm.Model(coords=coords) as model_1:\n    ## Priors for the Beta Coefficients\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n\n    ## Construct Utility matrix and Pivot\n    u0 = beta_ic * wide_heating_df[\"ic.ec\"] + beta_oc * wide_heating_df[\"oc.ec\"]\n    u1 = beta_ic * wide_heating_df[\"ic.er\"] + beta_oc * wide_heating_df[\"oc.er\"]\n    u2 = beta_ic * wide_heating_df[\"ic.gc\"] + beta_oc * wide_heating_df[\"oc.gc\"]\n    u3 = beta_ic * wide_heating_df[\"ic.gr\"] + beta_oc * wide_heating_df[\"oc.gr\"]\n    u4 = np.zeros(N)  # Outside Good\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    ## Apply Softmax Transform\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n\n    ## Likelihood\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#choice-a-naive-model-2",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#choice-a-naive-model-2",
    "title": "Choice Models and Subjective Utility",
    "section": "Choice: A Naive Model",
    "text": "Choice: A Naive Model\nUtility determines choice probability of choice:\n\\[\\text{softmax}(\\color{green}{u})_{j} = \\frac{\\exp(\\color{green}{u_{j}})}{\\sum_{q=1}^{J}\\exp(\\color{green}{u_{q}})}\\]\nchoices determine market share where:\n\\[ s_{j}(\\mathbf{\\color{blue}{\\beta}}) = P(\\color{green}{u_{j}} > \\color{green}{u_{k}}; ∀k ̸= j) \\]"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#what-are-we-missing",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#what-are-we-missing",
    "title": "Discrete Choice Models in PyMC",
    "section": "What are we missing?",
    "text": "What are we missing?"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#choice-bayesian-estimation",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#choice-bayesian-estimation",
    "title": "Choice Models and Subjective Utility",
    "section": "Choice: Bayesian Estimation",
    "text": "Choice: Bayesian Estimation\nTo evaluate the integrals in the Bayesian model we use MCMC to estimate conditional probabilities of the joint distribution.\n\\[\\underbrace{\\color{blue}{\\beta}}_{\\text{prior draws}} \\sim Normal(0, 1) \\]\n\\[ \\underbrace{p(\\color{blue}{\\beta} | D)}_{\\text{posterior draws}} = \\frac{p(\\mathbb{\\color{blue}{\\beta}})p(D | \\color{blue}{\\beta} )}{\\int_{i}^{n} p(D | \\mathbf{\\color{blue}{\\beta_{i}}})p(\\mathbf{\\color{blue}{\\beta_{i}}}) } \\]\nPriors can be used flexibly regularise and improve reliability of estimation across structural causal models."
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#does-the-model-fit",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#does-the-model-fit",
    "title": "Discrete Choice Models in PyMC",
    "section": "Does the Model Fit?",
    "text": "Does the Model Fit?"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#interpreting-the-model-coefficients",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#interpreting-the-model-coefficients",
    "title": "Choice Models and Subjective Utility",
    "section": "Interpreting the Model Coefficients",
    "text": "Interpreting the Model Coefficients\nRate of Substitution\n\n\nThe beta coefficients in the mode are interpreted as drivers of utility. However, the precision in these latent terms is relative to the variance of unobserved factors.\nThe utility scale is not fixed, but the ratio \\(\\frac{\\beta_{ic}}{\\beta_{oc}}\\) is invariant.\n\n\n\n\nRate of Substitution"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#model-posterior-predictive-fits",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#model-posterior-predictive-fits",
    "title": "Choice Models and Subjective Utility",
    "section": "Model Posterior Predictive Fits",
    "text": "Model Posterior Predictive Fits\nThe model fit fails to recapture the observed data points\n\nModel Fit"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#augmenting-the-model-product-specific-intercepts",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#augmenting-the-model-product-specific-intercepts",
    "title": "Choice Models and Subjective Utility",
    "section": "Augmenting the Model: Product Specific Intercepts",
    "text": "Augmenting the Model: Product Specific Intercepts\n\\[\n\\begin{split}\n\\begin{pmatrix}\n\\color{red}{u_{gc}} \\\\\n\\color{purple}{u_{gr}} \\\\\n\\color{orange}{u_{ec}} \\\\\n\\color{teal}{u_{er}} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  \\color{red}{\\alpha_{gc}} + \\color{blue}{\\beta_{ic}}gc_{ic} + \\color{blue}{\\beta_{oc}}gc_{oc} \\\\\n  \\color{purple}{\\alpha_{gr}} + \\color{blue}{\\beta_{ic}}gr_{ic} + \\color{blue}{\\beta_{oc}}gr_{oc}  \\\\\n  \\color{orange}{\\alpha_{ec}} + \\color{blue}{\\beta_{ic}}ec_{ic} + \\color{blue}{\\beta_{oc}}ec_{oc}  \\\\\n  \\color{teal}{\\alpha_{er}} + \\color{blue}{\\beta_{ic}}er_{ic} + \\color{blue}{\\beta_{oc}}er_{oc}  \\\\\n   0 + 0 + 0\n\\end{pmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#augmenting-the-model",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#augmenting-the-model",
    "title": "Choice Models and Subjective Utility",
    "section": "Augmenting the Model:",
    "text": "Augmenting the Model:\nProduct Specific Intercepts\n\\[\n\\begin{split}\n\\begin{pmatrix}\n\\color{red}{u_{gc}} \\\\\n\\color{purple}{u_{gr}} \\\\\n\\color{orange}{u_{ec}} \\\\\n\\color{teal}{u_{er}} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  \\color{red}{\\alpha_{gc}} + \\color{blue}{\\beta_{ic}}gc_{ic} + \\color{blue}{\\beta_{oc}}gc_{oc} \\\\\n  \\color{purple}{\\alpha_{gr}} + \\color{blue}{\\beta_{ic}}gr_{ic} + \\color{blue}{\\beta_{oc}}gr_{oc}  \\\\\n  \\color{orange}{\\alpha_{ec}} + \\color{blue}{\\beta_{ic}}ec_{ic} + \\color{blue}{\\beta_{oc}}ec_{oc}  \\\\\n  \\color{teal}{\\alpha_{er}} + \\color{blue}{\\beta_{ic}}er_{ic} + \\color{blue}{\\beta_{oc}}er_{oc}  \\\\\n   0 + 0 + 0\n\\end{pmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#augmenting-the-model-1",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#augmenting-the-model-1",
    "title": "Choice Models and Subjective Utility",
    "section": "Augmenting the Model:",
    "text": "Augmenting the Model:\nProduct Specific Intercepts\n\\[\n\\begin{split}\n\\begin{pmatrix}\n\\color{red}{u_{gc}} \\\\\n\\color{purple}{u_{gr}} \\\\\n\\color{orange}{u_{ec}} \\\\\n\\color{teal}{u_{er}} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  \\color{red}{\\alpha_{gc}} + \\color{blue}{\\beta_{ic}}gc_{ic} + \\color{blue}{\\beta_{oc}}gc_{oc} \\\\\n  \\color{purple}{\\alpha_{gr}} + \\color{blue}{\\beta_{ic}}gr_{ic} + \\color{blue}{\\beta_{oc}}gr_{oc}  \\\\\n  \\color{orange}{\\alpha_{ec}} + \\color{blue}{\\beta_{ic}}ec_{ic} + \\color{blue}{\\beta_{oc}}ec_{oc}  \\\\\n  \\color{teal}{\\alpha_{er}} + \\color{blue}{\\beta_{ic}}er_{ic} + \\color{blue}{\\beta_{oc}}er_{oc}  \\\\\n   0 + 0 + 0\n\\end{pmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#augmenting-the-model-posterior-predictions",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#augmenting-the-model-posterior-predictions",
    "title": "Choice Models and Subjective Utility",
    "section": "Augmenting the Model: Posterior Predictions",
    "text": "Augmenting the Model: Posterior Predictions\n\nModel Fit"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#agenda",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#agenda",
    "title": "Choice Models and Subjective Utility",
    "section": "Agenda",
    "text": "Agenda\n\n\nHistory and Background\n\n\n\n\nA Naive Utilty Model\n\n\n\n\nAn Augmented Model\n\n\n\n\nAdding Correlation Structure\n\n\n\n\nCounterfactual Questions\n\n\n\n\nIndividual Heterogenous Utility\n\n\n\n\nConclusion\n\nThe World in the Model"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#adding-correlation-structure",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#adding-correlation-structure",
    "title": "Choice Models and Subjective Utility",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\nDependence in Market Share\n\\[ \\alpha_{i} \\sim Normal(\\mathbf{0}, \\Sigma) \\]\n\\[\n\\begin{split}\n\\begin{pmatrix}\n\\color{red}{u_{gc}} \\\\\n\\color{purple}{u_{gr}} \\\\\n\\color{orange}{u_{ec}} \\\\\n\\color{teal}{u_{er}} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  \\color{red}{\\alpha_{gc}} + \\color{blue}{\\beta_{ic}}gc_{ic} + \\color{blue}{\\beta_{oc}}gc_{oc} \\\\\n  \\color{purple}{\\alpha_{gr}} + \\color{blue}{\\beta_{ic}}gr_{ic} + \\color{blue}{\\beta_{oc}}gr_{oc}  \\\\\n  \\color{orange}{\\alpha_{ec}} + \\color{blue}{\\beta_{ic}}ec_{ic} + \\color{blue}{\\beta_{oc}}ec_{oc}  \\\\\n  \\color{teal}{\\alpha_{er}} + \\color{blue}{\\beta_{ic}}er_{ic} + \\color{blue}{\\beta_{oc}}er_{oc}  \\\\\n   0 + 0 + 0\n\\end{pmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html",
    "title": "Discrete Choice Models in PyMC",
    "section": "",
    "text": "A Naive Utilty Model\nAn Augmented Model\nAdding Correlation Structure\nCounterfactual Reasoning\nIndividual Heterogenous Utility"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#adding-correlation-structure-1",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#adding-correlation-structure-1",
    "title": "Choice Models and Subjective Utility",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\nDependence in Market Share\n\\[ \\alpha_{i} \\sim Normal(\\mathbf{0}, \\color{brown}{\\Gamma}) \\]\n\\[\n\\begin{split}\n\\begin{pmatrix}\n\\color{red}{u_{gc}} \\\\\n\\color{purple}{u_{gr}} \\\\\n\\color{orange}{u_{ec}} \\\\\n\\color{teal}{u_{er}} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  \\color{red}{\\alpha_{gc}} + \\color{blue}{\\beta_{ic}}gc_{ic} + \\color{blue}{\\beta_{oc}}gc_{oc} \\\\\n  \\color{purple}{\\alpha_{gr}} + \\color{blue}{\\beta_{ic}}gr_{ic} + \\color{blue}{\\beta_{oc}}gr_{oc}  \\\\\n  \\color{orange}{\\alpha_{ec}} + \\color{blue}{\\beta_{ic}}ec_{ic} + \\color{blue}{\\beta_{oc}}ec_{oc}  \\\\\n  \\color{teal}{\\alpha_{er}} + \\color{blue}{\\beta_{ic}}er_{ic} + \\color{blue}{\\beta_{oc}}er_{oc}  \\\\\n   0 + 0 + 0\n\\end{pmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#adding-correlation-structure-2",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#adding-correlation-structure-2",
    "title": "Choice Models and Subjective Utility",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\nPriors on Parameters determine Market Structure\n\\[ \\begin{split} \\color{brown}{\\Gamma} =\n\\begin{pmatrix}\n  \\color{red}{1} + \\gamma + \\gamma + \\gamma \\\\\n  \\gamma + \\color{blue}{1} + \\gamma + \\gamma  \\\\\n   \\gamma + \\gamma  + \\color{orange}{1} + \\gamma \\\\\n  \\gamma + \\gamma + \\gamma + \\color{teal}{1}  \n\\end{pmatrix}\n\\end{split} \\]"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#counterfactual-reasoning",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#counterfactual-reasoning",
    "title": "Choice Models and Subjective Utility",
    "section": "Counterfactual Reasoning",
    "text": "Counterfactual Reasoning\nFrom Probability to Causation\n\n“[C]ontrary to the views of a number of pessimistic statisticians and philosophers you can get from probabilities to causes after all. Not always, not even ussually - but in just the right circumstances and with just the right kind of starting information, it is in principle possible.” - Nancy Cartwright in Nature’s Capacities and their Measurement\n\n\n“One of the functions of theoretical economics is to provide fully articulated artificial economic systems that can serve as laboratories in which policies that would be prohibitively expensive to experiment with in actual economies can be tested out at a much lower cost” - Mary Morgan quoted in Hunting Causes and Using Them"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#counterfactual-reasoning-1",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#counterfactual-reasoning-1",
    "title": "Choice Models and Subjective Utility",
    "section": "Counterfactual Reasoning",
    "text": "Counterfactual Reasoning\nCeteris Paribus Laws\nWith a fitted PyMC model we can counterfactually reset the values for the input data and regenerate the posterior predictive distribution holding else equal in the data generating process.\n\nWhat would the market share be like if prices for electrical systems increased 20%?\n\n # update values of predictors with new 20% \n # price increase in operating costs for electrical options\nwith model_3:\n    pm.set_data({\"oc_ec\": wide_heating_df[\"oc.ec\"] * 1.2, \n                 \"oc_er\": wide_heating_df[\"oc.er\"] * 1.2})\n    # use the updated values and predict outcomes and probabilities:\n    idata_new_policy = pm.sample_posterior_predictive(\n        idata_m3,\n        var_names=[\"p\", \"y_cat\"],\n        return_inferencedata=True,\n        predictions=True,\n        extend_inferencedata=False,\n        random_seed=100,\n    )\n\nidata_new_policy"
  },
  {
    "objectID": "oss/causalpy/instrumental_variables.html",
    "href": "oss/causalpy/instrumental_variables.html",
    "title": "Bayesian IV Regression in CausalPy",
    "section": "",
    "text": "Instrumental Variable Regression\nIn this project I sought to add the functionality for bayesian instrumental variable analysis to the CausalPy package. I adapted the work of Juan Orduz to contribute the base classes to the package and demonstrated how these classes can be used to esitmate instrumental regression by replicating the results of an Acemologu paper on the efficacy of political institutions. The demonstration can be seen here\n\n\n\nComparison with OLS"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#mcfadden-and-bart",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#mcfadden-and-bart",
    "title": "Choice Models and Subjective Utility",
    "section": "McFadden and BART",
    "text": "McFadden and BART\n\n\n\n\n“Transport projects involve sinking money in expensive capital investments, which have a long life and wide repercussions. There is no escape from the attempt both to estimate the demand for their services over twenty or thirty years and to assess their repercussions on the economy as a whole.” - Denys Munby, Transport, 1968 ”\n\n\n\n\n\n\n\nBay Area Rapid Transit\n\n\n\n\n\nDublin Metrolink"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#reveal-preference-and-predicting-demand",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#reveal-preference-and-predicting-demand",
    "title": "Product Choice Models and Subjective Utility",
    "section": "Reveal Preference and Predicting Demand",
    "text": "Reveal Preference and Predicting Demand\n\nThe assumption of revealed preference theory is that if a person chooses A over B then their subjective utility for A is greater than for B.\nSurvey data estimated about 15% of users would adopt the newly introduced BART system. McFadden’s random utility model estimated 6%.\nHe was right.\nCopernican Shift: He estimated utility to predict choice."
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#general-applicability-of-choice-problems",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#general-applicability-of-choice-problems",
    "title": "Choice Models and Subjective Utility",
    "section": "General Applicability of Choice Problems",
    "text": "General Applicability of Choice Problems\n\n\nThese models offer the possibility of predicting choice in diverse domains: policy, brand, school, car and partners.\n\n\n\n\nQuestion: What are the attributes that drive these choices? How well are they measurable?\n\n\n\n\nQuestion: How do changes in these attributes influence the predicted market demand for these choices?"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#revealed-preference-and-predicting-demand",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#revealed-preference-and-predicting-demand",
    "title": "Choice Models and Subjective Utility",
    "section": "Revealed Preference and Predicting Demand",
    "text": "Revealed Preference and Predicting Demand\nSelf Centred Utility Maximisers?\n\n\n\n\nThe assumption of revealed preference theory is that if a person chooses A over B then their latent subjective utility for A is greater than for B.\n\n\n\n\nSurvey data estimated about 15% of users would adopt the newly introduced BART system. McFadden’s random utility model estimated 6%.\n\n\n\n\nHe was right.\n\n\n\n\n\n\nCopernican Shift: He estimated utility to predict choice, rather than infer utility from stated choice."
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#individual-heterogenous-utility",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#individual-heterogenous-utility",
    "title": "Choice Models and Subjective Utility",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\nRepeated Choice and Hierarchical Structure\n\n\n\nperson_id\nchoice_id\nchosen\nnabisco_price\nkeebler_price\n\n\n\n\n1\n1\nnabisco\n3.40\n2.00\n\n\n1\n2\nnabisco\n3.45\n2.50\n\n\n1\n3\nkeebler\n3.60\n2.70\n\n\n2\n1\nkeebler\n3.48\n2.20\n\n\n2\n2\nkeebler\n3.30\n2.25"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#counterfactual-reasoning-2",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#counterfactual-reasoning-2",
    "title": "Choice Models and Subjective Utility",
    "section": "Counterfactual Reasoning",
    "text": "Counterfactual Reasoning\n\nCounterfactual Shares"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#augmenting-the-model-2",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#augmenting-the-model-2",
    "title": "Choice Models and Subjective Utility",
    "section": "Augmenting the Model:",
    "text": "Augmenting the Model:\nwith pm.Model(coords=coords) as model_2:\n    ## Priors for the Beta Coefficients\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n    alphas = pm.Normal(\"alpha\", 0, 1, dims=\"alts_intercepts\")\n\n    ## Construct Utility matrix and Pivot using an intercept per alternative\n    u0 = alphas[0] + beta_ic * wide_heating_df[\"ic.ec\"] + beta_oc * wide_heating_df[\"oc.ec\"]\n    u1 = alphas[1] + beta_ic * wide_heating_df[\"ic.er\"] + beta_oc * wide_heating_df[\"oc.er\"]\n    u2 = alphas[2] + beta_ic * wide_heating_df[\"ic.gc\"] + beta_oc * wide_heating_df[\"oc.gc\"]\n    u3 = alphas[3] + beta_ic * wide_heating_df[\"ic.gr\"] + beta_oc * wide_heating_df[\"oc.gr\"]\n    u4 = np.zeros(N)  # Outside Good\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    ## Apply Softmax Transform\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n\n    ## Likelihood\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#augmenting-the-model-3",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#augmenting-the-model-3",
    "title": "Choice Models and Subjective Utility",
    "section": "Augmenting the Model:",
    "text": "Augmenting the Model:\n\nModel Structure"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#self-centred-utility",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#self-centred-utility",
    "title": "Choice Models and Subjective Utility",
    "section": "Self-centred Utility",
    "text": "Self-centred Utility\n\ncoperincon"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#individual-heterogenous-utility-1",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#individual-heterogenous-utility-1",
    "title": "Choice Models and Subjective Utility",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\n\\[\n\\begin{split}\n\\begin{pmatrix}\n\\color{red}{u_{i, nb}} \\\\\n\\color{purple}{u_{i, kb}} \\\\\n\\color{orange}{u_{i, sun}} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  (\\color{red}{\\alpha_{nb}} + \\sum_{i}^{K}\\beta_{i}) + \\color{blue}{\\beta_{p}}p_{nb} + \\color{green}{\\beta_{disp}}d_{nb} \\\\\n  (\\color{purple}{\\alpha_{kb}} + \\sum_{i}^{K}\\beta_{i}) +  \\color{blue}{\\beta_{p}}p_{kb} + \\color{green}{\\beta_{disp}}d_{kb}  \\\\\n  (\\color{orange}{\\alpha_{sun}}  + \\sum_{i}^{K}\\beta_{i})  + \\color{blue}{\\beta_{p}}p_{sun} + \\color{green}{\\beta_{disp}}d_{sun}  \\\\\n   0 + 0 + 0\n\\end{pmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#individual-heterogenous-utility-2",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#individual-heterogenous-utility-2",
    "title": "Choice Models and Subjective Utility",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\nIn Code\n\nwith pm.Model(coords=coords) as model_4:\n    beta_feat = pm.TruncatedNormal(\"beta_feat\", 0, 1, upper=10, lower=0)\n    beta_disp = pm.TruncatedNormal(\"beta_disp\", 0, 1, upper=10, lower=0)\n    ## Stronger Prior on Price to ensure \n    ## an increase in price negatively impacts utility\n    beta_price = pm.TruncatedNormal(\"beta_price\", 0, 1, upper=0, lower=-10)\n    alphas = pm.Normal(\"alpha\", 0, 1, dims=\"alts_intercepts\")\n    beta_individual = pm.Normal(\"beta_individual\", 0, 0.05,\n     dims=(\"individuals\", \"alts_intercepts\"))\n\n    u0 = (\n        (alphas[0] + beta_individual[person_indx, 0])\n        + beta_disp * c_df[\"disp.sunshine\"]\n        + beta_feat * c_df[\"feat.sunshine\"]\n        + beta_price * c_df[\"price.sunshine\"]\n    )\n    u1 = (\n        (alphas[1] + beta_individual[person_indx, 1])\n        + beta_disp * c_df[\"disp.keebler\"]\n        + beta_feat * c_df[\"feat.keebler\"]\n        + beta_price * c_df[\"price.keebler\"]\n    )\n    u2 = (\n        (alphas[2] + beta_individual[person_indx, 2])\n        + beta_disp * c_df[\"disp.nabisco\"]\n        + beta_feat * c_df[\"feat.nabisco\"]\n        + beta_price * c_df[\"price.nabisco\"]\n    )\n    u3 = np.zeros(N)  # Outside Good\n    s = pm.math.stack([u0, u1, u2, u3]).T\n    # Reconstruct the total data\n\n    ## Apply Softmax Transform\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n\n    ## Likelihood\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#counterfactual-reasoning-3",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#counterfactual-reasoning-3",
    "title": "Choice Models and Subjective Utility",
    "section": "Counterfactual Reasoning",
    "text": "Counterfactual Reasoning\nInterventions and Conditionalisation\n\n\n\nThere is a sharp distinction between conditional probability distributions and probability under intervention\nIn PyMC you can implement the do-operator to intervene on the graph that represents your data generating process."
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#individual-heterogenous-utility-3",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#individual-heterogenous-utility-3",
    "title": "Choice Models and Subjective Utility",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\nRecovered Posterior Predictive Distribution"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#individual-heterogenous-utility-4",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#individual-heterogenous-utility-4",
    "title": "Choice Models and Subjective Utility",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\nIndividual Preference\n\n\n\n\n\nIndividual preferences can be derived from the model in this manner.\nThe relationship between preferences over the product offering can be seen too"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#note-on-evaluating-bayesian-models",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#note-on-evaluating-bayesian-models",
    "title": "Choice Models and Subjective Utility",
    "section": "Note on Evaluating Bayesian Models",
    "text": "Note on Evaluating Bayesian Models"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#conclusion",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#conclusion",
    "title": "Choice Models and Subjective Utility",
    "section": "Conclusion",
    "text": "Conclusion\nThe World in the Model\nWe’ve seen a series of models, each one expanding on the last.\n\nModels articulate the relevant structure of the world.\nThey serve as microscopes. Simulation systems are tools to interrogate reality.\nBayesian Conditionalisation calibrates the system against the observed facts.\nBayesian Discrete choice models help us interrogate aspects of actors and their motivations under uncertainty.\nPyMC enables us to easily build and experiment with those models.\nCausal inference is plausible to degree that we can stand behind the structural assumptions. Bayesian models enforce tranparency and justification of structural commitments.\n\n\n“Models… [are] like sonnets for the poet, [a] means to express accounts of life in exact, short form using languages that may easily abstract or analogise, and involve imaginative choices and even a certain degree of playfulness in expression” - Mary Morgan in The World in the Model\n\n\n\n\nDiscrete Choice with PyMC"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#note-on-evaluating-models",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#note-on-evaluating-models",
    "title": "Choice Models and Subjective Utility",
    "section": "Note on Evaluating Models",
    "text": "Note on Evaluating Models"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#note-on-evaluating-models-1",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#note-on-evaluating-models-1",
    "title": "Choice Models and Subjective Utility",
    "section": "Note on Evaluating Models",
    "text": "Note on Evaluating Models\n\n\n\n\n\n\n\n\n\n\nBayesian Models aim to replicate the DGP holistically"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#note-on-model-evaluation",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#note-on-model-evaluation",
    "title": "Choice Models and Subjective Utility",
    "section": "Note on Model Evaluation",
    "text": "Note on Model Evaluation\nReplicating the Super Soldier Program"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#note-on-model-evaluation-1",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#note-on-model-evaluation-1",
    "title": "Choice Models and Subjective Utility",
    "section": "Note on Model Evaluation",
    "text": "Note on Model Evaluation\nReplicating the Super Soldier Program\n\n\n\n\n\n\n\n\n\n\nBayesian Models aim to replicate the DGP holistically"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#augmenting-the-model-4",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#augmenting-the-model-4",
    "title": "Choice Models and Subjective Utility",
    "section": "Augmenting the Model",
    "text": "Augmenting the Model\nPosterior Predictions\n\nModel Fit"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#independence-of-irrelevant-alternatives",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#independence-of-irrelevant-alternatives",
    "title": "Choice Models and Subjective Utility",
    "section": "Independence of Irrelevant Alternatives",
    "text": "Independence of Irrelevant Alternatives\nNew Products Cannibalise Equally from all Alternatives\n\nSuppose a market choice between transport modes is determined by the above model.\nRed Bus or Car are you initial Options. Assume \\(s_{\\color{red}{bus}}(\\beta) = s_{car}(\\beta)\\). Market Share is 50% to each option.\nIntroduce the Blue Bus Option, then the Independent characteristics of the utility specification implies that \\(s_{\\color{red}{bus}}(\\beta) = s_{car}(\\beta) = s_{\\color{blue}{bus}}(\\beta)\\)\nThis implies an implausible substitution pattern for real markets.1\n\nWhat kind of Monster have we Created!?"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#products-cannibalise-equally-from-all-alternatives",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#products-cannibalise-equally-from-all-alternatives",
    "title": "Choice Models and Subjective Utility",
    "section": "Products Cannibalise Equally from all Alternatives",
    "text": "Products Cannibalise Equally from all Alternatives"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#discrete-choice",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#discrete-choice",
    "title": "Choice Models and Subjective Utility",
    "section": "Discrete Choice",
    "text": "Discrete Choice\n\n\n\n\n\n\nI am not an Economist\n\n\n\nI’m a data scientist at Personio, where we work on cool problems ranging across: revenue optimisation, customer churn, experimentation, survey design and people analytics at scale.\nI’m a Bayesian Statistician and a reformed philosopher.\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\n\nDisclaimer\n\n\nNone of Personio’s data was used in this presentation\n\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\nThe worked examples used here can be found here"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#preliminaries",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#preliminaries",
    "title": "Choice Models and Subjective Utility",
    "section": "Preliminaries",
    "text": "Preliminaries\n\n\n\n\n\n\nI am not an Economist\n\n\n\nI’m a data scientist at Personio - where we work on cool problems ranging across themes of:\n\nrevenue optimisation\ncustomer churn\nexperimentation\nsurvey design\nproduct analytics\n\nAlso a Bayesian statistician, reformed philosopher and logician.\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\n\nDisclaimer\n\n\nNone of Personio’s data was used in this presentation\n\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\nThe worked examples used here can be found here"
  },
  {
    "objectID": "oss/bambi/mr_p.html",
    "href": "oss/bambi/mr_p.html",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "",
    "text": "Regression as Stratification\nIn this project I sought to understand the procedure of post-stratification adjustment used in election forecasting and regression modelling of the same. I published the documentation for the technique of using Multilevel Regression and post-stratification (MrP) with the bambi package here. In this work i tried to elaborate precisely how and why the careful modeller would want to make stratum specific adjustments to the predictions of regression models.\nI adapted the work of Martin, Philips and Gelmen’s “Multilevel Regression and Poststratification Case Studies” that can be found here. You can download the worked example here\n\n\n\nComparison with OLS"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#adding-correlation-structure-3",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#adding-correlation-structure-3",
    "title": "Choice Models and Subjective Utility",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\nwith pm.Model(coords=coords) as model_3:\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n\n    beta_income = pm.Normal(\"beta_income\", 0, 1 dims=\"alts_intercepts\")\n\n    chol, corr, stds = pm.LKJCholeskyCov(\n        \"chol\", n=4, eta=2.0, \n        sd_dist=pm.Exponential.dist(1.0, shape=4)\n    )\n    alphas = pm.MvNormal(\"alpha\", mu=0, chol=chol, dims=\"alts_intercepts\")\n\n    u0 = (\n        alphas[0]\n        + beta_ic * wide_heating_df[\"ic.gc\"]\n        + beta_oc * wide_heating_df[\"oc.gc\"]\n        + beta_income[0] * wide_heating_df[\"income\"]\n    )\n    u1 = (\n        alphas[1]\n        + beta_ic * wide_heating_df[\"ic.gc\"]\n        + beta_oc * wide_heating_df[\"oc.gc\"]\n        + beta_income[1] * wide_heating_df[\"income\"]\n    )\n    u2 = (\n        alphas[2]\n        + beta_ic * wide_heating_df[\"ic.gc\"]\n        + beta_oc * wide_heating_df[\"oc.gc\"]\n        + beta_income[2] * wide_heating_df[\"income\"]\n    )\n    u3 = (\n        alphas[3]\n        + beta_ic * wide_heating_df[\"ic.gr\"]\n        + beta_oc * wide_heating_df[\"oc.gr\"]\n        + beta_income[3] * wide_heating_df[\"income\"]\n    )\n    u4 = np.zeros(N)  # pivot\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/utility_and_choice/discrete_choice_and_utility.html#adding-correlation-structure-4",
    "href": "talks/utility_and_choice/discrete_choice_and_utility.html#adding-correlation-structure-4",
    "title": "Choice Models and Subjective Utility",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\n\nCorrelation Structure"
  }
]