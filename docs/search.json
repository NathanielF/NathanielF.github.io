[
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "import pymc as pm\nimport bambi as bmb\nimport pandas as pd\nimport arviz as az\nfrom bambi.plots import plot_cap\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import stats\n\n:"
  },
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#modelling-improvement-as-lift-across-pooled-experiments",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#modelling-improvement-as-lift-across-pooled-experiments",
    "title": "Examined Algorithms",
    "section": "Modelling Improvement as Lift across Pooled Experiments",
    "text": "Modelling Improvement as Lift across Pooled Experiments\nIt’s useful fact that the Lift measurement of success can be more nicely modelled under a log transform. In this analysis we’ll follow the example in Demetri’s blog: https://dpananos.github.io/posts/2022-07-20-pooling-experiments/ and demonstrate how we can pool information across seperate experiments. In particular we’ll see why this type of modelling is apt for planning expected amount of cumulative gains over successive experiments. First observe how the Lift measurement can can be transformed to facilitate modelling.\n\nnp.random.seed(11)\nfig, axs = plt.subplots(2, 2, figsize=(20, 8))\naxs = axs.flatten()\ncounts_success_control = np.random.normal(10, 2, 100)\ncounts_success_treatment = np.random.normal(8, 2, 100)\naxs[0].hist(counts_success_control, alpha=.3, label='control', edgecolor='black')\naxs[0].hist(counts_success_treatment, alpha=0.2, label='treatment', edgecolor='black')\naxs[0].legend()\naxs[0].set_title(\"Successes in Treatment and Control for 100 Experiments\")\nrr = counts_success_treatment / counts_success_control\naxs[1].hist(rr, alpha=0.3, label='RR/Lift', color='y', edgecolor='black')\naxs[1].legend()\naxs[1].set_title(\"RR/Lift\")\naxs[2].hist(np.log(rr), label='logged Lift', alpha=0.4, color='green', edgecolor='black')\naxs[2].set_title(\"Logged RR/Lift\")\naxs[3].hist(np.exp(np.log(rr)), label='exponentiated Logged Lift', alpha=0.4, color='purple', edgecolor='black')\naxs[3].set_title(\" Exp Logged RR/Lift\")\n\nText(0.5, 1.0, ' Exp Logged RR/Lift')"
  },
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#example-experiments-with-count-successes",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#example-experiments-with-count-successes",
    "title": "Examined Algorithms",
    "section": "Example Experiment(s) with Count Successes",
    "text": "Example Experiment(s) with Count Successes\nThe original data was modelled in the Stan probabilistic programming language. We’ll use this opportunity to translate the code into a pymc implementation.\nIn the scenario we have 12 seperate experiments with 50,000 units on either arm of the experiment. The were desigined to detect conversion in the arm of each trial. Only four of the experiments were successful in the sense that they showed a positive lift distinguishable from statistical noise under a 5% p-value threshold. Management wishes to achieve a total Lift of 2x over the next year. We want to determine how plausible that goal is given our track record so far.\n\ndf_pooling = pd.read_csv('pooling_data.csv')\ndf_pooling\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      n_per_group\n      y_txt\n      y_control\n      estimated_relative_lift\n      pvals\n      experiment\n      estimated_sd_relative_lift\n      estimated_log_relative_lift\n    \n  \n  \n    \n      0\n      1\n      50000\n      551\n      492\n      1.119919\n      0.035510\n      1\n      0.061704\n      0.113256\n    \n    \n      1\n      2\n      50000\n      510\n      490\n      1.040816\n      0.272968\n      2\n      0.062941\n      0.040005\n    \n    \n      2\n      3\n      50000\n      548\n      509\n      1.076621\n      0.119989\n      3\n      0.061233\n      0.073827\n    \n    \n      3\n      4\n      50000\n      537\n      511\n      1.050881\n      0.218777\n      4\n      0.061475\n      0.049629\n    \n    \n      4\n      5\n      50000\n      558\n      508\n      1.098425\n      0.065669\n      5\n      0.060997\n      0.093878\n    \n    \n      5\n      6\n      50000\n      542\n      489\n      1.108384\n      0.051774\n      6\n      0.062048\n      0.102904\n    \n    \n      6\n      7\n      50000\n      533\n      495\n      1.076768\n      0.123029\n      7\n      0.062100\n      0.073964\n    \n    \n      7\n      8\n      50000\n      544\n      468\n      1.162393\n      0.008903\n      8\n      0.062729\n      0.150481\n    \n    \n      8\n      9\n      50000\n      519\n      521\n      0.996161\n      0.512433\n      9\n      0.061694\n      -0.003846\n    \n    \n      9\n      10\n      50000\n      532\n      469\n      1.134328\n      0.024447\n      10\n      0.063023\n      0.126041\n    \n    \n      10\n      11\n      50000\n      555\n      487\n      1.139630\n      0.018467\n      11\n      0.061767\n      0.130704\n    \n    \n      11\n      12\n      50000\n      525\n      497\n      1.056338\n      0.197962\n      12\n      0.062264\n      0.054808\n    \n  \n\n\n\n\n\nfig, axs = plt.subplots(2, 2, figsize=(20, 10))\naxs = axs.flatten()\naxs[0].hist(df_pooling['y_control'], alpha=.3, label='control', edgecolor='black')\naxs[0].hist(df_pooling['y_txt'], alpha=0.2, label='treatment', edgecolor='black')\naxs[0].legend()\naxs[0].set_title(\"Successes in Treatment and Control for 100 Experiments\")\nrr = df_pooling['y_txt'] / df_pooling['y_control']\naxs[1].hist(rr, alpha=0.3, label='RR/Lift', color='y', edgecolor='black')\naxs[1].legend()\naxs[1].set_title(\"RR/Lift\")\naxs[2].hist(np.log(rr), label='logged Lift', alpha=0.4, color='green', edgecolor='black')\naxs[2].set_title(\"Logged RR/Lift\")\naxs[3].hist(np.exp(np.log(rr)), label='exponentiated Logged Lift', alpha=0.4, color='purple', edgecolor='black')\naxs[3].set_title(\" Exp Logged RR/Lift\")\n\nText(0.5, 1.0, ' Exp Logged RR/Lift')"
  },
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#the-model",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#the-model",
    "title": "Examined Algorithms",
    "section": "The Model",
    "text": "The Model\nWe want to pool the information across our 12 experiments and to do so we model them hierarchically as a draws from an overarching normal distribution. The assumptions means we have to set priors on the shape of our parameters. We allow the hierarchical Normal distribution be configured with a centre of mass drawn from the StudentT distribution ensuring that we can have heavy tails in the distribution to account for outlier experiments with massive returns. The code and structure of the model are displayed below:\n\nwith pm.Model() as model:\n     pass\n\nmodel.add_coord('exp_id', list(range(12)), mutable=True)\n\nwith model:\n    exp_id = pm.MutableData(\"exp\", list(range(12)))\n    # Priors for the Hierarchical Log Lift Distribution\n    mu_metric = pm.StudentT('mu_metric', mu=0, sigma=2.5, nu=3)\n    sig_ex = pm.HalfCauchy('sig_ex', 0.01)\n\n    # Priors for the Individual effects for each experiment\n    z_ex = pm.Normal('z_ex', 0, 1, dims='exp_id')\n    \n    # Convenience wrappers for inputting fresh data\n    est_lift_sd = pm.MutableData('est_lift_sd', df_pooling['estimated_sd_relative_lift'], dims='exp_id')\n    est_log_lift = pm.MutableData('est_log_lift', df_pooling['estimated_log_relative_lift'], dims='exp_id')\n\n    ## pooling the indivdual experiemnts, ensuring shrinkage to the overall mean\n    true_log_rr = pm.Deterministic('true_log_rr', mu_metric + z_ex[exp_id]*sig_ex, dims='exp_id')\n\n    ## Likelihood model for Logged Lift using observed values\n    estimated_log_relative_lift = pm.Normal(\"estimated_log_relative_lift\", mu=true_log_rr[exp_id], sigma=est_lift_sd[exp_id], \n                                            observed=est_log_lift, dims=\"exp_id\")\n                \n    estimated_relative_lift = pm.Deterministic('estimated_relative_lift', pm.math.exp(estimated_log_relative_lift[exp_id]), dims='exp_id')\n    \n\n\npm.model_to_graphviz(model)"
  },
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#the-estimation-step",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#the-estimation-step",
    "title": "Examined Algorithms",
    "section": "The Estimation Step",
    "text": "The Estimation Step\nIn the Bayesian workflow we sample both the priors, the prior predictive and posterior_predictive distributions. These allow us to assess model fit and the degree to which our model captures the observed data.\n\nwith model:\n    idata = pm.sample()\n    idata.extend(pm.sample_prior_predictive(samples=50, random_seed=1))\n    idata.extend(pm.sample_posterior_predictive(idata, var_names=[\"estimated_log_relative_lift\", \"mu_metric\", \"sig_ex\"]))\n\nAuto-assigning NUTS sampler...\nINFO:pymc:Auto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nINFO:pymc:Initializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nINFO:pymc:Multiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu_metric, sig_ex, z_ex]\nINFO:pymc:NUTS: [mu_metric, sig_ex, z_ex]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:07<00:00 Sampling 4 chains, 21 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 26 seconds.\nINFO:pymc:Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 26 seconds.\nThere was 1 divergence after tuning. Increase `target_accept` or reparameterize.\nERROR:pymc:There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\nThere were 6 divergences after tuning. Increase `target_accept` or reparameterize.\nERROR:pymc:There were 6 divergences after tuning. Increase `target_accept` or reparameterize.\nThere were 11 divergences after tuning. Increase `target_accept` or reparameterize.\nERROR:pymc:There were 11 divergences after tuning. Increase `target_accept` or reparameterize.\nThere were 3 divergences after tuning. Increase `target_accept` or reparameterize.\nERROR:pymc:There were 3 divergences after tuning. Increase `target_accept` or reparameterize.\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:01<00:00]"
  },
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#plotting-and-convergence-checks",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#plotting-and-convergence-checks",
    "title": "Examined Algorithms",
    "section": "Plotting and Convergence checks",
    "text": "Plotting and Convergence checks\n\naz.plot_trace(idata, var_names=['mu_metric', 'z_ex', 'true_log_rr', 'sig_ex'], figsize=(20, 8));\n\n\n\n\n\naz.plot_ppc(idata, figsize=(20, 7), kind='scatter');\n\n\n\n\n\naz.summary(idata)\n\n/Users/nathanielforde/Documents/Gitlab/async_research_club/.venv/lib/python3.9/site-packages/arviz/stats/diagnostics.py:586: RuntimeWarning: invalid value encountered in double_scalars\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      mu_metric\n      0.083\n      0.018\n      0.050\n      0.118\n      0.000\n      0.000\n      4133.0\n      2445.0\n      1.0\n    \n    \n      z_ex[0]\n      0.041\n      0.987\n      -1.774\n      1.968\n      0.014\n      0.017\n      4873.0\n      2808.0\n      1.0\n    \n    \n      z_ex[1]\n      -0.088\n      0.995\n      -2.017\n      1.718\n      0.014\n      0.017\n      5093.0\n      2906.0\n      1.0\n    \n    \n      z_ex[2]\n      -0.037\n      0.971\n      -1.795\n      1.796\n      0.014\n      0.016\n      4757.0\n      3008.0\n      1.0\n    \n    \n      z_ex[3]\n      -0.079\n      1.011\n      -1.973\n      1.788\n      0.014\n      0.017\n      4925.0\n      2573.0\n      1.0\n    \n    \n      z_ex[4]\n      0.046\n      0.974\n      -1.860\n      1.815\n      0.013\n      0.016\n      5477.0\n      2826.0\n      1.0\n    \n    \n      z_ex[5]\n      0.041\n      0.994\n      -1.815\n      1.892\n      0.015\n      0.019\n      4581.0\n      2276.0\n      1.0\n    \n    \n      z_ex[6]\n      -0.003\n      0.965\n      -1.727\n      1.879\n      0.015\n      0.016\n      4196.0\n      2759.0\n      1.0\n    \n    \n      z_ex[7]\n      0.165\n      0.987\n      -1.713\n      1.959\n      0.015\n      0.017\n      4426.0\n      2569.0\n      1.0\n    \n    \n      z_ex[8]\n      -0.183\n      0.992\n      -2.046\n      1.649\n      0.014\n      0.016\n      4770.0\n      2848.0\n      1.0\n    \n    \n      z_ex[9]\n      0.077\n      0.998\n      -1.664\n      2.037\n      0.014\n      0.017\n      4990.0\n      2714.0\n      1.0\n    \n    \n      z_ex[10]\n      0.115\n      0.998\n      -1.752\n      1.988\n      0.014\n      0.018\n      5001.0\n      2612.0\n      1.0\n    \n    \n      z_ex[11]\n      -0.082\n      0.997\n      -1.909\n      1.840\n      0.015\n      0.018\n      4634.0\n      2564.0\n      1.0\n    \n    \n      sig_ex\n      0.010\n      0.010\n      0.000\n      0.027\n      0.000\n      0.000\n      2690.0\n      1933.0\n      1.0\n    \n    \n      true_log_rr[0]\n      0.084\n      0.021\n      0.046\n      0.126\n      0.000\n      0.000\n      4049.0\n      2457.0\n      1.0\n    \n    \n      true_log_rr[1]\n      0.082\n      0.021\n      0.042\n      0.121\n      0.000\n      0.000\n      3788.0\n      2655.0\n      1.0\n    \n    \n      true_log_rr[2]\n      0.083\n      0.022\n      0.042\n      0.123\n      0.000\n      0.000\n      4097.0\n      2861.0\n      1.0\n    \n    \n      true_log_rr[3]\n      0.082\n      0.022\n      0.040\n      0.121\n      0.000\n      0.000\n      3632.0\n      2416.0\n      1.0\n    \n    \n      true_log_rr[4]\n      0.084\n      0.021\n      0.044\n      0.123\n      0.000\n      0.000\n      4032.0\n      2419.0\n      1.0\n    \n    \n      true_log_rr[5]\n      0.084\n      0.022\n      0.042\n      0.124\n      0.000\n      0.000\n      4236.0\n      2668.0\n      1.0\n    \n    \n      true_log_rr[6]\n      0.083\n      0.021\n      0.045\n      0.123\n      0.000\n      0.000\n      3702.0\n      2731.0\n      1.0\n    \n    \n      true_log_rr[7]\n      0.086\n      0.022\n      0.043\n      0.126\n      0.000\n      0.000\n      4035.0\n      2712.0\n      1.0\n    \n    \n      true_log_rr[8]\n      0.080\n      0.022\n      0.040\n      0.121\n      0.000\n      0.000\n      3619.0\n      2592.0\n      1.0\n    \n    \n      true_log_rr[9]\n      0.085\n      0.022\n      0.042\n      0.123\n      0.000\n      0.000\n      3831.0\n      2801.0\n      1.0\n    \n    \n      true_log_rr[10]\n      0.085\n      0.022\n      0.045\n      0.124\n      0.000\n      0.000\n      3814.0\n      2450.0\n      1.0\n    \n    \n      true_log_rr[11]\n      0.082\n      0.022\n      0.043\n      0.124\n      0.000\n      0.000\n      3962.0\n      2829.0\n      1.0\n    \n    \n      estimated_relative_lift[0]\n      1.120\n      0.000\n      1.120\n      1.120\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[1]\n      1.041\n      0.000\n      1.041\n      1.041\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[2]\n      1.077\n      0.000\n      1.077\n      1.077\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[3]\n      1.051\n      0.000\n      1.051\n      1.051\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[4]\n      1.098\n      0.000\n      1.098\n      1.098\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[5]\n      1.108\n      0.000\n      1.108\n      1.108\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[6]\n      1.077\n      0.000\n      1.077\n      1.077\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[7]\n      1.162\n      0.000\n      1.162\n      1.162\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[8]\n      0.996\n      0.000\n      0.996\n      0.996\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[9]\n      1.134\n      0.000\n      1.134\n      1.134\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[10]\n      1.140\n      0.000\n      1.140\n      1.140\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[11]\n      1.056\n      0.000\n      1.056\n      1.056\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n  \n\n\n\n\n\nSimulate Draws from the Posterior and Calculate Lift\nThe Stan implementation has the functionality to enable the calculation of generated quantities on the fly within a model run. We need to replicate that functionality outside of our model making use of the estimated posterior distributions on the model parameters. We calculate the effect size engendered by an observed difference in proportions of conversion across the experiments, then calculate whether the simulated was large enough that we had the power to detect it against a baseline of 1% conversion. The quantities are used to define the amount of detected lift in our posterior distribution. This in turn can be used to project the amount of cumulative lift we will see over 12 experiments.\n\nidata.stack(sample=[\"chain\", \"draw\"], inplace=True)\n\n\ngenerated_quanties = []\nfor i in range(12):\n    generated_data = pd.DataFrame({'mu_metric': idata['posterior']['mu_metric'].values,\n        'sig_ex': idata['posterior']['sig_ex'].values\n        }\n    )\n    generated_data['log_rr_over_the_year'] = generated_data.apply(lambda x: np.random.normal(x['mu_metric'], x['sig_ex'], 1)[0], axis=1)\n    generated_data['rr_over_the_year'] = np.exp(generated_data['log_rr_over_the_year'])\n    ## Calculate effect size for proportions against base 0.01\n    generated_data['es'] = generated_data.apply(lambda x: 2*np.arcsin(np.sqrt(x['rr_over_the_year'] * 0.01)) -  2*np.arcsin(np.sqrt(0.01)), axis=1)\n    ## Calculate power based on difference from baseline with known sample size\n    generated_data['power'] = generated_data.apply(lambda x: 1 - stats.norm.cdf( 1.644854 - x['es'] * np.sqrt(50_000/2), 0, 1), axis=1)\n    ## Weight lift by our ability to detect given power in experiment\n    generated_data['detected_lift'] = generated_data['power']* np.log(generated_data['rr_over_the_year'])\n    generated_data['experiment'] = i\n    generated_data['draw'] = generated_data.index\n    generated_quanties.append(generated_data)\n\nforecast_df = pd.concat(generated_quanties)\nforecast_df\n\n\n\n\n\n  \n    \n      \n      mu_metric\n      sig_ex\n      log_rr_over_the_year\n      rr_over_the_year\n      es\n      power\n      detected_lift\n      experiment\n      draw\n    \n  \n  \n    \n      0\n      0.053614\n      0.003751\n      0.054232\n      1.055729\n      0.005526\n      0.220312\n      0.011948\n      0\n      0\n    \n    \n      1\n      0.089769\n      0.006848\n      0.078595\n      1.081766\n      0.008058\n      0.355404\n      0.027933\n      0\n      1\n    \n    \n      2\n      0.082105\n      0.008848\n      0.075538\n      1.078464\n      0.007739\n      0.336777\n      0.025440\n      0\n      2\n    \n    \n      3\n      0.076757\n      0.031000\n      0.098424\n      1.103431\n      0.010142\n      0.483548\n      0.047593\n      0\n      3\n    \n    \n      4\n      0.076757\n      0.031000\n      0.081759\n      1.085194\n      0.008389\n      0.375086\n      0.030667\n      0\n      4\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3995\n      0.102766\n      0.012876\n      0.118111\n      1.125369\n      0.012232\n      0.613782\n      0.072494\n      11\n      3995\n    \n    \n      3996\n      0.066032\n      0.000387\n      0.065700\n      1.067907\n      0.006714\n      0.279849\n      0.018386\n      11\n      3996\n    \n    \n      3997\n      0.062600\n      0.007694\n      0.067979\n      1.070343\n      0.006951\n      0.292589\n      0.019890\n      11\n      3997\n    \n    \n      3998\n      0.090900\n      0.005162\n      0.103968\n      1.109565\n      0.010729\n      0.520525\n      0.054118\n      11\n      3998\n    \n    \n      3999\n      0.064391\n      0.010588\n      0.065984\n      1.068209\n      0.006743\n      0.281419\n      0.018569\n      11\n      3999\n    \n  \n\n48000 rows × 9 columns\n\n\n\n\n\nGenerate Cumulative Lift Curves for N-Experiments\nWe can now line up the draws for each of our experiments and calculate the cumulative lift by taking the cumulative sum and then exponentiating to return us to the raw Lift scale.\n\ndraws_per_experiment = forecast_df.pivot('experiment', 'draw', 'detected_lift')\n## Probability of Independent events sum on the log scale\ndraws_per_experiment = np.exp(draws_per_experiment.cumsum()).T\ndraws_per_experiment\n\n\n\n\n\n  \n    \n      experiment\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n    \n    \n      draw\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      1.108264\n      1.184080\n      1.270116\n      1.344759\n      1.382252\n      1.493910\n      1.574159\n      1.636984\n      1.770258\n      1.896812\n      2.051490\n      2.125217\n    \n    \n      1\n      1.111839\n      1.173511\n      1.255733\n      1.316744\n      1.336851\n      1.431039\n      1.513762\n      1.624544\n      1.733171\n      1.984078\n      2.259019\n      2.370773\n    \n    \n      2\n      1.019156\n      1.058173\n      1.080910\n      1.120521\n      1.163374\n      1.190512\n      1.231332\n      1.257548\n      1.287433\n      1.320996\n      1.347873\n      1.399662\n    \n    \n      3\n      1.027886\n      1.062590\n      1.088567\n      1.124628\n      1.149337\n      1.189305\n      1.218267\n      1.258150\n      1.290688\n      1.328989\n      1.368187\n      1.412470\n    \n    \n      4\n      1.022567\n      1.037980\n      1.046979\n      1.064810\n      1.079649\n      1.086973\n      1.094045\n      1.097738\n      1.104774\n      1.108017\n      1.125943\n      1.132601\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3995\n      1.022991\n      1.064212\n      1.091823\n      1.118289\n      1.166716\n      1.182341\n      1.208105\n      1.252252\n      1.310564\n      1.319768\n      1.351317\n      1.375962\n    \n    \n      3996\n      1.039176\n      1.084013\n      1.129208\n      1.178795\n      1.230675\n      1.283502\n      1.337933\n      1.395778\n      1.451701\n      1.512018\n      1.576763\n      1.643312\n    \n    \n      3997\n      1.022346\n      1.057272\n      1.118954\n      1.183094\n      1.205393\n      1.225715\n      1.343847\n      1.407721\n      1.506250\n      1.565272\n      1.670593\n      1.755775\n    \n    \n      3998\n      1.037186\n      1.077082\n      1.122035\n      1.166914\n      1.206876\n      1.252481\n      1.300103\n      1.352597\n      1.397989\n      1.453273\n      1.510253\n      1.565866\n    \n    \n      3999\n      1.026173\n      1.057943\n      1.072373\n      1.120320\n      1.155082\n      1.198584\n      1.220144\n      1.246629\n      1.275155\n      1.311698\n      1.351460\n      1.378449\n    \n  \n\n4000 rows × 12 columns"
  },
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#plotting-predicted-trajectories",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#plotting-predicted-trajectories",
    "title": "Examined Algorithms",
    "section": "Plotting Predicted Trajectories",
    "text": "Plotting Predicted Trajectories\nNote how there are cumulative lift curves that go up and down due to the possibility of failed experiments. We plot here a a sample of 100 possible curves along with the key quantiles.\n\nfig, ax = plt.subplots(figsize=(20, 10))\nax.plot(draws_per_experiment.sample(100).T, color='grey', alpha=0.2);\nax.plot(draws_per_experiment.mean(), color='slateblue', label='Expected', linewidth=5)\nax.plot(draws_per_experiment.quantile(0.75), color='slateblue', label='p75', alpha=0.5, linewidth=3)\nax.plot(draws_per_experiment.quantile(0.25), color='slateblue', label='p25', alpha=0.5, linewidth=3)\nax.plot(draws_per_experiment.quantile(0.99), color='slateblue', label='p99', alpha=0.5, linewidth=3)\nax.set_title(\"Credible Cumulative Lift Curves\", fontsize=20)\nax.set_ylabel(\"Lift\")\nax.set_xlabel(\"Number of Experiments\")\nax.legend()\n\n<matplotlib.legend.Legend at 0x180b0c520>\n\n\n\n\n\nWe can now also ask how credible a target lift of 2x over the course of 12 experiments really is?\n\nfig, ax = plt.subplots(figsize=(20, 6))\nN, bins, patches = ax.hist(draws_per_experiment[11], color='slateblue', edgecolor='grey', alpha=0.3, bins=30);\nax.axvline(2)\nfor i in range(9, len(patches)):\n    patches[i].set_facecolor('red')\nax.set_title(\"Proportion of Credible Curves which achieve a cumulative Lift more than 2\", fontsize=20)\nax.set_xlabel(\"Lift\")\n\nText(0.5, 0, 'Lift')"
  },
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#generate-new-views-with-new-experimental-data",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#generate-new-views-with-new-experimental-data",
    "title": "Examined Algorithms",
    "section": "Generate new views with new Experimental data",
    "text": "Generate new views with new Experimental data\nWe can make use of PYMC’s mutable data input to feed in new experimental data and try and predict implied effects using the posterior predictive distribution. We will continue to assume that we have 50,000 observations per experiment on each arm. But now let’s assume we have 20 experiments, with a similar pattern of Lift observed on each of the experiments.\n\nN = 20\ncontrol = np.random.gumbel(2.2, 0.07, N)\ntreatment = np.random.gumbel(2.0, 0.05, N)\nsd = np.random.normal(0.1, 0.01, N)\n\nfig, axs = plt.subplots(2, 2, figsize=(20, 8))\naxs = axs.flatten()\naxs[0].hist(control, alpha=.3, label='control', edgecolor='black')\naxs[0].hist(treatment, alpha=0.2, label='treatment', edgecolor='black')\naxs[0].legend()\naxs[0].set_title(\"Successes in Treatment and Control for 100 Experiments\")\nrr = treatment / control\nlog_rr = np.log(rr)\naxs[1].hist(rr, alpha=0.3, label='RR/Lift', color='y', edgecolor='black')\naxs[1].legend()\naxs[1].set_title(\"RR/Lift\")\naxs[2].hist(log_rr, label='logged Lift', alpha=0.4, color='green', edgecolor='black')\naxs[2].set_title(\"Logged RR/Lift\")\naxs[3].hist(np.exp(log_rr), label='exponentiated Logged Lift', alpha=0.4, color='purple', edgecolor='black')\naxs[3].set_title(\" Exp Logged RR/Lift\")\n\nText(0.5, 1.0, ' Exp Logged RR/Lift')\n\n\n\n\n\nHere we can pass in the new data to our old model and re-generate the posterior predictive distribution.\n\n# Do the posterior predictions\ncoords = {'exp_id': list(range(N))}\nwith model:\n    pm.set_data({\"est_log_lift\": log_rr, 'exp':list(range(N)), 'est_lift_sd': sd}, coords=coords)\n    ppc = pm.sample_posterior_predictive(idata, var_names=[\"estimated_log_relative_lift\"])\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00<00:00]\n    \n    \n\n\n\naz.plot_ppc(ppc, figsize=(20, 10), kind='scatter');\n\n\n\n\n\nppc.stack(sample=[\"chain\", \"draw\"], inplace=True)\npredicted = ppc['posterior_predictive']['estimated_log_relative_lift'].to_dataframe().reset_index()\npredicted\n\n\n\n\n\n  \n    \n      \n      exp_id\n      chain\n      draw\n      estimated_log_relative_lift\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      0.129563\n    \n    \n      1\n      0\n      0\n      1\n      0.160167\n    \n    \n      2\n      0\n      0\n      2\n      0.015636\n    \n    \n      3\n      0\n      0\n      3\n      0.131316\n    \n    \n      4\n      0\n      0\n      4\n      0.155715\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      79995\n      19\n      3\n      995\n      0.047107\n    \n    \n      79996\n      19\n      3\n      996\n      0.152401\n    \n    \n      79997\n      19\n      3\n      997\n      0.257834\n    \n    \n      79998\n      19\n      3\n      998\n      0.037314\n    \n    \n      79999\n      19\n      3\n      999\n      -0.037456\n    \n  \n\n80000 rows × 4 columns\n\n\n\n\npredicted['rr_over_the_year'] = np.exp(predicted['estimated_log_relative_lift'])\n    ## Calculate effect size for proportions against base 0.01\npredicted['es'] = predicted.apply(lambda x: 2*np.arcsin(np.sqrt(x['rr_over_the_year'] * 0.01)) -  2*np.arcsin(np.sqrt(0.01)), axis=1)\n    ## Calculate power based on difference from baseline with known sample size\npredicted['power'] = predicted.apply(lambda x: 1 - stats.norm.cdf( 1.644854 - x['es'] * np.sqrt(50_000/2), 0, 1), axis=1)\n    ## Weight lift by our ability to detect given power in experiment\npredicted['detected_lift'] = predicted['power']* np.log(predicted['rr_over_the_year'])\npredicted\n\n\n\n\n\n  \n    \n      \n      exp_id\n      chain\n      draw\n      estimated_log_relative_lift\n      rr_over_the_year\n      es\n      power\n      detected_lift\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      0.129563\n      1.138330\n      0.013457\n      0.685424\n      0.088805\n    \n    \n      1\n      0\n      0\n      1\n      0.160167\n      1.173706\n      0.016767\n      0.842840\n      0.134995\n    \n    \n      2\n      0\n      0\n      2\n      0.015636\n      1.015759\n      0.001578\n      0.081447\n      0.001273\n    \n    \n      3\n      0\n      0\n      3\n      0.131316\n      1.140329\n      0.013646\n      0.695916\n      0.091385\n    \n    \n      4\n      0\n      0\n      4\n      0.155715\n      1.168493\n      0.016282\n      0.823705\n      0.128263\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      79995\n      19\n      3\n      995\n      0.047107\n      1.048235\n      0.004791\n      0.187461\n      0.008831\n    \n    \n      79996\n      19\n      3\n      996\n      0.152401\n      1.164627\n      0.015922\n      0.808573\n      0.123227\n    \n    \n      79997\n      19\n      3\n      997\n      0.257834\n      1.294124\n      0.027678\n      0.996847\n      0.257021\n    \n    \n      79998\n      19\n      3\n      998\n      0.037314\n      1.038019\n      0.003786\n      0.147719\n      0.005512\n    \n    \n      79999\n      19\n      3\n      999\n      -0.037456\n      0.963237\n      -0.003729\n      0.012726\n      -0.000477\n    \n  \n\n80000 rows × 8 columns\n\n\n\n\npredicted_curves  = predicted.pivot(['chain', 'draw'], 'exp_id', 'detected_lift')\npredicted_curves\n\n\n\n\n\n  \n    \n      \n      exp_id\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n    \n    \n      chain\n      draw\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      0.088805\n      0.001197\n      -0.000484\n      0.171321\n      0.307768\n      -0.000114\n      -0.000358\n      -0.000050\n      0.119042\n      0.035483\n      0.008818\n      0.017530\n      0.126463\n      0.044351\n      0.021847\n      0.077193\n      0.055479\n      8.898377e-03\n      0.010240\n      0.001336\n    \n    \n      1\n      0.134995\n      -0.000085\n      0.169952\n      -0.000517\n      0.005521\n      0.006376\n      -0.000489\n      0.134169\n      0.245929\n      0.038492\n      -0.000515\n      0.002976\n      0.209037\n      0.062656\n      0.002869\n      0.139614\n      0.254202\n      4.575356e-02\n      0.286258\n      0.206261\n    \n    \n      2\n      0.001273\n      0.191886\n      0.001264\n      0.090177\n      -0.000060\n      0.330653\n      0.202727\n      -0.000368\n      0.015053\n      0.305181\n      -0.000259\n      0.049178\n      -0.000262\n      0.015266\n      0.090864\n      0.060138\n      0.008056\n      1.787840e-01\n      0.003004\n      -0.000283\n    \n    \n      3\n      0.091385\n      -0.000165\n      0.175993\n      0.210623\n      -0.000398\n      0.000092\n      0.008886\n      0.169622\n      0.016613\n      -0.000175\n      -0.000333\n      -0.000151\n      0.069115\n      0.003197\n      0.009931\n      0.008144\n      0.069742\n      5.125832e-04\n      0.223892\n      -0.000056\n    \n    \n      4\n      0.128263\n      -0.000264\n      0.029506\n      0.074044\n      -0.000028\n      0.013448\n      0.034857\n      0.181062\n      0.059433\n      0.119991\n      0.022186\n      0.181830\n      0.078825\n      0.042061\n      0.229243\n      0.173755\n      0.131305\n      -3.022104e-08\n      0.019529\n      0.023409\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3\n      995\n      0.010141\n      0.046869\n      0.261628\n      0.187072\n      0.207434\n      0.041217\n      0.029719\n      0.098765\n      -0.000002\n      0.050363\n      -0.000449\n      0.215972\n      0.084759\n      0.035756\n      0.187955\n      0.000416\n      0.005068\n      -3.511800e-04\n      0.208030\n      0.008831\n    \n    \n      996\n      0.020431\n      0.113550\n      -0.000424\n      0.009520\n      0.283799\n      0.010694\n      -0.000157\n      0.030434\n      0.104618\n      0.032070\n      0.117800\n      0.152587\n      -0.000257\n      -0.000003\n      -0.000008\n      0.029872\n      0.224894\n      2.599086e-01\n      0.092621\n      0.123227\n    \n    \n      997\n      0.082644\n      0.004597\n      0.028806\n      0.001567\n      0.009130\n      0.000728\n      0.010956\n      0.001111\n      0.212021\n      0.240963\n      0.067241\n      0.182531\n      0.301767\n      0.221201\n      0.000654\n      0.001957\n      0.137934\n      2.372076e-01\n      0.014318\n      0.257021\n    \n    \n      998\n      -0.000116\n      0.085200\n      -0.000517\n      0.005099\n      0.029492\n      0.040047\n      -0.000026\n      0.002956\n      0.104428\n      0.082584\n      0.000486\n      0.001877\n      0.032360\n      0.001402\n      0.003078\n      0.001904\n      -0.000390\n      -1.231683e-04\n      -0.000350\n      0.005512\n    \n    \n      999\n      0.002641\n      0.138605\n      0.201605\n      0.103506\n      0.180542\n      0.034928\n      0.012054\n      -0.000369\n      0.047570\n      0.086689\n      0.257165\n      0.102930\n      0.221825\n      0.178257\n      0.010981\n      0.324702\n      0.091735\n      2.999001e-02\n      0.200721\n      -0.000477\n    \n  \n\n4000 rows × 20 columns\n\n\n\n\nfig, axs = plt.subplots(1, 2, figsize=(20, 6))\nnp.random.seed(19)\naxs = axs.flatten()\nN, bins, patches = axs[0].hist(np.exp(predicted_curves.cumsum(axis=1))[11], color='slateblue', edgecolor='grey', alpha=0.3, bins=40);\naxs[0].axvline(2)\nfor i in range(1, len(patches)):\n    patches[i].set_facecolor('red')\nax.set_title(\"Proportion of Credible Curves which achieve a cumulative Lift more than 2\", fontsize=20)\nax.set_xlabel(\"Lift\")\naxs[0].set_title(\"Proportion of Curves which achieve 2x Lift after 20 experiments\")\naxs[0].legend()\naxs[1].plot(np.exp(predicted_curves.cumsum(axis=1)).sample(100).T, color='grey');\naxs[1].plot(np.exp(predicted_curves.cumsum(axis=1)).mean(), color='red', label='Expected Growth curve');\naxs[1].set_title(\"Sample Set of Possible Growth Curves\")\naxs[1].set_ylim(0, 10)\n\nWARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n(0.0, 10.0)"
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html",
    "href": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html",
    "title": "Factor Analysis and Construct Validity in Psychology",
    "section": "",
    "text": "In a prescient paper in the 1970s Paul Meehl wrote about the lack of cumulative success in the psychologocial sciences, and how this should be attributed to poor methodology rather than the sheer difficulty of the subject. He elaborates an impressive list of problems for modeling any psychological process. Common themes criss-cross the list and interact with one another in ways which could make you despair for the discipline. So it is, I think, somewhat surprising that Meehl locates the main problem not in the subject, but in the method of significance testing.\nWe’ll narrow our focus shortly, but first consider the breadth of the issues."
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#meehls-problems-plaguing-psychology",
    "href": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#meehls-problems-plaguing-psychology",
    "title": "Factor Analysis and Construct Validity in Psychology",
    "section": "Meehl’s Problems Plaguing Psychology",
    "text": "Meehl’s Problems Plaguing Psychology\n\n\n\n\n\n\n\nProblem\nDescription\n\n\n\n\nResponse-Classification Problem\nDifficulty attributing mental process to observed behaviour\n\n\nSituation-Taxonomy Problem\nDifficulty isolating the stimulus from rough description of environment\n\n\nUnit of Measurement\nChoice of scale e.g. ratio or interval, continuous or discrete\n\n\nIndividual Differences\nCommon psychological dispositions arise from idiosyncratic mixture of influences\n\n\nPolygenetic Hereditry\nCommon psychological dispositions have complex causal roots\n\n\nDivergent Causality\nVery sensitive to initial conditions, slight differences at source result in large differences in outcomes\n\n\nIdiographic Problem\nOften relates to the specific discovery of facts rather than generalisable laws\n\n\nUnknown Critical Events\nPaucity of medical history or local context\n\n\nNuisance Variables\nDifficulty deciphering wealth of related variables\n\n\nFeedback Loops\nInterventions lead to changing behaviour, disrupting the study\n\n\nAutoCatalytic Processes\nIndividual influences on their own psychological disposition during tests\n\n\nRandom Walk\nDifferences in dispositional response often due to random flux, rather than different causal influence\n\n\nSheer Number of Variables\nCumulative influence of small random-drift can have decisive impact on the psychological disposition\n\n\nImportance of Cultural Factors\nWeight of an individual variable may vary with cultural context\n\n\nContext-Dependent Stochastilogicals\nAny derived rule of behaviour is likely only probabilisitic and subject to contextual variation.\n\n\nOpen Concepts\nLatent psychological factors under study “evolve” as we include/remove indicative measures\n\n\nIntentionality, Purpose and Meaning\nPurpose drives behaviour and changes in behaviour\n\n\nRule Governance\nPeople follow rules influencing their behaviour\n\n\nUniquely Human Events & Powers\nThere are some behaviours which have no animal/ape analogies to compare. Limiting data\n\n\nEthical Constraints on Research\nConstraints on some decisive testing methodologies due to ethical abuses.\n\n\n\nWe’re going to focus on two of the problems that most clearly relate the issue of significance testing: open concepts, context-dependent stochastilogicals. These issues are tightly coupled with the ability to falsify psychological hypotheses since they both serve as reasons to doubt contrary evidence and therefore deny us a decisive rejection of our hypotheses even when they conflict with observable data. This difficulty directly undermines the paradigm of null-hypothesis testing in psychology, since they imply that we are incapable of placing the null under severe scrutiny."
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#the-recipe-open-concepts-and-context-sensitive-stocastologicals",
    "href": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#the-recipe-open-concepts-and-context-sensitive-stocastologicals",
    "title": "Factor Analysis and Construct Validity in Psychology",
    "section": "The Recipe: Open Concepts and Context Sensitive Stocastologicals",
    "text": "The Recipe: Open Concepts and Context Sensitive Stocastologicals\nThe ugly word “stochastological” is to be contrasted with the, perhaps more familar but also pompous, notion of a nomological inference - one which is valid by appeal to a wholly general law.\n\\[ \\text{(Nomological): } \\forall x(Fx \\rightarrow Gx) \\]\nMeehl argues that the idiosyncratic and individual specific patterns of causation in psychology short-circuit any appeals to over-arching rules. Even when we can consistently measure a disposition common across individuals, the nature of the observed patterns support probabilistic inference not law-like generalisations. Even then the underlying probabilities are liable to change with the context. You might be prone to defer to authority in 9 of 10 cases, but always exhibit knee jerk refusal when prompted by a political opponent.\n\\[ \\text{(Stochastic): }  \\underbrace{P(Gx | Fx) \\geq 0.90}_{context = c} \\]\nCombine this issue with contingencies of measurement and the dynamic nature of the scales, and we have a recipe for undermining any null-hypothsis significance test. Consider iteritive model building which tests for predictive aspects of some latent factor, say risk-aversion, on performance at a related task. Say the factor was initially derived from a set of observational measures:\n\\[ \\underbrace{ feature4,  \\overbrace{ \\text{ feature5, } \\overbrace{feature1 , feature2, feature3 }^{\\text{initial feature measures}}}^{\\text{final feature measures}}}_{\\text{Intuitive Concept}} \\twoheadrightarrow \\text{Construct} \\]\nNow each iteration was designed to test the imagined psychological constructand the features are observational traits associated with risk-taking. Each additional feature seemed reasonable at time. We may even have gotten better predictive accuracy. But the model build and the changing measurements raises question over what we’re constructing and whether it truly “captures” the pre-theoretical psychological concept.\nNow we have a measure of risk-aversion and we try to render an intuitive hypothesis mathematically precise with respect to our construct. This is itself an art, but for the moment assume we can state \\((H)\\). Since the hypothesis test is supposed to infer the falsity of the assumptions from evidence contrary to the main hypothesis i.e. if when assuming the hypothesis and our auxilary commitments (regarding measurement and context) we find evidence inconsistent with our expectations, then we should reject the hypothesis! In practice the hypothesis is ussually far more entrenched in the minds of the experimenters than the extensive auxillary commitments, so we normally seek the source of predictive error in mistakes made than with the key hypothesis.\n\\[ (H \\wedge A) \\rightarrow O , \\neg O \\vdash \\neg (H \\wedge A) \\text{ ....so not A}\\]\nIn this way we preserve the hypothesis and remove it from exposure to a strict test of its accuracy. This experiment is really only validating the consistency of the data with a nebulous range of auxilary commitments, and as such allows the experimenter to move the goal-posts almost at whim.\nMeehl concedes that there may be some justification for this procedure if we would grant that \\((H)\\) has in some sense a greater “verisimilitude” than the auxilary commitments. Psychology differs from other disciplines in that the objects of measurement and the manner of measurement are not tightly bound. The truth “content” of claims about measurement of length, for instance, stand or fall with claims about the measurement instrument - they are so intimately connected that the measurement apparatus is almost definitional. You might object that appeals to versimilitude in some way puts the cart before the horse. We’re designing an experiement to test \\((H)\\), how can we presume it’s truth in testing!? This is fair but only points to difficulty of making \\((H)\\) precise and the distance between mental phenomena and our measurement of it"
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#open-concepts-and-factor-analysis",
    "href": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#open-concepts-and-factor-analysis",
    "title": "Factor Analysis and Construct Validity in Psychology",
    "section": "Open Concepts and Factor Analysis",
    "text": "Open Concepts and Factor Analysis\nIf we’re lucky we have a clear idea of how to measure the psychological phenomenon of interest and you can devise a survey to capture the details. If instead you just have some data and you think there might be a latent factor driving the observations, then the the factor analysis effectively tries to group the observed variable by their correlations. The underlying statistical model for \\(m\\) factors states:\n\nAssume the model \\[\\mathbf{X} = \\mathbf{\\mu}^{p \\times 1}  + \\mathbf{L}^{p \\times m} \\mathbf{F}^{m \\times 1} +\\epsilon^{p \\times 1} \\]\nwhere \\(\\mathbf{X}\\) is our data matrix recording the observational data while \\(\\mu\\) is the mean vector with entries for each column in \\(\\mathbf{X}\\). So our observational data is deemed a function modifying the multivariate mean as a linear function of latent factors \\(\\mathbf{F}\\) with factor loadings \\(l_{i, j} \\in \\mathbf{L}\\), for each of the observed variables \\(i\\) and for each \\(j\\) of the \\(m\\) factors.\nThe model assumes \\(\\mathbf{F}\\) and \\(\\epsilon\\) are independent and \\(E(\\mathbf{F}) = \\mathbf{0}, Cov(\\mathbf{F}) = \\mathbf{I}\\) In addition note that \\(E(\\epsilon) = \\mathbf{0}\\) and \\(Cov(\\epsilon) = \\Psi\\) where \\(\\Psi\\) is a diagonal matrix.\nThen we can show that \\[Cov(\\mathbf{X}) = \\mathbf{LL}^{'} + \\Psi\\]\n\nThat’s a bit abstract, but the point is just that each factor is a linear construct of the observed data, and we can choose how many constructs to build. An important consequence of this fact is that we can express the variance of each observed feature in terms of the loadings and a random variance, so if we can explain a high portion of their variance in a low number of factors we can be reasonably sure that the dimensional reduction remains representative of the diversity in the original data set.\n\\[ Var(X_i) = \\underbrace{l_{i, j}^{2} + l_{i, 2}^{2} ... l_{i, m}^{2}}\\_{communalities} + \\psi_{i}\\]\n\nProof. \\[ (\\mathbf{X} - \\mu) = (\\mathbf{L}\\mathbf{F} + \\epsilon)\\] \\[ \\Rightarrow (\\mathbf{X} - \\mu)(\\mathbf{X} - \\mu)^{'} = (\\mathbf{L}\\mathbf{F} + \\epsilon)(\\mathbf{L}\\mathbf{F} + \\epsilon)^{'}\\] \\[ = (\\mathbf{L}\\mathbf{F} + \\epsilon)((\\mathbf{L}\\mathbf{F})^{'} + \\epsilon^{'}) \\] \\[ = \\mathbf{L}\\mathbf{F}(\\mathbf{L}\\mathbf{F})^{'} + \\epsilon(\\mathbf{L}\\mathbf{F})^{'} + \\mathbf{L}\\mathbf{F}\\epsilon^{'} + \\epsilon\\epsilon^{'}\\] \\[ \\Rightarrow E((\\mathbf{X} - \\mu)(\\mathbf{X} - \\mu)^{'}) = E( (\\mathbf{L}\\mathbf{F} + \\epsilon)(\\mathbf{L}\\mathbf{F} + \\epsilon)^{'})\\] \\[ \\Rightarrow Cov(\\mathbf{X}) = \\mathbf{L}E(\\mathbf{F}\\mathbf{F}^{'})\\mathbf{L}^{'} + E(\\epsilon\\mathbf{F}^{'})\\mathbf{L}^{'} + \\mathbf{L}E(\\mathbf{F}\\epsilon^{'}) + E(\\epsilon\\epsilon^{'}) \\] \\[ = \\mathbf{L}\\mathbf{L}^{'} + \\Psi \\]\n\n\nEstimating the latent factor values\nVarious techniques can be applied to estimate the loadings \\(\\mathbf{L}\\) based on a strategic decomposition of sample covariance matrix to derive the principal factors. The typical technique is to use the eigenvalue decomposition, but a maximum likelihood method is also feasible. It is another question altogeher for how to estimate the factor scores \\(f_{j} \\in \\mathbf{F}\\) from our derived factor loadings.\nThis is a two step, where we base an estimate on a set of prior estimates. One or more latent factors are assumed, observable features postulated to be related are grouped and from these we derive a recipe for constructing the latent factor as a composite of the observed features. From this recipe, we can then by a process of optimisation derive estimates for the values of the latent feature. We won’t dwell on the details here, but I want to stress the level of abstraction! When it comes to test the hypotheses about the underlying psychological phenomenon, this method has certain mathematical appeal but it is not above question. It is these kind of contingencies: the bespoke assumptions of the model, but correlation and covariance relationship between the observed features and richness of the data required to discover the expected values of \\(\\mathbf{L}, \\mathbf{f}_j\\) respectively, coupled with the difficulty of interpreting the factors in light of the original psychological concept, that undermine cumulative progress in psychology."
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#weighing-the-hypothesis",
    "href": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#weighing-the-hypothesis",
    "title": "Factor Analysis and Construct Validity in Psychology",
    "section": "Weighing the Hypothesis",
    "text": "Weighing the Hypothesis\nThese observations suggest that the notion construct validity is not easily resolved and as such our initial hypotheses are plagued by innumerable auxilary commitments. It is with these considerations in mind that Paul Meehl can write:\n\n“[T]he almost universal reliance on merely refutng the null hypothesis as the standard method for corroborating substantive theories in the soft areas is a terrible mistake, is basically unsound, poor scientific strategy and one of the worst things that ever happened in theory of psychology” - Theoretical Risks and Tabular Asteriks, Sir Karl, Sir Ronald and the Slow Progress of Soft Psychology\n\nThe accumulation of auxilary commitments makes the cumulative confirmation of substantive psychological theory proportionaly unlikely. At best we may get lucky in some cases, but the character of the object under study is so dynamic that simple significance tests are next to useless. The “distance” between the latent factor and our measurement of it supply an almost endless set of auxilary commitments which can come under pressure when evaluating a given hypothesis.\nBut there is a tension since difficulty of measurement does not necessarily undermine the theory. In particular, there are theory’s which have a high degree of intuitive plausibility (“verisimilitude”) but escape our ability to properly measure. Any measurement construct is at best an attempted proxy. There are some historic measures of construct validity such as Cronbach’s alpha which at least test for a directional consistency in the observed features."
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#factor-reliability-an-example-in-code.",
    "href": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#factor-reliability-an-example-in-code.",
    "title": "Factor Analysis and Construct Validity in Psychology",
    "section": "Factor Reliability: An Example in Code.",
    "text": "Factor Reliability: An Example in Code."
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html",
    "href": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "import pandas as pd\nfrom matplotlib import pyplot as plt\nimport nltk\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom factor_analyzer import FactorAnalyzer\nfrom sklearn.decomposition import FactorAnalysis\nimport random\nimport seaborn as sns\nrandom.seed(30)"
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#create-the-fake-customer-purchase-data",
    "href": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#create-the-fake-customer-purchase-data",
    "title": "Examined Algorithms",
    "section": "Create the Fake Customer Purchase data",
    "text": "Create the Fake Customer Purchase data\nWe create two fake data sets with a discernible structures, but we don’t want the models to work too easily so we sample again from these data pairing both sets as if we have a customer and their products.\n\nX, y = make_blobs(n_samples=10000, \n                  centers=5, \n                  n_features=6,\n                 random_state=0\n                 )\n\nprods = ['product_desc_' + str(x) for x in range(0, 6)]\ndf_products = pd.DataFrame(data=X, columns=prods)\ndf_products['Product_Class'] = y\ndf_products['Product_ID'] = df_products.index\n\nX, y = make_blobs(n_samples=1000, \n                  centers=10,\n                  n_features=8,\n                  random_state=0)\ncusts = ['customer_desc_' + str(x) for x in range(0, 8)]\ndf_customer = pd.DataFrame(data=X,\n                           columns=custs)\n\ndf_customer['Customer_Class'] = y\ndf_customer['Customer_ID'] = df_customer.index\n\n# Randomly Select some of the customers to pair with random purchases\nday1 = pd.DataFrame(zip(\n        [random.randint(0, 1000) for x in range(0, 500)],\n        [random.randint(0, 10000) for x in range(0, 500)]\n        ), \n        columns=['Customer_ID', 'Product_ID']\n                   )\n\nday2 = pd.DataFrame(zip(\n        [random.randint(0, 1000) for x in range(0, 500)],\n        [random.randint(0, 10000) for x in range(0, 500)]\n        ), \n        columns=['Customer_ID', 'Product_ID'])\n\npurchases = pd.concat([day1, day2],\n                      axis=0, \n                      ignore_index=True)\npurchases.head()\n\n\n\n\n\n  \n    \n      \n      Customer_ID\n      Product_ID\n    \n  \n  \n    \n      0\n      552\n      773\n    \n    \n      1\n      827\n      8608\n    \n    \n      2\n      296\n      518\n    \n    \n      3\n      625\n      5394\n    \n    \n      4\n      30\n      8543\n    \n  \n\n\n\n\n\ndf_purchases = None\nfor purchase in range(0, len(purchases)):\n    cust_id = purchases['Customer_ID'][purchase]\n    prod_id = purchases['Product_ID'][purchase]\n    cust = df_customer[df_customer['Customer_ID'] == \n                       cust_id]\n    cust.reset_index(inplace=True, drop=True)\n    prod = df_products[df_products['Product_ID'] == \n                       prod_id]\n    prod.reset_index(inplace=True, drop=True)\n    temp = pd.concat([prod, cust], axis=1)\n    if df_purchases is None:\n        df_purchases = pd.concat([prod, cust], axis=1)\n    else:\n        df_purchases = df_purchases.append(\n            pd.concat([prod, cust], axis=1)\n        )\ndf_purchases.reset_index(inplace=True, drop=True)\ndf_purchases\n\n\n\n\n\n  \n    \n      \n      product_desc_0\n      product_desc_1\n      product_desc_2\n      product_desc_3\n      product_desc_4\n      product_desc_5\n      Product_Class\n      Product_ID\n      customer_desc_0\n      customer_desc_1\n      customer_desc_2\n      customer_desc_3\n      customer_desc_4\n      customer_desc_5\n      customer_desc_6\n      customer_desc_7\n      Customer_Class\n      Customer_ID\n    \n  \n  \n    \n      0\n      -1.233276\n      8.747108\n      9.165746\n      -0.007541\n      4.652380\n      1.913970\n      1\n      773\n      -6.374916\n      -2.257586\n      7.688019\n      -7.992522\n      7.593467\n      -9.193683\n      10.376847\n      -0.388405\n      8.0\n      552.0\n    \n    \n      1\n      -0.170403\n      8.219852\n      9.710727\n      -1.550177\n      5.811414\n      1.492459\n      1\n      8608\n      9.294317\n      -2.231715\n      6.061894\n      -0.438840\n      1.246116\n      8.820684\n      -9.950039\n      -7.391761\n      1.0\n      827.0\n    \n    \n      2\n      -2.822567\n      8.036298\n      10.929105\n      -0.935760\n      6.077699\n      1.265250\n      1\n      518\n      -8.793580\n      4.805865\n      6.167272\n      5.770659\n      7.451190\n      4.144325\n      1.196351\n      5.414350\n      2.0\n      296.0\n    \n    \n      3\n      1.019780\n      6.766354\n      -9.152836\n      -8.611808\n      -8.749707\n      6.637419\n      2\n      5394\n      -7.047638\n      -2.131113\n      5.768863\n      -8.094387\n      6.223832\n      -8.834962\n      9.629384\n      -1.935227\n      8.0\n      625.0\n    \n    \n      4\n      4.321656\n      7.430975\n      8.847703\n      5.921750\n      -0.868445\n      5.483210\n      3\n      8543\n      -0.169121\n      1.346768\n      -10.211423\n      1.709859\n      1.655568\n      1.891809\n      7.856076\n      4.333816\n      4.0\n      30.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      2.151425\n      3.179085\n      2.337353\n      0.559543\n      -1.629433\n      2.493002\n      0\n      8383\n      -4.926378\n      -1.474424\n      2.848533\n      -10.733950\n      4.237231\n      5.048239\n      -5.263423\n      -7.005510\n      5.0\n      272.0\n    \n    \n      996\n      0.475735\n      4.378010\n      2.597665\n      1.569648\n      -1.131376\n      3.110801\n      0\n      6141\n      7.821350\n      -1.948967\n      6.039587\n      1.739432\n      2.351800\n      8.325224\n      -10.263796\n      -7.450850\n      1.0\n      616.0\n    \n    \n      997\n      4.621406\n      7.791088\n      8.592775\n      6.423716\n      0.915721\n      4.727899\n      3\n      406\n      -9.236352\n      2.760513\n      -7.290240\n      9.169618\n      -0.188279\n      -2.443252\n      -4.153840\n      6.140598\n      3.0\n      266.0\n    \n    \n      998\n      -1.720297\n      8.174031\n      9.746428\n      -1.849028\n      5.046019\n      0.951072\n      1\n      9737\n      -1.411479\n      0.830250\n      5.167232\n      -9.188249\n      3.176105\n      4.570698\n      -6.300042\n      -7.561958\n      5.0\n      263.0\n    \n    \n      999\n      2.804878\n      4.882689\n      -0.262516\n      0.311810\n      -1.867281\n      4.639076\n      0\n      3486\n      -6.696620\n      1.945182\n      -7.520248\n      8.545794\n      3.743541\n      -3.216961\n      -4.505348\n      3.400242\n      3.0\n      589.0\n    \n  \n\n1000 rows × 18 columns"
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#preliminary-plotting-do-the-factors-seperate-the-structure",
    "href": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#preliminary-plotting-do-the-factors-seperate-the-structure",
    "title": "Examined Algorithms",
    "section": "Preliminary Plotting: Do the Factors seperate the structure?",
    "text": "Preliminary Plotting: Do the Factors seperate the structure?\nA worthwhile plot to apply with any dimensional reduction technique (such as PCA or factor analysis) is to check if and how the data seperates when plotted on the reduced plane. In lieu of knowledge of the data we can always compare this representation to the output of a clustering algorithm.\n\ncust_desc = [x for x in df_purchases.columns if \n             'customer_desc' in x]\nX = df_customer[cust_desc]\nkmeans = KMeans(init='k-means++',\n                n_clusters=3, \n                n_init=30\n               )\nkmeans.fit(X)\nclusters = kmeans.predict(X)\nX['cluster'] = clusters\n\nsklearn_fa = FactorAnalysis(n_components=2, \n                            rotation='varimax'\n                           )\nY_fa = pd.DataFrame(sklearn_fa.fit_transform(X[cust_desc]))\nX = pd.concat([X, Y_fa], axis=1)\nX[[0, 1, 'cluster']].plot.scatter(x=0,\n                      y=1,\n                      c='cluster',\n                      colormap='viridis')\nplt.title(\"Factor Analysis Representation - Coloured by inferred Clusters\")\nplt.ylabel(\"Factor 1\")\nplt.xlabel(\"Factor 2\")\nplt.style.use('default')\nplt.show()\n\n/Users/nathanielforde/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  if __name__ == '__main__':\n\n\n\n\n\nWe can see here that the factors do a pretty good job of seperating the classes (0, 1), but mix up (2, 1). In addition we can see that there are 7 distinct clusters on the factor analysis representation which suggests that our choice three clustering classes is too low."
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#choosing-the-number-of-factors",
    "href": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#choosing-the-number-of-factors",
    "title": "Examined Algorithms",
    "section": "Choosing the number of Factors",
    "text": "Choosing the number of Factors\nOne suggestive way in which to determine the number of factors which we should extract is to build the skree plot of the eigenvalues and select the number of features where there is a higher relative eigenvalues.\n\nimport numpy as np\nfeature_names = ['customer_desc_' + str(x) for x in range(0, 8)]\n\nfa = FactorAnalyzer(n_factors=3)\nfa.fit(X[feature_names], 10)\nev, v = fa.get_eigenvalues()\nplt.plot(range(1,X[feature_names].shape[1]+1),ev)\nplt.show()\n\n\n\n\nOn the basis of this plot we should probably choose no more than two factors at most but we’ll continue with 3 for purposes of illustration. The factor loadings are linear functions of the observed features and so we may interpret the newly created factors by observing which of the observed features play a greater role in their composition.\n\nfa_loading_matrix = pd.DataFrame(fa.loadings_, \n                                 columns=['FA{}'.format(i) for \n                                          i in range(1, 3+1)], \n                              index=feature_names)\nfa_loading_matrix['Highest_loading'] = fa_loading_matrix.idxmax(axis=1)\nfa_loading_matrix = fa_loading_matrix.sort_values('Highest_loading')\nfa_loading_matrix\n\n\n\n\n\n  \n    \n      \n      FA1\n      FA2\n      FA3\n      Highest_loading\n    \n  \n  \n    \n      customer_desc_1\n      0.727724\n      0.024771\n      0.091443\n      FA1\n    \n    \n      customer_desc_3\n      0.826998\n      0.068252\n      0.014691\n      FA1\n    \n    \n      customer_desc_5\n      0.637687\n      -0.003234\n      0.489801\n      FA1\n    \n    \n      customer_desc_7\n      0.787048\n      0.081877\n      -0.414776\n      FA1\n    \n    \n      customer_desc_4\n      0.015911\n      1.001115\n      0.175406\n      FA2\n    \n    \n      customer_desc_6\n      -0.172587\n      0.077989\n      -0.684129\n      FA2\n    \n    \n      customer_desc_0\n      -0.229668\n      -0.628541\n      0.355323\n      FA3\n    \n    \n      customer_desc_2\n      -0.304156\n      0.217303\n      0.440891\n      FA3\n    \n  \n\n\n\n\nWe can see here that there is probably only one sensible factor to be derived from our dataset.\n\nimport seaborn as sns\n\nplt.figure(figsize=(25,5))\n\n# plot the heatmap for correlation matrix\nax = sns.heatmap(fa_loading_matrix.drop('Highest_loading', axis=1).T, \n                vmin=-1, vmax=1, center=0,\n                cmap=sns.diverging_palette(220, 20, n=200),\n                square=True, annot=True, fmt='.2f')\n\nax.set_yticklabels(\n    ax.get_yticklabels(),\n    rotation=0);\n\n\n\n\n\ncommunalities = pd.DataFrame(fa.get_communalities(), \n                             index=list(feature_names))\nfeatures_comm = list(communalities[communalities[0] > 0.33].index)\nprint('Total variables/features with communalities >0.33: {}'.format(len(features_comm)))\ncommunalities\n\nTotal variables/features with communalities >0.33: 8\n\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      customer_desc_0\n      0.574065\n    \n    \n      customer_desc_1\n      0.538558\n    \n    \n      customer_desc_2\n      0.334116\n    \n    \n      customer_desc_3\n      0.688800\n    \n    \n      customer_desc_4\n      1.033252\n    \n    \n      customer_desc_5\n      0.646560\n    \n    \n      customer_desc_6\n      0.503901\n    \n    \n      customer_desc_7\n      0.798188"
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#testing-the-validity-of-a-hypothetical-factor",
    "href": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#testing-the-validity-of-a-hypothetical-factor",
    "title": "Examined Algorithms",
    "section": "Testing the Validity of a Hypothetical Factor",
    "text": "Testing the Validity of a Hypothetical Factor\nCronbach’s Alpha is a statistical measure of the reliability of the factor analysis derived from a test of the covariances matrix of features which make up the proposed latent factor. A cronbach alpha closer to 1 is desired.\n\\(\\alpha = \\frac{K}{K-1}\\left(1-\\frac{\\sum \\sigma^2_{x_i}}{\\sigma^2_T}\\right)\\)\nwhere\n\\(\\sigma^2_T = \\sum \\sigma^2_{x_i} + 2 \\sum_{i < j}^K {\\rm cov}(x_i,x_j)\\)\na combination of the observational measure of variance and inter-metric covariances for each observational variable. This ties this measure to the Factor analysis model since the covariances can be re-expressed in terms of the factor loadings.\n\\(\\sigma^2_T = \\sum \\sigma^2_{x_i} + 2 \\sum_{i < j}^K (l_{i, 1} + \\epsilon_{i})(l_{j, 1} + \\epsilon_{j})\\)\nwhich means that if the factors loadings are fairly high relative the the random components of the variance then we’ll get a ratio that come close to one. Conversely low loadings will ensure that the denominator drags the ratio down.\n\nimport numpy as np\n\ndef CronbachAlpha(observed_measures):\n    observed_measures = np.asarray(observed_measures)\n    sample_vars = observed_measures.var(axis=1, ddof=1)\n    total_scores = observed_measures.sum(axis=0)\n    nitems = len(observed_measures)\n\n    return nitems / (nitems-1.) * (1 - sample_vars.sum() / total_scores.var(ddof=1))\n\n\n#Collate the observed features\n\nfactor1 = [X['customer_desc_1'], X['customer_desc_3'], \n             X['customer_desc_5'], X['customer_desc_7']]\n\nfactor2 = [X['customer_desc_4'], X['customer_desc_0']]\n\nfactor3 = [X['customer_desc_5'], X['customer_desc_6'], \n             X['customer_desc_2']]\n#Get cronbach alpha\nfactor1_alpha = CronbachAlpha(factor1)\n#factor2_alpha = CronbachAlpha(factor2)\n#factor3_alpha = CronbachAlpha(factor3)\nprint(factor1_alpha, \n      factor2_alpha, \n      factor3_alpha\n     )\n\n0.7889764629492505 -3.382959230225132 -0.8158399732248119\n\n\nWhich shows as expected that only one of the proposed factors is sensible."
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html",
    "href": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "import random\nimport statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nfrom scipy.optimize import minimize_scalar, minimize\nfrom IPython.display import Latex\nfrom stargazer.stargazer import Stargazer\nfrom IPython.core.display import HTML\nThis notebook is a python port of some of the code in “Learning Microeconometrics with R” by Christopher P Adams. It corresponds to the blog post: https://nathanielf.github.io//post/mle_utility_and_choice/"
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#the-problem-modelling-discrete-choice-by-latent-utility-metrics",
    "href": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#the-problem-modelling-discrete-choice-by-latent-utility-metrics",
    "title": "Examined Algorithms",
    "section": "The Problem: Modelling Discrete Choice by Latent Utility Metrics",
    "text": "The Problem: Modelling Discrete Choice by Latent Utility Metrics\nSome assumptions about the form of the utility distribution are crucial as our modeling efforts will go wrong if we know nothing about the latent utilities. We assume that the latent utility can be expressed as by the revealed preferences i.e. as the share or proportion of choices made by the customers.\nThe utility is some function of product and consumer’s properties, perhaps mostly driven by price\n\\[ utility = \\mathbf{X'}\\beta + e\\]\nand market share is an expression of that utility \\[ demand_A = utility_{A} > 0 \\]\nIn a choice context we’re trying to determine if the implicit utility measure is sufficient to drive a purchase, and as such OLS models are inappropriate\n\nN = 1000\na = 2\nb = -3\ne = np.random.normal(0, 1, N)\nconsumer_desc = np.random.uniform(3, 1, N)\nconsumer_desc1 = np.random.uniform(2, 5, N)\nutility = 2 + 3*consumer_desc + -4*consumer_desc1 + e\n## Predicting choice over two options\ndemand_A = utility > 0\nX = pd.DataFrame({'product_desc': consumer_desc, 'product_desc1': consumer_desc1})\nX = sm.add_constant(X)\nlm1 = sm.OLS(demand_A,X)\nlm1_results = lm1.fit()\nprint(lm1_results.summary())\nprint(round(lm1_results.params, 5))\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.200\nModel:                            OLS   Adj. R-squared:                  0.198\nMethod:                 Least Squares   F-statistic:                     124.5\nDate:                Sat, 20 Feb 2021   Prob (F-statistic):           5.42e-49\nTime:                        16:01:07   Log-Likelihood:                 58.448\nNo. Observations:                1000   AIC:                            -110.9\nDf Residuals:                     997   BIC:                            -96.17\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconst             0.2095      0.040      5.291      0.000       0.132       0.287\nproduct_desc      0.1120      0.013      8.939      0.000       0.087       0.137\nproduct_desc1    -0.1045      0.008    -12.630      0.000      -0.121      -0.088\n==============================================================================\nOmnibus:                      472.919   Durbin-Watson:                   1.983\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1999.877\nSkew:                           2.298   Prob(JB):                         0.00\nKurtosis:                       8.183   Cond. No.                         23.8\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nconst            0.20953\nproduct_desc     0.11204\nproduct_desc1   -0.10449\ndtype: float64\n\n\nThis stems from the fact that we’re’trying to estimate a conditional probability over a binary choice not a continuous measure. The revealed preference assumption says that we can predict the purchase if the utility of good is positive.\n\\[Pr(demand_A = 1) = utility > 0 \\] \\[= Pr(\\mathbf{X'}\\beta + e > 0) \\] \\[ = Pr(e > - \\mathbf{X'}\\beta ) \\] \\[ = 1 - F(\\mathbf{X'}\\beta ) \\]\nwhere \\(F\\) is the distribution of the unobserved random variable \\(e\\). The challenge is using the correct distribution as this feeds the method of statistical estimation of the parameters \\(\\beta\\)"
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#maximum-likelihood-fits-over-candidate-distributions",
    "href": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#maximum-likelihood-fits-over-candidate-distributions",
    "title": "Examined Algorithms",
    "section": "Maximum Likelihood Fits over Candidate Distributions",
    "text": "Maximum Likelihood Fits over Candidate Distributions\nThere are a number of candidate distributions which might serve to replace \\(F\\) and estimate the share of purchases\n\ndef log_binomial_dist(params, *args):\n    p = params[0]\n    p_hat = args[0]\n    N = args[1]\n    return -((p_hat*N)*np.log(p) + (1-p_hat)*N*np.log(1-(p)))\n\nres = minimize(log_binomial_dist, x0 = [.1], args =(.34, 100), bounds = ((0, .99),))\nprint(res)\n    \n\n\n      fun: 64.10354778811556\n hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n      jac: array([0.])\n  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n     nfev: 16\n      nit: 6\n     njev: 8\n   status: 0\n  success: True\n        x: array([0.33999999])\n\n\n\ndef ll_ols(params, *args):\n    X, y = args[0], args[1]\n    beta = [params[0], params[1], params[2]]\n    mu, sd, = params[3], params[4]\n    z = (y - X.dot(beta)) / sd\n    log_lik = -sum(np.log(stats.norm.pdf(z)) - np.log(sd))\n    return log_lik\n\nx = np.random.normal(5, 2, 1000)\nx1 = np.random.normal(6, 1, 1000)\nx2 = np.random.uniform(2, 7, 1000)\ny = 1 + .3*x + 5*x1 + np.random.normal(0, 1, 1000)\n\nX1 = pd.DataFrame({'consumer_desc': x, 'consumer_desc1': x1})\nX1 = sm.add_constant(X1)\n\nres = minimize(ll_ols, x0 =[2, 1, 4, 2, 1], method = 'Nelder-Mead', args =(X1, y))\nprint(res)\n\n final_simplex: (array([[1.27170093, 0.28846111, 4.97078397, 2.33608166, 0.98080792],\n       [1.27163611, 0.28845751, 4.97079439, 2.33611095, 0.98081281],\n       [1.27179597, 0.28845924, 4.97076792, 2.33606271, 0.98080085],\n       [1.27178217, 0.28845315, 4.97077749, 2.33606068, 0.98081083],\n       [1.2716755 , 0.28844586, 4.97079715, 2.33609652, 0.98080788],\n       [1.27174903, 0.28845715, 4.97077654, 2.33606353, 0.98081551]]), array([1399.55005084, 1399.55005089, 1399.55005094, 1399.55005095,\n       1399.55005095, 1399.55005106]))\n           fun: 1399.5500508414766\n       message: 'Optimization terminated successfully.'\n          nfev: 384\n           nit: 238\n        status: 0\n       success: True\n             x: array([1.27170093, 0.28846111, 4.97078397, 2.33608166, 0.98080792])\n\n\n\ndef log_probit_dist(params, *args):\n    X, y = args[0], args[1]\n    beta = [params[0], params[1], params[2]]\n    mu, sd, = params[3], params[4]\n    Xb = X.dot(beta)\n    q = 2*y-1\n    log_lik = np.log(stats.norm.cdf(q*Xb))\n    return -sum(log_lik)\n\n### Optimise the probit model for determining the parameters required toe estimate the underlying utility\n### True values of the parameters 2, 3, -4\nres = minimize(log_probit_dist, x0 =[0, 0 ,0 , 0, 1], args =(X, demand_A), options={'disp': True})\nprint(res)\n\nOptimization terminated successfully.\n         Current function value: 94.044202\n         Iterations: 17\n         Function evaluations: 108\n         Gradient evaluations: 18\n      fun: 94.04420183563171\n hess_inv: array([[ 0.72799404, -0.03552851, -0.26060106,  0.        ,  0.        ],\n       [-0.03552851,  0.09513159, -0.08023828,  0.        ,  0.        ],\n       [-0.26060106, -0.08023828,  0.18756252,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  1.        ]])\n      jac: array([ 3.81469727e-06, -9.53674316e-06,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00])\n  message: 'Optimization terminated successfully.'\n     nfev: 108\n      nit: 17\n     njev: 18\n   status: 0\n  success: True\n        x: array([ 2.14197608,  2.42768647, -3.51990129,  0.        ,  1.        ])\n\n\nThese estimates are still incorrect but an awful lot closer than the fits achieved by the ols model in the first section. We can validate the above optimisation against the inbuilt model of statsmodels\n\nprobit_mod = sm.Probit(demand_A, X)\nprobit_res = probit_mod.fit()\nprobit_margeff = probit_res.get_margeff()\nprint('Parameters: ', probit_res.params)\nprint('Marginal effects: ')\nprint(probit_margeff.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.094044\n         Iterations 10\nParameters:  const            2.141972\nproduct_desc     2.427687\nproduct_desc1   -3.519900\ndtype: float64\nMarginal effects: \n       Probit Marginal Effects       \n=====================================\nDep. Variable:                      y\nMethod:                          dydx\nAt:                           overall\n=================================================================================\n                   dy/dx    std err          z      P>|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nproduct_desc      0.1285      0.011     12.137      0.000       0.108       0.149\nproduct_desc1    -0.1863      0.015    -12.674      0.000      -0.215      -0.157\n================================================================================="
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#mcfaddans-bart-discrete-choice-model",
    "href": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#mcfaddans-bart-discrete-choice-model",
    "title": "Examined Algorithms",
    "section": "McFaddan’s BART Discrete Choice Model",
    "text": "McFaddan’s BART Discrete Choice Model\nThe idea is a generalisation of the above to estimate the difference in utilities across multiple products.\n\\[ U_{i,j} = \\mathbf{X'}_{i, j}\\beta + v_{i,j} \\]\nwhere the each individual’s \\(i\\) utility for a given good \\(j\\) is expressed as a linear weighted function of the product characteristics \\(\\mathbf{X}\\). Since we need to predict demand based on utility we’re really interested in estimating the differnce in utility\n\\[U_{i,A} > U_{i, B} \\]\njust when\n\\[ \\mathbf{X'}_{i, A}\\beta + v_{i,A} > \\mathbf{X'}_{i, B}\\beta + v_{i,B}\\]\nor\n\\[  v_{i,A} -  v_{i,B} > - (\\mathbf{X'}_{i, A}  - \\mathbf{X'}_{i, B})\\beta \\]\nbut then the probability of demand is just\n\\[Pr(demand_A = 1 | \\mathbf{X'}_{i, A}, \\mathbf{X'}_{i, B}) = Pr\\Bigg( v_{i,A} -  v_{i,B} > - (\\mathbf{X'}_{i, A}  - \\mathbf{X'}_{i, B})\\beta \\Bigg) \\] \\[ = Pr\\Bigg( -v_{i,A} -  v_{i,B} < (\\mathbf{X'}_{i, A}  - \\mathbf{X'}_{i, B})\\beta \\Bigg) \\] \\[ = F\\Bigg( (\\mathbf{X'}_{i, A}  - \\mathbf{X'}_{i, B})\\beta \\Bigg) \\]"
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#an-example-in-two-products-two-models",
    "href": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#an-example-in-two-products-two-models",
    "title": "Examined Algorithms",
    "section": "An Example in Two Products & Two Models",
    "text": "An Example in Two Products & Two Models\n\nN = 100\nX_A = sm.add_constant(np.random.rand(N,2))\nX_B = sm.add_constant(np.random.rand(N, 2))\nbeta = np.array([1, -2, 3])\n\n#probit we only need one normal error term since sums of normals are normal\nv = np.random.normal(0, 1, N)\ny = (X_A.dot(beta) - X_B.dot(beta)) + v > 0\nX_diff = X_A - X_B\nX_diff[:, 0] =  1\nX_diff = pd.DataFrame(X_diff, columns=['const', 'product_desc', 'product_desc1'])\n\nAgain we can try two classification algorithms which attempt to characterise the error terms \\(v_{1}, v_{2}\\) that the McFaddan model assumes\n\nprobit_mod = sm.Probit(y, X_diff)\nprobit_res = probit_mod.fit()\nprobit_margeff = probit_res.get_margeff()\n\nOptimization terminated successfully.\n         Current function value: 0.426413\n         Iterations 6\n\n\n\nv1 = np.random.weibull(1, N)\nv2 = np.random.weibull(1, N)\ny = (X_A.dot(beta) - X_B.dot(beta)) + (v1 - v2) > 0\nlogit_mod = sm.Logit(y, X_diff)\nlogit_res = logit_mod.fit()\nlogit_margeff = logit_res.get_margeff()\n\nOptimization terminated successfully.\n         Current function value: 0.499093\n         Iterations 6\n\n\n\nstargazer = Stargazer([probit_res, logit_res])\nstargazer.custom_columns(['Probit Model', 'Logit Model'], [1, 1])\n#stargazer.add_custom_notes([str(probit_margeff.summary()), str(logit_margeff.summary())])\nHTML(stargazer.render_html())\n\n\nDependent variable:yProbit ModelLogit Model(1)(2)const-0.255-0.049(0.156)(0.246)product_desc-1.491***-2.157***(0.445)(0.690)product_desc12.964***3.833***(0.578)(0.888)Observations100100R2Adjusted R2Residual Std. Error1.000 (df=97)1.000 (df=97)F Statistic (df=2; 97) (df=2; 97)Note:\n \n  *p<0.1;\n  **p<0.05;\n  ***p<0.01"
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#generalising-to-multiple-choices-the-multinomial-distribution",
    "href": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#generalising-to-multiple-choices-the-multinomial-distribution",
    "title": "Examined Algorithms",
    "section": "Generalising to Multiple choices: The Multinomial Distribution",
    "text": "Generalising to Multiple choices: The Multinomial Distribution\n\nnp.random.seed(100)\nN = 1000\nmu = [0,0]\nrho = 0.1\ncov = [[1, rho], [rho, 1]]\n\n# u is N*2\nu = np.random.multivariate_normal(mu, cov, 1000)\nx1 = np.random.uniform(0, 1, size=(N,2)) #np.random.rand(N,2)\nx2 = np.random.uniform(0, 1, size=(N,2)) #np.random.rand(N,2)\n\nU = -1 + -3*x1 + 4*x2 + u\n\ny = np.zeros(shape=(N, 2))\ny[:,0] = ((U[:,0] > 0) & (U[:,0] > U[:,1]))\ny[:,1] = (U[:,1] > 0 & (U[:,1] > U[:,0]))\n\n\nW1 = pd.DataFrame({'x1':x1[:,0], 'x2':x2[:,0]})\nW2 = pd.DataFrame({'x1':x1[:,1], 'x2':x2[:,1]})\n\n\nOptimization terminated successfully.\n         Current function value: 542.609688\n         Iterations: 185\n         Function evaluations: 338\n\n\n final_simplex: (array([[-1.74855767, -5.18018459,  7.07552039],\n       [-1.74856002, -5.18016799,  7.07549515],\n       [-1.74856866, -5.18022325,  7.07555337],\n       [-1.74862589, -5.18016727,  7.07561422]]), array([542.60968847, 542.60968847, 542.60968848, 542.60968848]))\n           fun: 542.6096884689198\n       message: 'Optimization terminated successfully.'\n          nfev: 338\n           nit: 185\n        status: 0\n       success: True\n             x: array([-1.74855767, -5.18018459,  7.07552039])\n\n\n\ny_full = np.ones(shape=(N*2,1))\nclass_1 = np.where(((U[:,0] > 0) & (U[:,0] > U[:,1])), 'class_1', 'class_0')\nclass_2 = np.where((U[:,1] > 0 & (U[:,1] > U[:,0])), 'class_2', 'class_0')\ny_full = np.append(class_1, class_2)\nW_full = sm.add_constant(W1.append(W2)).reset_index(drop=True)\nmn_logit = sm.MNLogit(y_full, W_full)\nmn_logit_res = mn_logit.fit()\nmn_logit_res.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.630391\n         Iterations 7\n\n\n\n\nMNLogit Regression Results\n\n  Dep. Variable:           y          No. Observations:       2000  \n\n\n  Model:                MNLogit       Df Residuals:           1994  \n\n\n  Method:                 MLE         Df Model:                  4  \n\n\n  Date:            Thu, 25 Feb 2021   Pseudo R-squ.:        0.2924  \n\n\n  Time:                21:23:52       Log-Likelihood:       -1260.8 \n\n\n  converged:             True         LL-Null:              -1781.9 \n\n\n  Covariance Type:     nonrobust      LLR p-value:        2.587e-224\n\n\n\n\n  y=class_1    coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  const        -2.6666     0.219   -12.201  0.000    -3.095    -2.238\n\n\n  x1           -4.7724     0.315   -15.143  0.000    -5.390    -4.155\n\n\n  x2            6.2800     0.360    17.461  0.000     5.575     6.985\n\n\n  y=class_2    coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  const        -2.7293     0.212   -12.865  0.000    -3.145    -2.313\n\n\n  x1           -4.4876     0.299   -15.004  0.000    -5.074    -3.901\n\n\n  x2            6.4438     0.348    18.497  0.000     5.761     7.127\n\n\n\n\n\nfrom scipy.special import softmax\n\ndef cdf(W, beta):\n    Wb = np.dot(W, beta)\n    eXB = np.exp(Wb)\n    eXB = eXB /eXB.sum(1)[:, None]\n    return eXB\n\ndef take_log(probs):\n    epsilon = 1e-20 \n    return np.log(probs)\n\ndef calc_ll(logged, d):\n    ll = d * logged\n    return ll\n\ndef ll_mn_logistic(params, *args):\n    y, W, n_params, n_classes = args[0], args[1], args[2], args[3]\n    beta = [params[i] for i in range(0, len(params))]\n    beta = np.array(beta).reshape(n_params, -1, order='F')\n    beta[:,0] = [0 for i in range(0, n_params)]\n    \n    ## onehot_encode\n    d = pd.get_dummies(y, prefix='Flag').to_numpy()\n    \n    probs = cdf(W, beta)\n    logged = take_log(probs)\n    ll = calc_ll(logged, d)\n    \n    return -np.sum(ll)\n\nn_params = 3 \nn_classes = 3\nz = np.random.rand(3,3).flatten()\n#probs = ll_mn_logistic(list(z), *[y_full, W_full, n_params, n_classes])\n\n\nres = minimize(ll_mn_logistic, x0 =z, args =(y_full, W_full, n_params, n_classes), \n             options={'disp': True, 'maxiter':1000})\nres\n\nWarning: Desired error not necessarily achieved due to precision loss.\n         Current function value: 1260.782799\n         Iterations: 21\n         Function evaluations: 470\n         Gradient evaluations: 47\n\n\n      fun: 1260.782798764732\n hess_inv: array([[ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.15840199,  0.05230628,\n        -0.24846581,  0.08569624, -0.08026449, -0.11217585],\n       [ 0.        ,  0.        ,  0.        ,  0.05230628,  0.05960721,\n        -0.11533024,  0.03950015, -0.02890953, -0.06618335],\n       [ 0.        ,  0.        ,  0.        , -0.24846581, -0.11533024,\n         0.42805566, -0.14239161,  0.11728476,  0.20865474],\n       [ 0.        ,  0.        ,  0.        ,  0.08569624,  0.03950015,\n        -0.14239161,  0.0776523 , -0.03556134, -0.10565188],\n       [ 0.        ,  0.        ,  0.        , -0.08026449, -0.02890953,\n         0.11728476, -0.03556134,  0.07906259,  0.01257425],\n       [ 0.        ,  0.        ,  0.        , -0.11217585, -0.06618335,\n         0.20865474, -0.10565188,  0.01257425,  0.18307424]])\n      jac: array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.05175781e-05,\n       1.52587891e-05, 0.00000000e+00, 3.05175781e-05, 3.05175781e-05,\n       3.05175781e-05])\n  message: 'Desired error not necessarily achieved due to precision loss.'\n     nfev: 470\n      nit: 21\n     njev: 47\n   status: 2\n  success: False\n        x: array([ 0.09640129,  0.00798646,  0.70648002, -2.66658562, -4.77244682,\n        6.27997417, -2.72927898, -4.48757328,  6.44379724])\n\n\n\nbart_data = pd.read_csv('../data_files/mcfaddan_bart.csv')\nbart_data.head()\n\n\n\n\n\n  \n    \n      \n      HOUSEID\n      TRAVDAY\n      SAMPSTRAT\n      HOMEOWN\n      HHSIZE\n      HHVEHCNT\n      HHFAMINC\n      PC\n      SPHONE\n      TAB\n      ...\n      SMPLSRCE\n      WTHHFIN\n      HBHUR\n      HTHTNRNT\n      HTPPOPDN\n      HTRESDN\n      HTEEMPDN\n      HBHTNRNT\n      HBPPOPDN\n      HBRESDN\n    \n  \n  \n    \n      0\n      30000007\n      2\n      3\n      1\n      3\n      5\n      7\n      2\n      1\n      2\n      ...\n      2\n      187.314320\n      T\n      50\n      1500\n      750\n      750\n      20\n      750\n      300\n    \n    \n      1\n      30000008\n      5\n      2\n      1\n      2\n      4\n      8\n      1\n      1\n      2\n      ...\n      2\n      69.513032\n      R\n      5\n      300\n      300\n      150\n      5\n      300\n      300\n    \n    \n      2\n      30000012\n      5\n      3\n      1\n      1\n      2\n      10\n      1\n      1\n      3\n      ...\n      2\n      79.419586\n      C\n      80\n      17000\n      17000\n      5000\n      60\n      17000\n      7000\n    \n    \n      3\n      30000019\n      5\n      3\n      1\n      2\n      2\n      3\n      1\n      5\n      5\n      ...\n      2\n      279.143588\n      S\n      40\n      300\n      300\n      150\n      50\n      750\n      300\n    \n    \n      4\n      30000029\n      3\n      3\n      1\n      2\n      2\n      5\n      2\n      5\n      1\n      ...\n      2\n      103.240304\n      S\n      40\n      1500\n      750\n      750\n      40\n      1500\n      750\n    \n  \n\n5 rows × 58 columns\n\n\n\n\nbart_data['CHOICE'] = np.nan\nbart_data['CHOICE'] = np.where(bart_data['CAR']==1, 'car', bart_data['CHOICE'])\nbart_data['CHOICE'] = np.where(bart_data['BUS']==1, 'bus', bart_data['CHOICE'])\nbart_data['CHOICE'] = np.where(bart_data['TRAIN']==1, 'rail', bart_data['CHOICE'])\nbart_data['car1'] = bart_data['CHOICE'] == 'car'\nbart_data['train1'] = bart_data['CHOICE'] == 'rail'\nbart_data['home'] = np.where(bart_data['HOMEOWN'] == 1, 1, np.nan)\nbart_data['home'] = np.where(bart_data['HOMEOWN'] > 1, 0, bart_data['home'])\nbart_data['income'] = np.where(bart_data['HHFAMINC'] > 0, bart_data['HHFAMINC'], np.nan)\nbart_data['density'] = np.where(bart_data['HTPPOPDN']==-9, np.nan, bart_data['HTPPOPDN']/1000)\nbart_data['urban1'] = bart_data['URBAN']==1\ny = bart_data[(bart_data['WRKCOUNT'] > 0) & ((bart_data['MSACAT'] == 1) | (bart_data['MSACAT'] == 2))]\ny['rail'] = y['RAIL'] == 1\ny['row_sum'] = y[['car1','train1','home','HHSIZE','income', 'urban1','density','MSACAT', 'rail']].sum(axis=1) == 0\ny\n\n/Users/nathanielforde/.local/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  del sys.path[0]\n/Users/nathanielforde/.local/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \n\n\n\n\n\n\n  \n    \n      \n      HOUSEID\n      TRAVDAY\n      SAMPSTRAT\n      HOMEOWN\n      HHSIZE\n      HHVEHCNT\n      HHFAMINC\n      PC\n      SPHONE\n      TAB\n      ...\n      HBRESDN\n      CHOICE\n      car1\n      train1\n      home\n      income\n      density\n      urban1\n      rail\n      row_sum\n    \n  \n  \n    \n      1\n      30000008\n      5\n      2\n      1\n      2\n      4\n      8\n      1\n      1\n      2\n      ...\n      300\n      car\n      True\n      False\n      1.0\n      8.0\n      0.3\n      False\n      False\n      False\n    \n    \n      9\n      30000085\n      1\n      2\n      1\n      1\n      2\n      9\n      1\n      1\n      4\n      ...\n      17000\n      nan\n      False\n      False\n      1.0\n      9.0\n      17.0\n      True\n      False\n      False\n    \n    \n      15\n      30000130\n      1\n      1\n      1\n      2\n      1\n      5\n      -9\n      1\n      -9\n      ...\n      17000\n      rail\n      False\n      True\n      1.0\n      5.0\n      30.0\n      True\n      True\n      False\n    \n    \n      17\n      30000144\n      3\n      2\n      2\n      3\n      0\n      5\n      5\n      1\n      1\n      ...\n      3000\n      bus\n      False\n      False\n      0.0\n      5.0\n      3.0\n      True\n      False\n      False\n    \n    \n      18\n      30000145\n      5\n      2\n      2\n      2\n      2\n      7\n      1\n      1\n      2\n      ...\n      3000\n      car\n      True\n      False\n      0.0\n      7.0\n      7.0\n      True\n      False\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      129674\n      40794087\n      2\n      1\n      2\n      1\n      1\n      5\n      2\n      -9\n      -9\n      ...\n      3000\n      car\n      True\n      False\n      0.0\n      5.0\n      7.0\n      True\n      True\n      False\n    \n    \n      129688\n      40794241\n      3\n      2\n      1\n      2\n      2\n      6\n      1\n      1\n      5\n      ...\n      3000\n      car\n      True\n      False\n      1.0\n      6.0\n      7.0\n      True\n      False\n      False\n    \n    \n      129690\n      40794260\n      5\n      2\n      1\n      4\n      1\n      11\n      1\n      1\n      2\n      ...\n      1500\n      car\n      True\n      False\n      1.0\n      11.0\n      3.0\n      True\n      False\n      False\n    \n    \n      129693\n      40794294\n      5\n      2\n      1\n      2\n      2\n      10\n      1\n      1\n      5\n      ...\n      7000\n      car\n      True\n      False\n      1.0\n      10.0\n      7.0\n      True\n      False\n      False\n    \n    \n      129695\n      50515573\n      3\n      1\n      1\n      1\n      0\n      10\n      1\n      1\n      5\n      ...\n      17000\n      nan\n      False\n      False\n      1.0\n      10.0\n      30.0\n      True\n      True\n      False\n    \n  \n\n39057 rows × 67 columns\n\n\n\n\nfeatures = [\"car1\",\"train1\",\"home\",\"HHSIZE\",\"income\",\"urban1\",\"density\", 'rail', 'CHOICE']\ny_focus = y[features]\ny_focus.dropna(inplace=True)\ny_focus = y_focus[y_focus['CHOICE'] != 'nan']\ny_focus[features].groupby('rail').mean().T\n\n/Users/nathanielforde/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  This is separate from the ipykernel package so we can avoid doing imports until\n\n\n\n\n\n\n  \n    \n      rail\n      False\n      True\n    \n  \n  \n    \n      car1\n      0.973861\n      0.877925\n    \n    \n      train1\n      0.010036\n      0.096610\n    \n    \n      home\n      0.749231\n      0.730127\n    \n    \n      HHSIZE\n      2.462420\n      2.487870\n    \n    \n      income\n      7.054552\n      7.518840\n    \n    \n      urban1\n      0.869753\n      0.910272\n    \n    \n      density\n      4.661936\n      7.559076\n    \n  \n\n\n\n\n\ny_focus[features + ['CHOICE']]\n\n\n\n\n\n  \n    \n      \n      home\n      HHSIZE\n      income\n      urban1\n      density\n      CHOICE\n    \n  \n  \n    \n      1\n      1.0\n      2\n      8.0\n      False\n      0.3\n      car\n    \n    \n      15\n      1.0\n      2\n      5.0\n      True\n      30.0\n      rail\n    \n    \n      17\n      0.0\n      3\n      5.0\n      True\n      3.0\n      bus\n    \n    \n      18\n      0.0\n      2\n      7.0\n      True\n      7.0\n      car\n    \n    \n      33\n      1.0\n      3\n      11.0\n      True\n      1.5\n      car\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      129667\n      1.0\n      2\n      8.0\n      False\n      0.3\n      car\n    \n    \n      129674\n      0.0\n      1\n      5.0\n      True\n      7.0\n      car\n    \n    \n      129688\n      1.0\n      2\n      6.0\n      True\n      7.0\n      car\n    \n    \n      129690\n      1.0\n      4\n      11.0\n      True\n      3.0\n      car\n    \n    \n      129693\n      1.0\n      2\n      10.0\n      True\n      7.0\n      car\n    \n  \n\n34043 rows × 6 columns\n\n\n\n\n# Without Rail\ny_focus_nr = y_focus[y_focus['rail'] == 0]\ny_focus_r = y_focus[y_focus['rail'] == 1]\n\n\nlogit_mod_nr = sm.Logit(np.array(y_focus_nr['car1']), \n                       sm.add_constant(y_focus_nr[[\"home\",\"HHSIZE\",\"income\",\"urban1\",\"density\"]]).astype(float))\nlogit_res_nr = logit_mod_nr.fit()\nlogit_margeff_nr = logit_res_nr.get_margeff()\n\n\nlogit_mod_r = sm.Logit(np.array(y_focus_r['car1']), \n                       sm.add_constant(y_focus_r[[\"home\",\"HHSIZE\",\"income\",\"urban1\",\"density\"]]).astype(float))\nlogit_res_r = logit_mod_r.fit()\nlogit_margeff_r = logit_res.get_margeff()\n\nOptimization terminated successfully.\n         Current function value: 0.114130\n         Iterations 9\nOptimization terminated successfully.\n         Current function value: 0.300803\n         Iterations 7\n\n\n\nstargazer = Stargazer([logit_res_nr, logit_res_r])\nstargazer.custom_columns(['Probability of Car - No Rail', 'Probability of Car -Rail'], [1, 1])\n#stargazer.add_custom_notes([str(probit_margeff.summary()), str(logit_margeff.summary())])\nHTML(stargazer.render_html())\n\n\nDependent variable:yProbability of Car - No RailProbability of Car -Rail(1)(2)HHSIZE-0.0180.050*(0.033)(0.026)const3.775***3.506***(0.270)(0.211)density-0.055***-0.109***(0.008)(0.003)home0.633***0.498***(0.095)(0.071)income0.133***-0.068***(0.019)(0.013)urban1-1.160***-0.312*(0.246)(0.187)Observations22,41911,624R2Adjusted R2Residual Std. Error1.000 (df=22413)1.000 (df=11618)F Statistic (df=5; 22413) (df=5; 11618)Note:\n \n  *p<0.1;\n  **p<0.05;\n  ***p<0.01\n \n\n\n\nMN_logit_mod_r = sm.MNLogit(np.array(y_focus_r['CHOICE']), \n                       y_focus_r[[\"home\",\"HHSIZE\",\"income\",\"urban1\",\"density\"]].astype(float))\nMN_logit_res_r = MN_logit_mod_r.fit()\nMN_logit_res_r.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.387108\n         Iterations 8\n\n\n\n\nMNLogit Regression Results\n\n  Dep. Variable:           y          No. Observations:      11624  \n\n\n  Model:                MNLogit       Df Residuals:          11614  \n\n\n  Method:                 MLE         Df Model:                  8  \n\n\n  Date:            Mon, 01 Mar 2021   Pseudo R-squ.:        0.1071  \n\n\n  Time:                23:06:01       Log-Likelihood:       -4499.7 \n\n\n  converged:             True         LL-Null:              -5039.6 \n\n\n  Covariance Type:     nonrobust      LLR p-value:        9.100e-228\n\n\n\n\n   y=car     coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  home        1.0460     0.128     8.159  0.000     0.795     1.297\n\n\n  HHSIZE      0.2411     0.046     5.234  0.000     0.151     0.331\n\n\n  income      0.2235     0.021    10.829  0.000     0.183     0.264\n\n\n  urban1      1.7447     0.150    11.619  0.000     1.450     2.039\n\n\n  density    -0.0847     0.006   -13.677  0.000    -0.097    -0.073\n\n\n  y=rail     coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  home        0.3757     0.140     2.689  0.007     0.102     0.650\n\n\n  HHSIZE     -0.0495     0.051    -0.973  0.331    -0.149     0.050\n\n\n  income      0.1658     0.022     7.466  0.000     0.122     0.209\n\n\n  urban1     -0.0949     0.167    -0.569  0.569    -0.422     0.232\n\n\n  density     0.0202     0.007     3.107  0.002     0.007     0.033\n\n\n\n\n\nn_params = 5 \nn_classes = 3\nz = np.random.rand(n_params,n_classes).flatten()\n\nres = minimize(ll_mn_logistic, x0 =z, args =(y_focus_r['CHOICE'], \n                                             y_focus_r[[\"home\",\"HHSIZE\",\"income\",\"urban1\",\"density\"]].astype(float), n_params, n_classes), \n             options={'disp': True, 'maxiter':1000})\nres\n\nWarning: Desired error not necessarily achieved due to precision loss.\n         Current function value: 4499.748263\n         Iterations: 33\n         Function evaluations: 896\n         Gradient evaluations: 56\n\n\n      fun: 4499.748263266686\n hess_inv: array([[ 1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  2.49790461e-02,\n         1.22239651e-02, -1.08077892e-03, -4.52948507e-02,\n         9.24499328e-04,  2.18160612e-02,  1.53707043e-02,\n        -2.13296546e-03, -3.42635201e-02,  5.36330909e-04],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  1.22239651e-02,\n         8.71917753e-03, -1.06847160e-03, -2.23203076e-02,\n         3.46191120e-04,  1.19746027e-02,  1.05025079e-02,\n        -1.74296384e-03, -1.65180529e-02,  1.47031357e-04],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00, -1.08077892e-03,\n        -1.06847160e-03,  4.43252403e-04,  1.14187551e-03,\n        -5.55280050e-05, -8.98456837e-04, -1.17483721e-03,\n         4.60854131e-04,  6.94275713e-04, -3.43435221e-05],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00, -4.52948507e-02,\n        -2.23203076e-02,  1.14187551e-03,  8.77146917e-02,\n        -1.70194689e-03, -4.37327155e-02, -2.84783896e-02,\n         3.45189389e-03,  6.85283602e-02, -1.05179786e-03],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  9.24499328e-04,\n         3.46191120e-04, -5.55280050e-05, -1.70194689e-03,\n         6.40830138e-05,  8.65122620e-04,  4.47609224e-04,\n        -8.69531233e-05, -1.35865208e-03,  4.78235451e-05],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  2.18160612e-02,\n         1.19746027e-02, -8.98456837e-04, -4.37327155e-02,\n         8.65122620e-04,  2.42731374e-02,  1.45500604e-02,\n        -2.32279638e-03, -3.32914355e-02,  5.72986223e-04],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  1.53707043e-02,\n         1.05025079e-02, -1.17483721e-03, -2.84783896e-02,\n         4.47609224e-04,  1.45500604e-02,  1.36673618e-02,\n        -2.18820823e-03, -2.15013079e-02,  1.87660290e-04],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00, -2.13296546e-03,\n        -1.74296384e-03,  4.60854131e-04,  3.45189389e-03,\n        -8.69531233e-05, -2.32279638e-03, -2.18820823e-03,\n         7.07813187e-04,  1.90500574e-03, -5.20898364e-05],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00, -3.42635201e-02,\n        -1.65180529e-02,  6.94275713e-04,  6.85283602e-02,\n        -1.35865208e-03, -3.32914355e-02, -2.15013079e-02,\n         1.90500574e-03,  6.33830924e-02, -1.09402475e-03],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  5.36330909e-04,\n         1.47031357e-04, -3.43435221e-05, -1.05179786e-03,\n         4.78235451e-05,  5.72986223e-04,  1.87660290e-04,\n        -5.20898364e-05, -1.09402475e-03,  5.33476436e-05]])\n      jac: array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  6.10351562e-05, -6.10351562e-05, -6.10351562e-05,\n        0.00000000e+00, -1.83105469e-04,  6.10351562e-05,  2.44140625e-04,\n        3.66210938e-04,  6.10351562e-05,  4.88281250e-04])\n  message: 'Desired error not necessarily achieved due to precision loss.'\n     nfev: 896\n      nit: 33\n     njev: 56\n   status: 2\n  success: False\n        x: array([ 0.98480009,  0.00651008,  0.0863602 ,  0.13983792,  0.32455478,\n        1.04604352,  0.24109143,  0.22348356,  1.74469334, -0.08468886,\n        0.3757215 , -0.04946345,  0.16579691, -0.09489807,  0.02024353])\n\n\n\nMN_logit_mod_r = sm.MNLogit(np.array(y_focus_r['CHOICE']), \n                       y_focus_r[[\"home\",\"HHSIZE\",\"income\",\"urban1\",\"density\"]].astype(float))\nMN_logit_res_r = MN_logit_mod_r.fit()\nMN_logit_res_r.summary()\nnr = y_focus_nr[['home', 'HHSIZE', 'income', 'urban1', 'density', 'rail']].astype(float)\nr = y_focus_r[['home', 'HHSIZE', 'income', 'urban1', 'density', 'rail']].astype(float)\nfull = y_focus[['home', 'HHSIZE', 'income', 'urban1', 'density', 'rail']].astype(float)\nnr_nd = nr\nnr_nd['density'] = 0\nnr_d = nr\nnr_d['density'] = r['density'].mean()\nfull_d = full\nfull_d[full_d['rail'] == 0]['density'] = r['density'].mean()\n\n\nres = MN_logit_res_r.predict(full.drop('rail', axis=1)).mean()\n\nres = pd.DataFrame(res).T\nres.columns = ['Bus', 'Car', 'Train']\nres.round(3) * 100\n\n/Users/nathanielforde/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  if __name__ == '__main__':\n\n\n\n\n\n\n  \n    \n      \n      Bus\n      Car\n      Train\n    \n  \n  \n    \n      0\n      2.7\n      88.5\n      8.8"
  },
  {
    "objectID": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html",
    "href": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "import random\nimport statsmodels.api as sm\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html#simultaneity-bias-in-an-ols-regression",
    "href": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html#simultaneity-bias-in-an-ols-regression",
    "title": "Examined Algorithms",
    "section": "Simultaneity Bias in an OLS regression",
    "text": "Simultaneity Bias in an OLS regression\nThe coefficients in an ordinary least squares (OLS) regression are imprecisely estimated when there is a relationship between the feature variables and the error terms. This is true even when the functional form is properly specified! In our case we can see that the values for the price feature are too high due to the simultaneity bias.\n\nA Supply and Demand Example\n\nN = 1000\nnp.random.seed(0)\nmu, sigma = 0, 7\ne_1 = np.random.normal(mu, sigma, N)\nW = np.random.normal(100, 20, N)\nH = np.random.uniform(0, 10, N)\n\n# True Values\na0, a1, a2, a3 = 2, 3, 5, 10\n\nmu, sigma = 0, 6\ne_2 = np.random.normal(mu, sigma, N)\nF = np.random.uniform(6, 30, N)\nO = np.random.normal(7, 4, N)\n\n#True Values\nb0, b1, b2, b3 = 4, 6, 2, 7\n\nP = np.random.normal(5, 2, N) + .2*(e_1+e_2)\n\ns = a0 + a1*P + a2*W + a3*H + e_1\nd = b0 + b1*P +b2*F + b3*O + e_2\n\n# True coeffs             3       5       10\nX_supply = pd.DataFrame({'P': P , 'W':W, 'H':H})\nX_supply = sm.add_constant(X_supply)\nmodel = sm.OLS(s, X_supply)\nresults = model.fit()\nresults.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:            y          R-squared:             0.997 \n\n\n  Model:                   OLS         Adj. R-squared:        0.997 \n\n\n  Method:             Least Squares    F-statistic:        9.625e+04\n\n\n  Date:             Thu, 11 Feb 2021   Prob (F-statistic):    0.00  \n\n\n  Time:                 22:42:02       Log-Likelihood:      -3203.0 \n\n\n  No. Observations:        1000        AIC:                   6414. \n\n\n  Df Residuals:             996        BIC:                   6434. \n\n\n  Df Model:                   3                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n           coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  const    -3.3455     1.102    -3.037  0.002    -5.508    -1.184\n\n\n  P         4.2631     0.068    62.574  0.000     4.129     4.397\n\n\n  W         4.9892     0.010   512.078  0.000     4.970     5.008\n\n\n  H         9.9924     0.066   152.473  0.000     9.864    10.121\n\n\n\n\n  Omnibus:        0.846   Durbin-Watson:         2.049\n\n\n  Prob(Omnibus):  0.655   Jarque-Bera (JB):      0.738\n\n\n  Skew:          -0.058   Prob(JB):              0.691\n\n\n  Kurtosis:       3.066   Cond. No.               598.\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe price coefficient estimates are statistically significant and the model has a high R squared figure but fundamentally incorrect due to the simultaneity bias."
  },
  {
    "objectID": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html#detour-a-simpler-example",
    "href": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html#detour-a-simpler-example",
    "title": "Examined Algorithms",
    "section": "Detour: A simpler Example",
    "text": "Detour: A simpler Example\nWe’ll explain some the details with a simpler example first, and then proceed to show how to estimate the supply and demand system using instrumental variable regression.\n\nnp.random.seed(1235)\nz = np.random.uniform(0, 1, 1000)\ne_3 = np.random.normal(0, 3, 1000)\ne_1 = np.random.normal(0, 1, 1000)\nx = -1 + 5*z + 2*(e_1) + e_3\ny = 2 + 3*x + e_3\n\nX = pd.DataFrame({'X': x})\nX = sm.add_constant(X)\n\nZ = pd.DataFrame({'Z': z})\nZ = sm.add_constant(Z)\n\nOn the OLS linear model \\[ Y = \\beta X + \\epsilon \\] the beta coefficients can be estimated as follows: \\[ \\hat{\\beta} = (X^{'}X)^{-1}X^{'}y \\] under a number of conditions, but crucially we require that: \\[ E(X'\\epsilon) = 0 \\]\n\nbeta_hat_OLS = linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\nbeta_hat_OLS\n#True coefficient values 2,  3\n\narray([0.99580253, 3.60164346])\n\n\nThe failure of this assumption leads to skewed coefficient estimates. On the IV variable regression model we allow that the last assumption fails, so that: \\[ Y = \\beta X + \\psi \\] and we allow that \\[ E(X'\\psi) \\neq 0 \\] and choose an instrument Z as a proxy for X so specifically that \\[ E(Z'\\psi) = 0\\] then then estimator can be stated: \\[ Z'Y = Z'X\\beta + Z'\\psi \\Rightarrow \\hat\\beta =  [X'Z(Z'Z)^{-1}Z'X]^{-1}X'Z(Z'Z)^{-1}Z'y \\]\n\ndef iv_estimate(Z, X, y):\n    return linalg.inv(X.T.dot(Z).dot(linalg.inv(Z.T.dot(Z)).dot(Z.T.dot(X)))).dot(X.T.dot(Z).dot(linalg.inv(\n            Z.T.dot(Z)).dot(Z.T.dot(y))))\n\n\niv_estimate(Z, X, y)\n#True coefficient values 2,  3\n\narray([2.22432093, 2.84420838])\n\n\nwhich (a) gives better estimates of the coefficients and (b) can, thankfully, be simplified when we have as many instruments as variables that need to be “instrumented” to: \\[ (Z'X)^{-1}Z'y\\]. A good instrument needs to be carefully chosen so as to correlate with \\(X\\) with but not be influenced by the error terms in \\(y\\).\n\nlinalg.inv(Z.T.dot(X)).dot(Z.T).dot(y)\n\narray([2.22432093, 2.84420838])\n\n\n\ndef bootstrap_iv_estimator(reps, y, X_in, Z_in=None):\n    np.random.seed(100)\n    bs_mat = np.zeros(shape=(reps, X_in.shape[1]))\n    N = len(y)\n    if Z_in is None:\n        Z_in = X_in\n    for i in range(0, reps):\n        index_bs = np.random.randint(N, size=N)\n        y_bs = y[index_bs]\n        X_bs = X_in.iloc[index_bs,]\n        Z_bs = Z_in.iloc[index_bs,]\n        bs_mat[i,] = linalg.inv(Z_bs.T.dot(X_bs)).dot(Z_bs.T).dot(y_bs)\n    bs_mat = pd.DataFrame(bs_mat, columns = ['const', 'X'])\n    summary = pd.concat([bs_mat.mean(), bs_mat.std(), bs_mat.quantile(0.05), bs_mat.quantile(0.95)], axis=1)\n    summary.columns = ['coefs', 'std', '0.05', '0.95']\n    return summary\n\n\nestimates = bootstrap_iv_estimator(100, y, X) #OLS\nestimates\n\n\n\n\n\n  \n    \n      \n      coefs\n      std\n      0.05\n      0.95\n    \n  \n  \n    \n      const\n      0.996938\n      0.068499\n      0.892036\n      1.113305\n    \n    \n      X\n      3.599339\n      0.014494\n      3.576504\n      3.622485\n    \n  \n\n\n\n\n\nestimates = bootstrap_iv_estimator(100, y, X, Z) #IV\nestimates\n\n\n\n\n\n  \n    \n      \n      coefs\n      std\n      0.05\n      0.95\n    \n  \n  \n    \n      const\n      2.220228\n      0.169027\n      2.001413\n      2.465313\n    \n    \n      X\n      2.852359\n      0.080436\n      2.732629\n      2.967042"
  },
  {
    "objectID": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html#returning-to-our-model-of-supply-and-demand",
    "href": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html#returning-to-our-model-of-supply-and-demand",
    "title": "Examined Algorithms",
    "section": "Returning to our model of Supply and Demand",
    "text": "Returning to our model of Supply and Demand\n\nN = 1000\nnp.random.seed(0)\nmu, sigma = 0, 7\ne_1 = np.random.normal(mu, sigma, N)\nW = np.random.normal(100, 20, N)\nH = np.random.uniform(0, 10, N)\n\n# True Values\na0, a1, a2, a3 = 2, 3, 5, 10\n\nmu, sigma = 0, 6\ne_2 = np.random.normal(mu, sigma, N)\nF = np.random.uniform(6, 30, N)\nO = np.random.normal(7, 4, N)\n\n#True Values\nb0, b1, b2, b3 = 4, 6, 2, 7\n\nP = np.random.normal(5, 2, N) + .2*(e_1+e_2)\n\ns = a0 + a1*P + a2*W + a3*H + e_1\nd = b0 + b1*P +b2*F + b3*O + e_2\n\nX_supply = pd.DataFrame({'P': P, 'W':W, 'H': H})\nX_supply = sm.add_constant(X_supply)\n\nZ_supply = pd.DataFrame({'F': F, 'O': O, 'W':W, 'H': H})\nZ_supply = sm.add_constant(Z_supply)\n\niv_estimate(Z_supply, X_supply, s)\n#True coefficient values 2, 3, 5, 10\n\narray([1.33465635, 3.32553682, 4.98887685, 9.97598787])"
  },
  {
    "objectID": "posts/post-with-code/bayesian_model_comparison/Hierarchical_Claims.html",
    "href": "posts/post-with-code/bayesian_model_comparison/Hierarchical_Claims.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "import numpy as np \nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport pymc3 as pm\nimport arviz as az\nimport theano.tensor as tt\nRANDOM_SEED = 13\n\n\nusetable = pd.read_csv('../data_files/insurace_res_usedata_tbl.csv')\nprint(usetable.columns)\nn_data = len(usetable)\nn_time = len(usetable['dev_year'].unique())\nn_cohort = len(usetable['acc_year'].unique())\ncohort_id, cohort = pd.factorize(usetable['acc_year'], sort=True)\ncohort_maxtime = usetable.groupby('acc_year')['dev_lag'].max().to_list()\nt_values = list(np.sort(usetable['dev_lag'].unique()).astype(int))\nt_idx, _  = pd.factorize(usetable['dev_lag'], sort=True)\npremium = usetable.groupby('acc_year')['premium'].mean().to_list()\nloss_real = usetable['cum_loss']\ncoords = {\"t_idx\": t_idx, \"cohort\": cohort, \"t_values\": t_values, 'obs': range(n_data),'cohort_id':cohort_id}\n\nIndex(['Unnamed: 0', 'grcode', 'grname', 'acc_year', 'dev_year', 'dev_lag',\n       'premium', 'cum_loss', 'loss_ratio'],\n      dtype='object')\n\n\n\nparams = {'mu_LR': [0, 0.5], 'sd_LR': [0, 0.5], 'loss_sd': [0, .7], 'omega': [0, .5], 'theta': [0, .5], \n         'tune': 2000, 'target_accept':.9}\n\ndef make_model(model_data, params,  growth_function ='logistic'):   \n    with pm.Model(coords=coords) as basic_model:\n\n        # Priors for unknown model parameters\n        mu_LR = pm.Normal('mu_LR', params['mu_LR'][0],  params['mu_LR'][1]);\n        sd_LR = pm.Lognormal('sd_LR', params['sd_LR'][0], params['sd_LR'][1]);\n\n        LR = pm.Lognormal('LR', mu_LR, sd_LR, dims='cohort')\n\n        loss_sd = pm.Lognormal('loss_sd', params['loss_sd'][0], params['loss_sd'][1]);\n\n        ## Parameters for the growth factor\n        omega = pm.Lognormal('omega', params['omega'][0], params['omega'][1]);\n        theta = pm.Lognormal('theta', params['theta'][0], params['theta'][1]);\n        \n        t = pm.Data(\"t\", t_values, dims='t_values')\n        if growth_function == 'logistic':\n            gf = pm.Deterministic('gf', (t**omega /  (t**omega + theta**omega)), dims='t_values')\n        else:\n            gf = pm.Deterministic('gf', 1-(pm.math.exp(-(t/theta)**omega)), dims='t_values')\n        ## Premium\n        prem = pm.Data(\"premium\", premium, dims='cohort')\n\n        t_indx = pm.Data(\"t_idx\", t_idx, dims='obs')\n        cohort_idx = pm.Data('c_idx', cohort_id, dims='obs')\n        obs = pm.Data('obs_idx', range(n_data), dims='obs')\n        lm = pm.Deterministic('lm', LR[cohort_idx] * prem[cohort_idx] *gf[t_indx], dims=('obs'))\n        #max_loss = pm.Deterministic('max_loss', tt.max(lm[t_idx]))\n\n        # Likelihood (sampling distribution) of observations\n        loss = pm.Normal('loss', lm, loss_sd * prem[cohort_idx], observed=loss_real, dims='obs')\n\n        prior_checks = pm.sample_prior_predictive(samples=100, random_seed=100)\n\n        trace = pm.sample(tune=params['tune'], init=\"adapt_diag\", \n                          target_accept=params['target_accept'])\n        ppc = pm.sample_posterior_predictive(trace, var_names=[\"loss\", \"LR\", \"lm\"], random_seed=100)\n\n        idata = az.from_pymc3(prior=prior_checks, posterior_predictive=ppc, trace=trace)\n        \n    return basic_model, idata, trace\n\n\nlogistic_model, logistic_idata, logistic_trace = make_model(usetable, params)\n#weibull_model, weibull_idata, weibull_trace = make_model(usetable, params, 'weibull')\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [theta, omega, loss_sd, LR, sd_LR, mu_LR]\n\n\n\n    \n        \n      \n      100.00% [6000/6000 00:11<00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 2_000 tune and 1_000 draw iterations (4_000 + 2_000 draws total) took 11 seconds.\nThe number of effective samples is smaller than 25% for some parameters.\n\n\n\n    \n        \n      \n      100.00% [2000/2000 00:03<00:00]\n    \n    \n\n\n\naz.hdi(logistic_idata, var_names='lm').to_dataframe()\n\n\n\n\n\n  \n    \n      \n      \n      lm\n    \n    \n      hdi\n      obs\n      \n    \n  \n  \n    \n      lower\n      0\n      144.536642\n    \n    \n      1\n      345.880432\n    \n    \n      2\n      460.309382\n    \n    \n      3\n      522.841780\n    \n    \n      4\n      558.101366\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      higher\n      50\n      26942.142200\n    \n    \n      51\n      35365.145610\n    \n    \n      52\n      11878.581162\n    \n    \n      53\n      27013.067386\n    \n    \n      54\n      15285.784189\n    \n  \n\n110 rows × 1 columns\n\n\n\n\naxes = az.plot_forest(logistic_idata,\n                           kind='ridgeplot',\n                           var_names=['gf'],\n                           combined=True,\n                           ridgeplot_overlap=3,\n                           colors='white',\n                           figsize=(9, 7))\naxes[0].set_title('Loss Ratio Plots')\n\nText(0.5, 1.0, 'Loss Ratio Plots')\n\n\n\n\n\n\ng = pm.model_to_graphviz(logistic_model)\ng\n\n\n\n\n\nlogistic_idata\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:   (chain: 2, cohort: 10, draw: 1000, obs: 55, t_values: 10)\nCoordinates:\n  * chain     (chain) int64 0 1\n  * draw      (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * cohort    (cohort) int64 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\n  * t_values  (t_values) int64 1 2 3 4 5 6 7 8 9 10\n  * obs       (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    mu_LR     (chain, draw) float64 -0.187 -0.09882 ... -0.03967 0.007514\n    sd_LR     (chain, draw) float64 0.6209 0.3373 0.2758 ... 0.2711 0.2706\n    LR        (chain, draw, cohort) float64 0.6702 0.8334 ... 0.8889 0.7807\n    loss_sd   (chain, draw) float64 0.03186 0.02677 0.03494 ... 0.0332 0.02944\n    omega     (chain, draw) float64 2.047 2.063 2.117 ... 1.928 1.942 1.925\n    theta     (chain, draw) float64 1.73 1.75 1.699 1.712 ... 1.81 1.803 1.801\n    gf        (chain, draw, t_values) float64 0.2457 0.5738 ... 0.9568 0.9644\n    lm        (chain, draw, obs) float64 157.6 368.0 ... 2.567e+04 1.084e+04\nAttributes:\n    created_at:                 2021-09-15T19:52:34.368634\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2\n    sampling_time:              11.485157012939453\n    tuning_steps:               2000xarray.DatasetDimensions:chain: 2cohort: 10draw: 1000obs: 55t_values: 10Coordinates: (5)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])t_values(t_values)int641 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (8)mu_LR(chain, draw)float64-0.187 -0.09882 ... 0.007514array([[-0.18696659, -0.0988205 , -0.00810847, ...,  0.05311949,\n         0.20880435, -0.01687328],\n       [-0.11414088, -0.15425932, -0.22073582, ...,  0.0246603 ,\n        -0.03966546,  0.00751418]])sd_LR(chain, draw)float640.6209 0.3373 ... 0.2711 0.2706array([[0.62089146, 0.33727945, 0.27581849, ..., 0.42592517, 0.45086283,\n        0.48777715],\n       [0.32289577, 0.44645476, 0.49395078, ..., 0.32981233, 0.27107624,\n        0.27061255]])LR(chain, draw, cohort)float640.6702 0.8334 ... 0.8889 0.7807array([[[0.67022333, 0.83343362, 1.50476005, ..., 0.84746063,\n         0.75324862, 0.97680847],\n        [0.67852191, 0.85033978, 1.5128585 , ..., 0.8348323 ,\n         0.82923175, 0.78441512],\n        [0.65084057, 0.81301633, 1.50223526, ..., 0.86576549,\n         0.81385395, 1.1107245 ],\n        ...,\n        [0.69134537, 0.87360691, 1.53672948, ..., 0.88517673,\n         0.80569189, 0.80970752],\n        [0.67295471, 0.8505383 , 1.55445959, ..., 0.9003714 ,\n         0.81393564, 0.9427412 ],\n        [0.68806353, 0.85636222, 1.52987837, ..., 0.84520793,\n         0.77860489, 0.83172496]],\n\n       [[0.68118037, 0.84469149, 1.52453007, ..., 0.89498748,\n         0.80324074, 1.07317632],\n        [0.66690657, 0.85809314, 1.53552822, ..., 0.83513642,\n         0.8337994 , 0.70777644],\n        [0.69604056, 0.83549505, 1.52540707, ..., 0.85311843,\n         0.83007865, 0.58221531],\n        ...,\n        [0.66498216, 0.85115548, 1.55624145, ..., 0.84190874,\n         0.84255311, 0.92859606],\n        [0.67333019, 0.82537763, 1.546271  , ..., 0.88271328,\n         0.97106623, 0.93680019],\n        [0.66692557, 0.85914168, 1.55441355, ..., 0.88582965,\n         0.88886908, 0.78072028]]])loss_sd(chain, draw)float640.03186 0.02677 ... 0.0332 0.02944array([[0.03186241, 0.02677491, 0.03494346, ..., 0.02950565, 0.03100382,\n        0.0271407 ],\n       [0.02958521, 0.03073347, 0.03223621, ..., 0.03685306, 0.03319626,\n        0.02944148]])omega(chain, draw)float642.047 2.063 2.117 ... 1.942 1.925array([[2.04684299, 2.06348333, 2.11704502, ..., 2.02350399, 1.90782721,\n        2.02067407],\n       [2.10779663, 1.94099992, 2.01335649, ..., 1.92760786, 1.94155975,\n        1.92504071]])theta(chain, draw)float641.73 1.75 1.699 ... 1.803 1.801array([[1.72969432, 1.75027019, 1.6993544 , ..., 1.82016814, 1.74772066,\n        1.78329374],\n       [1.76581379, 1.75222828, 1.76108597, ..., 1.80953151, 1.80345161,\n        1.80096094]])gf(chain, draw, t_values)float640.2457 0.5738 ... 0.9568 0.9644array([[[0.24572263, 0.57375952, 0.75530888, ..., 0.95830295,\n         0.96694015, 0.97318136],\n        [0.23956308, 0.56837426, 0.75248162, ..., 0.9583462 ,\n         0.96703728, 0.97330626],\n        [0.24553678, 0.58537157, 0.76910278, ..., 0.9637262 ,\n         0.9715035 , 0.9770701 ],\n        ...,\n        [0.22935886, 0.54751893, 0.73323759, ..., 0.95238508,\n         0.96209965, 0.96915217],\n        [0.2563239 , 0.56395803, 0.73707071, ..., 0.94794775,\n         0.95798322, 0.96536932],\n        [0.23705689, 0.55767741, 0.74097726, ..., 0.95404246,\n         0.96342013, 0.97022592]],\n\n       [[0.23173901, 0.56524966, 0.75345663, ..., 0.96024765,\n         0.96871358, 0.9747871 ],\n        [0.25186615, 0.56382847, 0.73956758, ..., 0.95014575,\n         0.95992601, 0.96709377],\n        [0.24242684, 0.56368527, 0.74506788, ..., 0.95466309,\n         0.96388892, 0.97058815],\n        ...,\n        [0.24173286, 0.54807935, 0.72601378, ..., 0.94609631,\n         0.95656922, 0.96426648],\n        [0.24141408, 0.55004295, 0.72870832, ..., 0.94747034,\n         0.95775477, 0.96529882],\n        [0.24369253, 0.55027846, 0.72757083, ..., 0.94636716,\n         0.95677735, 0.96442982]]])lm(chain, draw, obs)float64157.6 368.0 ... 2.567e+04 1.084e+04array([[[  157.60741315,   368.01149881,   484.45793209, ...,\n          9713.72058482, 22681.4259549 , 13676.08451242],\n        [  155.55919791,   369.07124816,   488.62052768, ...,\n         10425.52435542, 24735.02909624, 10707.12910397],\n        [  152.93367252,   364.60127339,   479.03907065, ...,\n         10487.33492388, 25002.31377805, 15539.25209443],\n        ...,\n        [  151.74783701,   362.24811303,   485.12283192, ...,\n          9698.09893366, 23151.02546387, 10581.58893866],\n        [  165.07711826,   363.19893028,   474.68655257, ...,\n         10949.17187914, 24090.11955022, 13768.56860626],\n        [  156.09646279,   367.21763655,   487.91633048, ...,\n          9686.61005323, 22787.79407511, 11234.13198617]],\n\n       [[  151.0682531 ,   368.4803803 ,   491.1705532 , ...,\n          9768.92945857, 23828.02983572, 14170.24714991],\n        [  160.74842881,   359.85201422,   472.01390953, ...,\n         11021.31677114, 24672.35959206, 10157.1789823 ],\n        [  161.48314057,   375.47685914,   496.29778013, ...,\n         10560.92705732, 24556.02304463,  8042.1379405 ],\n        ...,\n        [  153.83587622,   348.79108035,   462.02640537, ...,\n         10688.95096964, 24234.98892586, 12789.97614652],\n        [  155.56168039,   354.43502325,   469.5628732 , ...,\n         12303.07167812, 28031.57876309, 12885.95974292],\n        [  155.53621549,   351.21400257,   464.37046077, ...,\n         11367.94919334, 25669.79609618, 10840.38943469]]])Attributes: (6)created_at :2021-09-15T19:52:34.368634arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2sampling_time :11.485157012939453tuning_steps :2000\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 2, cohort: 10, draw: 1000, obs: 55)\nCoordinates:\n  * chain    (chain) int64 0 1\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\n  * cohort   (cohort) int64 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\nData variables:\n    loss     (chain, draw, obs) float64 104.3 378.5 ... 2.644e+04 9.016e+03\n    LR       (chain, draw, cohort) float64 0.6702 0.8334 1.505 ... 0.8889 0.7807\n    lm       (chain, draw, obs) float64 157.6 368.0 ... 2.567e+04 1.084e+04\nAttributes:\n    created_at:                 2021-09-15T19:52:34.700411\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 2cohort: 10draw: 1000obs: 55Coordinates: (4)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])Data variables: (3)loss(chain, draw, obs)float64104.3 378.5 ... 2.644e+04 9.016e+03array([[[  104.25299878,   378.46062043,   519.61667247, ...,\n          8141.80268618, 21296.98149285, 13873.7213711 ],\n        [  168.57110391,   346.97788603,   520.63643116, ...,\n         10456.73879506, 25180.98640336,  9559.26030343],\n        [  109.58118079,   367.78282075,   464.86966193, ...,\n         11868.35118362, 22050.81611823, 19408.3032024 ],\n        ...,\n        [  106.27795502,   318.52680823,   510.43588738, ...,\n         11021.18578426, 22826.81417456, 12527.08543234],\n        [  171.64762377,   365.27633379,   457.72868241, ...,\n         12032.70208003, 22850.73831518, 12817.78476422],\n        [  138.78409136,   360.61093994,   480.48285626, ...,\n         10137.62921603, 20979.07259156, 13465.6251659 ]],\n\n       [[   97.16103159,   355.99598682,   512.81621348, ...,\n         11204.55137772, 23671.12813507, 12157.79410608],\n        [  131.26783254,   355.3390368 ,   460.1770658 , ...,\n         10682.5367999 , 23307.75834914,  8833.09949403],\n        [  194.26536002,   369.60172602,   492.15711635, ...,\n         10816.10853934, 23849.85945329,  5220.66454384],\n        ...,\n        [  103.44871872,   337.44440949,   461.41297666, ...,\n         10020.64412924, 24559.0585406 , 15059.02844165],\n        [  154.30464618,   327.38538816,   474.78895535, ...,\n         15633.51420392, 24639.37731399, 14192.66494558],\n        [  132.40295851,   395.31181651,   455.54853151, ...,\n         10602.40295703, 26440.182822  ,  9015.82177028]]])LR(chain, draw, cohort)float640.6702 0.8334 ... 0.8889 0.7807array([[[0.67022333, 0.83343362, 1.50476005, ..., 0.84746063,\n         0.75324862, 0.97680847],\n        [0.67852191, 0.85033978, 1.5128585 , ..., 0.8348323 ,\n         0.82923175, 0.78441512],\n        [0.65084057, 0.81301633, 1.50223526, ..., 0.86576549,\n         0.81385395, 1.1107245 ],\n        ...,\n        [0.69134537, 0.87360691, 1.53672948, ..., 0.88517673,\n         0.80569189, 0.80970752],\n        [0.67295471, 0.8505383 , 1.55445959, ..., 0.9003714 ,\n         0.81393564, 0.9427412 ],\n        [0.68806353, 0.85636222, 1.52987837, ..., 0.84520793,\n         0.77860489, 0.83172496]],\n\n       [[0.68118037, 0.84469149, 1.52453007, ..., 0.89498748,\n         0.80324074, 1.07317632],\n        [0.66690657, 0.85809314, 1.53552822, ..., 0.83513642,\n         0.8337994 , 0.70777644],\n        [0.69604056, 0.83549505, 1.52540707, ..., 0.85311843,\n         0.83007865, 0.58221531],\n        ...,\n        [0.66498216, 0.85115548, 1.55624145, ..., 0.84190874,\n         0.84255311, 0.92859606],\n        [0.67333019, 0.82537763, 1.546271  , ..., 0.88271328,\n         0.97106623, 0.93680019],\n        [0.66692557, 0.85914168, 1.55441355, ..., 0.88582965,\n         0.88886908, 0.78072028]]])lm(chain, draw, obs)float64157.6 368.0 ... 2.567e+04 1.084e+04array([[[  157.60741315,   368.01149881,   484.45793209, ...,\n          9713.72058482, 22681.4259549 , 13676.08451242],\n        [  155.55919791,   369.07124816,   488.62052768, ...,\n         10425.52435542, 24735.02909624, 10707.12910397],\n        [  152.93367252,   364.60127339,   479.03907065, ...,\n         10487.33492388, 25002.31377805, 15539.25209443],\n        ...,\n        [  151.74783701,   362.24811303,   485.12283192, ...,\n          9698.09893366, 23151.02546387, 10581.58893866],\n        [  165.07711826,   363.19893028,   474.68655257, ...,\n         10949.17187914, 24090.11955022, 13768.56860626],\n        [  156.09646279,   367.21763655,   487.91633048, ...,\n          9686.61005323, 22787.79407511, 11234.13198617]],\n\n       [[  151.0682531 ,   368.4803803 ,   491.1705532 , ...,\n          9768.92945857, 23828.02983572, 14170.24714991],\n        [  160.74842881,   359.85201422,   472.01390953, ...,\n         11021.31677114, 24672.35959206, 10157.1789823 ],\n        [  161.48314057,   375.47685914,   496.29778013, ...,\n         10560.92705732, 24556.02304463,  8042.1379405 ],\n        ...,\n        [  153.83587622,   348.79108035,   462.02640537, ...,\n         10688.95096964, 24234.98892586, 12789.97614652],\n        [  155.56168039,   354.43502325,   469.5628732 , ...,\n         12303.07167812, 28031.57876309, 12885.95974292],\n        [  155.53621549,   351.21400257,   464.37046077, ...,\n         11367.94919334, 25669.79609618, 10840.38943469]]])Attributes: (4)created_at :2021-09-15T19:52:34.700411arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 2, draw: 1000, obs: 55)\nCoordinates:\n  * chain    (chain) int64 0 1\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    loss     (chain, draw, obs) float64 -4.662 -4.996 -5.873 ... -9.298 -8.718\nAttributes:\n    created_at:                 2021-09-15T19:52:34.697576\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 2draw: 1000obs: 55Coordinates: (3)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (1)loss(chain, draw, obs)float64-4.662 -4.996 ... -9.298 -8.718array([[[ -4.66204085,  -4.99560314,  -5.8731991 , ...,  -9.83460231,\n          -8.44534821,  -8.71364938],\n        [ -4.55001055,  -5.15331207,  -6.69084417, ...,  -9.36860769,\n          -8.58761574,  -8.78868814],\n        [ -4.6063776 ,  -4.87522004,  -5.46053709, ...,  -9.09921933,\n          -8.79322533,  -9.84534805],\n        ...,\n        [ -4.47998193,  -4.79602114,  -6.09651578, ..., -10.0247998 ,\n          -8.28211525,  -8.86372827],\n        [ -4.89349193,  -4.82706018,  -5.39305197, ...,  -8.8306795 ,\n          -8.39185554,  -8.74504146],\n        [ -4.57138313,  -5.04378999,  -6.57694207, ..., -10.27797608,\n          -8.28719365,  -8.49661696]],\n\n       [[ -4.46588533,  -5.04744865,  -6.52047369, ...,  -9.93369734,\n          -8.29693424,  -8.96962124],\n        [ -4.74537964,  -4.71708996,  -5.27260529, ...,  -8.7861692 ,\n          -8.59379556,  -9.13006987],\n        [ -4.7742971 ,  -5.29597798,  -6.5881131 , ...,  -9.0816783 ,\n          -8.56772943, -11.11150275],\n        ...,\n        [ -4.65643627,  -4.58216115,  -4.86888165, ...,  -8.97653424,\n          -8.5695356 ,  -8.59666319],\n        [ -4.62960387,  -4.60504662,  -5.11415089, ...,  -8.396745  ,\n         -11.84581071,  -8.51334596],\n        [ -4.5772725 ,  -4.46633895,  -4.95876778, ...,  -8.58177161,\n          -9.29750017,  -8.71840896]]])Attributes: (4)created_at :2021-09-15T19:52:34.697576arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:           (chain: 2, draw: 1000)\nCoordinates:\n  * chain             (chain) int64 0 1\n  * draw              (draw) int64 0 1 2 3 4 5 6 ... 993 994 995 996 997 998 999\nData variables:\n    energy            (chain, draw) float64 418.4 411.7 410.1 ... 411.3 415.6\n    tree_size         (chain, draw) float64 7.0 7.0 7.0 7.0 ... 7.0 7.0 7.0 7.0\n    step_size         (chain, draw) float64 0.3709 0.3709 ... 0.3642 0.3642\n    mean_tree_accept  (chain, draw) float64 0.9878 1.0 0.6001 ... 0.9535 0.9023\n    step_size_bar     (chain, draw) float64 0.4186 0.4186 ... 0.4023 0.4023\n    max_energy_error  (chain, draw) float64 -0.3033 -0.9073 ... 0.1527 0.2162\n    diverging         (chain, draw) bool False False False ... False False False\n    energy_error      (chain, draw) float64 -0.08659 -0.9073 ... 0.05396 0.01052\n    depth             (chain, draw) int64 3 3 3 3 3 3 3 3 4 ... 4 3 3 4 3 3 3 3\n    lp                (chain, draw) float64 -409.4 -398.4 ... -405.5 -405.0\nAttributes:\n    created_at:                 2021-09-15T19:52:34.379572\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2\n    sampling_time:              11.485157012939453\n    tuning_steps:               2000xarray.DatasetDimensions:chain: 2draw: 1000Coordinates: (2)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (10)energy(chain, draw)float64418.4 411.7 410.1 ... 411.3 415.6array([[418.43883273, 411.65728711, 410.0767806 , ..., 410.51405687,\n        408.58036529, 407.35921943],\n       [412.80822536, 411.47900652, 413.17891412, ..., 411.18710152,\n        411.29444508, 415.58466124]])tree_size(chain, draw)float647.0 7.0 7.0 7.0 ... 7.0 7.0 7.0 7.0array([[ 7.,  7.,  7., ...,  7.,  7.,  7.],\n       [ 7., 15.,  7., ...,  7.,  7.,  7.]])step_size(chain, draw)float640.3709 0.3709 ... 0.3642 0.3642array([[0.37085185, 0.37085185, 0.37085185, ..., 0.37085185, 0.37085185,\n        0.37085185],\n       [0.36415012, 0.36415012, 0.36415012, ..., 0.36415012, 0.36415012,\n        0.36415012]])mean_tree_accept(chain, draw)float640.9878 1.0 0.6001 ... 0.9535 0.9023array([[0.98779061, 1.        , 0.60011115, ..., 0.93739839, 0.98545547,\n        0.9950132 ],\n       [0.43950928, 0.89151176, 0.84991465, ..., 0.90923646, 0.95350635,\n        0.90231548]])step_size_bar(chain, draw)float640.4186 0.4186 ... 0.4023 0.4023array([[0.41864123, 0.41864123, 0.41864123, ..., 0.41864123, 0.41864123,\n        0.41864123],\n       [0.40234341, 0.40234341, 0.40234341, ..., 0.40234341, 0.40234341,\n        0.40234341]])max_energy_error(chain, draw)float64-0.3033 -0.9073 ... 0.1527 0.2162array([[-0.30333558, -0.90728229,  0.69568241, ..., -0.31374491,\n        -0.2200231 , -0.40118408],\n       [ 1.42927712,  0.33138796,  0.36049719, ...,  0.18928538,\n         0.15266867,  0.21622557]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])energy_error(chain, draw)float64-0.08659 -0.9073 ... 0.01052array([[-0.08658624, -0.90728229,  0.52606004, ..., -0.05300835,\n         0.02450066, -0.40118408],\n       [ 0.38322751,  0.00231922,  0.16376489, ...,  0.12195152,\n         0.05395713,  0.01051789]])depth(chain, draw)int643 3 3 3 3 3 3 3 ... 4 3 3 4 3 3 3 3array([[3, 3, 3, ..., 3, 3, 3],\n       [3, 4, 3, ..., 3, 3, 3]])lp(chain, draw)float64-409.4 -398.4 ... -405.5 -405.0array([[-409.43533657, -398.44740163, -404.77855576, ..., -402.57284061,\n        -403.52737457, -400.50675223],\n       [-401.88597959, -402.44777691, -407.25696595, ..., -404.72076691,\n        -405.45093988, -404.98509764]])Attributes: (6)created_at :2021-09-15T19:52:34.379572arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2sampling_time :11.485157012939453tuning_steps :2000\n                      \n                  \n            \n            \n            \n                  \n                  prior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:         (LR_log___dim_0: 10, chain: 1, cohort: 10, draw: 100, obs: 55, t_values: 10)\nCoordinates:\n  * chain           (chain) int64 0\n  * draw            (draw) int64 0 1 2 3 4 5 6 7 8 ... 92 93 94 95 96 97 98 99\n  * LR_log___dim_0  (LR_log___dim_0) int64 0 1 2 3 4 5 6 7 8 9\n  * cohort          (cohort) int64 1988 1989 1990 1991 ... 1994 1995 1996 1997\n  * obs             (obs) int64 0 1 2 3 4 5 6 7 8 ... 46 47 48 49 50 51 52 53 54\n  * t_values        (t_values) int64 1 2 3 4 5 6 7 8 9 10\nData variables:\n    theta           (chain, draw) float64 0.5739 1.253 3.127 ... 1.24 1.031\n    LR_log__        (chain, draw, LR_log___dim_0) float64 -0.6171 ... 1.319\n    mu_LR           (chain, draw) float64 -0.8749 0.1713 ... -0.09251 -1.244\n    loss_sd_log__   (chain, draw) float64 0.1178 -0.06791 ... -0.3026 -1.538\n    theta_log__     (chain, draw) float64 -0.5553 0.2252 1.14 ... 0.2152 0.0308\n    omega_log__     (chain, draw) float64 0.4461 0.07193 ... 0.1054 -0.7762\n    LR              (chain, draw, cohort) float64 0.5395 0.2832 ... 3.739\n    loss_sd         (chain, draw) float64 1.125 0.9343 0.4603 ... 0.7389 0.2147\n    sd_LR_log__     (chain, draw) float64 -0.8523 -0.5681 -1.487 ... -0.3 0.7881\n    omega           (chain, draw) float64 1.562 1.075 1.892 ... 1.111 0.4602\n    sd_LR           (chain, draw) float64 0.4264 0.5666 0.2261 ... 0.7408 2.199\n    lm              (chain, draw, obs) float64 363.6 452.0 ... 1.058e+05\n    gf              (chain, draw, t_values) float64 0.7042 0.8755 ... 0.7399\nAttributes:\n    created_at:                 2021-09-15T19:52:34.707602\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:LR_log___dim_0: 10chain: 1cohort: 10draw: 100obs: 55t_values: 10Coordinates: (6)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 6 ... 94 95 96 97 98 99array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])LR_log___dim_0(LR_log___dim_0)int640 1 2 3 4 5 6 7 8 9array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])t_values(t_values)int641 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])Data variables: (13)theta(chain, draw)float640.5739 1.253 3.127 ... 1.24 1.031array([[0.57389012, 1.25260178, 3.12725504, 2.33099523, 1.5559494 ,\n        0.7639599 , 0.82833537, 0.80557966, 2.20412348, 0.43069581,\n        2.9093212 , 0.99399548, 0.86455246, 1.32475578, 1.44164061,\n        1.10965243, 1.07620636, 1.29017089, 2.16036625, 2.32360565,\n        0.55518025, 0.90958767, 1.01729211, 0.84623772, 0.82608081,\n        1.62644821, 1.1668848 , 1.25656072, 0.83539401, 0.65453681,\n        1.83275323, 0.46182585, 1.40712532, 1.19493509, 0.94248559,\n        0.93866154, 0.83764564, 2.25087097, 1.27622612, 0.57064121,\n        0.52945567, 0.88550147, 0.75327822, 1.37076844, 0.93745489,\n        1.51501646, 0.62072393, 1.25873035, 2.29427686, 0.68248132,\n        1.06593879, 0.66061023, 0.77861607, 1.41031485, 2.15389213,\n        0.5953411 , 0.56525208, 1.340487  , 0.69754309, 1.16981863,\n        1.33030513, 1.57734588, 0.8597188 , 0.35135396, 0.82764502,\n        0.6132911 , 0.7778497 , 1.74519489, 0.71525789, 0.83759542,\n        2.2658658 , 4.32680955, 1.6694292 , 1.51162302, 0.54751728,\n        1.73666316, 1.22802165, 0.55897645, 0.70077877, 0.67783987,\n        0.70416641, 1.29484782, 0.81285727, 0.7551851 , 1.18750981,\n        2.39590503, 1.30989887, 0.78067317, 1.26049998, 1.12601871,\n        0.74383829, 1.38761336, 1.04839036, 1.80706916, 0.89511397,\n        0.67815156, 1.02856998, 1.19869844, 1.24008379, 1.03128319]])LR_log__(chain, draw, LR_log___dim_0)float64-0.6171 -1.262 ... -3.077 1.319array([[[-6.17143188e-01, -1.26166054e+00, -6.22430937e-01,\n         -1.06125666e+00, -8.31483299e-01, -3.16974749e-01,\n         -2.83178673e-01, -1.61489186e+00, -7.51912234e-01,\n         -1.01947626e+00],\n        [ 4.70138943e-01, -4.07337019e-02,  4.41652909e-01,\n         -6.38739983e-01,  4.02340063e-01, -2.97949948e-01,\n          4.11264185e-01,  8.09602667e-02,  8.74517052e-01,\n          1.11387272e+00],\n        [ 4.26835799e-01,  1.36009991e+00,  6.65032809e-01,\n          6.50516967e-01,  5.06113871e-01,  2.54364564e-01,\n          4.35217322e-01,  5.51644954e-01,  7.92007920e-01,\n          3.54152889e-01],\n        [-1.23916176e+00, -3.71620985e-01, -7.31328680e-01,\n         -1.12553935e+00, -3.65059826e-02, -7.39102641e-01,\n          1.01824470e+00,  8.16408078e-01, -2.43349269e+00,\n         -4.80237285e-01],\n        [ 3.45828028e-01,  1.56488574e+00, -6.28437141e-02,\n          2.47892004e-01,  1.50385087e+00,  1.51489840e+00,\n          1.11068047e+00, -6.43011394e-01,  1.83483899e+00,\n          3.31755049e-01],\n...\n        [ 1.24232366e+00, -1.79165022e+00, -1.05836185e+00,\n         -3.09225403e+00,  6.56009661e-02, -7.50671829e-02,\n          5.89357991e-01,  8.63156384e-01, -2.81532734e-01,\n         -1.24847464e+00],\n        [-6.86571297e-01, -3.02133976e-01,  3.05860017e-01,\n          5.61005282e-04, -1.20440890e-01,  1.16642382e+00,\n         -2.26335350e-01, -9.18702845e-01, -8.41696486e-01,\n         -3.58155741e-01],\n        [ 1.09884268e+00,  1.13119969e+00, -6.78816255e-01,\n         -5.85593856e-01,  7.15943176e-01,  2.24503315e+00,\n         -4.51843600e-01,  3.07867675e-01,  9.31380940e-01,\n         -2.27855222e+00],\n        [-3.37456137e-01,  2.42518433e-01, -4.85528021e-01,\n         -3.01018200e-01, -5.69692766e-01, -9.29658170e-01,\n         -9.53172168e-01,  3.29390486e-02, -1.36732900e-01,\n          7.26723678e-01],\n        [-2.69919207e+00, -2.03199061e+00, -1.40009199e+00,\n         -1.02835591e+00,  2.65043778e+00,  3.31279141e-01,\n         -1.79784476e+00,  2.42504669e+00, -3.07654973e+00,\n          1.31886562e+00]]])mu_LR(chain, draw)float64-0.8749 0.1713 ... -0.09251 -1.244array([[-0.87488274,  0.1713402 ,  0.5765179 , -0.12621802,  0.49066039,\n         0.25710942,  0.11058983, -0.53502167, -0.09474792,  0.12750072,\n        -0.22901349,  0.21758174, -0.29179753,  0.40842354,  0.3363604 ,\n        -0.05220557, -0.26564019,  0.51486634, -0.21906781, -0.55915912,\n         0.80949083,  0.77080259, -0.12593957, -0.42121787,  0.09225935,\n         0.4685411 ,  0.36550017,  0.68077806, -0.16311903,  0.02783801,\n         0.1111998 , -0.7216085 , -0.37817615,  0.40822701,  0.37522238,\n        -0.22797346,  0.59481113, -0.84530841, -0.67819952, -0.61621726,\n        -0.27221958, -0.33408587,  0.00365728, -0.30646937,  0.64987404,\n        -0.86654781, -0.49165505,  0.17875388, -0.80678925,  0.73535693,\n        -0.5940088 , -0.2748731 , -0.47002308, -0.41396618,  0.05443173,\n         0.2539048 , -0.43111367,  0.62473487, -0.03980562, -0.44486574,\n        -0.44089919,  0.00931947,  0.11892231,  0.00677427, -0.8177647 ,\n        -0.52210494,  0.30651944,  0.36810261,  0.51346072, -0.71609531,\n        -0.92059415,  0.18304661, -0.16588857, -0.34460899,  1.01730378,\n        -0.27535721,  0.37522667, -0.65349617,  0.29028667, -0.55226155,\n         0.34506074,  0.34344503, -0.78334376,  0.45248706,  0.3894112 ,\n         0.21411644,  0.05443599,  0.01414182, -0.28941291, -0.5997256 ,\n        -0.852976  ,  0.18458198,  0.93828671, -0.18845168,  0.91596804,\n         0.00150872, -0.03801173,  0.0019788 , -0.09250706, -1.24357577]])loss_sd_log__(chain, draw)float640.1178 -0.06791 ... -0.3026 -1.538array([[ 0.11776897, -0.06791383, -0.77586015,  0.81418801, -1.54295884,\n        -0.08635401, -0.3073733 ,  0.44099133, -0.60868549, -0.42565676,\n         0.83953297,  1.90751457,  0.31553409, -0.58763441, -0.02813683,\n         0.61956292,  1.1102456 ,  1.41804546,  1.07121072,  1.2829134 ,\n         0.67823319,  0.11953965,  0.49559607,  0.91112716,  0.26419822,\n        -0.55153079,  0.64974865,  1.42332632,  0.40688404,  1.0925064 ,\n         0.0515981 , -0.0156497 ,  0.47990747, -0.15405117, -1.44822335,\n         0.44352918, -1.63864786, -0.29389226,  0.71212342, -0.27237528,\n        -0.51608351, -0.00461874,  0.19355207, -0.40968742,  1.59689048,\n         0.80746392,  0.90807538, -0.15603457,  0.24094942, -1.39879746,\n         0.38607813, -0.58429016, -0.56743807,  0.07267915, -0.20197823,\n        -0.65064838,  0.31415215, -0.38924771,  0.91696663, -0.48047711,\n        -0.79327309, -0.11503221,  0.74234383, -0.37075465, -0.29457163,\n        -1.81513457,  0.20137675,  0.56713415,  0.07135635, -0.19635674,\n        -0.4044409 , -0.17024778, -1.04476412,  0.16943208, -0.98912851,\n        -0.66173423,  1.065507  ,  1.40047042, -0.45946529, -0.33103924,\n         1.22237942,  1.01016131, -0.64361443,  0.76310702,  0.76479435,\n        -0.22690514,  0.57659883,  1.36853881, -0.45579136,  0.67822079,\n         0.29416748, -1.19540042, -0.00414685, -0.5183479 ,  0.94649573,\n        -0.45149198, -0.10859997, -0.04805518, -0.302624  , -1.53838435]])theta_log__(chain, draw)float64-0.5553 0.2252 ... 0.2152 0.0308array([[-0.55531734,  0.22522281,  1.14015564,  0.84629531,  0.44208591,\n        -0.26923997, -0.18833717, -0.21619318,  0.79032991, -0.84235321,\n         1.06791979, -0.00602262, -0.1455433 ,  0.28122813,  0.36578178,\n         0.10404684,  0.07344223,  0.25477469,  0.77027777,  0.84312014,\n        -0.58846245, -0.09476389,  0.0171443 , -0.16695497, -0.19106268,\n         0.48639862,  0.15433763,  0.2283784 , -0.17985179, -0.42382746,\n         0.60581933, -0.7725674 ,  0.34154884,  0.17809187, -0.05923465,\n        -0.06330032, -0.17716014,  0.81131724,  0.24390738, -0.56099462,\n        -0.63590583, -0.12160116, -0.28332064,  0.31537149, -0.06458664,\n         0.4154263 , -0.47686886,  0.23010355,  0.8304177 , -0.38202012,\n         0.06385591, -0.41459128, -0.25023721,  0.34381298,  0.7672765 ,\n        -0.51862076, -0.57048348,  0.29303298, -0.360191  ,  0.15684872,\n         0.28540834,  0.45574361, -0.15114992, -1.04596112, -0.18917094,\n        -0.48891559, -0.25122197,  0.55686623, -0.33511211, -0.17722009,\n         0.81795694,  1.46483045,  0.51248177,  0.41318392, -0.60236126,\n         0.55196555,  0.20540446, -0.58164794, -0.35556303, -0.3888442 ,\n        -0.35074057,  0.25839318, -0.20719975, -0.2807924 ,  0.17185852,\n         0.87376104,  0.26994994, -0.24759869,  0.23150845,  0.11868815,\n        -0.29593161,  0.32758526,  0.047256  ,  0.59170629, -0.11080422,\n        -0.38838448,  0.02816947,  0.18123634,  0.21517895,  0.03080384]])omega_log__(chain, draw)float640.4461 0.07193 ... 0.1054 -0.7762array([[ 0.44612894,  0.0719305 ,  0.63779866, -0.23449358,  0.23706241,\n         0.33419761, -0.60235794, -0.42556604,  0.44956226,  0.46859577,\n         0.47679333, -1.10255173, -1.1406529 , -0.30581554,  1.40485057,\n         0.08792286, -0.3153778 ,  0.92777277,  0.08014537, -0.71142477,\n        -0.18382993, -0.94314418,  0.37741105,  0.89061229,  0.32631473,\n         0.1187519 ,  0.29710071, -0.56646717,  0.24871394, -0.12211726,\n         0.52034076, -0.13353552, -0.33864184, -0.33580391,  1.31280722,\n        -0.55840032,  0.46812679, -0.28309119, -0.25267346, -0.43320864,\n         0.69851556, -0.43096099, -0.05757817,  0.00526835, -0.25980818,\n        -0.12478563,  1.12182973, -0.2414983 , -1.18449009, -0.46079041,\n         0.75950166,  0.27624921, -0.34634081, -0.53000413,  0.73357778,\n        -0.08395659,  0.37989795, -0.10431087, -0.02411082, -0.0916873 ,\n         0.64151539,  0.34238158,  0.25391359,  0.20807299,  0.07147848,\n        -0.03081378, -0.15740337,  0.33052872, -0.54743424, -0.17367713,\n         0.6791806 , -0.14902945, -0.04004145,  0.24734065,  0.68666321,\n        -0.46438288, -0.5281403 ,  0.10112753, -0.33912545, -0.01100623,\n        -0.95018485, -0.51196821,  0.10753402, -0.2633491 , -0.26221749,\n         0.2735264 ,  0.05936596, -1.15266121,  0.35999557, -0.64225228,\n         0.67848939,  0.24234314, -0.12805771,  0.67731315,  1.03797529,\n         0.3334401 , -0.50685985,  0.09829753,  0.10543659, -0.77619435]])LR(chain, draw, cohort)float640.5395 0.2832 ... 0.04612 3.739array([[[5.39483441e-01, 2.83183398e-01, 5.36638317e-01, 3.46020707e-01,\n         4.35402974e-01, 7.28349146e-01, 7.53385166e-01, 1.98912181e-01,\n         4.71464140e-01, 3.60783849e-01],\n        [1.60021652e+00, 9.60084765e-01, 1.55527582e+00, 5.27957240e-01,\n         1.49531975e+00, 7.42338495e-01, 1.50872389e+00, 1.08432781e+00,\n         2.39771704e+00, 3.04613240e+00],\n        [1.53240102e+00, 3.89658259e+00, 1.94455432e+00, 1.91653136e+00,\n         1.65883222e+00, 1.28964188e+00, 1.54529885e+00, 1.73610649e+00,\n         2.20782512e+00, 1.42497303e+00],\n        [2.89626892e-01, 6.89615568e-01, 4.81269113e-01, 3.24477413e-01,\n         9.64152326e-01, 4.77542250e-01, 2.76833126e+00, 2.26235901e+00,\n         8.77298833e-02, 6.18636581e-01],\n        [1.41315957e+00, 4.78212851e+00, 9.39090229e-01, 1.28132155e+00,\n         4.49898074e+00, 4.54895891e+00, 3.03642390e+00, 5.25706928e-01,\n         6.26412548e+00, 1.39341149e+00],\n        [9.57696979e-01, 1.86763571e+00, 4.13374817e+00, 1.56821742e+00,\n         1.25487332e+00, 2.51191010e+00, 8.10945988e+00, 6.10722752e-01,\n         1.41395916e+00, 9.82281959e+00],\n        [1.71912712e+00, 1.49080130e-01, 4.37317875e+00, 6.93848752e-01,\n         2.87455238e-01, 2.20250288e+00, 8.11983450e-01, 7.26701892e-01,\n...\n         9.69395468e-01, 6.87721577e+01, 1.36484576e-01, 1.13037865e-01,\n         4.30151712e-02, 2.71561166e+02],\n        [3.40901443e+00, 2.08439147e+00, 6.84379805e+00, 9.55929917e+00,\n         1.77361679e+00, 1.82087509e+01, 4.04976821e+00, 1.13211119e+01,\n         1.86859175e+00, 1.89121247e+00],\n        [3.46365246e+00, 1.66684876e-01, 3.47023822e-01, 4.53995073e-02,\n         1.06780054e+00, 9.27681160e-01, 1.80283061e+00, 2.37063152e+00,\n         7.54626213e-01, 2.86942154e-01],\n        [5.03298776e-01, 7.39239018e-01, 1.35779223e+00, 1.00056116e+00,\n         8.86529488e-01, 3.21049079e+00, 7.97450632e-01, 3.99036318e-01,\n         4.30978753e-01, 6.98964209e-01],\n        [3.00069125e+00, 3.09937257e+00, 5.07217053e-01, 5.56775119e-01,\n         2.04611562e+00, 9.44072847e+00, 6.36453703e-01, 1.36052095e+00,\n         2.53801160e+00, 1.02432399e-01],\n        [7.13583274e-01, 1.27445474e+00, 6.15372182e-01, 7.40064303e-01,\n         5.65699214e-01, 3.94688604e-01, 3.85516160e-01, 1.03348754e+00,\n         8.72203161e-01, 2.06829310e+00],\n        [6.72598317e-02, 1.31074344e-01, 2.46574280e-01, 3.57594394e-01,\n         1.41602364e+01, 1.39274851e+00, 1.65655532e-01, 1.13027571e+01,\n         4.61181024e-02, 3.73917734e+00]]])loss_sd(chain, draw)float641.125 0.9343 ... 0.7389 0.2147array([[1.12498418, 0.93434098, 0.46030768, 2.257342  , 0.21374772,\n        0.91726945, 0.73537603, 1.55424723, 0.54406558, 0.65334056,\n        2.31528541, 6.73632528, 1.37099135, 0.55564015, 0.97225532,\n        1.85811573, 3.03510372, 4.12904219, 2.91891135, 3.60713346,\n        1.97039335, 1.12697793, 1.64147638, 2.48712433, 1.30238633,\n        0.5760673 , 1.91505942, 4.15090475, 1.5021299 , 2.98173813,\n        1.05295248, 0.98447212, 1.61592487, 0.85722816, 0.23498741,\n        1.55819668, 0.19424251, 0.7453568 , 2.03831486, 0.76156841,\n        0.59685355, 0.99539191, 1.21355257, 0.66385773, 4.93765477,\n        2.24221434, 2.47954574, 0.85552962, 1.27245668, 0.24689368,\n        1.47119961, 0.55750146, 0.56697613, 1.07538545, 0.81711272,\n        0.5217074 , 1.36909803, 0.67756641, 2.50169031, 0.61848824,\n        0.45236175, 0.89133743, 2.10085379, 0.69021326, 0.74485059,\n        0.162816  , 1.22308549, 1.76320672, 1.07396387, 0.82171904,\n        0.66734982, 0.8434558 , 0.35177479, 1.18463189, 0.37190066,\n        0.51595577, 2.90231009, 4.05710805, 0.63162129, 0.71817699,\n        3.39525688, 2.74604394, 0.52539   , 2.14493023, 2.14855249,\n        0.79699638, 1.77997412, 3.9296046 , 0.63394609, 1.97036892,\n        1.34200864, 0.30258277, 0.99586174, 0.59550356, 2.57666449,\n        0.63667753, 0.89708921, 0.9530812 , 0.73887686, 0.21472775]])sd_LR_log__(chain, draw)float64-0.8523 -0.5681 ... -0.3 0.7881array([[-0.8523256 , -0.5681305 , -1.48665774,  0.01665864, -0.12444433,\n        -0.22508822,  0.0662139 ,  0.01110696,  0.15868399, -0.37620709,\n        -0.6481959 ,  0.04756972, -0.21185755, -0.59299178, -0.182731  ,\n        -0.63551152,  0.79308547,  0.34669533, -0.97904062, -0.06740066,\n        -0.77030801,  1.02335698, -0.69849967, -0.54858599, -0.11935643,\n        -0.71453345,  0.47450239, -0.00969879,  0.44729885,  0.37984656,\n        -0.74886019, -0.59694299,  0.64813129,  0.47613781, -0.60862707,\n        -0.07863258, -0.75379258,  0.05394207,  0.37352783,  0.21483822,\n        -0.70752146, -0.32037996,  0.38981315, -0.21906046,  1.03739658,\n        -0.17164884, -0.30831469,  0.38159182,  0.0964586 , -0.17422947,\n         1.14932697, -0.08260478,  0.23314968,  0.13499362, -0.15991552,\n        -0.5738708 ,  0.85181199, -0.36107539,  0.54684332, -0.11475888,\n        -0.00444933, -0.271599  ,  0.37653109, -0.80471945,  0.97163113,\n        -0.72371806,  0.06512423,  0.47468043, -1.00759436, -0.03977029,\n         0.15052473, -0.84244998,  0.1111954 , -0.34246087, -0.06310059,\n         0.99513682,  0.2614989 , -0.0081727 , -0.20790817, -0.67925147,\n        -0.25721495, -0.10803006,  0.21119011, -0.54702147,  0.61845394,\n        -0.11514234, -0.3522091 , -0.29568756,  0.36849758,  0.21793363,\n         0.88799679,  0.25653719,  0.58526349,  1.03885612, -0.22796101,\n         0.32458646, -0.08739078,  0.50863217, -0.29999152,  0.78808336]])omega(chain, draw)float641.562 1.075 1.892 ... 1.111 0.4602array([[1.56225289, 1.07458066, 1.89231067, 0.79097131, 1.26752023,\n        1.39681914, 0.5475191 , 0.65339983, 1.56762583, 1.59774901,\n        1.61090048, 0.33202277, 0.31961028, 0.73652247, 4.07491779,\n        1.09190388, 0.72951322, 2.52887052, 1.08344456, 0.49094421,\n        0.83207731, 0.38940156, 1.4585037 , 2.43662111, 1.38585147,\n        1.1260905 , 1.34595085, 0.56752688, 1.28237514, 0.88504458,\n        1.68260091, 0.87499639, 0.71273768, 0.71476325, 3.71659238,\n        0.57212355, 1.59699987, 0.75345108, 0.77672147, 0.64842519,\n        2.01076562, 0.64988426, 0.94404809, 1.00528225, 0.7711995 ,\n        0.8826861 , 3.07046718, 0.78545014, 0.30590212, 0.63078487,\n        2.13721089, 1.31817633, 0.7072714 , 0.58860254, 2.08251808,\n        0.91947117, 1.46213538, 0.90094518, 0.97617753, 0.91239041,\n        1.89935696, 1.40829757, 1.28906041, 1.23130304, 1.07409503,\n        0.96965612, 0.85435936, 1.39170375, 0.57843202, 0.84056825,\n        1.97226099, 0.86154374, 0.96074961, 1.28061528, 1.987074  ,\n        0.62852286, 0.58970061, 1.10641774, 0.71239307, 0.98905411,\n        0.38666954, 0.59931484, 1.11352875, 0.76847358, 0.76934368,\n        1.31459206, 1.06116352, 0.31579525, 1.43332306, 0.52610615,\n        1.97089823, 1.27423136, 0.87980261, 1.96858134, 2.82349447,\n        1.39576144, 0.60238419, 1.103291  , 1.11119564, 0.46015387]])sd_LR(chain, draw)float640.4264 0.5666 ... 0.7408 2.199array([[0.42642209, 0.56658368, 0.22612717, 1.01679817, 0.88298741,\n        0.79844578, 1.06845524, 1.01116888, 1.17196753, 0.68646016,\n        0.52298845, 1.04871932, 0.80907994, 0.55267134, 0.8329922 ,\n        0.52966448, 2.21020544, 1.41438574, 0.37567134, 0.93482058,\n        0.46287048, 2.78251998, 0.4973309 , 0.5777662 , 0.88749141,\n        0.4894204 , 1.60721423, 0.99034809, 1.56408166, 1.46206023,\n        0.47290527, 0.55049193, 1.91196459, 1.60984486, 0.54409737,\n        0.92437949, 0.47057846, 1.05542345, 1.45285099, 1.23966133,\n        0.49286427, 0.72587318, 1.47670485, 0.80327315, 2.82186097,\n        0.84227489, 0.73468409, 1.46461414, 1.10126398, 0.8401041 ,\n        3.15606807, 0.92071496, 1.26257045, 1.14452948, 0.85221578,\n        0.56334063, 2.34389012, 0.69692646, 1.72779033, 0.8915811 ,\n        0.99556055, 0.76215982, 1.45722085, 0.44721338, 2.64225081,\n        0.48494584, 1.0672916 , 1.60750041, 0.36509621, 0.96101016,\n        1.16244405, 0.43065414, 1.11761327, 0.7100209 , 0.93884903,\n        2.70509444, 1.29887552, 0.9918606 , 0.81228163, 0.50699635,\n        0.773202  , 0.89760061, 1.23514715, 0.57867084, 1.85605625,\n        0.89123928, 0.70313308, 0.74401985, 1.44556115, 1.24350453,\n        2.43025646, 1.29244683, 1.79546401, 2.82598257, 0.7961553 ,\n        1.38345842, 0.91631895, 1.66301492, 0.7408245 , 2.19917736]])lm(chain, draw, obs)float64363.6 452.0 ... 1.393e+03 1.058e+05array([[[3.63586344e+02, 4.52004030e+02, 4.80050323e+02, ...,\n         1.74248189e+04, 2.16622228e+04, 1.44767740e+04],\n        [6.73495425e+02, 9.54254126e+02, 1.10077873e+03, ...,\n         5.53406168e+04, 7.84103498e+04, 7.63308220e+04],\n        [1.51973891e+02, 4.40389457e+02, 7.04447107e+02, ...,\n         1.20074665e+04, 3.47951982e+04, 8.41392159e+03],\n        ...,\n        [1.29275537e+03, 1.83085465e+03, 2.10618368e+03, ...,\n         5.99623720e+04, 8.49212392e+04, 2.62740876e+03],\n        [3.00821669e+02, 4.30051269e+02, 4.96767448e+02, ...,\n         2.01637950e+04, 2.88259343e+04, 5.19124828e+04],\n        [3.19557383e+01, 3.70507050e+01, 3.99353603e+01, ...,\n         1.20158548e+03, 1.39316416e+03, 1.05770463e+05]]])gf(chain, draw, t_values)float640.7042 0.8755 ... 0.7304 0.7399array([[[0.70423484, 0.87549214, 0.92981535, 0.95405553, 0.9671339 ,\n         0.97507705, 0.98030598, 0.98395463, 0.98661528, 0.98862353],\n        [0.4397886 , 0.62312239, 0.71880211, 0.77689541, 0.81569534,\n         0.84335026, 0.86401169, 0.88000818, 0.89274339, 0.90311221],\n        [0.10362979, 0.30029807, 0.48035688, 0.61438231, 0.70848302,\n         0.77434966, 0.8212351 , 0.85537809, 0.88082887, 0.90022056],\n        [0.33863225, 0.46975303, 0.54972892, 0.60518664, 0.64648496,\n         0.67870678, 0.70469655, 0.7261912 , 0.74432036, 0.75985477],\n        [0.36346583, 0.57889167, 0.69681155, 0.76795562, 0.81451854,\n         0.84693205, 0.87058348, 0.88848725, 0.90244344, 0.91358541],\n        [0.59292719, 0.79319304, 0.87109128, 0.90990553, 0.93240122,\n         0.94679063, 0.95665138, 0.96376016, 0.96908782, 0.97320487],\n        [0.52575673, 0.6183686 , 0.66921337, 0.70310774, 0.7279638 ,\n         0.74727678, 0.76288122, 0.77585146, 0.78686662, 0.79638106],\n        [0.53525654, 0.64431972, 0.70247126, 0.74021092, 0.76725344,\n         0.78784695, 0.80419428, 0.8175696 , 0.82876883, 0.83831844],\n        [0.22462027, 0.46198695, 0.61852007, 0.71793657, 0.78313921,\n         0.82776462, 0.85954517, 0.88296832, 0.90074093, 0.91456169],\n        [0.79345344, 0.92080566, 0.95694122, 0.9723696 , 0.98049424,\n         0.98535136, 0.98851254, 0.99069905, 0.99228223, 0.99347016],\n...\n        [0.64181514, 0.87537833, 0.93982881, 0.96495661, 0.97714108,\n         0.98393041, 0.98809055, 0.99082103, 0.99270871, 0.99406781],\n        [0.39713443, 0.61439167, 0.72759741, 0.79397269, 0.83663171,\n         0.86595956, 0.88716659, 0.90311019, 0.9154717 , 0.92529784],\n        [0.48960751, 0.63836083, 0.71605695, 0.76460411, 0.798092  ,\n         0.82271025, 0.84163459, 0.85667234, 0.86893193, 0.87913289],\n        [0.23779221, 0.54975842, 0.73064268, 0.82695681, 0.88116276,\n         0.91391499, 0.93498184, 0.94924842, 0.95932411, 0.96668948],\n        [0.57758201, 0.90635774, 0.96816329, 0.98561457, 0.99228692,\n         0.99537608, 0.99700294, 0.99794238, 0.99852365, 0.99890312],\n        [0.63229895, 0.81899602, 0.88849829, 0.92251565, 0.94205031,\n         0.95447765, 0.96296374, 0.96906659, 0.97363229, 0.97715586],\n        [0.49575789, 0.5988251 , 0.65584172, 0.69383563, 0.72162392,\n         0.74314104, 0.76046247, 0.7748066 , 0.78694441, 0.79739179],\n        [0.45017679, 0.63755935, 0.7334373 , 0.79076283, 0.82859779,\n         0.85531065, 0.87511355, 0.89034527, 0.90240364, 0.91217334],\n        [0.4405067 , 0.62974341, 0.72743891, 0.78606014, 0.82481181,\n         0.85219095, 0.87249273, 0.88810812, 0.90046845, 0.91048055],\n        [0.49645643, 0.57561057, 0.62042586, 0.65106736, 0.67401798,\n         0.69217567, 0.70707985, 0.71964241, 0.73044606, 0.73988461]]])Attributes: (4)created_at :2021-09-15T19:52:34.707602arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  prior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 1, draw: 100, obs: 55)\nCoordinates:\n  * chain    (chain) int64 0\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 9 ... 90 91 92 93 94 95 96 97 98 99\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    loss     (chain, draw, obs) float64 -2.671e+03 1.343e+03 ... 9.672e+04\nAttributes:\n    created_at:                 2021-09-15T19:52:34.712755\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 1draw: 100obs: 55Coordinates: (3)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 6 ... 94 95 96 97 98 99array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (1)loss(chain, draw, obs)float64-2.671e+03 1.343e+03 ... 9.672e+04array([[[-2.67100326e+03,  1.34255155e+03,  1.40821615e+03, ...,\n          5.44546338e+04, -2.82037218e+04,  2.25204026e+04],\n        [ 6.91120357e+02,  1.25047752e+03,  2.34623536e+03, ...,\n          3.67907448e+04,  1.27985416e+05,  4.26089822e+04],\n        [-1.05994085e+02,  7.82732937e+02,  3.65909060e+02, ...,\n         -1.07724563e+04,  2.61310680e+04,  2.36614675e+04],\n        ...,\n        [ 8.83043472e+02,  2.15344855e+03,  3.03744893e+03, ...,\n          4.12138121e+04, -2.16126888e+04,  5.72089335e+03],\n        [ 1.13652047e+03,  1.26135401e+03,  1.58322237e+03, ...,\n         -4.63703410e+04,  6.47498746e+04,  1.04504263e+05],\n        [ 1.67803663e+02,  2.66545568e+02, -9.96214671e+01, ...,\n         -2.17424973e+04, -9.03439475e+03,  9.67249413e+04]]])Attributes: (4)created_at :2021-09-15T19:52:34.712755arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (obs: 55)\nCoordinates:\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    loss     (obs) int32 133 333 431 570 615 ... 26012 31677 12604 23446 12292\nAttributes:\n    created_at:                 2021-09-15T19:52:34.713725\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:obs: 55Coordinates: (1)obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (1)loss(obs)int32133 333 431 ... 12604 23446 12292array([  133,   333,   431,   570,   615,   615,   615,   614,   614,\n         614,   934,  1746,  2365,  2579,  2763,  2966,  2940,  2978,\n        2978,  2030,  4864,  6880,  8087,  8595,  8743,  8763,  8762,\n        4537, 11527, 15123, 16656, 17321, 18076, 18308,  7564, 16061,\n       22465, 25204, 26517, 27124,  8343, 19900, 26732, 30079, 31249,\n       12565, 26922, 33867, 38338, 13437, 26012, 31677, 12604, 23446,\n       12292], dtype=int32)Attributes: (4)created_at :2021-09-15T19:52:34.713725arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  constant_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:   (cohort: 10, obs: 55, t_values: 10)\nCoordinates:\n  * t_values  (t_values) int64 1 2 3 4 5 6 7 8 9 10\n  * cohort    (cohort) int64 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\n  * obs       (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    t         (t_values) int32 1 2 3 4 5 6 7 8 9 10\n    premium   (cohort) int32 957 3695 6138 17533 ... 46095 51512 52481 56978\n    t_idx     (obs) int32 0 1 2 3 4 5 6 7 8 9 0 1 2 ... 3 4 0 1 2 3 0 1 2 0 1 0\n    c_idx     (obs) int32 0 0 0 0 0 0 0 0 0 0 1 1 1 ... 5 5 6 6 6 6 7 7 7 8 8 9\n    obs_idx   (obs) float64 0.0 1.0 2.0 3.0 4.0 5.0 ... 50.0 51.0 52.0 53.0 54.0\nAttributes:\n    created_at:                 2021-09-15T19:52:34.715851\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:cohort: 10obs: 55t_values: 10Coordinates: (3)t_values(t_values)int641 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (5)t(t_values)int321 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=int32)premium(cohort)int32957 3695 6138 ... 51512 52481 56978array([  957,  3695,  6138, 17533, 29341, 37194, 46095, 51512, 52481,\n       56978], dtype=int32)t_idx(obs)int320 1 2 3 4 5 6 7 ... 2 3 0 1 2 0 1 0array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2,\n       3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3,\n       4, 0, 1, 2, 3, 0, 1, 2, 0, 1, 0], dtype=int32)c_idx(obs)int320 0 0 0 0 0 0 0 ... 6 6 7 7 7 8 8 9array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2,\n       2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5,\n       5, 6, 6, 6, 6, 7, 7, 7, 8, 8, 9], dtype=int32)obs_idx(obs)float640.0 1.0 2.0 3.0 ... 52.0 53.0 54.0array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,\n       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,\n       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51.,\n       52., 53., 54.])Attributes: (4)created_at :2021-09-15T19:52:34.715851arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n              \n            \n            \n\n\n\nusetable[['grcode','grname','acc_year','dev_year','dev_lag','premium','cum_loss','loss_ratio']].head(10)\n\n\n\n\n\n  \n    \n      \n      grcode\n      grname\n      acc_year\n      dev_year\n      dev_lag\n      premium\n      cum_loss\n      loss_ratio\n    \n  \n  \n    \n      0\n      43\n      IDS Property Cas Ins Co\n      1988\n      1988\n      1\n      957\n      133\n      0.138976\n    \n    \n      1\n      43\n      IDS Property Cas Ins Co\n      1988\n      1989\n      2\n      957\n      333\n      0.347962\n    \n    \n      2\n      43\n      IDS Property Cas Ins Co\n      1988\n      1990\n      3\n      957\n      431\n      0.450366\n    \n    \n      3\n      43\n      IDS Property Cas Ins Co\n      1988\n      1991\n      4\n      957\n      570\n      0.595611\n    \n    \n      4\n      43\n      IDS Property Cas Ins Co\n      1988\n      1992\n      5\n      957\n      615\n      0.642633\n    \n    \n      5\n      43\n      IDS Property Cas Ins Co\n      1988\n      1993\n      6\n      957\n      615\n      0.642633\n    \n    \n      6\n      43\n      IDS Property Cas Ins Co\n      1988\n      1994\n      7\n      957\n      615\n      0.642633\n    \n    \n      7\n      43\n      IDS Property Cas Ins Co\n      1988\n      1995\n      8\n      957\n      614\n      0.641588\n    \n    \n      8\n      43\n      IDS Property Cas Ins Co\n      1988\n      1996\n      9\n      957\n      614\n      0.641588\n    \n    \n      9\n      43\n      IDS Property Cas Ins Co\n      1988\n      1997\n      10\n      957\n      614\n      0.641588\n    \n  \n\n\n\n\n\nprediction_coords = {\"obs\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}\nwith logistic_model:\n    pm.set_data({'premium':np.array([800]*10), \n                 'obs_idx':[21, 11, 12, 13, 14, 15, 16, 17, 18, 50]\n                })\n    ppc = pm.sample_posterior_predictive(logistic_trace,  var_names=[\"lm\"])\n    \n\n\n    \n        \n      \n      100.00% [2000/2000 00:01<00:00]\n    \n    \n\n\n\npd.DataFrame(ppc['lm']).T[0:10].plot(legend=False)\n#logistic_idata.constant_data.to_dataframe()\n#usetable\n#cohort_id\n\n<AxesSubplot:>\n\n\n\n\n\n\nyears = usetable['acc_year'].unique()\nfig, axs = plt.subplots((int(len(years)/2)), 2, figsize=(20,10))\naxs = axs.flatten()\nfor ax, year in zip(axs, years):\n    usetable[usetable['acc_year'] == year]['loss_ratio'].plot(ax=ax, title=\"Loss Ratio Curve for: \" + str(year))\n\n\n\n\n\nfrom graphviz import Digraph, Graph\n\n# Packages\np = Digraph()\np.node('Echo', 'Echo')\np.node('Severen', 'Severen')\np.node('Panacea', 'Panacea')\n\n\n## Frameworks\nf = Digraph()\nf.node('ACDC', 'ACDC Framework')\nf.node('Jenkins', 'Jenkins')\n\np.edge('Echo', 'Panacea')\n\np.subgraph(f)\n\nf.edge('Jenkins', 'Echo')\np\n\n\n\n\n\ng = Digraph('G', filename='cluster_edge.gv')\ng.attr(compound='true')\ng.node('e', 'Echo')\ng.node('s', 'Severen')\ng.node('p', 'Panacea')\ng.node('j', 'Jenkins')\ng.node('slack', 'Slack')\n\n\nwith g.subgraph(name='cluster0') as c:\n    c.edges(['jb', 'ac', 'bd', 'cd'])\n\nwith g.subgraph(name='cluster1') as c:\n    c.edges(['ps', 'pe'])\n\n\ng\n\n\n\n\n\nidata\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:   (chain: 2, cohort: 10, draw: 1000, obs: 55, t_values: 10)\nCoordinates:\n  * chain     (chain) int64 0 1\n  * draw      (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * cohort    (cohort) int64 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\n  * t_values  (t_values) int64 1 2 3 4 5 6 7 8 9 10\n  * obs       (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    mu_LR     (chain, draw) float64 -0.187 -0.09882 ... -0.03967 0.007514\n    sd_LR     (chain, draw) float64 0.6209 0.3373 0.2758 ... 0.2711 0.2706\n    LR        (chain, draw, cohort) float64 0.6702 0.8334 ... 0.8889 0.7807\n    loss_sd   (chain, draw) float64 0.03186 0.02677 0.03494 ... 0.0332 0.02944\n    omega     (chain, draw) float64 2.047 2.063 2.117 ... 1.928 1.942 1.925\n    theta     (chain, draw) float64 1.73 1.75 1.699 1.712 ... 1.81 1.803 1.801\n    gf        (chain, draw, t_values) float64 0.2457 0.5738 ... 0.9568 0.9644\n    lm        (chain, draw, obs) float64 157.6 368.0 ... 2.567e+04 1.084e+04\nAttributes:\n    created_at:                 2021-07-10T22:10:08.100990\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2\n    sampling_time:              10.192897081375122\n    tuning_steps:               2000xarray.DatasetDimensions:chain: 2cohort: 10draw: 1000obs: 55t_values: 10Coordinates: (5)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])t_values(t_values)int641 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (8)mu_LR(chain, draw)float64-0.187 -0.09882 ... 0.007514array([[-0.18696659, -0.0988205 , -0.00810847, ...,  0.05311949,\n         0.20880435, -0.01687328],\n       [-0.11414088, -0.15425932, -0.22073582, ...,  0.0246603 ,\n        -0.03966546,  0.00751418]])sd_LR(chain, draw)float640.6209 0.3373 ... 0.2711 0.2706array([[0.62089146, 0.33727945, 0.27581849, ..., 0.42592517, 0.45086283,\n        0.48777715],\n       [0.32289577, 0.44645476, 0.49395078, ..., 0.32981233, 0.27107624,\n        0.27061255]])LR(chain, draw, cohort)float640.6702 0.8334 ... 0.8889 0.7807array([[[0.67022333, 0.83343362, 1.50476005, ..., 0.84746063,\n         0.75324862, 0.97680847],\n        [0.67852191, 0.85033978, 1.5128585 , ..., 0.8348323 ,\n         0.82923175, 0.78441512],\n        [0.65084057, 0.81301633, 1.50223526, ..., 0.86576549,\n         0.81385395, 1.1107245 ],\n        ...,\n        [0.69134537, 0.87360691, 1.53672948, ..., 0.88517673,\n         0.80569189, 0.80970752],\n        [0.67295471, 0.8505383 , 1.55445959, ..., 0.9003714 ,\n         0.81393564, 0.9427412 ],\n        [0.68806353, 0.85636222, 1.52987837, ..., 0.84520793,\n         0.77860489, 0.83172496]],\n\n       [[0.68118037, 0.84469149, 1.52453007, ..., 0.89498748,\n         0.80324074, 1.07317632],\n        [0.66690657, 0.85809314, 1.53552822, ..., 0.83513642,\n         0.8337994 , 0.70777644],\n        [0.69604056, 0.83549505, 1.52540707, ..., 0.85311843,\n         0.83007865, 0.58221531],\n        ...,\n        [0.66498216, 0.85115548, 1.55624145, ..., 0.84190874,\n         0.84255311, 0.92859606],\n        [0.67333019, 0.82537763, 1.546271  , ..., 0.88271328,\n         0.97106623, 0.93680019],\n        [0.66692557, 0.85914168, 1.55441355, ..., 0.88582965,\n         0.88886908, 0.78072028]]])loss_sd(chain, draw)float640.03186 0.02677 ... 0.0332 0.02944array([[0.03186241, 0.02677491, 0.03494346, ..., 0.02950565, 0.03100382,\n        0.0271407 ],\n       [0.02958521, 0.03073347, 0.03223621, ..., 0.03685306, 0.03319626,\n        0.02944148]])omega(chain, draw)float642.047 2.063 2.117 ... 1.942 1.925array([[2.04684299, 2.06348333, 2.11704502, ..., 2.02350399, 1.90782721,\n        2.02067407],\n       [2.10779663, 1.94099992, 2.01335649, ..., 1.92760786, 1.94155975,\n        1.92504071]])theta(chain, draw)float641.73 1.75 1.699 ... 1.803 1.801array([[1.72969432, 1.75027019, 1.6993544 , ..., 1.82016814, 1.74772066,\n        1.78329374],\n       [1.76581379, 1.75222828, 1.76108597, ..., 1.80953151, 1.80345161,\n        1.80096094]])gf(chain, draw, t_values)float640.2457 0.5738 ... 0.9568 0.9644array([[[0.24572263, 0.57375952, 0.75530888, ..., 0.95830295,\n         0.96694015, 0.97318136],\n        [0.23956308, 0.56837426, 0.75248162, ..., 0.9583462 ,\n         0.96703728, 0.97330626],\n        [0.24553678, 0.58537157, 0.76910278, ..., 0.9637262 ,\n         0.9715035 , 0.9770701 ],\n        ...,\n        [0.22935886, 0.54751893, 0.73323759, ..., 0.95238508,\n         0.96209965, 0.96915217],\n        [0.2563239 , 0.56395803, 0.73707071, ..., 0.94794775,\n         0.95798322, 0.96536932],\n        [0.23705689, 0.55767741, 0.74097726, ..., 0.95404246,\n         0.96342013, 0.97022592]],\n\n       [[0.23173901, 0.56524966, 0.75345663, ..., 0.96024765,\n         0.96871358, 0.9747871 ],\n        [0.25186615, 0.56382847, 0.73956758, ..., 0.95014575,\n         0.95992601, 0.96709377],\n        [0.24242684, 0.56368527, 0.74506788, ..., 0.95466309,\n         0.96388892, 0.97058815],\n        ...,\n        [0.24173286, 0.54807935, 0.72601378, ..., 0.94609631,\n         0.95656922, 0.96426648],\n        [0.24141408, 0.55004295, 0.72870832, ..., 0.94747034,\n         0.95775477, 0.96529882],\n        [0.24369253, 0.55027846, 0.72757083, ..., 0.94636716,\n         0.95677735, 0.96442982]]])lm(chain, draw, obs)float64157.6 368.0 ... 2.567e+04 1.084e+04array([[[  157.60741315,   368.01149881,   484.45793209, ...,\n          9713.72058482, 22681.4259549 , 13676.08451242],\n        [  155.55919791,   369.07124816,   488.62052768, ...,\n         10425.52435542, 24735.02909624, 10707.12910397],\n        [  152.93367252,   364.60127339,   479.03907065, ...,\n         10487.33492388, 25002.31377805, 15539.25209443],\n        ...,\n        [  151.74783701,   362.24811303,   485.12283192, ...,\n          9698.09893366, 23151.02546387, 10581.58893866],\n        [  165.07711826,   363.19893028,   474.68655257, ...,\n         10949.17187914, 24090.11955022, 13768.56860626],\n        [  156.09646279,   367.21763655,   487.91633048, ...,\n          9686.61005323, 22787.79407511, 11234.13198617]],\n\n       [[  151.0682531 ,   368.4803803 ,   491.1705532 , ...,\n          9768.92945857, 23828.02983572, 14170.24714991],\n        [  160.74842881,   359.85201422,   472.01390953, ...,\n         11021.31677114, 24672.35959206, 10157.1789823 ],\n        [  161.48314057,   375.47685914,   496.29778013, ...,\n         10560.92705732, 24556.02304463,  8042.1379405 ],\n        ...,\n        [  153.83587622,   348.79108035,   462.02640537, ...,\n         10688.95096964, 24234.98892586, 12789.97614652],\n        [  155.56168039,   354.43502325,   469.5628732 , ...,\n         12303.07167812, 28031.57876309, 12885.95974292],\n        [  155.53621549,   351.21400257,   464.37046077, ...,\n         11367.94919334, 25669.79609618, 10840.38943469]]])Attributes: (6)created_at :2021-07-10T22:10:08.100990arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2sampling_time :10.192897081375122tuning_steps :2000\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 2, cohort: 10, draw: 1000, obs: 55)\nCoordinates:\n  * chain    (chain) int64 0 1\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\n  * cohort   (cohort) int64 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\nData variables:\n    loss     (chain, draw, obs) float64 104.3 378.5 ... 2.644e+04 9.016e+03\n    LR       (chain, draw, cohort) float64 0.6702 0.8334 1.505 ... 0.8889 0.7807\n    lm       (chain, draw, obs) float64 157.6 368.0 ... 2.567e+04 1.084e+04\nAttributes:\n    created_at:                 2021-07-10T22:10:08.573587\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 2cohort: 10draw: 1000obs: 55Coordinates: (4)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])Data variables: (3)loss(chain, draw, obs)float64104.3 378.5 ... 2.644e+04 9.016e+03array([[[  104.25299878,   378.46062043,   519.61667247, ...,\n          8141.80268618, 21296.98149285, 13873.7213711 ],\n        [  168.57110391,   346.97788603,   520.63643116, ...,\n         10456.73879506, 25180.98640336,  9559.26030343],\n        [  109.58118079,   367.78282075,   464.86966193, ...,\n         11868.35118362, 22050.81611823, 19408.3032024 ],\n        ...,\n        [  106.27795502,   318.52680823,   510.43588738, ...,\n         11021.18578426, 22826.81417456, 12527.08543234],\n        [  171.64762377,   365.27633379,   457.72868241, ...,\n         12032.70208003, 22850.73831518, 12817.78476422],\n        [  138.78409136,   360.61093994,   480.48285626, ...,\n         10137.62921603, 20979.07259156, 13465.6251659 ]],\n\n       [[   97.16103159,   355.99598682,   512.81621348, ...,\n         11204.55137772, 23671.12813507, 12157.79410608],\n        [  131.26783254,   355.3390368 ,   460.1770658 , ...,\n         10682.5367999 , 23307.75834914,  8833.09949403],\n        [  194.26536002,   369.60172602,   492.15711635, ...,\n         10816.10853934, 23849.85945329,  5220.66454384],\n        ...,\n        [  103.44871872,   337.44440949,   461.41297666, ...,\n         10020.64412924, 24559.0585406 , 15059.02844165],\n        [  154.30464618,   327.38538816,   474.78895535, ...,\n         15633.51420392, 24639.37731399, 14192.66494558],\n        [  132.40295851,   395.31181651,   455.54853151, ...,\n         10602.40295703, 26440.182822  ,  9015.82177028]]])LR(chain, draw, cohort)float640.6702 0.8334 ... 0.8889 0.7807array([[[0.67022333, 0.83343362, 1.50476005, ..., 0.84746063,\n         0.75324862, 0.97680847],\n        [0.67852191, 0.85033978, 1.5128585 , ..., 0.8348323 ,\n         0.82923175, 0.78441512],\n        [0.65084057, 0.81301633, 1.50223526, ..., 0.86576549,\n         0.81385395, 1.1107245 ],\n        ...,\n        [0.69134537, 0.87360691, 1.53672948, ..., 0.88517673,\n         0.80569189, 0.80970752],\n        [0.67295471, 0.8505383 , 1.55445959, ..., 0.9003714 ,\n         0.81393564, 0.9427412 ],\n        [0.68806353, 0.85636222, 1.52987837, ..., 0.84520793,\n         0.77860489, 0.83172496]],\n\n       [[0.68118037, 0.84469149, 1.52453007, ..., 0.89498748,\n         0.80324074, 1.07317632],\n        [0.66690657, 0.85809314, 1.53552822, ..., 0.83513642,\n         0.8337994 , 0.70777644],\n        [0.69604056, 0.83549505, 1.52540707, ..., 0.85311843,\n         0.83007865, 0.58221531],\n        ...,\n        [0.66498216, 0.85115548, 1.55624145, ..., 0.84190874,\n         0.84255311, 0.92859606],\n        [0.67333019, 0.82537763, 1.546271  , ..., 0.88271328,\n         0.97106623, 0.93680019],\n        [0.66692557, 0.85914168, 1.55441355, ..., 0.88582965,\n         0.88886908, 0.78072028]]])lm(chain, draw, obs)float64157.6 368.0 ... 2.567e+04 1.084e+04array([[[  157.60741315,   368.01149881,   484.45793209, ...,\n          9713.72058482, 22681.4259549 , 13676.08451242],\n        [  155.55919791,   369.07124816,   488.62052768, ...,\n         10425.52435542, 24735.02909624, 10707.12910397],\n        [  152.93367252,   364.60127339,   479.03907065, ...,\n         10487.33492388, 25002.31377805, 15539.25209443],\n        ...,\n        [  151.74783701,   362.24811303,   485.12283192, ...,\n          9698.09893366, 23151.02546387, 10581.58893866],\n        [  165.07711826,   363.19893028,   474.68655257, ...,\n         10949.17187914, 24090.11955022, 13768.56860626],\n        [  156.09646279,   367.21763655,   487.91633048, ...,\n          9686.61005323, 22787.79407511, 11234.13198617]],\n\n       [[  151.0682531 ,   368.4803803 ,   491.1705532 , ...,\n          9768.92945857, 23828.02983572, 14170.24714991],\n        [  160.74842881,   359.85201422,   472.01390953, ...,\n         11021.31677114, 24672.35959206, 10157.1789823 ],\n        [  161.48314057,   375.47685914,   496.29778013, ...,\n         10560.92705732, 24556.02304463,  8042.1379405 ],\n        ...,\n        [  153.83587622,   348.79108035,   462.02640537, ...,\n         10688.95096964, 24234.98892586, 12789.97614652],\n        [  155.56168039,   354.43502325,   469.5628732 , ...,\n         12303.07167812, 28031.57876309, 12885.95974292],\n        [  155.53621549,   351.21400257,   464.37046077, ...,\n         11367.94919334, 25669.79609618, 10840.38943469]]])Attributes: (4)created_at :2021-07-10T22:10:08.573587arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 2, draw: 1000, obs: 55)\nCoordinates:\n  * chain    (chain) int64 0 1\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    loss     (chain, draw, obs) float64 -4.662 -4.996 -5.873 ... -9.298 -8.718\nAttributes:\n    created_at:                 2021-07-10T22:10:08.569795\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 2draw: 1000obs: 55Coordinates: (3)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (1)loss(chain, draw, obs)float64-4.662 -4.996 ... -9.298 -8.718array([[[ -4.66204085,  -4.99560314,  -5.8731991 , ...,  -9.83460231,\n          -8.44534821,  -8.71364938],\n        [ -4.55001055,  -5.15331207,  -6.69084417, ...,  -9.36860769,\n          -8.58761574,  -8.78868814],\n        [ -4.6063776 ,  -4.87522004,  -5.46053709, ...,  -9.09921933,\n          -8.79322533,  -9.84534805],\n        ...,\n        [ -4.47998193,  -4.79602114,  -6.09651578, ..., -10.0247998 ,\n          -8.28211525,  -8.86372827],\n        [ -4.89349193,  -4.82706018,  -5.39305197, ...,  -8.8306795 ,\n          -8.39185554,  -8.74504146],\n        [ -4.57138313,  -5.04378999,  -6.57694207, ..., -10.27797608,\n          -8.28719365,  -8.49661696]],\n\n       [[ -4.46588533,  -5.04744865,  -6.52047369, ...,  -9.93369734,\n          -8.29693424,  -8.96962124],\n        [ -4.74537964,  -4.71708996,  -5.27260529, ...,  -8.7861692 ,\n          -8.59379556,  -9.13006987],\n        [ -4.7742971 ,  -5.29597798,  -6.5881131 , ...,  -9.0816783 ,\n          -8.56772943, -11.11150275],\n        ...,\n        [ -4.65643627,  -4.58216115,  -4.86888165, ...,  -8.97653424,\n          -8.5695356 ,  -8.59666319],\n        [ -4.62960387,  -4.60504662,  -5.11415089, ...,  -8.396745  ,\n         -11.84581071,  -8.51334596],\n        [ -4.5772725 ,  -4.46633895,  -4.95876778, ...,  -8.58177161,\n          -9.29750017,  -8.71840896]]])Attributes: (4)created_at :2021-07-10T22:10:08.569795arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:           (chain: 2, draw: 1000)\nCoordinates:\n  * chain             (chain) int64 0 1\n  * draw              (draw) int64 0 1 2 3 4 5 6 ... 993 994 995 996 997 998 999\nData variables:\n    depth             (chain, draw) int64 3 3 3 3 3 3 3 3 4 ... 4 3 3 4 3 3 3 3\n    step_size         (chain, draw) float64 0.3709 0.3709 ... 0.3642 0.3642\n    max_energy_error  (chain, draw) float64 -0.3033 -0.9073 ... 0.1527 0.2162\n    mean_tree_accept  (chain, draw) float64 0.9878 1.0 0.6001 ... 0.9535 0.9023\n    tree_size         (chain, draw) float64 7.0 7.0 7.0 7.0 ... 7.0 7.0 7.0 7.0\n    step_size_bar     (chain, draw) float64 0.4186 0.4186 ... 0.4023 0.4023\n    diverging         (chain, draw) bool False False False ... False False False\n    energy            (chain, draw) float64 418.4 411.7 410.1 ... 411.3 415.6\n    lp                (chain, draw) float64 -409.4 -398.4 ... -405.5 -405.0\n    energy_error      (chain, draw) float64 -0.08659 -0.9073 ... 0.05396 0.01052\nAttributes:\n    created_at:                 2021-07-10T22:10:08.109313\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2\n    sampling_time:              10.192897081375122\n    tuning_steps:               2000xarray.DatasetDimensions:chain: 2draw: 1000Coordinates: (2)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (10)depth(chain, draw)int643 3 3 3 3 3 3 3 ... 4 3 3 4 3 3 3 3array([[3, 3, 3, ..., 3, 3, 3],\n       [3, 4, 3, ..., 3, 3, 3]])step_size(chain, draw)float640.3709 0.3709 ... 0.3642 0.3642array([[0.37085185, 0.37085185, 0.37085185, ..., 0.37085185, 0.37085185,\n        0.37085185],\n       [0.36415012, 0.36415012, 0.36415012, ..., 0.36415012, 0.36415012,\n        0.36415012]])max_energy_error(chain, draw)float64-0.3033 -0.9073 ... 0.1527 0.2162array([[-0.30333558, -0.90728229,  0.69568241, ..., -0.31374491,\n        -0.2200231 , -0.40118408],\n       [ 1.42927712,  0.33138796,  0.36049719, ...,  0.18928538,\n         0.15266867,  0.21622557]])mean_tree_accept(chain, draw)float640.9878 1.0 0.6001 ... 0.9535 0.9023array([[0.98779061, 1.        , 0.60011115, ..., 0.93739839, 0.98545547,\n        0.9950132 ],\n       [0.43950928, 0.89151176, 0.84991465, ..., 0.90923646, 0.95350635,\n        0.90231548]])tree_size(chain, draw)float647.0 7.0 7.0 7.0 ... 7.0 7.0 7.0 7.0array([[ 7.,  7.,  7., ...,  7.,  7.,  7.],\n       [ 7., 15.,  7., ...,  7.,  7.,  7.]])step_size_bar(chain, draw)float640.4186 0.4186 ... 0.4023 0.4023array([[0.41864123, 0.41864123, 0.41864123, ..., 0.41864123, 0.41864123,\n        0.41864123],\n       [0.40234341, 0.40234341, 0.40234341, ..., 0.40234341, 0.40234341,\n        0.40234341]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])energy(chain, draw)float64418.4 411.7 410.1 ... 411.3 415.6array([[418.43883273, 411.65728711, 410.0767806 , ..., 410.51405687,\n        408.58036529, 407.35921943],\n       [412.80822536, 411.47900652, 413.17891412, ..., 411.18710152,\n        411.29444508, 415.58466124]])lp(chain, draw)float64-409.4 -398.4 ... -405.5 -405.0array([[-409.43533657, -398.44740163, -404.77855576, ..., -402.57284061,\n        -403.52737457, -400.50675223],\n       [-401.88597959, -402.44777691, -407.25696595, ..., -404.72076691,\n        -405.45093988, -404.98509764]])energy_error(chain, draw)float64-0.08659 -0.9073 ... 0.01052array([[-0.08658624, -0.90728229,  0.52606004, ..., -0.05300835,\n         0.02450066, -0.40118408],\n       [ 0.38322751,  0.00231922,  0.16376489, ...,  0.12195152,\n         0.05395713,  0.01051789]])Attributes: (6)created_at :2021-07-10T22:10:08.109313arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2sampling_time :10.192897081375122tuning_steps :2000\n                      \n                  \n            \n            \n            \n                  \n                  prior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:         (LR_log___dim_0: 10, chain: 1, cohort: 10, draw: 100, obs: 55, t_values: 10)\nCoordinates:\n  * chain           (chain) int64 0\n  * draw            (draw) int64 0 1 2 3 4 5 6 7 8 ... 92 93 94 95 96 97 98 99\n  * cohort          (cohort) int64 1988 1989 1990 1991 ... 1994 1995 1996 1997\n  * t_values        (t_values) int64 1 2 3 4 5 6 7 8 9 10\n  * LR_log___dim_0  (LR_log___dim_0) int64 0 1 2 3 4 5 6 7 8 9\n  * obs             (obs) int64 0 1 2 3 4 5 6 7 8 ... 46 47 48 49 50 51 52 53 54\nData variables:\n    LR              (chain, draw, cohort) float64 0.5395 0.2832 ... 3.739\n    omega_log__     (chain, draw) float64 0.4461 0.07193 ... 0.1054 -0.7762\n    sd_LR_log__     (chain, draw) float64 -0.8523 -0.5681 -1.487 ... -0.3 0.7881\n    mu_LR           (chain, draw) float64 -0.8749 0.1713 ... -0.09251 -1.244\n    loss_sd_log__   (chain, draw) float64 0.1178 -0.06791 ... -0.3026 -1.538\n    gf              (chain, draw, t_values) float64 0.7042 0.8755 ... 0.7399\n    theta           (chain, draw) float64 0.5739 1.253 3.127 ... 1.24 1.031\n    sd_LR           (chain, draw) float64 0.4264 0.5666 0.2261 ... 0.7408 2.199\n    loss_sd         (chain, draw) float64 1.125 0.9343 0.4603 ... 0.7389 0.2147\n    LR_log__        (chain, draw, LR_log___dim_0) float64 -0.6171 ... 1.319\n    omega           (chain, draw) float64 1.562 1.075 1.892 ... 1.111 0.4602\n    theta_log__     (chain, draw) float64 -0.5553 0.2252 1.14 ... 0.2152 0.0308\n    lm              (chain, draw, obs) float64 363.6 452.0 ... 1.058e+05\nAttributes:\n    created_at:                 2021-07-10T22:10:08.582306\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:LR_log___dim_0: 10chain: 1cohort: 10draw: 100obs: 55t_values: 10Coordinates: (6)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 6 ... 94 95 96 97 98 99array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])t_values(t_values)int641 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])LR_log___dim_0(LR_log___dim_0)int640 1 2 3 4 5 6 7 8 9array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (13)LR(chain, draw, cohort)float640.5395 0.2832 ... 0.04612 3.739array([[[5.39483441e-01, 2.83183398e-01, 5.36638317e-01, 3.46020707e-01,\n         4.35402974e-01, 7.28349146e-01, 7.53385166e-01, 1.98912181e-01,\n         4.71464140e-01, 3.60783849e-01],\n        [1.60021652e+00, 9.60084765e-01, 1.55527582e+00, 5.27957240e-01,\n         1.49531975e+00, 7.42338495e-01, 1.50872389e+00, 1.08432781e+00,\n         2.39771704e+00, 3.04613240e+00],\n        [1.53240102e+00, 3.89658259e+00, 1.94455432e+00, 1.91653136e+00,\n         1.65883222e+00, 1.28964188e+00, 1.54529885e+00, 1.73610649e+00,\n         2.20782512e+00, 1.42497303e+00],\n        [2.89626892e-01, 6.89615568e-01, 4.81269113e-01, 3.24477413e-01,\n         9.64152326e-01, 4.77542250e-01, 2.76833126e+00, 2.26235901e+00,\n         8.77298833e-02, 6.18636581e-01],\n        [1.41315957e+00, 4.78212851e+00, 9.39090229e-01, 1.28132155e+00,\n         4.49898074e+00, 4.54895891e+00, 3.03642390e+00, 5.25706928e-01,\n         6.26412548e+00, 1.39341149e+00],\n        [9.57696979e-01, 1.86763571e+00, 4.13374817e+00, 1.56821742e+00,\n         1.25487332e+00, 2.51191010e+00, 8.10945988e+00, 6.10722752e-01,\n         1.41395916e+00, 9.82281959e+00],\n        [1.71912712e+00, 1.49080130e-01, 4.37317875e+00, 6.93848752e-01,\n         2.87455238e-01, 2.20250288e+00, 8.11983450e-01, 7.26701892e-01,\n...\n         9.69395468e-01, 6.87721577e+01, 1.36484576e-01, 1.13037865e-01,\n         4.30151712e-02, 2.71561166e+02],\n        [3.40901443e+00, 2.08439147e+00, 6.84379805e+00, 9.55929917e+00,\n         1.77361679e+00, 1.82087509e+01, 4.04976821e+00, 1.13211119e+01,\n         1.86859175e+00, 1.89121247e+00],\n        [3.46365246e+00, 1.66684876e-01, 3.47023822e-01, 4.53995073e-02,\n         1.06780054e+00, 9.27681160e-01, 1.80283061e+00, 2.37063152e+00,\n         7.54626213e-01, 2.86942154e-01],\n        [5.03298776e-01, 7.39239018e-01, 1.35779223e+00, 1.00056116e+00,\n         8.86529488e-01, 3.21049079e+00, 7.97450632e-01, 3.99036318e-01,\n         4.30978753e-01, 6.98964209e-01],\n        [3.00069125e+00, 3.09937257e+00, 5.07217053e-01, 5.56775119e-01,\n         2.04611562e+00, 9.44072847e+00, 6.36453703e-01, 1.36052095e+00,\n         2.53801160e+00, 1.02432399e-01],\n        [7.13583274e-01, 1.27445474e+00, 6.15372182e-01, 7.40064303e-01,\n         5.65699214e-01, 3.94688604e-01, 3.85516160e-01, 1.03348754e+00,\n         8.72203161e-01, 2.06829310e+00],\n        [6.72598317e-02, 1.31074344e-01, 2.46574280e-01, 3.57594394e-01,\n         1.41602364e+01, 1.39274851e+00, 1.65655532e-01, 1.13027571e+01,\n         4.61181024e-02, 3.73917734e+00]]])omega_log__(chain, draw)float640.4461 0.07193 ... 0.1054 -0.7762array([[ 0.44612894,  0.0719305 ,  0.63779866, -0.23449358,  0.23706241,\n         0.33419761, -0.60235794, -0.42556604,  0.44956226,  0.46859577,\n         0.47679333, -1.10255173, -1.1406529 , -0.30581554,  1.40485057,\n         0.08792286, -0.3153778 ,  0.92777277,  0.08014537, -0.71142477,\n        -0.18382993, -0.94314418,  0.37741105,  0.89061229,  0.32631473,\n         0.1187519 ,  0.29710071, -0.56646717,  0.24871394, -0.12211726,\n         0.52034076, -0.13353552, -0.33864184, -0.33580391,  1.31280722,\n        -0.55840032,  0.46812679, -0.28309119, -0.25267346, -0.43320864,\n         0.69851556, -0.43096099, -0.05757817,  0.00526835, -0.25980818,\n        -0.12478563,  1.12182973, -0.2414983 , -1.18449009, -0.46079041,\n         0.75950166,  0.27624921, -0.34634081, -0.53000413,  0.73357778,\n        -0.08395659,  0.37989795, -0.10431087, -0.02411082, -0.0916873 ,\n         0.64151539,  0.34238158,  0.25391359,  0.20807299,  0.07147848,\n        -0.03081378, -0.15740337,  0.33052872, -0.54743424, -0.17367713,\n         0.6791806 , -0.14902945, -0.04004145,  0.24734065,  0.68666321,\n        -0.46438288, -0.5281403 ,  0.10112753, -0.33912545, -0.01100623,\n        -0.95018485, -0.51196821,  0.10753402, -0.2633491 , -0.26221749,\n         0.2735264 ,  0.05936596, -1.15266121,  0.35999557, -0.64225228,\n         0.67848939,  0.24234314, -0.12805771,  0.67731315,  1.03797529,\n         0.3334401 , -0.50685985,  0.09829753,  0.10543659, -0.77619435]])sd_LR_log__(chain, draw)float64-0.8523 -0.5681 ... -0.3 0.7881array([[-0.8523256 , -0.5681305 , -1.48665774,  0.01665864, -0.12444433,\n        -0.22508822,  0.0662139 ,  0.01110696,  0.15868399, -0.37620709,\n        -0.6481959 ,  0.04756972, -0.21185755, -0.59299178, -0.182731  ,\n        -0.63551152,  0.79308547,  0.34669533, -0.97904062, -0.06740066,\n        -0.77030801,  1.02335698, -0.69849967, -0.54858599, -0.11935643,\n        -0.71453345,  0.47450239, -0.00969879,  0.44729885,  0.37984656,\n        -0.74886019, -0.59694299,  0.64813129,  0.47613781, -0.60862707,\n        -0.07863258, -0.75379258,  0.05394207,  0.37352783,  0.21483822,\n        -0.70752146, -0.32037996,  0.38981315, -0.21906046,  1.03739658,\n        -0.17164884, -0.30831469,  0.38159182,  0.0964586 , -0.17422947,\n         1.14932697, -0.08260478,  0.23314968,  0.13499362, -0.15991552,\n        -0.5738708 ,  0.85181199, -0.36107539,  0.54684332, -0.11475888,\n        -0.00444933, -0.271599  ,  0.37653109, -0.80471945,  0.97163113,\n        -0.72371806,  0.06512423,  0.47468043, -1.00759436, -0.03977029,\n         0.15052473, -0.84244998,  0.1111954 , -0.34246087, -0.06310059,\n         0.99513682,  0.2614989 , -0.0081727 , -0.20790817, -0.67925147,\n        -0.25721495, -0.10803006,  0.21119011, -0.54702147,  0.61845394,\n        -0.11514234, -0.3522091 , -0.29568756,  0.36849758,  0.21793363,\n         0.88799679,  0.25653719,  0.58526349,  1.03885612, -0.22796101,\n         0.32458646, -0.08739078,  0.50863217, -0.29999152,  0.78808336]])mu_LR(chain, draw)float64-0.8749 0.1713 ... -0.09251 -1.244array([[-0.87488274,  0.1713402 ,  0.5765179 , -0.12621802,  0.49066039,\n         0.25710942,  0.11058983, -0.53502167, -0.09474792,  0.12750072,\n        -0.22901349,  0.21758174, -0.29179753,  0.40842354,  0.3363604 ,\n        -0.05220557, -0.26564019,  0.51486634, -0.21906781, -0.55915912,\n         0.80949083,  0.77080259, -0.12593957, -0.42121787,  0.09225935,\n         0.4685411 ,  0.36550017,  0.68077806, -0.16311903,  0.02783801,\n         0.1111998 , -0.7216085 , -0.37817615,  0.40822701,  0.37522238,\n        -0.22797346,  0.59481113, -0.84530841, -0.67819952, -0.61621726,\n        -0.27221958, -0.33408587,  0.00365728, -0.30646937,  0.64987404,\n        -0.86654781, -0.49165505,  0.17875388, -0.80678925,  0.73535693,\n        -0.5940088 , -0.2748731 , -0.47002308, -0.41396618,  0.05443173,\n         0.2539048 , -0.43111367,  0.62473487, -0.03980562, -0.44486574,\n        -0.44089919,  0.00931947,  0.11892231,  0.00677427, -0.8177647 ,\n        -0.52210494,  0.30651944,  0.36810261,  0.51346072, -0.71609531,\n        -0.92059415,  0.18304661, -0.16588857, -0.34460899,  1.01730378,\n        -0.27535721,  0.37522667, -0.65349617,  0.29028667, -0.55226155,\n         0.34506074,  0.34344503, -0.78334376,  0.45248706,  0.3894112 ,\n         0.21411644,  0.05443599,  0.01414182, -0.28941291, -0.5997256 ,\n        -0.852976  ,  0.18458198,  0.93828671, -0.18845168,  0.91596804,\n         0.00150872, -0.03801173,  0.0019788 , -0.09250706, -1.24357577]])loss_sd_log__(chain, draw)float640.1178 -0.06791 ... -0.3026 -1.538array([[ 0.11776897, -0.06791383, -0.77586015,  0.81418801, -1.54295884,\n        -0.08635401, -0.3073733 ,  0.44099133, -0.60868549, -0.42565676,\n         0.83953297,  1.90751457,  0.31553409, -0.58763441, -0.02813683,\n         0.61956292,  1.1102456 ,  1.41804546,  1.07121072,  1.2829134 ,\n         0.67823319,  0.11953965,  0.49559607,  0.91112716,  0.26419822,\n        -0.55153079,  0.64974865,  1.42332632,  0.40688404,  1.0925064 ,\n         0.0515981 , -0.0156497 ,  0.47990747, -0.15405117, -1.44822335,\n         0.44352918, -1.63864786, -0.29389226,  0.71212342, -0.27237528,\n        -0.51608351, -0.00461874,  0.19355207, -0.40968742,  1.59689048,\n         0.80746392,  0.90807538, -0.15603457,  0.24094942, -1.39879746,\n         0.38607813, -0.58429016, -0.56743807,  0.07267915, -0.20197823,\n        -0.65064838,  0.31415215, -0.38924771,  0.91696663, -0.48047711,\n        -0.79327309, -0.11503221,  0.74234383, -0.37075465, -0.29457163,\n        -1.81513457,  0.20137675,  0.56713415,  0.07135635, -0.19635674,\n        -0.4044409 , -0.17024778, -1.04476412,  0.16943208, -0.98912851,\n        -0.66173423,  1.065507  ,  1.40047042, -0.45946529, -0.33103924,\n         1.22237942,  1.01016131, -0.64361443,  0.76310702,  0.76479435,\n        -0.22690514,  0.57659883,  1.36853881, -0.45579136,  0.67822079,\n         0.29416748, -1.19540042, -0.00414685, -0.5183479 ,  0.94649573,\n        -0.45149198, -0.10859997, -0.04805518, -0.302624  , -1.53838435]])gf(chain, draw, t_values)float640.7042 0.8755 ... 0.7304 0.7399array([[[0.70423484, 0.87549214, 0.92981535, 0.95405553, 0.9671339 ,\n         0.97507705, 0.98030598, 0.98395463, 0.98661528, 0.98862353],\n        [0.4397886 , 0.62312239, 0.71880211, 0.77689541, 0.81569534,\n         0.84335026, 0.86401169, 0.88000818, 0.89274339, 0.90311221],\n        [0.10362979, 0.30029807, 0.48035688, 0.61438231, 0.70848302,\n         0.77434966, 0.8212351 , 0.85537809, 0.88082887, 0.90022056],\n        [0.33863225, 0.46975303, 0.54972892, 0.60518664, 0.64648496,\n         0.67870678, 0.70469655, 0.7261912 , 0.74432036, 0.75985477],\n        [0.36346583, 0.57889167, 0.69681155, 0.76795562, 0.81451854,\n         0.84693205, 0.87058348, 0.88848725, 0.90244344, 0.91358541],\n        [0.59292719, 0.79319304, 0.87109128, 0.90990553, 0.93240122,\n         0.94679063, 0.95665138, 0.96376016, 0.96908782, 0.97320487],\n        [0.52575673, 0.6183686 , 0.66921337, 0.70310774, 0.7279638 ,\n         0.74727678, 0.76288122, 0.77585146, 0.78686662, 0.79638106],\n        [0.53525654, 0.64431972, 0.70247126, 0.74021092, 0.76725344,\n         0.78784695, 0.80419428, 0.8175696 , 0.82876883, 0.83831844],\n        [0.22462027, 0.46198695, 0.61852007, 0.71793657, 0.78313921,\n         0.82776462, 0.85954517, 0.88296832, 0.90074093, 0.91456169],\n        [0.79345344, 0.92080566, 0.95694122, 0.9723696 , 0.98049424,\n         0.98535136, 0.98851254, 0.99069905, 0.99228223, 0.99347016],\n...\n        [0.64181514, 0.87537833, 0.93982881, 0.96495661, 0.97714108,\n         0.98393041, 0.98809055, 0.99082103, 0.99270871, 0.99406781],\n        [0.39713443, 0.61439167, 0.72759741, 0.79397269, 0.83663171,\n         0.86595956, 0.88716659, 0.90311019, 0.9154717 , 0.92529784],\n        [0.48960751, 0.63836083, 0.71605695, 0.76460411, 0.798092  ,\n         0.82271025, 0.84163459, 0.85667234, 0.86893193, 0.87913289],\n        [0.23779221, 0.54975842, 0.73064268, 0.82695681, 0.88116276,\n         0.91391499, 0.93498184, 0.94924842, 0.95932411, 0.96668948],\n        [0.57758201, 0.90635774, 0.96816329, 0.98561457, 0.99228692,\n         0.99537608, 0.99700294, 0.99794238, 0.99852365, 0.99890312],\n        [0.63229895, 0.81899602, 0.88849829, 0.92251565, 0.94205031,\n         0.95447765, 0.96296374, 0.96906659, 0.97363229, 0.97715586],\n        [0.49575789, 0.5988251 , 0.65584172, 0.69383563, 0.72162392,\n         0.74314104, 0.76046247, 0.7748066 , 0.78694441, 0.79739179],\n        [0.45017679, 0.63755935, 0.7334373 , 0.79076283, 0.82859779,\n         0.85531065, 0.87511355, 0.89034527, 0.90240364, 0.91217334],\n        [0.4405067 , 0.62974341, 0.72743891, 0.78606014, 0.82481181,\n         0.85219095, 0.87249273, 0.88810812, 0.90046845, 0.91048055],\n        [0.49645643, 0.57561057, 0.62042586, 0.65106736, 0.67401798,\n         0.69217567, 0.70707985, 0.71964241, 0.73044606, 0.73988461]]])theta(chain, draw)float640.5739 1.253 3.127 ... 1.24 1.031array([[0.57389012, 1.25260178, 3.12725504, 2.33099523, 1.5559494 ,\n        0.7639599 , 0.82833537, 0.80557966, 2.20412348, 0.43069581,\n        2.9093212 , 0.99399548, 0.86455246, 1.32475578, 1.44164061,\n        1.10965243, 1.07620636, 1.29017089, 2.16036625, 2.32360565,\n        0.55518025, 0.90958767, 1.01729211, 0.84623772, 0.82608081,\n        1.62644821, 1.1668848 , 1.25656072, 0.83539401, 0.65453681,\n        1.83275323, 0.46182585, 1.40712532, 1.19493509, 0.94248559,\n        0.93866154, 0.83764564, 2.25087097, 1.27622612, 0.57064121,\n        0.52945567, 0.88550147, 0.75327822, 1.37076844, 0.93745489,\n        1.51501646, 0.62072393, 1.25873035, 2.29427686, 0.68248132,\n        1.06593879, 0.66061023, 0.77861607, 1.41031485, 2.15389213,\n        0.5953411 , 0.56525208, 1.340487  , 0.69754309, 1.16981863,\n        1.33030513, 1.57734588, 0.8597188 , 0.35135396, 0.82764502,\n        0.6132911 , 0.7778497 , 1.74519489, 0.71525789, 0.83759542,\n        2.2658658 , 4.32680955, 1.6694292 , 1.51162302, 0.54751728,\n        1.73666316, 1.22802165, 0.55897645, 0.70077877, 0.67783987,\n        0.70416641, 1.29484782, 0.81285727, 0.7551851 , 1.18750981,\n        2.39590503, 1.30989887, 0.78067317, 1.26049998, 1.12601871,\n        0.74383829, 1.38761336, 1.04839036, 1.80706916, 0.89511397,\n        0.67815156, 1.02856998, 1.19869844, 1.24008379, 1.03128319]])sd_LR(chain, draw)float640.4264 0.5666 ... 0.7408 2.199array([[0.42642209, 0.56658368, 0.22612717, 1.01679817, 0.88298741,\n        0.79844578, 1.06845524, 1.01116888, 1.17196753, 0.68646016,\n        0.52298845, 1.04871932, 0.80907994, 0.55267134, 0.8329922 ,\n        0.52966448, 2.21020544, 1.41438574, 0.37567134, 0.93482058,\n        0.46287048, 2.78251998, 0.4973309 , 0.5777662 , 0.88749141,\n        0.4894204 , 1.60721423, 0.99034809, 1.56408166, 1.46206023,\n        0.47290527, 0.55049193, 1.91196459, 1.60984486, 0.54409737,\n        0.92437949, 0.47057846, 1.05542345, 1.45285099, 1.23966133,\n        0.49286427, 0.72587318, 1.47670485, 0.80327315, 2.82186097,\n        0.84227489, 0.73468409, 1.46461414, 1.10126398, 0.8401041 ,\n        3.15606807, 0.92071496, 1.26257045, 1.14452948, 0.85221578,\n        0.56334063, 2.34389012, 0.69692646, 1.72779033, 0.8915811 ,\n        0.99556055, 0.76215982, 1.45722085, 0.44721338, 2.64225081,\n        0.48494584, 1.0672916 , 1.60750041, 0.36509621, 0.96101016,\n        1.16244405, 0.43065414, 1.11761327, 0.7100209 , 0.93884903,\n        2.70509444, 1.29887552, 0.9918606 , 0.81228163, 0.50699635,\n        0.773202  , 0.89760061, 1.23514715, 0.57867084, 1.85605625,\n        0.89123928, 0.70313308, 0.74401985, 1.44556115, 1.24350453,\n        2.43025646, 1.29244683, 1.79546401, 2.82598257, 0.7961553 ,\n        1.38345842, 0.91631895, 1.66301492, 0.7408245 , 2.19917736]])loss_sd(chain, draw)float641.125 0.9343 ... 0.7389 0.2147array([[1.12498418, 0.93434098, 0.46030768, 2.257342  , 0.21374772,\n        0.91726945, 0.73537603, 1.55424723, 0.54406558, 0.65334056,\n        2.31528541, 6.73632528, 1.37099135, 0.55564015, 0.97225532,\n        1.85811573, 3.03510372, 4.12904219, 2.91891135, 3.60713346,\n        1.97039335, 1.12697793, 1.64147638, 2.48712433, 1.30238633,\n        0.5760673 , 1.91505942, 4.15090475, 1.5021299 , 2.98173813,\n        1.05295248, 0.98447212, 1.61592487, 0.85722816, 0.23498741,\n        1.55819668, 0.19424251, 0.7453568 , 2.03831486, 0.76156841,\n        0.59685355, 0.99539191, 1.21355257, 0.66385773, 4.93765477,\n        2.24221434, 2.47954574, 0.85552962, 1.27245668, 0.24689368,\n        1.47119961, 0.55750146, 0.56697613, 1.07538545, 0.81711272,\n        0.5217074 , 1.36909803, 0.67756641, 2.50169031, 0.61848824,\n        0.45236175, 0.89133743, 2.10085379, 0.69021326, 0.74485059,\n        0.162816  , 1.22308549, 1.76320672, 1.07396387, 0.82171904,\n        0.66734982, 0.8434558 , 0.35177479, 1.18463189, 0.37190066,\n        0.51595577, 2.90231009, 4.05710805, 0.63162129, 0.71817699,\n        3.39525688, 2.74604394, 0.52539   , 2.14493023, 2.14855249,\n        0.79699638, 1.77997412, 3.9296046 , 0.63394609, 1.97036892,\n        1.34200864, 0.30258277, 0.99586174, 0.59550356, 2.57666449,\n        0.63667753, 0.89708921, 0.9530812 , 0.73887686, 0.21472775]])LR_log__(chain, draw, LR_log___dim_0)float64-0.6171 -1.262 ... -3.077 1.319array([[[-6.17143188e-01, -1.26166054e+00, -6.22430937e-01,\n         -1.06125666e+00, -8.31483299e-01, -3.16974749e-01,\n         -2.83178673e-01, -1.61489186e+00, -7.51912234e-01,\n         -1.01947626e+00],\n        [ 4.70138943e-01, -4.07337019e-02,  4.41652909e-01,\n         -6.38739983e-01,  4.02340063e-01, -2.97949948e-01,\n          4.11264185e-01,  8.09602667e-02,  8.74517052e-01,\n          1.11387272e+00],\n        [ 4.26835799e-01,  1.36009991e+00,  6.65032809e-01,\n          6.50516967e-01,  5.06113871e-01,  2.54364564e-01,\n          4.35217322e-01,  5.51644954e-01,  7.92007920e-01,\n          3.54152889e-01],\n        [-1.23916176e+00, -3.71620985e-01, -7.31328680e-01,\n         -1.12553935e+00, -3.65059826e-02, -7.39102641e-01,\n          1.01824470e+00,  8.16408078e-01, -2.43349269e+00,\n         -4.80237285e-01],\n        [ 3.45828028e-01,  1.56488574e+00, -6.28437141e-02,\n          2.47892004e-01,  1.50385087e+00,  1.51489840e+00,\n          1.11068047e+00, -6.43011394e-01,  1.83483899e+00,\n          3.31755049e-01],\n...\n        [ 1.24232366e+00, -1.79165022e+00, -1.05836185e+00,\n         -3.09225403e+00,  6.56009661e-02, -7.50671829e-02,\n          5.89357991e-01,  8.63156384e-01, -2.81532734e-01,\n         -1.24847464e+00],\n        [-6.86571297e-01, -3.02133976e-01,  3.05860017e-01,\n          5.61005282e-04, -1.20440890e-01,  1.16642382e+00,\n         -2.26335350e-01, -9.18702845e-01, -8.41696486e-01,\n         -3.58155741e-01],\n        [ 1.09884268e+00,  1.13119969e+00, -6.78816255e-01,\n         -5.85593856e-01,  7.15943176e-01,  2.24503315e+00,\n         -4.51843600e-01,  3.07867675e-01,  9.31380940e-01,\n         -2.27855222e+00],\n        [-3.37456137e-01,  2.42518433e-01, -4.85528021e-01,\n         -3.01018200e-01, -5.69692766e-01, -9.29658170e-01,\n         -9.53172168e-01,  3.29390486e-02, -1.36732900e-01,\n          7.26723678e-01],\n        [-2.69919207e+00, -2.03199061e+00, -1.40009199e+00,\n         -1.02835591e+00,  2.65043778e+00,  3.31279141e-01,\n         -1.79784476e+00,  2.42504669e+00, -3.07654973e+00,\n          1.31886562e+00]]])omega(chain, draw)float641.562 1.075 1.892 ... 1.111 0.4602array([[1.56225289, 1.07458066, 1.89231067, 0.79097131, 1.26752023,\n        1.39681914, 0.5475191 , 0.65339983, 1.56762583, 1.59774901,\n        1.61090048, 0.33202277, 0.31961028, 0.73652247, 4.07491779,\n        1.09190388, 0.72951322, 2.52887052, 1.08344456, 0.49094421,\n        0.83207731, 0.38940156, 1.4585037 , 2.43662111, 1.38585147,\n        1.1260905 , 1.34595085, 0.56752688, 1.28237514, 0.88504458,\n        1.68260091, 0.87499639, 0.71273768, 0.71476325, 3.71659238,\n        0.57212355, 1.59699987, 0.75345108, 0.77672147, 0.64842519,\n        2.01076562, 0.64988426, 0.94404809, 1.00528225, 0.7711995 ,\n        0.8826861 , 3.07046718, 0.78545014, 0.30590212, 0.63078487,\n        2.13721089, 1.31817633, 0.7072714 , 0.58860254, 2.08251808,\n        0.91947117, 1.46213538, 0.90094518, 0.97617753, 0.91239041,\n        1.89935696, 1.40829757, 1.28906041, 1.23130304, 1.07409503,\n        0.96965612, 0.85435936, 1.39170375, 0.57843202, 0.84056825,\n        1.97226099, 0.86154374, 0.96074961, 1.28061528, 1.987074  ,\n        0.62852286, 0.58970061, 1.10641774, 0.71239307, 0.98905411,\n        0.38666954, 0.59931484, 1.11352875, 0.76847358, 0.76934368,\n        1.31459206, 1.06116352, 0.31579525, 1.43332306, 0.52610615,\n        1.97089823, 1.27423136, 0.87980261, 1.96858134, 2.82349447,\n        1.39576144, 0.60238419, 1.103291  , 1.11119564, 0.46015387]])theta_log__(chain, draw)float64-0.5553 0.2252 ... 0.2152 0.0308array([[-0.55531734,  0.22522281,  1.14015564,  0.84629531,  0.44208591,\n        -0.26923997, -0.18833717, -0.21619318,  0.79032991, -0.84235321,\n         1.06791979, -0.00602262, -0.1455433 ,  0.28122813,  0.36578178,\n         0.10404684,  0.07344223,  0.25477469,  0.77027777,  0.84312014,\n        -0.58846245, -0.09476389,  0.0171443 , -0.16695497, -0.19106268,\n         0.48639862,  0.15433763,  0.2283784 , -0.17985179, -0.42382746,\n         0.60581933, -0.7725674 ,  0.34154884,  0.17809187, -0.05923465,\n        -0.06330032, -0.17716014,  0.81131724,  0.24390738, -0.56099462,\n        -0.63590583, -0.12160116, -0.28332064,  0.31537149, -0.06458664,\n         0.4154263 , -0.47686886,  0.23010355,  0.8304177 , -0.38202012,\n         0.06385591, -0.41459128, -0.25023721,  0.34381298,  0.7672765 ,\n        -0.51862076, -0.57048348,  0.29303298, -0.360191  ,  0.15684872,\n         0.28540834,  0.45574361, -0.15114992, -1.04596112, -0.18917094,\n        -0.48891559, -0.25122197,  0.55686623, -0.33511211, -0.17722009,\n         0.81795694,  1.46483045,  0.51248177,  0.41318392, -0.60236126,\n         0.55196555,  0.20540446, -0.58164794, -0.35556303, -0.3888442 ,\n        -0.35074057,  0.25839318, -0.20719975, -0.2807924 ,  0.17185852,\n         0.87376104,  0.26994994, -0.24759869,  0.23150845,  0.11868815,\n        -0.29593161,  0.32758526,  0.047256  ,  0.59170629, -0.11080422,\n        -0.38838448,  0.02816947,  0.18123634,  0.21517895,  0.03080384]])lm(chain, draw, obs)float64363.6 452.0 ... 1.393e+03 1.058e+05array([[[3.63586344e+02, 4.52004030e+02, 4.80050323e+02, ...,\n         1.74248189e+04, 2.16622228e+04, 1.44767740e+04],\n        [6.73495425e+02, 9.54254126e+02, 1.10077873e+03, ...,\n         5.53406168e+04, 7.84103498e+04, 7.63308220e+04],\n        [1.51973891e+02, 4.40389457e+02, 7.04447107e+02, ...,\n         1.20074665e+04, 3.47951982e+04, 8.41392159e+03],\n        ...,\n        [1.29275537e+03, 1.83085465e+03, 2.10618368e+03, ...,\n         5.99623720e+04, 8.49212392e+04, 2.62740876e+03],\n        [3.00821669e+02, 4.30051269e+02, 4.96767448e+02, ...,\n         2.01637950e+04, 2.88259343e+04, 5.19124828e+04],\n        [3.19557383e+01, 3.70507050e+01, 3.99353603e+01, ...,\n         1.20158548e+03, 1.39316416e+03, 1.05770463e+05]]])Attributes: (4)created_at :2021-07-10T22:10:08.582306arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  prior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 1, draw: 100, obs: 55)\nCoordinates:\n  * chain    (chain) int64 0\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 9 ... 90 91 92 93 94 95 96 97 98 99\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    loss     (chain, draw, obs) float64 -2.671e+03 1.343e+03 ... 9.672e+04\nAttributes:\n    created_at:                 2021-07-10T22:10:08.587369\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 1draw: 100obs: 55Coordinates: (3)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 6 ... 94 95 96 97 98 99array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (1)loss(chain, draw, obs)float64-2.671e+03 1.343e+03 ... 9.672e+04array([[[-2.67100326e+03,  1.34255155e+03,  1.40821615e+03, ...,\n          5.44546338e+04, -2.82037218e+04,  2.25204026e+04],\n        [ 6.91120357e+02,  1.25047752e+03,  2.34623536e+03, ...,\n          3.67907448e+04,  1.27985416e+05,  4.26089822e+04],\n        [-1.05994085e+02,  7.82732937e+02,  3.65909060e+02, ...,\n         -1.07724563e+04,  2.61310680e+04,  2.36614675e+04],\n        ...,\n        [ 8.83043472e+02,  2.15344855e+03,  3.03744893e+03, ...,\n          4.12138121e+04, -2.16126888e+04,  5.72089335e+03],\n        [ 1.13652047e+03,  1.26135401e+03,  1.58322237e+03, ...,\n         -4.63703410e+04,  6.47498746e+04,  1.04504263e+05],\n        [ 1.67803663e+02,  2.66545568e+02, -9.96214671e+01, ...,\n         -2.17424973e+04, -9.03439475e+03,  9.67249413e+04]]])Attributes: (4)created_at :2021-07-10T22:10:08.587369arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (obs: 55)\nCoordinates:\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    loss     (obs) int32 133 333 431 570 615 ... 26012 31677 12604 23446 12292\nAttributes:\n    created_at:                 2021-07-10T22:10:08.588339\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:obs: 55Coordinates: (1)obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (1)loss(obs)int32133 333 431 ... 12604 23446 12292array([  133,   333,   431,   570,   615,   615,   615,   614,   614,\n         614,   934,  1746,  2365,  2579,  2763,  2966,  2940,  2978,\n        2978,  2030,  4864,  6880,  8087,  8595,  8743,  8763,  8762,\n        4537, 11527, 15123, 16656, 17321, 18076, 18308,  7564, 16061,\n       22465, 25204, 26517, 27124,  8343, 19900, 26732, 30079, 31249,\n       12565, 26922, 33867, 38338, 13437, 26012, 31677, 12604, 23446,\n       12292], dtype=int32)Attributes: (4)created_at :2021-07-10T22:10:08.588339arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  constant_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:   (cohort: 10, obs: 55, t_values: 10)\nCoordinates:\n  * t_values  (t_values) int64 1 2 3 4 5 6 7 8 9 10\n  * cohort    (cohort) int64 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\n  * obs       (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    t         (t_values) int32 1 2 3 4 5 6 7 8 9 10\n    premium   (cohort) int32 957 3695 6138 17533 ... 46095 51512 52481 56978\n    t_idx     (obs) int32 0 1 2 3 4 5 6 7 8 9 0 1 2 ... 3 4 0 1 2 3 0 1 2 0 1 0\n    c_idx     (obs) int32 0 0 0 0 0 0 0 0 0 0 1 1 1 ... 5 5 6 6 6 6 7 7 7 8 8 9\nAttributes:\n    created_at:                 2021-07-10T22:10:08.590166\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:cohort: 10obs: 55t_values: 10Coordinates: (3)t_values(t_values)int641 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (4)t(t_values)int321 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=int32)premium(cohort)int32957 3695 6138 ... 51512 52481 56978array([  957,  3695,  6138, 17533, 29341, 37194, 46095, 51512, 52481,\n       56978], dtype=int32)t_idx(obs)int320 1 2 3 4 5 6 7 ... 2 3 0 1 2 0 1 0array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2,\n       3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3,\n       4, 0, 1, 2, 3, 0, 1, 2, 0, 1, 0], dtype=int32)c_idx(obs)int320 0 0 0 0 0 0 0 ... 6 6 7 7 7 8 8 9array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2,\n       2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5,\n       5, 6, 6, 6, 6, 7, 7, 7, 8, 8, 9], dtype=int32)Attributes: (4)created_at :2021-07-10T22:10:08.590166arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n              \n            \n            \n\n\n\naz.plot_ppc(idata, alpha=0.3, kind=\"cumulative\", figsize=(12, 6), textsize=14)\n\narray([<AxesSubplot:xlabel='loss'>], dtype=object)\n\n\n\n\n\n\nmodel_compare = az.compare(\n    {\n        \"Logistic Growth Model\": logistic_idata,\n        \"Weibull Growth Model\": weibull_idata,\n    }\n)\naz.plot_compare(model_compare, figsize=(12, 4), insample_dev=False)\n\nplt.show()\n\n/Users/nathanielforde/anaconda/envs/examined_algorithms/lib/python3.6/site-packages/arviz/stats/stats.py:151: UserWarning: \nThe scale is now log by default. Use 'scale' argument or 'stats.ic_scale' rcParam if you rely on a specific value.\nA higher log-score (or a lower deviance) indicates a model with better predictive accuracy.\n  \"\\nThe scale is now log by default. Use 'scale' argument or \"\n/Users/nathanielforde/anaconda/envs/examined_algorithms/lib/python3.6/site-packages/arviz/stats/stats.py:683: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  \"Estimated shape parameter of Pareto distribution is greater than 0.7 for \"\n/Users/nathanielforde/anaconda/envs/examined_algorithms/lib/python3.6/site-packages/arviz/stats/stats.py:683: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  \"Estimated shape parameter of Pareto distribution is greater than 0.7 for \"\n\n\n\n\n\n\naz.plot_trace(logistic_idata, var_names=[\"gf\", \"LR\"], circ_var_names=[\"LR\"])\n\nTypeError: plot_trace() got an unexpected keyword argument 'circ_var_names'\n\n\n\nlogistic_model.logp\n\n<pymc3.model.LoosePointFunc at 0x14d286b70>\n\n\n\nlogp = logistic_model.logp\nlnp = np.array([logp(logistic_trace.point(i,chain=c)) for c in logistic_trace.chains for i in range(len(logistic_trace))])\n\n\n\n\n0       0.0\n1       0.0\n2       0.0\n3       0.0\n4       0.0\n       ... \n1995    0.0\n1996    0.0\n1997    0.0\n1998    0.0\n1999    0.0\nLength: 2000, dtype: float64\n\n\n\nlogistic_trace.report.log_marginal_likelihood\n\nAttributeError: 'SamplerReport' object has no attribute 'log_marginal_likelihood'\n\n\n\nmodel_compare\n\n\n\n\n\n  \n    \n      \n      rank\n      loo\n      p_loo\n      d_loo\n      weight\n      se\n      dse\n      warning\n      loo_scale\n    \n  \n  \n    \n      Weibull Growth Model\n      0\n      -387.915\n      10.6734\n      0\n      0.811983\n      11.1849\n      0\n      True\n      log\n    \n    \n      Logistic Growth Model\n      1\n      -391.599\n      10.2344\n      3.68346\n      0.188017\n      11.874\n      3.81681\n      True\n      log"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nMissing Data and Bayesian Imputation with PyMC\n\n\nCausal Narratives in Survey Analysis\n\n\n\n\nbayesian\n\n\nmissing data\n\n\ncausal inference\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2024\n\n\n13 min\n\n\n\n\n\n\n  \n\n\n\n\nModel Evaluation and Discrete Choice Scenarios\n\n\nBayesian Mixer London\n\n\n\n\nbayesian\n\n\ndiscrete choice\n\n\n\n\n\n\n\n\n\n\n\nMar 26, 2024\n\n\n14 min\n\n\n\n\n\n\n  \n\n\n\n\nNon Parametric Causal Inference with PyMC\n\n\nPropensity Scores, Debiased ML and Causal Mediation\n\n\n\n\nbayesian\n\n\npropensity scores\n\n\ncausal inference\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2024\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nMultilevel Regression and Post-Stratification\n\n\nStratum Specific effect modification\n\n\n\n\nbayesian\n\n\nmissing data\n\n\ncausal inference\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nSurvival Regression Models in PyMC\n\n\nTime to Attrition in People Analytics\n\n\n\n\nbayesian\n\n\nsurvival analysis\n\n\npeople analytics\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2023\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nDiscrete Choice and Random Utility Models\n\n\nPyCon Ireland\n\n\n\n\nbayesian\n\n\ndiscrete choice\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2023\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\nTenuous Relations and Timeseries Analysis\n\n\nBerlin Timeseries\n\n\n\n\nbayesian\n\n\nVAR\n\n\ntimeseries analysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2023\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/tenuous_relations/first_talk.html",
    "href": "talks/tenuous_relations/first_talk.html",
    "title": "Tenuous Relations and Timeseries Analysis",
    "section": "",
    "text": "Hierarchical Bayesian VARs"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Measurement, Latent Factors and the Garden of Forking Paths\n\n\nConfirmatory Factor Analysis and Structural Equations in PyMC\n\n\n\n\n \n\n\n\n\n\n\nFreedom, Hierarchies and Confounded Estimates\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\nGAMs and GPs: Flexibility and Calibration\n\n\n\n\n\n\n\n\n\nApr 7, 2024\n\n\n\n\n\n\n\n\nFactor Analysis and Construct Validity in Psychology\n\n\n\n\n\n\n\n\n\nJan 22, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "Welcome to my site!\nYou’ll find a range of writings on data science, philosophy, statistics and inference more generally. I’ll update this site with semi-regular blog entries, and any talks or presentations on these topics.\n\n\nI’m a data scientist specialising in probabilistic modelling for the study of risk and causal inference. I have experience in model development, deployment, multivariate testing and monitoring.\nI’m interested in questions of inference and measurement in the face of natural variation and confounding.\nMy academic background is in mathematical logic and philosophy where I mostly imagined possible worlds and modal logics.\n\n\n\n\nPersonio | Staff Data Scientist | December 2021 - Present\n\nWorking with Product and Engineering to measure risk and quantify impact. Revenue Ops to forecasts and optimise processes.\n\nPyMC | Open Source Contributor | November 2021 - Present\n\nHelping to document some of the more esoteric applications of Bayesian modelling in PyMC."
  },
  {
    "objectID": "notes/Statistics/Introduction - Statistics Topics.html",
    "href": "notes/Statistics/Introduction - Statistics Topics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "The idea is that this section of my notes will serve to capture my zettelkasten style notes on topics in Statistics."
  },
  {
    "objectID": "notes/Uncertain Things.html",
    "href": "notes/Uncertain Things.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "This is a space for my notes. They’ll be sporadic, haphazard in their level of detail but I aim for them to be regular and consistently updated log of topics of interest or distraction."
  },
  {
    "objectID": "notes/Logic/Introduction - Logic Topics.html",
    "href": "notes/Logic/Introduction - Logic Topics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "The idea is that this section will serve to capture any and all notes on topics in logic -mathematical, philosophical or any style in between."
  },
  {
    "objectID": "notes/Philosophy/Introduction - Philosophy Topics.html",
    "href": "notes/Philosophy/Introduction - Philosophy Topics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "The idea is that this section will serve to capture the philosophical topics in my Zettlekasten notes."
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.html",
    "href": "oss/pymc/bayesian_var_model.html",
    "title": "Bayesian Vector Autoregressive Models in PyMC",
    "section": "",
    "text": "Bayesian Vector Autoregressive Models\nIn this project I demonstrated how to fit a hierarchical bayesian autoregressive model in PyMC. The work drew on a PyMC labs blogpost showing how to fit a simple VAR model in PyMC. We applied these types of model to econometric timeseries data to analyse the relationships between GDP, investment and consumption for Ireland.\nThe project culminated in a publication to the official PyMC documentation that can be found online here or downloaded as notebook here\n\n\n\nIreland’s GDP v Peers"
  },
  {
    "objectID": "oss/pymc/discrete_choice.html",
    "href": "oss/pymc/discrete_choice.html",
    "title": "Discrete Choice Models in PyMC",
    "section": "",
    "text": "In this project I demonstrated how to fit a discrete choice models using random utility components in PyMC. I applied these types of model to micro-econometric data over product choice to estimate market share and the correlation among good within an market.\nThe project culminated in a publication to the official PyMC documentation that can be found online here or downloaded as notebook here\n\n\n\nDiscrete Choice"
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.myst.html",
    "href": "oss/pymc/bayesian_var_model.myst.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "(Bayesian Vector Autoregressive Models)= # Bayesian Vector Autoregressive Models\n:::{post} November, 2022 :tags: time series, vector autoregressive model, hierarchical model :category: intermediate :author: Nathaniel Forde :::\n```{code-cell} ipython3 import os\nimport arviz as az import matplotlib.pyplot as plt import numpy as np import pandas as pd import pymc as pm import statsmodels.api as sm\nfrom pymc.sampling_jax import sample_blackjax_nuts"
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.myst.html#vectorautoregression-models",
    "href": "oss/pymc/bayesian_var_model.myst.html#vectorautoregression-models",
    "title": "Examined Algorithms",
    "section": "V(ector)A(uto)R(egression) Models",
    "text": "V(ector)A(uto)R(egression) Models\nIn this notebook we will outline an application of the Bayesian Vector Autoregressive Modelling. We will draw on the work in the PYMC Labs blogpost (see {cite:t}vieira2022BVAR). This will be a three part series. In the first we want to show how to fit Bayesian VAR models in PYMC. In the second we will show how to extract extra insight from the fitted model with Impulse Response analysis and make forecasts from the fitted VAR model. In the third and final post we will show in some more detail the benefits of using hierarchical priors with Bayesian VAR models. Specifically, we’ll outline how and why there are actually a range of carefully formulated industry standard priors which work with Bayesian VAR modelling.\nIn this post we will (i) demonstrate the basic pattern on a simple VAR model on fake data and show how the model recovers the true data generating parameters and (ii) we will show an example applied to macro-economic data and compare the results to those achieved on the same data with statsmodels MLE fits and (iii) show an example of estimating a hierarchical bayesian VAR model over a number of countries."
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.myst.html#autoregressive-models-in-general",
    "href": "oss/pymc/bayesian_var_model.myst.html#autoregressive-models-in-general",
    "title": "Examined Algorithms",
    "section": "Autoregressive Models in General",
    "text": "Autoregressive Models in General\nThe idea of a simple autoregressive model is to capture the manner in which past observations of the timeseries are predictive of the current observation. So in traditional fashion, if we model this as a linear phenomena we get simple autoregressive models where the current value is predicted by a weighted linear combination of the past values and an error term.\n\\[ y_t = \\alpha + \\beta_{y0} \\cdot y_{t-1} + \\beta_{y1} \\cdot y_{t-2} ... + \\epsilon \\]\nfor however many lags are deemed appropriate to the predict the current observation.\nA VAR model is kind of generalisation of this framework in that it retains the linear combination approach but allows us to model multiple timeseries at once. So concretely this mean that \\(\\mathbf{y}_{t}\\) as a vector where:\n\\[ \\mathbf{y}_{T} =  \\nu + A_{1}\\mathbf{y}_{T-1} + A_{2}\\mathbf{y}_{T-2} ... A_{p}\\mathbf{y}_{T-p} + \\mathbf{e}_{t}  \\]\nwhere the As are coefficient matrices to be combined with the past values of each individual timeseries. For example consider an economic example where we aim to model the relationship and mutual influence of each variable on themselves and one another.\n\\[ \\begin{bmatrix} gdp \\\\ inv \\\\ con \\end{bmatrix}_{T} = \\nu + A_{1}\\begin{bmatrix} gdp \\\\ inv \\\\ con \\end{bmatrix}_{T-1} +\n    A_{2}\\begin{bmatrix} gdp \\\\ inv \\\\ con \\end{bmatrix}_{T-2} ... A_{p}\\begin{bmatrix} gdp \\\\ inv \\\\ con \\end{bmatrix}_{T-p} + \\mathbf{e}_{t} \\]\nThis structure is compact representation using matrix notation. The thing we are trying to estimate when we fit a VAR model is the A matrices that determine the nature of the linear combination that best fits our timeseries data. Such timeseries models can have an auto-regressive or a moving average representation, and the details matter for some of the implication of a VAR model fit.\nWe’ll see in the next notebook of the series how the moving-average representation of a VAR lends itself to the interpretation of the covariance structure in our model as representing a kind of impulse-response relationship between the component timeseries.\n\nA Concrete Specification with Two lagged Terms\nThe matrix notation is convenient to suggest the broad patterns of the model, but it is useful to see the algebra is a simple case. Consider the case of Ireland’s GDP and consumption described as:\n\\[ gdp_{t} = \\beta_{gdp1} \\cdot gdp_{t-1} + \\beta_{gdp2} \\cdot gdp_{t-2} +  \\beta_{cons1} \\cdot cons_{t-1} + \\beta_{cons2} \\cdot cons_{t-2}  + \\epsilon_{gdp}\\] \\[ cons_{t} = \\beta_{cons1} \\cdot cons_{t-1} + \\beta_{cons2} \\cdot cons_{t-2} +  \\beta_{gdp1} \\cdot gdp_{t-1} + \\beta_{gdp2} \\cdot gdp_{t-2}  + \\epsilon_{cons}\\]\nIn this way we can see that if we can estimate the \\(\\beta\\) terms we have an estimate for the bi-directional effects of each variable on the other. This is a useful feature of the modelling. In what follows i should stress that i’m not an economist and I’m aiming to show only the functionality of these models not give you a decisive opinion about the economic relationships determining Irish GDP figures.\n\n\nCreating some Fake Data\n{code-cell} ipython3 def simulate_var(     intercepts, coefs_yy, coefs_xy, coefs_xx, coefs_yx, noises=(1, 1), *, warmup=100, steps=200 ):     draws_y = np.zeros(warmup + steps)     draws_x = np.zeros(warmup + steps)     draws_y[:2] = intercepts[0]     draws_x[:2] = intercepts[1]     for step in range(2, warmup + steps):         draws_y[step] = (             intercepts[0]             + coefs_yy[0] * draws_y[step - 1]             + coefs_yy[1] * draws_y[step - 2]             + coefs_xy[0] * draws_x[step - 1]             + coefs_xy[1] * draws_x[step - 2]             + rng.normal(0, noises[0])         )         draws_x[step] = (             intercepts[1]             + coefs_xx[0] * draws_x[step - 1]             + coefs_xx[1] * draws_x[step - 2]             + coefs_yx[0] * draws_y[step - 1]             + coefs_yx[1] * draws_y[step - 2]             + rng.normal(0, noises[1])         )     return draws_y[warmup:], draws_x[warmup:]\nFirst we generate some fake data with known parameters.\n```{code-cell} ipython3 var_y, var_x = simulate_var( intercepts=(18, 8), coefs_yy=(-0.8, 0), coefs_xy=(0.9, 0), coefs_xx=(1.3, -0.7), coefs_yx=(-0.1, 0.3), )\ndf = pd.DataFrame({“x”: var_x, “y”: var_y}) df.head()\n\n```{code-cell} ipython3\nfig, axs = plt.subplots(2, 1, figsize=(10, 3))\naxs[0].plot(df[\"x\"], label=\"x\")\naxs[0].set_title(\"Series X\")\naxs[1].plot(df[\"y\"], label=\"y\")\naxs[1].set_title(\"Series Y\");"
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.myst.html#handling-multiple-lags-and-different-dimensions",
    "href": "oss/pymc/bayesian_var_model.myst.html#handling-multiple-lags-and-different-dimensions",
    "title": "Examined Algorithms",
    "section": "Handling Multiple Lags and Different Dimensions",
    "text": "Handling Multiple Lags and Different Dimensions\nWhen Modelling multiple timeseries and accounting for potentially any number lags to incorporate in our model we need to abstract some of the model definition to helper functions. An example will make this a bit clearer.\n```{code-cell} ipython3 ### Define a helper function that will construct our autoregressive step for the marginal contribution of each lagged ### term in each of the respective time series equations def calc_ar_step(lag_coefs, n_eqs, n_lags, df): ars = [] for j in range(n_eqs): ar = pm.math.sum( [ pm.math.sum(lag_coefs[j, i] * df.values[n_lags - (i + 1) : -(i + 1)], axis=-1) for i in range(n_lags) ], axis=0, ) ars.append(ar) beta = pm.math.stack(ars, axis=-1)\nreturn beta\n\nMake the model in such a way that it can handle different specifications of the likelihood term\n\n\nand can be run for simple prior predictive checks. This latter functionality is important for debugging of\n\n\nshape handling issues. Building a VAR model involves quite a few moving parts and it is handy to\n### inspect the shape implied in the prior predictive checks. def make_model(n_lags, n_eqs, df, priors, mv_norm=True, prior_checks=True): coords = { “lags”: np.arange(n_lags) + 1, “equations”: df.columns.tolist(), “cross_vars”: df.columns.tolist(), “time”: [x for x in df.index[n_lags:]], }\nwith pm.Model(coords=coords) as model:\n    lag_coefs = pm.Normal(\n        \"lag_coefs\",\n        mu=priors[\"lag_coefs\"][\"mu\"],\n        sigma=priors[\"lag_coefs\"][\"sigma\"],\n        dims=[\"equations\", \"lags\", \"cross_vars\"],\n    )\n    alpha = pm.Normal(\n        \"alpha\", mu=priors[\"alpha\"][\"mu\"], sigma=priors[\"alpha\"][\"sigma\"], dims=(\"equations\",)\n    )\n    data_obs = pm.Data(\"data_obs\", df.values[n_lags:], dims=[\"time\", \"equations\"], mutable=True)\n\n    betaX = calc_ar_step(lag_coefs, n_eqs, n_lags, df)\n    betaX = pm.Deterministic(\n        \"betaX\",\n        betaX,\n        dims=[\n            \"time\",\n        ],\n    )\n    mean = alpha + betaX\n\n    if mv_norm:\n        n = df.shape[1]\n        ## Under the hood the LKJ prior will retain the correlation matrix too.\n        noise_chol, _, _ = pm.LKJCholeskyCov(\n            \"noise_chol\",\n            eta=priors[\"noise_chol\"][\"eta\"],\n            n=n,\n            sd_dist=pm.HalfNormal.dist(sigma=priors[\"noise_chol\"][\"sigma\"]),\n        )\n        obs = pm.MvNormal(\n            \"obs\", mu=mean, chol=noise_chol, observed=data_obs, dims=[\"time\", \"equations\"]\n        )\n    else:\n        ## This is an alternative likelihood that can recover sensible estimates of the coefficients\n        ## But lacks the multivariate correlation between the timeseries.\n        sigma = pm.HalfNormal(\"noise\", sigma=priors[\"noise\"][\"sigma\"], dims=[\"equations\"])\n        obs = pm.Normal(\n            \"obs\", mu=mean, sigma=sigma, observed=data_obs, dims=[\"time\", \"equations\"]\n        )\n\n    if prior_checks:\n        idata = pm.sample_prior_predictive()\n        return model, idata\n    else:\n        idata = pm.sample_prior_predictive()\n        idata.extend(pm.sample(draws=2000, random_seed=130))\n        pm.sample_posterior_predictive(idata, extend_inferencedata=True, random_seed=rng)\nreturn model, idata\n\nThe model has a deterministic component in the auto-regressive calculation which is required at each timestep, but the key point here is that we model the likelihood of the VAR as a multivariate normal distribution with a particular covariance relationship. The estimation of these covariance relationship gives the main insight in the manner in which our component timeseries relate to one another. \n\nWe will inspect the structure of a VAR with 2 lags and 2 equations\n\n```{code-cell} ipython3\nn_lags = 2\nn_eqs = 2\npriors = {\n    \"lag_coefs\": {\"mu\": 0.3, \"sigma\": 1},\n    \"alpha\": {\"mu\": 15, \"sigma\": 5},\n    \"noise_chol\": {\"eta\": 1, \"sigma\": 1},\n    \"noise\": {\"sigma\": 1},\n}\n\nmodel, idata = make_model(n_lags, n_eqs, df, priors)\npm.model_to_graphviz(model)\nAnother VAR with 3 lags and 2 equations.\n{code-cell} ipython3 n_lags = 3 n_eqs = 2 model, idata = make_model(n_lags, n_eqs, df, priors) for rv, shape in model.eval_rv_shapes().items():     print(f\"{rv:>11}: shape={shape}\") pm.model_to_graphviz(model)\nWe can inspect the correlation matrix between our timeseries which is implied by the prior specification, to see that we have allowed a flat uniform prior over their correlation.\n{code-cell} ipython3 ax = az.plot_posterior(     idata,     var_names=\"noise_chol_corr\",     hdi_prob=\"hide\",     group=\"prior\",     point_estimate=\"mean\",     grid=(2, 2),     kind=\"hist\",     ec=\"black\",     figsize=(10, 4), )\nNow we will fit the VAR with 2 lags and 2 equations\n{code-cell} ipython3 n_lags = 2 n_eqs = 2 model, idata_fake_data = make_model(n_lags, n_eqs, df, priors, prior_checks=False)\nWe’ll now plot some of the results to see that the parameters are being broadly recovered. The alpha parameters match well, but the individual lag coefficients show differences.\n{code-cell} ipython3 az.summary(idata_fake_data, var_names=[\"alpha\", \"lag_coefs\", \"noise_chol_corr\"])\n{code-cell} ipython3 az.plot_posterior(idata_fake_data, var_names=[\"alpha\"], ref_val=[18, 8]);\nNext we’ll plot the posterior predictive distribution to check that the fitted model can capture the patterns in the observed data. This is the primary test of goodness of fit.\n```{code-cell} ipython3 def shade_background(ppc, ax, idx, palette=“cividis”): palette = palette cmap = plt.get_cmap(palette) percs = np.linspace(51, 99, 100) colors = (percs - np.min(percs)) / (np.max(percs) - np.min(percs)) for i, p in enumerate(percs[::-1]): upper = np.percentile( ppc[:, idx, :], p, axis=1, ) lower = np.percentile( ppc[:, idx, :], 100 - p, axis=1, ) color_val = colors[i] ax[idx].fill_between( x=np.arange(ppc.shape[0]), y1=upper.flatten(), y2=lower.flatten(), color=cmap(color_val), alpha=0.1, )\ndef plot_ppc(idata, df, group=“posterior_predictive”): fig, axs = plt.subplots(2, 1, figsize=(25, 15)) df = pd.DataFrame(idata_fake_data[“observed_data”][“obs”].data, columns=[“x”, “y”]) axs = axs.flatten() ppc = az.extract_dataset(idata, group=group, num_samples=100)[“obs”] # Minus the lagged terms and the constant shade_background(ppc, axs, 0, “inferno”) axs[0].plot(np.arange(ppc.shape[0]), ppc[:, 0, :].mean(axis=1), color=“cyan”, label=“Mean”) axs[0].plot(df[“x”], “o”, mfc=“black”, mec=“white”, mew=1, markersize=7, label=“Observed”) axs[0].set_title(“VAR Series 1”) axs[0].legend() shade_background(ppc, axs, 1, “inferno”) axs[1].plot(df[“y”], “o”, mfc=“black”, mec=“white”, mew=1, markersize=7, label=“Observed”) axs[1].plot(np.arange(ppc.shape[0]), ppc[:, 1, :].mean(axis=1), color=“cyan”, label=“Mean”) axs[1].set_title(“VAR Series 2”) axs[1].legend()\nplot_ppc(idata_fake_data, df)\n\nAgain we can check the learned posterior distribution for the correlation parameter.\n\n```{code-cell} ipython3\nax = az.plot_posterior(\n    idata_fake_data,\n    var_names=\"noise_chol_corr\",\n    hdi_prob=\"hide\",\n    point_estimate=\"mean\",\n    grid=(2, 2),\n    kind=\"hist\",\n    ec=\"black\",\n    figsize=(10, 6),\n)"
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.myst.html#applying-the-theory-macro-economic-timeseries",
    "href": "oss/pymc/bayesian_var_model.myst.html#applying-the-theory-macro-economic-timeseries",
    "title": "Examined Algorithms",
    "section": "Applying the Theory: Macro Economic Timeseries",
    "text": "Applying the Theory: Macro Economic Timeseries\nThe data is from the World Bank’s World Development Indicators. In particular, we’re pulling annual values of GDP, consumption, and gross fixed capital formation (investment) for all countries from 1970. Timeseries models in general work best when we have a stable mean throughout the series, so for the estimation procedure we have taken the first difference and the natural log of each of these series.\n```{code-cell} ipython3 try: gdp_hierarchical = pd.read_csv( os.path.join(“..”, “data”, “gdp_data_hierarchical_clean.csv”), index_col=0 ) except FileNotFoundError: gdp_hierarchical = pd.read_csv(pm.get_data(“gdp_data_hierarchical_clean.csv”), …)\ngdp_hierarchical\n\n```{code-cell} ipython3\nfig, axs = plt.subplots(3, 1, figsize=(20, 10))\nfor country in gdp_hierarchical[\"country\"].unique():\n    temp = gdp_hierarchical[gdp_hierarchical[\"country\"] == country].reset_index()\n    axs[0].plot(temp[\"dl_gdp\"], label=f\"{country}\")\n    axs[1].plot(temp[\"dl_cons\"], label=f\"{country}\")\n    axs[2].plot(temp[\"dl_gfcf\"], label=f\"{country}\")\naxs[0].set_title(\"Differenced and Logged GDP\")\naxs[1].set_title(\"Differenced and Logged Consumption\")\naxs[2].set_title(\"Differenced and Logged Investment\")\naxs[0].legend()\naxs[1].legend()\naxs[2].legend()\nplt.suptitle(\"Macroeconomic Timeseries\");"
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.myst.html#irelands-economic-situation",
    "href": "oss/pymc/bayesian_var_model.myst.html#irelands-economic-situation",
    "title": "Examined Algorithms",
    "section": "Ireland’s Economic Situation",
    "text": "Ireland’s Economic Situation\nIreland is somewhat infamous for its GDP numbers that are largely the product of foreign direct investment and inflated beyond expectation in recent years by the investment and taxation deals offered to large multi-nationals. We’ll look here at just the relationship between GDP and consumption. We just want to show the mechanics of the VAR estimation, you shouldn’t read too much into the subsequent analysis.\n{code-cell} ipython3 ireland_df = gdp_hierarchical[gdp_hierarchical[\"country\"] == \"Ireland\"] ireland_df.reset_index(inplace=True, drop=True) ireland_df.head()\n{code-cell} ipython3 n_lags = 2 n_eqs = 2 priors = {     ## Set prior for expected positive relationship between the variables.     \"lag_coefs\": {\"mu\": 0.3, \"sigma\": 1},     \"alpha\": {\"mu\": 0, \"sigma\": 0.1},     \"noise_chol\": {\"eta\": 1, \"sigma\": 1},     \"noise\": {\"sigma\": 1}, } model, idata_ireland = make_model(     n_lags, n_eqs, ireland_df[[\"dl_gdp\", \"dl_cons\"]], priors, prior_checks=False ) idata_ireland\n{code-cell} ipython3 az.plot_trace(idata_ireland, var_names=[\"lag_coefs\", \"alpha\", \"betaX\"], kind=\"rank_vlines\");\n```{code-cell} ipython3 def plot_ppc_macro(idata, df, group=“posterior_predictive”): df = pd.DataFrame(idata[“observed_data”][“obs”].data, columns=[“dl_gdp”, “dl_cons”]) fig, axs = plt.subplots(2, 1, figsize=(20, 10)) axs = axs.flatten() ppc = az.extract_dataset(idata, group=group, num_samples=100)[“obs”]\nshade_background(ppc, axs, 0, \"inferno\")\naxs[0].plot(np.arange(ppc.shape[0]), ppc[:, 0, :].mean(axis=1), color=\"cyan\", label=\"Mean\")\naxs[0].plot(df[\"dl_gdp\"], \"o\", mfc=\"black\", mec=\"white\", mew=1, markersize=7, label=\"Observed\")\naxs[0].set_title(\"Differenced and Logged GDP\")\naxs[0].legend()\nshade_background(ppc, axs, 1, \"inferno\")\naxs[1].plot(df[\"dl_cons\"], \"o\", mfc=\"black\", mec=\"white\", mew=1, markersize=7, label=\"Observed\")\naxs[1].plot(np.arange(ppc.shape[0]), ppc[:, 1, :].mean(axis=1), color=\"cyan\", label=\"Mean\")\naxs[1].set_title(\"Differenced and Logged Consumption\")\naxs[1].legend()\nplot_ppc_macro(idata_ireland, ireland_df)\n\n```{code-cell} ipython3\nax = az.plot_posterior(\n    idata_ireland,\n    var_names=\"noise_chol_corr\",\n    hdi_prob=\"hide\",\n    point_estimate=\"mean\",\n    grid=(2, 2),\n    kind=\"hist\",\n    ec=\"black\",\n    figsize=(10, 6),\n)\n\nComparison with Statsmodels\nIt’s worthwhile comparing these model fits to the one achieved by Statsmodels just to see if we can recover a similar story.\n{code-cell} ipython3 VAR_model = sm.tsa.VAR(ireland_df[[\"dl_gdp\", \"dl_cons\"]]) results = VAR_model.fit(2, trend=\"c\")\n{code-cell} ipython3 results.params\nThe intercept parameters broadly agree with our Bayesian model with some differences in the implied relationships defined by the estimates for the lagged terms.\n{code-cell} ipython3 corr = pd.DataFrame(results.resid_corr, columns=[\"dl_gdp\", \"dl_cons\"]) corr.index = [\"dl_gdp\", \"dl_cons\"] corr\nThe residual correlation estimates reported by statsmodels agree quite closely with the multivariate gaussian correlation between the variables in our Bayesian model.\n{code-cell} ipython3 az.summary(idata_ireland, var_names=[\"alpha\", \"lag_coefs\", \"noise_chol_corr\"])\nWe plot the alpha parameter estimates against the Statsmodels estimates\n{code-cell} ipython3 az.plot_posterior(idata_ireland, var_names=[\"alpha\"], ref_val=[0.034145, 0.006996]);\n{code-cell} ipython3 az.plot_posterior(     idata_ireland,     var_names=[\"lag_coefs\"],     ref_val=[0.330003, -0.053677],     coords={\"equations\": \"dl_cons\", \"lags\": [1, 2], \"cross_vars\": \"dl_gdp\"}, );\nWe can see here again how the Bayesian VAR model recovers much of the same story. Similar magnitudes in the estimates for the alpha terms for both equations and a clear relationship between the first lagged GDP numbers and consumption along with a very similar covariance structure.\n+++"
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.myst.html#adding-a-bayesian-twist-hierarchical-vars",
    "href": "oss/pymc/bayesian_var_model.myst.html#adding-a-bayesian-twist-hierarchical-vars",
    "title": "Examined Algorithms",
    "section": "Adding a Bayesian Twist: Hierarchical VARs",
    "text": "Adding a Bayesian Twist: Hierarchical VARs\nIn addition we can add some hierarchical parameters if we want to model multiple countries and the relationship between these economic metrics at the national level. This is a useful technique in the cases where we have reasonably short timeseries data because it allows us to “borrow” information across the countries to inform the estimates of the key parameters.\n```{code-cell} ipython3 def make_hierarchical_model(n_lags, n_eqs, df, group_field, prior_checks=True): cols = [col for col in df.columns if col != group_field] coords = {“lags”: np.arange(n_lags) + 1, “equations”: cols, “cross_vars”: cols}\ngroups = df[group_field].unique()\n\nwith pm.Model(coords=coords) as model:\n    ## Hierarchical Priors\n    rho = pm.Beta(\"rho\", alpha=2, beta=2)\n    alpha_hat_location = pm.Normal(\"alpha_hat_location\", 0, 0.1)\n    alpha_hat_scale = pm.InverseGamma(\"alpha_hat_scale\", 3, 0.5)\n    beta_hat_location = pm.Normal(\"beta_hat_location\", 0, 0.1)\n    beta_hat_scale = pm.InverseGamma(\"beta_hat_scale\", 3, 0.5)\n    omega_global, _, _ = pm.LKJCholeskyCov(\n        \"omega_global\", n=n_eqs, eta=1.0, sd_dist=pm.Exponential.dist(1)\n    )\n\n    for grp in groups:\n        df_grp = df[df[group_field] == grp][cols]\n        z_scale_beta = pm.InverseGamma(f\"z_scale_beta_{grp}\", 3, 0.5)\n        z_scale_alpha = pm.InverseGamma(f\"z_scale_alpha_{grp}\", 3, 0.5)\n        lag_coefs = pm.Normal(\n            f\"lag_coefs_{grp}\",\n            mu=beta_hat_location,\n            sigma=beta_hat_scale * z_scale_beta,\n            dims=[\"equations\", \"lags\", \"cross_vars\"],\n        )\n        alpha = pm.Normal(\n            f\"alpha_{grp}\",\n            mu=alpha_hat_location,\n            sigma=alpha_hat_scale * z_scale_alpha,\n            dims=(\"equations\",),\n        )\n\n        betaX = calc_ar_step(lag_coefs, n_eqs, n_lags, df_grp)\n        betaX = pm.Deterministic(f\"betaX_{grp}\", betaX)\n        mean = alpha + betaX\n\n        n = df_grp.shape[1]\n        noise_chol, _, _ = pm.LKJCholeskyCov(\n            f\"noise_chol_{grp}\", eta=10, n=n, sd_dist=pm.Exponential.dist(1)\n        )\n        omega = pm.Deterministic(f\"omega_{grp}\", rho * omega_global + (1 - rho) * noise_chol)\n        obs = pm.MvNormal(f\"obs_{grp}\", mu=mean, chol=omega, observed=df_grp.values[n_lags:])\n\n    if prior_checks:\n        idata = pm.sample_prior_predictive()\n        return model, idata\n    else:\n        idata = pm.sample_prior_predictive()\n        idata.extend(sample_blackjax_nuts(2000, random_seed=120))\n        pm.sample_posterior_predictive(idata, extend_inferencedata=True)\nreturn model, idata\n\nThe model design allows for a non-centred parameterisation of the key likeihood for each of the individual country components by allowing the us to shift the country specific estimates away from the hierarchical mean. This is done by `rho * omega_global + (1 - rho) * noise_chol` line. The parameter `rho` determines the share of impact each country's data contributes to the estimation of the covariance relationship among the economic variables. Similar country specific adjustments are made with the `z_alpha_scale` and `z_beta_scale` parameters.\n\n```{code-cell} ipython3\ndf_final = gdp_hierarchical[[\"country\", \"dl_gdp\", \"dl_cons\", \"dl_gfcf\"]]\nmodel_full_test, idata_full_test = make_hierarchical_model(\n    2,\n    3,\n    df_final,\n    \"country\",\n    prior_checks=False,\n)\n{code-cell} ipython3 idata_full_test\n{code-cell} ipython3 az.plot_trace(     idata_full_test,     var_names=[\"rho\", \"alpha_hat_location\", \"beta_hat_location\", \"omega_global\"],     kind=\"rank_vlines\", );\nNext we’ll look at some of the summary statistics and how they vary across the countries.\n```{code-cell} ipython3\n\n```{code-cell} ipython3\naz.summary(\n    idata_full_test,\n    var_names=[\n        \"rho\",\n        \"alpha_hat_location\",\n        \"alpha_hat_scale\",\n        \"beta_hat_location\",\n        \"beta_hat_scale\",\n        \"z_scale_alpha_Ireland\",\n        \"z_scale_alpha_United States\",\n        \"z_scale_beta_Ireland\",\n        \"z_scale_beta_United States\",\n        \"alpha_Ireland\",\n        \"alpha_United States\",\n        \"omega_global_corr\",\n        \"lag_coefs_Ireland\",\n        \"lag_coefs_United States\",\n    ],\n)\n```{code-cell} ipython3 ax = az.plot_forest( idata_full_test, var_names=[ “alpha_Ireland”, “alpha_United States”, “alpha_Australia”, “alpha_Chile”, “alpha_New Zealand”, “alpha_South Africa”, “alpha_Canada”, “alpha_United Kingdom”, ], kind=“ridgeplot”, combined=True, ridgeplot_truncate=False, ridgeplot_quantiles=[0.25, 0.5, 0.75], ridgeplot_overlap=0.7, figsize=(10, 10), )\nax[0].axvline(0, color=“red”) ax[0].set_title(“Intercept Parameters for each country and Economic Measure”);\n\n```{code-cell} ipython3\nax = az.plot_forest(\n    idata_full_test,\n    var_names=[\n        \"lag_coefs_Ireland\",\n        \"lag_coefs_United States\",\n        \"lag_coefs_Australia\",\n        \"lag_coefs_Chile\",\n        \"lag_coefs_New Zealand\",\n        \"lag_coefs_South Africa\",\n        \"lag_coefs_Canada\",\n        \"lag_coefs_United Kingdom\",\n    ],\n    kind=\"ridgeplot\",\n    ridgeplot_truncate=False,\n    figsize=(10, 10),\n    coords={\"equations\": \"dl_cons\", \"lags\": 1, \"cross_vars\": \"dl_gdp\"},\n)\nax[0].axvline(0, color=\"red\")\nax[0].set_title(\"Lag Coefficient for the first lag of GDP on Consumption \\n by Country\");\nNext we’ll examine the correlation between the three variables and see what we’ve learned by including the hierarchical structure.\n{code-cell} ipython3 corr = pd.DataFrame(     az.summary(idata_full_test, var_names=[\"omega_global_corr\"])[\"mean\"].values.reshape(3, 3),     columns=[\"GDP\", \"CONS\", \"GFCF\"], ) corr.index = [\"GDP\", \"CONS\", \"GFCF\"] corr\n{code-cell} ipython3 ax = az.plot_posterior(     idata_full_test,     var_names=\"omega_global_corr\",     hdi_prob=\"hide\",     point_estimate=\"mean\",     grid=(3, 3),     kind=\"hist\",     ec=\"black\",     figsize=(10, 7), ) titles = [     \"GDP/GDP\",     \"GDP/CONS\",     \"GDP/GFCF\",     \"CONS/GDP\",     \"CONS/CONS\",     \"CONS/GFCF\",     \"GFCF/GDP\",     \"GFCF/CONS\",     \"GFCF/GFCF\", ] for ax, t in zip(ax.ravel(), titles):     ax.set_xlim(0.6, 1)     ax.set_title(t, fontsize=10) plt.suptitle(\"The Posterior Correlation Estimates\", fontsize=20);\nWe can see these estimates of the correlations between the 3 economic variables differ markedly from the simple case where we examined Ireland alone. In particular we can see that the correlation between GDF and CONS is now much higher. Which suggests that we have learned something about the relationship between these variables which would not be clear examining the Irish case alone.\nNext we’ll plot the model fits for each country to ensure that the predictive distribution can recover the observed data. It is important for the question of model adequacy that we can recover both the outlier case of Ireland and the more regular countries such as Australia and United States.\n{code-cell} ipython3 az.plot_ppc(idata_full_test);\nAnd to see the development of these model fits over time:\n```{code-cell} ipython3 countries = gdp_hierarchical[“country”].unique()\nfig, axs = plt.subplots(8, 3, figsize=(20, 40)) for ax, country in zip(axs, countries): temp = pd.DataFrame( idata_full_test[“observed_data”][f”obs_{country}”].data, columns=[“dl_gdp”, “dl_cons”, “dl_gfcf”], ) ppc = az.extract_dataset(idata_full_test, group=“posterior_predictive”, num_samples=100)[ f”obs_{country}” ] if country == “Ireland”: color = “viridis” else: color = “inferno” for i in range(3): shade_background(ppc, ax, i, color) ax[0].plot(np.arange(ppc.shape[0]), ppc[:, 0, :].mean(axis=1), color=“cyan”, label=“Mean”) ax[0].plot(temp[“dl_gdp”], “o”, mfc=“black”, mec=“white”, mew=1, markersize=7, label=“Observed”) ax[0].set_title(f”Posterior Predictive GDP: {country}“) ax[0].legend(loc=”lower left”) ax[1].plot( temp[“dl_cons”], “o”, mfc=“black”, mec=“white”, mew=1, markersize=7, label=“Observed” ) ax[1].plot(np.arange(ppc.shape[0]), ppc[:, 1, :].mean(axis=1), color=“cyan”, label=“Mean”) ax[1].set_title(f”Posterior Predictive Consumption: {country}“) ax[1].legend(loc=”lower left”) ax[2].plot( temp[“dl_gfcf”], “o”, mfc=“black”, mec=“white”, mew=1, markersize=7, label=“Observed” ) ax[2].plot(np.arange(ppc.shape[0]), ppc[:, 2, :].mean(axis=1), color=“cyan”, label=“Mean”) ax[2].set_title(f”Posterior Predictive Investment: {country}“) ax[2].legend(loc=”lower left”) plt.suptitle(“Posterior Predictive Checks on Hierarchical VAR”, fontsize=20);\n\nHere we can see that the model appears to have recovered reasonable posterior predictions for the observed data and the volatility of the Irish GDP figures is clear next to the other countries. Whether this is a cautionary tale about data quality or the corruption of metrics we leave to the economists to figure out.\n\n+++\n\n## Conclusion\n\nVAR modelling is a rich an interesting area of research within economics and there are a range of challenges and pitfalls which come with the interpretation and understanding of these models. We hope this example encourages you to continue exploring the potential of this kind of VAR modelling in the Bayesian framework. Whether you're interested in the relationship between grand economic theory or simpler questions about the impact of poor app performance on customer feedback, VAR models give you a powerful tool for interrogating these relationships over time. As we've seen Hierarchical VARs further enables the precise quantification of outliers within a cohort and does not throw away the information because of odd accounting practices engendered by international capitalism. \n\nIn the next post in this series we will spend some time digging into the implied relationships between the timeseries which result from fitting our VAR models.\n\n+++\n\n## References\n\n:::{bibliography}\n:filter: docname in docnames\n:::\n\n+++\n\n## Authors\n* Adapted from the PYMC labs [Blog post](https://www.pymc-labs.io/blog-posts/bayesian-vector-autoregression/) and Jim Savage's discussion [here](https://rpubs.com/jimsavage/hierarchical_var) by [Nathaniel Forde](https://nathanielf.github.io/) in November 2022 ([pymc-examples#456](https://github.com/pymc-devs/pymc-examples/pull/456))\n\n+++\n\n## Watermark\n\n```{code-cell} ipython3\n%load_ext watermark\n%watermark -n -u -v -iv -w -p pytensor,aeppl,xarray\n:::{include} ../page_footer.md :::"
  },
  {
    "objectID": "oss/pymc/missing_info.html",
    "href": "oss/pymc/missing_info.html",
    "title": "Missing Data Imputation in PyMC",
    "section": "",
    "text": "Missing Data Imputation and Employee Survey Data\nIn this project I demonstrate the technique of imputation for missing data using both the standard frequentist approach full information maximum likelihod and a more nuanced Bayesian method of chained equations.\nThe project culminated in a publication to the official PyMC documentation that can be found online here or downloaded as notebook here\nThe notebook demonstates these techniques applied to employee satisfaction data. In particular we show how Bayesian hierarchical methods can be used to help predict missing data values across various teams within an organisation based on the observed values andt the characteristics of the team dynamics which drove the observed data.\n\n\n\nDeviations from the Grand Mean by Team"
  },
  {
    "objectID": "oss/pymc/reliability_stats.html",
    "href": "oss/pymc/reliability_stats.html",
    "title": "Reliability Statistics in PyMC",
    "section": "",
    "text": "Reliability Statistics and Calibrated Prediction\nThis project was inspired by the need to apply survival analysis techniques in software engineering to predict and quantify the failure time distribution of software products. The focus was on parameteric modelling of failure distributions and the notion of calibrated predictions of failure times.\nThe project culminated in a publication to the official PyMC documentation that can be found online here or downloaded as notebook here\n\n\n\nLinearised MLE fits to Failure time data"
  },
  {
    "objectID": "oss/pymc/ordinal_regression.html",
    "href": "oss/pymc/ordinal_regression.html",
    "title": "Ordinal Regression Models in PyMC",
    "section": "",
    "text": "Ordinal Models of Regression\nIn this project I outline the strategy of latent variable ordinal regression Bayesian models to evaluate the categorical choice on Likert like scales. The main theme of the work is to try and articulate the modelling approach to survey data, comparing the risk of model misspecification to the assumed metric based analysis of ordinal response variables.\nThe project culminated in a publication to the official PyMC documentation that can be found online here and downloaded here\nThe notebook demonstates these techniques applied to simulated manager engagement evaluations and applied to rotten tomatoes movie ratings data. In particular we show how a latent variable formulate can help characterise the manner in which different factors such as working from home, salary can influence an individual’s ordinal rating response and why it’s important to understand the sources of variation in such responses.\n\n\n\nLatent and Explicit Ratings"
  },
  {
    "objectID": "oss/pymc/longitudinal_models.html",
    "href": "oss/pymc/longitudinal_models.html",
    "title": "Longitudinal Models in PyMC",
    "section": "",
    "text": "Longitudinal Analysis of Growth Trajectories\nIn this project I outline the strategy of using multi-level or hierarchical Bayesian models to evaluate the growth trajectories of individuals, and estimate the between individual effects. The main theme of the work is to try and disambiguate some of the complexities of mixed level (hierarchical modelling) when applied to longitudinal data.\nThe project culminated in a publication to the official PyMC documentation that can be found online here and downloaded here\nThe notebook demonstates these techniques applied to data of youth alcohol consuption. and behaviourial data. In particular we show how Bayesian hierarchical methods can be used to help characterise the manner in which different factors such can influence an individual’s trajectory and why it’s important to understand the sources of variation in such growth trajectories.\n\n\n\nWithin and Between Individual Trajectories"
  },
  {
    "objectID": "oss/pymc/autoregressive_forecasting.html",
    "href": "oss/pymc/autoregressive_forecasting.html",
    "title": "Autoregressive Forecasting in PyMC",
    "section": "",
    "text": "Autoregressive Forecasting in PyMC\nThis project stemmed from a gap in the PyMC documentation around how to use a fitted auto-regressive model to make forecasts about the future state of the world. In this project I demonstrate how fit and make predictictions with bayesian structural timeseries models.\nThe project culminated in a publication to the official PyMC documentation that can be found online here or downloaded as notebook here\nThe notebook demonstates these techniques applied to a series of fake data building in complexity as we add more structure.\n\n\n\nForecasting"
  },
  {
    "objectID": "opensource.html",
    "href": "opensource.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nJustifying Instruments in CausalPy\n\n\n\n\n\n\n\nregression\n\n\ninstrumental variables\n\n\ncausal inference\n\n\n \n\n\n\n\nJun 15, 2024\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nInverse Propensity Score Weighting in CausalPy\n\n\n\n\n\n\n\nregression\n\n\npropensity\n\n\ncausal inference\n\n\nweighting\n\n\n \n\n\n\n\nMay 11, 2024\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nBayesian Non Parametric Causal Inference in PyMC\n\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nnon parametric\n\n\ncausal inference\n\n\n \n\n\n\n\nFeb 27, 2024\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nFrailty Survival Models in PyMC\n\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nsurvival models\n\n\nfrailty models\n\n\n \n\n\n\n\nNov 28, 2023\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nMultilevel Regression and Post-Stratification\n\n\n\n\n\n\n\nregression\n\n\npost-stratification\n\n\nsurvey data\n\n\n \n\n\n\n\nSep 21, 2023\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nBayesian IV Regression in CausalPy\n\n\n\n\n\n\n\nregression\n\n\ninstrumental variables\n\n\ncausal inference\n\n\n \n\n\n\n\nAug 15, 2023\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nDiscrete Choice Models in PyMC\n\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\ndiscrete choice\n\n\nsubjective utility\n\n\n \n\n\n\n\nJul 15, 2023\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nOrdinal Regression Models in PyMC\n\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nordinal regression\n\n\nlikert scales\n\n\n \n\n\n\n\nJun 1, 2023\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nLongitudinal Models in PyMC\n\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nlongitudinal models\n\n\n \n\n\n\n\nApr 10, 2023\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nMissing Data Imputation in PyMC\n\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nmissing_data\n\n\nimputation\n\n\n \n\n\n\n\nFeb 10, 2023\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nReliability Statistics in PyMC\n\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\ntime-to-failure\n\n\ncalibration\n\n\n \n\n\n\n\nJan 10, 2023\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nBayesian Vector Autoregressive Models in PyMC\n\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nautoregressive\n\n\nhierarchical_models\n\n\n \n\n\n\n\nDec 15, 2022\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nAutoregressive Forecasting in PyMC\n\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nautoregressive\n\n\n \n\n\n\n\nAug 15, 2022\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nMeasurement, Latent Factors and the Garden of Forking Paths\n\n\nConfirmatory Factor Analysis and Structural Equations in PyMC\n\n\n\n\ncfa\n\n\nsem\n\n\nmeasurment\n\n\n\n\n\n\n\n\n\n\n\n41 min\n\n\n\n\n\n\n  \n\n\n\n\nFreedom, Hierarchies and Confounded Estimates\n\n\n\n\n\n\n\nTWFE\n\n\nmundlak\n\n\nhierarchical models\n\n\nDiD\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\n42 min\n\n\n\n\n\n\n  \n\n\n\n\nGAMs and GPs: Flexibility and Calibration\n\n\n\n\n\n\n\nprobability\n\n\ngeneralised additive models\n\n\ngaussian processes\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2024\n\n\n39 min\n\n\n\n\n\n\n  \n\n\n\n\nFactor Analysis and Construct Validity in Psychology\n\n\n\n\n\n\n\nconstruct_validity\n\n\nsignificance_tests\n\n\nfactor analysis\n\n\nreplicability\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2021\n\n\n15 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "oss/causalpy/instrumental_variables.html",
    "href": "oss/causalpy/instrumental_variables.html",
    "title": "Bayesian IV Regression in CausalPy",
    "section": "",
    "text": "Instrumental Variable Regression\nIn this project I sought to add the functionality for bayesian instrumental variable analysis to the CausalPy package. I adapted the work of Juan Orduz to contribute the base classes to the package and demonstrated how these classes can be used to esitmate instrumental regression by replicating the results of an Acemologu paper on the efficacy of political institutions. The demonstration can be seen here\n\n\n\nComparison with OLS"
  },
  {
    "objectID": "oss/bambi/mr_p.html",
    "href": "oss/bambi/mr_p.html",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "",
    "text": "Regression as Stratification\nIn this project I sought to understand the procedure of post-stratification adjustment used in election forecasting and regression modelling of the same. I published the documentation for the technique of using Multilevel Regression and post-stratification (MrP) with the bambi package here. In this work i tried to elaborate precisely how and why the careful modeller would want to make stratum specific adjustments to the predictions of regression models.\nI adapted the work of Martin, Philips and Gelmen’s “Multilevel Regression and Poststratification Case Studies” that can be found here. You can download the worked example here\n\n\n\nComparison with OLS"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#preliminaries",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#preliminaries",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Preliminaries",
    "text": "Preliminaries\n\n\n\n\n\n\nI am not an Economist\n\n\n\nI’m a data scientist at Personio\n\nBayesian statistician,\nReformed philosopher and logician.\n\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\n\nDisclaimer\n\n\nNone of Personio’s data was used in this presentation\n\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\nThe worked examples used here can be found here"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#agenda",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#agenda",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Agenda",
    "text": "Agenda\n\n\nHistory and Background\n\n\n\n\nModelling Choice Scenarios\n\n\n\n\nMarket Structure and Substitution Patterns\n\n\n\n\nModel Adequacy and Counterfactuals\n\n\n\n\nIndividual Heterogenous Utility\n\n\n\n\nConclusion\n\nThe World in the Model"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#mcfadden-and-bart",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#mcfadden-and-bart",
    "title": "Discrete Choice and Random Utility Models",
    "section": "McFadden and BART",
    "text": "McFadden and BART\n\n\n\n\n“Transport projects involve sinking money in expensive capital investments, which have a long life and wide repercussions. There is no escape from the attempt both to estimate the demand for their services over twenty or thirty years and to assess their repercussions on the economy as a whole.” - Denys Munby, Transport, 1968 ”\n\n\n\n\n\n\n\nBay Area Rapid Transit\n\n\n\n\n\nDublin Metrolink"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#revealed-preference-and-predicting-demand",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#revealed-preference-and-predicting-demand",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Revealed Preference and Predicting Demand",
    "text": "Revealed Preference and Predicting Demand\nSelf Centred Utility Maximisers?\n\n\n\n\nThe assumption of revealed preference theory is that if a person chooses A over B then their latent subjective utility for A is greater than for B.\n\n\n\n\nSurvey data estimated about 15% of users would adopt the newly introduced BART system. McFadden’s random utility model estimated 6%.\n\n\n\n\nHe was right.\n\n\n\n\n\n\nCopernican Shift: He estimated utility to predict choice, rather than infer utility from stated choice."
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#general-applicability-of-choice-problems",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#general-applicability-of-choice-problems",
    "title": "Discrete Choice and Random Utility Models",
    "section": "General Applicability of Choice Problems",
    "text": "General Applicability of Choice Problems\n\n\nThese models offer the possibility of predicting choice in diverse domains: policy, brand, school, car and partners.\n\n\n\n\nHard Question: What are the attributes that drive these choices? How well are they measurable?\n\n\n\n\nHarder Question: How do changes in these attributes influence the predicted market demand for these choices?"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#note-on-model-evaluation",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#note-on-model-evaluation",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Note on Model Evaluation",
    "text": "Note on Model Evaluation\nReplicating the Super Soldier Program"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#note-on-model-evaluation-1",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#note-on-model-evaluation-1",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Note on Model Evaluation",
    "text": "Note on Model Evaluation\nReplicating the Super Soldier Program\n\n\n\n\n\n\n\n\n\n\nBayesian Models aim to replicate the DGP holistically and answer the harder question"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-the-data",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-the-data",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Choice: The Data",
    "text": "Choice: The Data\nGas Central Heating and Electrical Central Heating described by their cost of installation and operation.\n\n\n\nchoice_id\nchosen\nic_gc\noc_gc\n…\noc_ec\n\n\n\n\n1\ngc\n866\n200\n…\n542\n\n\n2\nec\n802\n195\n…\n510\n\n\n3\ner\n759\n203\n…\n495\n\n\n4\ngr\n789\n220\n…\n502"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-a-naive-model",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-a-naive-model",
    "title": "Choice Models and Subjective Utility",
    "section": "Choice: A Naive Model",
    "text": "Choice: A Naive Model\nUnderspecified Utilities\nLet there be five goods described by their cost of installation and operation.\n\\[ \\begin{split} \\overbrace{\\begin{pmatrix}\n\\color{green}{u_{gc}}   \\\\\n\\color{green}{u_{gr}}   \\\\\n\\color{green}{u_{ec}}   \\\\\n\\color{green}{u_{er}}   \\\\\n\\color{green}{u_{hp}}   \\\\\n\\end{pmatrix}}^{utility} =  \\begin{pmatrix}\ngc_{ic} & gc_{oc}  \\\\\ngr_{ic} & gr_{oc}  \\\\\nec_{ic} & ec_{oc}  \\\\\ner_{ic} & er_{oc}  \\\\\nhp_{ic} & hp_{oc}  \\\\\n\\end{pmatrix} \\overbrace{\\begin{pmatrix}\n\\color{blue}{\\beta_{ic}}   \\\\\n\\color{blue}{\\beta_{oc}}   \\\\\n\\end{pmatrix}}^{parameters}  \\end{split}\n\\]"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-a-naive-model-1",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-a-naive-model-1",
    "title": "Choice Models and Subjective Utility",
    "section": "Choice: A Naive Model",
    "text": "Choice: A Naive Model\nThe utility calculation is fundamentally comparative. \\[ \\begin{split} \\begin{pmatrix}\n\\color{green}{u_{gc}}   \\\\\n\\color{green}{u_{gr}}   \\\\\n\\color{green}{u_{ec}}   \\\\\n\\color{green}{u_{er}}   \\\\\n\\color{red}{\\overbrace{0}^{\\text{outside good}}}   \\\\\n\\end{pmatrix} =  \\begin{pmatrix}\ngc_{ic} & gc_{oc}  \\\\\ngr_{ic} & gr_{oc}  \\\\\nec_{ic} & ec_{oc}  \\\\\ner_{ic} & er_{oc}  \\\\\n\\color{red}{0} & \\color{red}{0} \\\\\n\\end{pmatrix} \\begin{pmatrix}\n\\color{blue}{\\beta_{ic}}   \\\\\n\\color{blue}{\\beta_{oc}}   \\\\\n\\end{pmatrix}  \\end{split}\n\\]\nWe zero out one category in the data set to represent the “outside good” for comparison. Similar to dummy variables in Regression, this is required for the model to be identified."
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-a-naive-model-2",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-a-naive-model-2",
    "title": "Choice Models and Subjective Utility",
    "section": "Choice: A Naive Model",
    "text": "Choice: A Naive Model\nUtility determines choice probability of choice:\n\\[\\text{softmax}(\\color{green}{u})_{j} = \\frac{\\exp(\\color{green}{u_{j}})}{\\sum_{q=1}^{J}\\exp(\\color{green}{u_{q}})}\\]\nchoices determine market share where:\n\\[ s_{j}(\\mathbf{\\color{blue}{\\beta}}) = P(\\color{green}{u_{j}} > \\color{green}{u_{k}}; ∀k ̸= j) \\]"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-estimation",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-estimation",
    "title": "Choice Models and Subjective Utility",
    "section": "Choice: Estimation",
    "text": "Choice: Estimation\nThe model is traditionally estimated with maximum likelihood caclulations.\n\\[  L(\\color{blue}{\\beta}) = \\prod s_{j}(\\mathbf{\\color{blue}{\\beta}}) \\]\nor taking the log:\n\\[  l(\\color{blue}{\\beta}) = \\sum log(s_{j}(\\mathbf{\\color{blue}{\\beta}})) \\] \\[ \\text{ We find: } \\underset{\\color{blue}{\\beta}}{\\mathrm{argmax}} \\text{ } l(\\color{blue}{\\beta}) \\]\nResults are often brittle!"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-bayesian-estimation",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-bayesian-estimation",
    "title": "Choice Models and Subjective Utility",
    "section": "Choice: Bayesian Estimation",
    "text": "Choice: Bayesian Estimation\nTo evaluate the integrals in the Bayesian model we use MCMC to estimate conditional probabilities of the joint distribution.\n\\[\\underbrace{\\color{blue}{\\beta}}_{\\text{prior draws}} \\sim Normal(0, 1) \\]\n\\[ \\underbrace{p(\\color{blue}{\\beta} | D)}_{\\text{posterior draws}} = \\frac{p(\\mathbb{\\color{blue}{\\beta}})p(D | \\color{blue}{\\beta} )}{\\int_{i}^{n} p(D | \\mathbf{\\color{blue}{\\beta_{i}}})p(\\mathbf{\\color{blue}{\\beta_{i}}}) } \\]\nPriors can be used flexibly regularise and improve reliability of estimation across structural causal models."
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-naive-model-in-code",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-naive-model-in-code",
    "title": "Choice Models and Subjective Utility",
    "section": "The Naive Model in Code",
    "text": "The Naive Model in Code\nwith pm.Model(coords=coords) as model_1:\n    ## Priors for the Beta Coefficients\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n\n    ## Construct Utility matrix and Pivot\n    u0 = beta_ic * wide_heating_df[\"ic.ec\"] + beta_oc * wide_heating_df[\"oc.ec\"]\n    u1 = beta_ic * wide_heating_df[\"ic.er\"] + beta_oc * wide_heating_df[\"oc.er\"]\n    u2 = beta_ic * wide_heating_df[\"ic.gc\"] + beta_oc * wide_heating_df[\"oc.gc\"]\n    u3 = beta_ic * wide_heating_df[\"ic.gr\"] + beta_oc * wide_heating_df[\"oc.gr\"]\n    u4 = np.zeros(N)  # Outside Good\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    ## Apply Softmax Transform\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n\n    ## Likelihood\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#interpreting-the-model-coefficients",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#interpreting-the-model-coefficients",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Interpreting the Model Coefficients",
    "text": "Interpreting the Model Coefficients\nRate of Substitution\n\n\nThe beta coefficients in the model are interpreted as weights of utility. However, the precision in these latent terms is relative to the variance of unobserved factors.\nThe utility scale is not fixed, but the ratio \\(\\frac{\\beta_{ic}}{\\beta_{oc}}\\) is invariant.\n\n\n\n\nRate of Substitution"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#model-posterior-predictive-fits",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#model-posterior-predictive-fits",
    "title": "Choice Models and Subjective Utility",
    "section": "Model Posterior Predictive Fits",
    "text": "Model Posterior Predictive Fits\nThe model fit fails to recapture the observed data points\n\nModel Fit"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#augmenting-the-model-1",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#augmenting-the-model-1",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Augmenting the Model",
    "text": "Augmenting the Model\nPosterior Predictions\n\nModel Fit"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#augmenting-the-model-2",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#augmenting-the-model-2",
    "title": "Choice Models and Subjective Utility",
    "section": "Augmenting the Model:",
    "text": "Augmenting the Model:\nwith pm.Model(coords=coords) as model_2:\n    ## Priors for the Beta Coefficients\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n    alphas = pm.Normal(\"alpha\", 0, 1, dims=\"alts_intercepts\")\n\n    ## Construct Utility matrix and Pivot using an intercept per alternative\n    u0 = alphas[0] + beta_ic * wide_heating_df[\"ic.ec\"] + beta_oc * wide_heating_df[\"oc.ec\"]\n    u1 = alphas[1] + beta_ic * wide_heating_df[\"ic.er\"] + beta_oc * wide_heating_df[\"oc.er\"]\n    u2 = alphas[2] + beta_ic * wide_heating_df[\"ic.gc\"] + beta_oc * wide_heating_df[\"oc.gc\"]\n    u3 = alphas[3] + beta_ic * wide_heating_df[\"ic.gr\"] + beta_oc * wide_heating_df[\"oc.gr\"]\n    u4 = np.zeros(N)  # Outside Good\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    ## Apply Softmax Transform\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n\n    ## Likelihood\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#augmenting-the-model-3",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#augmenting-the-model-3",
    "title": "Choice Models and Subjective Utility",
    "section": "Augmenting the Model:",
    "text": "Augmenting the Model:\n\nModel Structure"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#augmenting-the-model-4",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#augmenting-the-model-4",
    "title": "Choice Models and Subjective Utility",
    "section": "Augmenting the Model",
    "text": "Augmenting the Model\nPosterior Predictions\n\nModel Fit"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#independence-of-irrelevant-alternatives",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#independence-of-irrelevant-alternatives",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Independence of Irrelevant Alternatives",
    "text": "Independence of Irrelevant Alternatives\nNew Products Cannibalise Equally from all Alternatives\n\nSuppose a market choice between transport modes is determined by the above model.\nRed Bus or Car are you initial Options. Assume \\(s_{\\color{red}{bus}}(\\beta) = s_{car}(\\beta)\\). Market Share is 50% to each option.\nIntroduce the Blue Bus Option, then the Independent characteristics of the utility specification implies that \\(s_{\\color{red}{bus}}(\\beta) = s_{car}(\\beta) = s_{\\color{blue}{bus}}(\\beta)\\)\nThis implies an implausible substitution pattern for real markets.1\n\nWhat kind of Monster have we Created!?"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure-1",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure-1",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\nPriors on Parameters determine Market Structure\n\\[ \\begin{split} \\color{brown}{\\Gamma} =\n\\begin{pmatrix}\n  \\color{red}{1} , \\gamma , \\gamma , \\gamma \\\\\n  \\gamma , \\color{blue}{1} , \\gamma , \\gamma  \\\\\n   \\gamma , \\gamma  , \\color{orange}{1} , \\gamma \\\\\n  \\gamma , \\gamma , \\gamma , \\color{teal}{1}  \n\\end{pmatrix}\n\\end{split} \\]"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure-2",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure-2",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\nCovariance in Code\nwith pm.Model(coords=coords) as model_3:\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n\n    beta_income = pm.Normal(\"beta_income\", 0, 1 dims=\"alts_intercepts\")\n\n    chol, corr, stds = pm.LKJCholeskyCov(\n        \"chol\", n=4, eta=2.0, \n        sd_dist=pm.Exponential.dist(1.0, shape=4)\n    )\n    alphas = pm.MvNormal(\"alpha\", mu=0, chol=chol, dims=\"alts_intercepts\")\n\n    u0 = (\n        alphas[0]\n        + beta_ic * wide_heating_df[\"ic.gc\"]\n        + beta_oc * wide_heating_df[\"oc.gc\"]\n        + beta_income[0] * wide_heating_df[\"income\"]\n    )\n    u1 = (\n        alphas[1]\n        + beta_ic * wide_heating_df[\"ic.gc\"]\n        + beta_oc * wide_heating_df[\"oc.gc\"]\n        + beta_income[1] * wide_heating_df[\"income\"]\n    )\n    u2 = (\n        alphas[2]\n        + beta_ic * wide_heating_df[\"ic.gc\"]\n        + beta_oc * wide_heating_df[\"oc.gc\"]\n        + beta_income[2] * wide_heating_df[\"income\"]\n    )\n    u3 = (\n        alphas[3]\n        + beta_ic * wide_heating_df[\"ic.gr\"]\n        + beta_oc * wide_heating_df[\"oc.gr\"]\n        + beta_income[3] * wide_heating_df[\"income\"]\n    )\n    u4 = np.zeros(N)  # pivot\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure-3",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure-3",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\nStructural Dependence\n\nCorrelation Structure"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure-4",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure-4",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\n\nCorrelation Structure"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#counterfactual-reasoning",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#counterfactual-reasoning",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Counterfactual Reasoning",
    "text": "Counterfactual Reasoning\nInterventions and Conditionalisation\n\n\n\nThere is a sharp distinction between conditional probability distributions and probability under intervention\nIn PyMC you can implement the do-operator to intervene on the graph that represents your data generating process."
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#counterfactual-reasoning-1",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#counterfactual-reasoning-1",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Counterfactual Reasoning",
    "text": "Counterfactual Reasoning\nPricing Experiments\n\nCounterfactual Shares"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#counterfactual-reasoning-2",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#counterfactual-reasoning-2",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Counterfactual Reasoning",
    "text": "Counterfactual Reasoning\nInterventions and Conditionalisation\n\n\n\nThere is a sharp distinction between conditional probability distributions and probability under intervention\nIn PyMC you can implement the do-operator to intervene on the graph that represents your data generating process."
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#counterfactual-reasoning-3",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#counterfactual-reasoning-3",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Counterfactual Reasoning",
    "text": "Counterfactual Reasoning\nInterventions and Conditionalisation\n\n\n\nThere is a sharp distinction between conditional probability distributions and probability under intervention\nIn PyMC you can implement the do-operator to intervene on the graph that represents your data generating process."
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#individual-heterogenous-utility",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#individual-heterogenous-utility",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\nRepeated Choice and Hierarchical Structure\n\n\n\nperson_id\nchoice_id\nchosen\nnabisco_price\nkeebler_price\n\n\n\n\n1\n1\nnabisco\n3.40\n2.00\n\n\n1\n2\nnabisco\n3.45\n2.50\n\n\n1\n3\nkeebler\n3.60\n2.70\n\n\n2\n1\nkeebler\n3.48\n2.20\n\n\n2\n2\nkeebler\n3.30\n2.25"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#individual-heterogenous-utility-1",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#individual-heterogenous-utility-1",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\n\\[\n\\begin{split}\n\\begin{pmatrix}\n\\color{red}{u_{i, nb}} \\\\\n\\color{purple}{u_{i, kb}} \\\\\n\\color{orange}{u_{i, sun}} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  (\\color{red}{\\alpha_{nb}} + \\beta_{i}) + \\color{blue}{\\beta_{p}}p_{nb} + \\color{green}{\\beta_{disp}}d_{nb} \\\\\n  (\\color{purple}{\\alpha_{kb}} + \\beta_{i}) +  \\color{blue}{\\beta_{p}}p_{kb} + \\color{green}{\\beta_{disp}}d_{kb}  \\\\\n  (\\color{orange}{\\alpha_{sun}}  + \\beta_{i})  + \\color{blue}{\\beta_{p}}p_{sun} + \\color{green}{\\beta_{disp}}d_{sun}  \\\\\n   0 + 0 + 0\n\\end{pmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#individual-heterogenous-utility-2",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#individual-heterogenous-utility-2",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\nIn Code\n\nwith pm.Model(coords=coords) as model_4:\n    beta_feat = pm.TruncatedNormal(\"beta_feat\", 0, 1, upper=10, lower=0)\n    beta_disp = pm.TruncatedNormal(\"beta_disp\", 0, 1, upper=10, lower=0)\n    ## Stronger Prior on Price to ensure \n    ## an increase in price negatively impacts utility\n    beta_price = pm.TruncatedNormal(\"beta_price\", 0, 1, upper=0, lower=-10)\n    alphas = pm.Normal(\"alpha\", 0, 1, dims=\"alts_intercepts\")\n    beta_individual = pm.Normal(\"beta_individual\", 0, 0.05,\n     dims=(\"individuals\", \"alts_intercepts\"))\n\n    u0 = (\n        (alphas[0] + beta_individual[person_indx, 0])\n        + beta_disp * c_df[\"disp.sunshine\"]\n        + beta_feat * c_df[\"feat.sunshine\"]\n        + beta_price * c_df[\"price.sunshine\"]\n    )\n    u1 = (\n        (alphas[1] + beta_individual[person_indx, 1])\n        + beta_disp * c_df[\"disp.keebler\"]\n        + beta_feat * c_df[\"feat.keebler\"]\n        + beta_price * c_df[\"price.keebler\"]\n    )\n    u2 = (\n        (alphas[2] + beta_individual[person_indx, 2])\n        + beta_disp * c_df[\"disp.nabisco\"]\n        + beta_feat * c_df[\"feat.nabisco\"]\n        + beta_price * c_df[\"price.nabisco\"]\n    )\n    u3 = np.zeros(N)  # Outside Good\n    s = pm.math.stack([u0, u1, u2, u3]).T\n    # Reconstruct the total data\n\n    ## Apply Softmax Transform\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n\n    ## Likelihood\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#individual-heterogenous-utility-3",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#individual-heterogenous-utility-3",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\nRecovered Posterior Predictive Distribution"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#individual-heterogenous-utility-4",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#individual-heterogenous-utility-4",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\nIndividual Preference\n\n\n\n\n\nIndividual preferences can be derived from the model in this manner.\nThe relationship between preferences over the product offering can be seen too\nMarket stable under stable preferences?"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#conclusion",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#conclusion",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Conclusion",
    "text": "Conclusion\nThe World in the Model\n“Models… [are] like sonnets for the poet, [a] means to express accounts of life in exact, short form using languages that may easily abstract or analogise, and involve imaginative choices and even a certain degree of playfulness in expression” - Mary Morgan in The World in the Model\n\n\n\nModels should articulate the relevant structure of this world and other possible ones.\nThey serve as microscopes. Simulation systems are tools to interrogate reality.\nBayesian Conditionalisation calibrates the system against the observed facts.\nBayesian Discrete choice models help us interrogate aspects of market demand under uncertainty.\nPyMC enables us to easily build and experiment with those models.\nCausal inference is plausible to degree that we can defend the structural assumptions. Bayesian models enforce tranparency and justification of structural commitments and necessary complexity.\n\n\n \n\n\n\n\n\nDiscrete Choice with PyMC"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-multnomial-model",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-multnomial-model",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Multnomial Model:",
    "text": "The Multnomial Model:\nProduct Specific Intercepts\n\\[\n\\begin{split}\n\\begin{pmatrix}\n\\color{red}{u_{gc}} \\\\\n\\color{purple}{u_{gr}} \\\\\n\\color{orange}{u_{ec}} \\\\\n\\color{teal}{u_{er}} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  \\color{red}{\\alpha_{gc}} + \\color{blue}{\\beta_{ic}}gc_{ic} + \\color{blue}{\\beta_{oc}}gc_{oc} \\\\\n  \\color{purple}{\\alpha_{gr}} + \\color{blue}{\\beta_{ic}}gr_{ic} + \\color{blue}{\\beta_{oc}}gr_{oc}  \\\\\n  \\color{orange}{\\alpha_{ec}} + \\color{blue}{\\beta_{ic}}ec_{ic} + \\color{blue}{\\beta_{oc}}ec_{oc}  \\\\\n  \\color{teal}{\\alpha_{er}} + \\color{blue}{\\beta_{ic}}er_{ic} + \\color{blue}{\\beta_{oc}}er_{oc}  \\\\\n   0 + 0 + 0\n\\end{pmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-choice-model",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-choice-model",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Choice Model",
    "text": "The Choice Model\nUtility determines choice probability of choice:\n\\[\\text{softmax}(\\color{green}{u})_{j} = \\frac{\\exp(\\color{green}{u_{j}})}{\\sum_{q=1}^{J}\\exp(\\color{green}{u_{q}})}\\]\nchoices determine market share where:\n\\[ s_{j}(\\mathbf{\\color{blue}{\\beta}}) = P(\\color{green}{u_{j}} > \\color{green}{u_{k}}; ∀k ̸= j) \\]"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-estimation-strategy",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-estimation-strategy",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Estimation Strategy",
    "text": "The Estimation Strategy\nThe model is traditionally estimated with maximum likelihood caclulations.\n\\[  L(\\color{blue}{\\beta}) = \\prod s_{j}(\\mathbf{\\color{blue}{\\beta}}) \\]\nor taking the log:\n\\[  l(\\color{blue}{\\beta}) = \\sum log(s_{j}(\\mathbf{\\color{blue}{\\beta}})) \\] \\[ \\text{ We find: } \\underset{\\color{blue}{\\beta}}{\\mathrm{argmax}} \\text{ } l(\\color{blue}{\\beta}) \\]\nResults are often brittle!"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#bayesian-estimation",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#bayesian-estimation",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Bayesian Estimation",
    "text": "Bayesian Estimation\nTo evaluate the integrals in the Bayesian model we use MCMC to estimate conditional probabilities of the joint distribution.\n\\[\\underbrace{\\color{blue}{\\beta}}_{\\text{prior draws}} \\sim Normal(0, 1) \\]\n\\[ \\underbrace{p(\\color{blue}{\\beta} | D)}_{\\text{posterior draws}} = \\frac{p(\\mathbb{\\color{blue}{\\beta}})p(D | \\color{blue}{\\beta} )}{\\int_{i}^{n} p(D | \\mathbf{\\color{blue}{\\beta_{i}}})p(\\mathbf{\\color{blue}{\\beta_{i}}}) } \\]\nPriors can be used flexibly to regularise and improve reliability of estimation across structural causal models."
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-model-in-code",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-model-in-code",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Model in Code:",
    "text": "The Model in Code:\nwith pm.Model(coords=coords) as model_2:\n    ## Priors for the Beta Coefficients\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n    alphas = pm.Normal(\"alpha\", 0, 1, dims=\"alts_intercepts\")\n\n    ## Construct Utility matrix and Pivot using an intercept per alternative\n    u0 = alphas[0] + beta_ic * wide_heating_df[\"ic.ec\"] + beta_oc * wide_heating_df[\"oc.ec\"]\n    u1 = alphas[1] + beta_ic * wide_heating_df[\"ic.er\"] + beta_oc * wide_heating_df[\"oc.er\"]\n    u2 = alphas[2] + beta_ic * wide_heating_df[\"ic.gc\"] + beta_oc * wide_heating_df[\"oc.gc\"]\n    u3 = alphas[3] + beta_ic * wide_heating_df[\"ic.gr\"] + beta_oc * wide_heating_df[\"oc.gr\"]\n    u4 = np.zeros(N)  # Outside Good\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    ## Apply Softmax Transform\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n\n    ## Likelihood\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#augmenting-the-model",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#augmenting-the-model",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Augmenting the Model:",
    "text": "Augmenting the Model:\n\nModel Structure"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-multnomial-model-1",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-multnomial-model-1",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Multnomial Model:",
    "text": "The Multnomial Model:\nUtility determines choice probability of choice:\n\\[\\text{softmax}(\\color{green}{u})_{j} = \\frac{\\exp(\\color{green}{u_{j}})}{\\sum_{q=1}^{J}\\exp(\\color{green}{u_{q}})}\\]\nchoices determine market share where:\n\\[ s_{j}(\\mathbf{\\color{blue}{\\beta}}) = P(\\color{green}{u_{j}} > \\color{green}{u_{k}}; ∀k ̸= j) \\]"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-maximum-likelihood-estimation-strategy",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-maximum-likelihood-estimation-strategy",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Maximum Likelihood Estimation Strategy",
    "text": "The Maximum Likelihood Estimation Strategy\nThe model is traditionally estimated with maximum likelihood caclulations.\n\\[  L(\\color{blue}{\\beta}) = \\prod s_{j}(\\mathbf{\\color{blue}{\\beta}}) \\]\nor taking the log:\n\\[  l(\\color{blue}{\\beta}) = \\sum log(s_{j}(\\mathbf{\\color{blue}{\\beta}})) \\] \\[ \\text{ We find: } \\underset{\\color{blue}{\\beta}}{\\mathrm{argmax}} \\text{ } l(\\color{blue}{\\beta}) \\]\nResults are often brittle!"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#model-structure",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#model-structure",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Model Structure:",
    "text": "Model Structure:\n\nModel Structure"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#model-fit",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#model-fit",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Model Fit",
    "text": "Model Fit\nPosterior Predictive Distribution\n\nModel Fit"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Problem:",
    "text": "The Problem:\nNew Products Cannibalise Equally from all Alternatives\n\nSuppose a market choice between transport modes is determined by the above model.\nRed Bus or Car are you initial Options. Assume \\(s_{\\color{red}{bus}}(\\beta) = s_{car}(\\beta)\\). Market Share is 50% to each option.\nIntroduce the Blue Bus Option, then the Independent characteristics of the utility specification implies that \\(s_{\\color{red}{bus}}(\\beta) = s_{car}(\\beta) = s_{\\color{blue}{bus}}(\\beta)\\)\nThis implies an implausible substitution pattern for real markets.1\n\nWhat kind of Monster have we Created!?"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem-iia",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem-iia",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Problem: IIA",
    "text": "The Problem: IIA\nNew Products Cannibalise Equally from all Alternatives\n\nSuppose a market choice between transport modes is determined by the above model.\nRed Bus or Car are you initial Options. Assume \\(s_{\\color{red}{bus}}(\\beta) = s_{car}(\\beta)\\). Market Share is 50% to each option.\nIntroduce the Blue Bus Option, then the Independent characteristics of the utility specification implies that \\(s_{\\color{red}{bus}}(\\beta) = s_{car}(\\beta) = s_{\\color{blue}{bus}}(\\beta)\\)\nThis implies an implausible substitution pattern for real markets.1\n\nWhat kind of Monster have we Created!?"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\nDependence in Market Share\n\\[ \\alpha_{i} \\sim Normal(\\mathbf{0}, \\color{brown}{\\Gamma}) \\]\n\\[\n\\begin{split}\n\\begin{pmatrix}\n\\color{red}{u_{gc}} \\\\\n\\color{purple}{u_{gr}} \\\\\n\\color{orange}{u_{ec}} \\\\\n\\color{teal}{u_{er}} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  \\color{red}{\\alpha_{gc}} + \\color{blue}{\\beta_{ic}}gc_{ic} + \\color{blue}{\\beta_{oc}}gc_{oc} \\\\\n  \\color{purple}{\\alpha_{gr}} + \\color{blue}{\\beta_{ic}}gr_{ic} + \\color{blue}{\\beta_{oc}}gr_{oc}  \\\\\n  \\color{orange}{\\alpha_{ec}} + \\color{blue}{\\beta_{ic}}ec_{ic} + \\color{blue}{\\beta_{oc}}ec_{oc}  \\\\\n  \\color{teal}{\\alpha_{er}} + \\color{blue}{\\beta_{ic}}er_{ic} + \\color{blue}{\\beta_{oc}}er_{oc}  \\\\\n   0 + 0 + 0\n\\end{pmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure-in-code",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure-in-code",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Adding Correlation Structure in Code",
    "text": "Adding Correlation Structure in Code\nwith pm.Model(coords=coords) as model_3:\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n\n    beta_income = pm.Normal(\"beta_income\", 0, 1 dims=\"alts_intercepts\")\n\n    chol, corr, stds = pm.LKJCholeskyCov(\n        \"chol\", n=4, eta=2.0, \n        sd_dist=pm.Exponential.dist(1.0, shape=4)\n    )\n    alphas = pm.MvNormal(\"alpha\", mu=0, chol=chol, dims=\"alts_intercepts\")\n\n    u0 = (\n        alphas[0]\n        + beta_ic * wide_heating_df[\"ic.gc\"]\n        + beta_oc * wide_heating_df[\"oc.gc\"]\n        + beta_income[0] * wide_heating_df[\"income\"]\n    )\n    u1 = (\n        alphas[1]\n        + beta_ic * wide_heating_df[\"ic.gc\"]\n        + beta_oc * wide_heating_df[\"oc.gc\"]\n        + beta_income[1] * wide_heating_df[\"income\"]\n    )\n    u2 = (\n        alphas[2]\n        + beta_ic * wide_heating_df[\"ic.gc\"]\n        + beta_oc * wide_heating_df[\"oc.gc\"]\n        + beta_income[2] * wide_heating_df[\"income\"]\n    )\n    u3 = (\n        alphas[3]\n        + beta_ic * wide_heating_df[\"ic.gr\"]\n        + beta_oc * wide_heating_df[\"oc.gr\"]\n        + beta_income[3] * wide_heating_df[\"income\"]\n    )\n    u4 = np.zeros(N)  # pivot\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-model",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-model",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Model:",
    "text": "The Model:\nUtilities in Code\nwith pm.Model(coords=coords) as model_2:\n    ## Priors for the Beta Coefficients\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n    alphas = pm.Normal(\"alpha\", 0, 1, dims=\"alts_intercepts\")\n\n    ## Construct Utility matrix and Pivot using an intercept per alternative\n    u0 = alphas[0] + beta_ic * wide_heating_df[\"ic.ec\"] + beta_oc * wide_heating_df[\"oc.ec\"]\n    u1 = alphas[1] + beta_ic * wide_heating_df[\"ic.er\"] + beta_oc * wide_heating_df[\"oc.er\"]\n    u2 = alphas[2] + beta_ic * wide_heating_df[\"ic.gc\"] + beta_oc * wide_heating_df[\"oc.gc\"]\n    u3 = alphas[3] + beta_ic * wide_heating_df[\"ic.gr\"] + beta_oc * wide_heating_df[\"oc.gr\"]\n    u4 = np.zeros(N)  # Outside Good\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    ## Apply Softmax Transform\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n\n    ## Likelihood\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#model-adequacy-and-counterfactuals",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#model-adequacy-and-counterfactuals",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Model Adequacy and Counterfactuals",
    "text": "Model Adequacy and Counterfactuals\nCeteris Paribus Laws\nWith a fitted PyMC model we can counterfactually reset the values for the input data and regenerate the posterior predictive distribution holding else equal in the data generating process.\n\nWhat would the market share be like if prices for electrical systems increased 20%?\n\n # update values of predictors with new 20% \n # price increase in operating costs for electrical options\nwith model_3:\n    pm.set_data({\"oc_ec\": wide_heating_df[\"oc.ec\"] * 1.2, \n                 \"oc_er\": wide_heating_df[\"oc.er\"] * 1.2})\n    # use the updated values and predict outcomes and probabilities:\n    idata_new_policy = pm.sample_posterior_predictive(\n        idata_m3,\n        var_names=[\"p\", \"y_cat\"],\n        return_inferencedata=True,\n        predictions=True,\n        extend_inferencedata=False,\n        random_seed=100,\n    )\n\nidata_new_policy"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#model-adequacy-and-counterfactuals-1",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#model-adequacy-and-counterfactuals-1",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Model Adequacy and Counterfactuals",
    "text": "Model Adequacy and Counterfactuals\nCeteris Paribus Laws\nWith a fitted PyMC model we can counterfactually reset the values for the input data and regenerate the posterior predictive distribution holding else equal in the data generating process.\n\nWhat would the market share be like if prices for electrical systems increased 20%?\n\n # update values of predictors with new 20% \n # price increase in operating costs for electrical options\nwith model_3:\n    pm.set_data({\"oc_ec\": wide_heating_df[\"oc.ec\"] * 1.2, \n                 \"oc_er\": wide_heating_df[\"oc.er\"] * 1.2})\n    # use the updated values and predict outcomes and probabilities:\n    idata_new_policy = pm.sample_posterior_predictive(\n        idata_m3,\n        var_names=[\"p\", \"y_cat\"],\n        return_inferencedata=True,\n        predictions=True,\n        extend_inferencedata=False,\n        random_seed=100,\n    )\n\nidata_new_policy"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#model-adequacy-and-counterfactuals-2",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#model-adequacy-and-counterfactuals-2",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Model Adequacy and Counterfactuals",
    "text": "Model Adequacy and Counterfactuals\nPricing Experiments\n\nCounterfactual Shares"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem-interpreting-the-model-coefficients",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem-interpreting-the-model-coefficients",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Problem: Interpreting the Model Coefficients",
    "text": "The Problem: Interpreting the Model Coefficients\nRate of Substitution\n\n\nThe beta coefficients in the model are interpreted as weights of utility. However, the precision in these latent terms is relative to the variance of unobserved factors.\nThe utility scale is not fixed, but the ratio \\(\\frac{\\beta_{ic}}{\\beta_{oc}}\\) is invariant.\n\n\n\n\nRate of Substitution"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem-model-structure",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem-model-structure",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Problem: Model Structure:",
    "text": "The Problem: Model Structure:\n\nThe Process of Bayesian Updating calibrates the parameter estimates against the data"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem-model-fit",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem-model-fit",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Problem: Model Fit",
    "text": "The Problem: Model Fit\nPosterior Predictive Distribution\n\nThe model successfully predicts observed market share"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem-interpreting-the-model",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem-interpreting-the-model",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Problem: Interpreting the Model",
    "text": "The Problem: Interpreting the Model\nRate of Substitution\n\n\nThe beta coefficients in the model are interpreted as weights of utility. However, the precision in these latent terms is relative to the variance of unobserved factors.\nThe utility scale is not fixed, but the ratio \\(\\frac{\\beta_{ic}}{\\beta_{oc}}\\) is invariant.\n\n\n\n\nRate of Substitution"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#model-adequacy-and-counterfactuals-3",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#model-adequacy-and-counterfactuals-3",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Model Adequacy and Counterfactuals",
    "text": "Model Adequacy and Counterfactuals\nInterventions and Conditionalisation\n\n\n\nThere is a sharp distinction between conditional probability distributions and probability under intervention\nIn PyMC you can implement the do-operator to intervene on the graph that represents your data generating process."
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#preliminaries",
    "href": "talks/missing_data/missing_data_causal.html#preliminaries",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Preliminaries",
    "text": "Preliminaries\n\n\n\n\n\n\nBackground\n\n\n\nI’m a data scientist at Personio\nContributor at PyMC and PyMC labs\n\nBayesian statistician,\nReformed philosopher and logician.\n\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\n\nDisclaimer\n\n\nNone of Personio’s data was used in this presentation\n\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\nThe worked examples used here can be found here"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#agenda",
    "href": "talks/missing_data/missing_data_causal.html#agenda",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Agenda",
    "text": "Agenda"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#agenda-1",
    "href": "talks/missing_data/missing_data_causal.html#agenda-1",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Agenda",
    "text": "Agenda\n\n\nTypology of Missing-ness\n\n\n\n\nMultivariate Missing-ness with FIML\n\n\n\n\nBayesian Imputation by Chained Equations\n\n\n\n\nHierarchical Structures impacting Missing-ness\n\n\n\n\nImputation and Causal Narratives\n\n\n\n\nConclusion\n\nMissing Data: a Gateway to Causal Inference"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#typology-of-missing-ness",
    "href": "talks/missing_data/missing_data_causal.html#typology-of-missing-ness",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Typology of Missing-ness",
    "text": "Typology of Missing-ness\n\n\nMissing Completely at Random (MCAR) for \\(Y\\)\n\n\\(P(M = 1 | Y_{obs}, Y_{miss}, \\phi) = P(M=1 | \\phi)\\)\nwhere \\(\\phi\\) is the haphazard circumstance of the world\nmissing-ness cannot be predicted from the \\(Y\\) variable\n\n\n\n\n\nMissing at Random (MAR) for \\(Y\\)\n\n\\(P(M = 1 | Y_{obs}, Y_{miss}, \\phi) = P(M=1 | Y_{obs}, \\phi)\\)\nmissing-ness can be predicted from \\(Y\\) variable\n\n\n\n\n\nMissing Not at Random (MNAR) for \\(Y\\)\n\n\\(P(M = 1 | Y_{obs}, Y_{miss}, \\phi)\\)\nnon-ignorable missing-ness. Need to account for why!"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#identifiability-under-missing-ness",
    "href": "talks/missing_data/missing_data_causal.html#identifiability-under-missing-ness",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Identifiability under Missing-ness",
    "text": "Identifiability under Missing-ness\n\n\nUnder MCAR and MAR there exist consistent identifiable estimators for functions of \\(Y\\)\n\n\n\n\nUnder MNAR there does not exist in general consistent estimators for functions of \\(Y.\\)\n\n\n\n\nWe cannot use data to distinguish cases which are MNAR V (MCAR or MAR).\nWe are required to make assumptions about the nature of the missing-ness to try and account for it."
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#the-data",
    "href": "talks/missing_data/missing_data_causal.html#the-data",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "The Data",
    "text": "The Data\n\nEmployee Empowerment Survey"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#the-metrics",
    "href": "talks/missing_data/missing_data_causal.html#the-metrics",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "The Metrics",
    "text": "The Metrics\n\nMetrics with Gaps"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#modelling-the-multivariate-structure",
    "href": "talks/missing_data/missing_data_causal.html#modelling-the-multivariate-structure",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Modelling the Multivariate Structure",
    "text": "Modelling the Multivariate Structure\nWe can use a MLE variant called Full information maximum likelihood to estimate the multivariate distribution.\n\\[ \\mathbf{Y} = MvNormal(\\mu, \\Sigma) \\]\nFIML controls for the missing observations in the data by maximising the likelihood based across the different patterns of missing-ness in the data."
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#modelling-the-multivariate-structure-1",
    "href": "talks/missing_data/missing_data_causal.html#modelling-the-multivariate-structure-1",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Modelling the Multivariate Structure",
    "text": "Modelling the Multivariate Structure\nFIML in code\n\ndata = df_employee[[\"worksat\", \"empower\", \"lmx\"]]\n\n\ndef split_data_by_missing_pattern(data):\n    # We want to extract our the pattern of missing-ness in our dataset\n    # and save each sub-set of our data in a structure that can be used to feed into a log-likelihood function\n    grouped_patterns = []\n    patterns = data.notnull().drop_duplicates().values\n    # A pattern is whether the \n    # values in each column e.g. \n    # [True, True, True] or [True, True, False]\n    observed = data.notnull()\n    for p in range(len(patterns)):\n        temp = observed[\n            (observed[\"worksat\"] == patterns[p][0])\n            & (observed[\"empower\"] == patterns[p][1])\n            & (observed[\"lmx\"] == patterns[p][2])\n        ]\n        grouped_patterns.append([patterns[p], temp.index, data.iloc[temp.index].dropna(axis=1)])\n\n    return grouped_patterns\n\n\ndef reconstitute_params(params_vector, n_vars):\n    # Convenience numpy function to construct mirrored COV matrix\n    # From flattened params_vector\n    mus = params_vector[0:n_vars]\n    cov_flat = params_vector[n_vars:]\n    indices = np.tril_indices(n_vars)\n    cov = np.empty((n_vars, n_vars))\n    for i, j, c in zip(indices[0], indices[1], cov_flat):\n        cov[i, j] = c\n        cov[j, i] = c\n    cov = cov + 1e-25\n    return mus, cov\n\n\ndef optimise_ll(flat_params, n_vars, grouped_patterns):\n    mus, cov = reconstitute_params(flat_params, n_vars)\n    # Check if COV is positive definite\n    if (np.linalg.eigvalsh(cov) < 0).any():\n        return 1e100\n    objval = 0.0\n    for obs_pattern, _, obs_data in grouped_patterns:\n        # This is the key (tricky) step because we're selecting the variables which pattern\n        # the full information set within each pattern of \"missing-ness\"\n        # e.g. when the observed pattern is [True, True, False] we want the first two variables\n        # of the mus vector and we want only the covariance relations between the relevant variables from the cov\n        # in the iteration.\n        obs_mus = mus[obs_pattern]\n        obs_cov = cov[obs_pattern][:, obs_pattern]\n        ll = np.sum(multivariate_normal(obs_mus, obs_cov).logpdf(obs_data))\n        objval = ll + objval\n    return -objval\n\n\ndef estimate(data):\n    n_vars = data.shape[1]\n    # Initialise\n    mus0 = np.zeros(n_vars)\n    cov0 = np.eye(n_vars)\n    # Flatten params for optimiser\n    params0 = np.append(mus0, cov0[np.tril_indices(n_vars)])\n    # Process Data\n    grouped_patterns = split_data_by_missing_pattern(data)\n    # Run the Optimiser.\n    try:\n        result = scipy.optimize.minimize(\n            optimise_ll, params0, args=(n_vars, grouped_patterns), method=\"Powell\"\n        )\n    except Exception as e:\n        raise e\n    mean, cov = reconstitute_params(result.x, n_vars)\n    return mean, cov\n\n\nfiml_mus, fiml_cov = estimate(data)"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#estimate-model-and-samples",
    "href": "talks/missing_data/missing_data_causal.html#estimate-model-and-samples",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Estimate Model and Samples",
    "text": "Estimate Model and Samples\n\n\n\n\n\n\n\n\n\n\nDiscrete Choice with PyMC"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#estimated-model-and-samples",
    "href": "talks/missing_data/missing_data_causal.html#estimated-model-and-samples",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Estimated Model and Samples",
    "text": "Estimated Model and Samples\n\n\n\n\n\n\n\n\nThe Estimated model Parameters are used to sample from the implied distribution.\nBut the approach lacks control and insight into why the data was missing in the first place.\nCannot directly account for MNAR cases."
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#the-joint-distribution-decomposed",
    "href": "talks/missing_data/missing_data_causal.html#the-joint-distribution-decomposed",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "The Joint Distribution Decomposed",
    "text": "The Joint Distribution Decomposed\nModelling the Outcome piece-wise\nStart with the observation that: \\[f(emp, lmx, climate, male) \\\\ = f(emp | lmx, climate, male) \\cdot  \\\\f(lmx | climate, male) \\cdot f(climate | male) \\cdot f(male)^{*}\\]\nwhich can be phrased as a set of linear models\n\\[ empower = \\alpha_{2} + \\beta_{3}male + \\beta_{4}climate + \\beta_{5}lmx \\\\\nlmx = \\alpha_{1} + \\beta_{1}climate + \\beta_{2}male \\\\\nclimate = \\alpha_{0} + \\beta_{0}male \\]"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#the-model",
    "href": "talks/missing_data/missing_data_causal.html#the-model",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "The Model",
    "text": "The Model\nSensitivity Analysis with Priors in PyMC\ndata = df_employee[[\"lmx\", \"empower\", \"climate\", \"male\"]]\nlmx_mean = data[\"lmx\"].mean()\nlmx_min = data[\"lmx\"].min()\nlmx_max = data[\"lmx\"].max()\nlmx_sd = data[\"lmx\"].std()\n\ncli_mean = data[\"climate\"].mean()\ncli_min = data[\"climate\"].min()\ncli_max = data[\"climate\"].max()\ncli_sd = data[\"climate\"].std()\n\n\npriors = {\n    \"climate\": {\"normal\": [lmx_mean, lmx_sd, lmx_sd], \n                \"uniform\": [lmx_min, lmx_max]},\n    \"lmx\": {\"normal\": [cli_mean, cli_sd, cli_sd], \n            \"uniform\": [cli_min, cli_max]},\n}\n\n\ndef make_model(priors, normal_pred_assumption=True):\n    coords = {\n        \"alpha_dim\": [\"lmx_imputed\", \"climate_imputed\", \"empower_imputed\"],\n        \"beta_dim\": [\n            \"lmxB_male\",\n            \"lmxB_climate\",\n            \"climateB_male\",\n            \"empB_male\",\n            \"empB_climate\",\n            \"empB_lmx\",\n        ],\n    }\n    with pm.Model(coords=coords) as model:\n        # Priors\n        beta = pm.Normal(\"beta\", 0, 1, size=6, dims=\"beta_dim\")\n        alpha = pm.Normal(\"alphas\", 10, 5, size=3, dims=\"alpha_dim\")\n        sigma = pm.HalfNormal(\"sigmas\", 5, size=3, dims=\"alpha_dim\")\n\n        if normal_pred_assumption:\n            mu_climate = pm.Normal(\n                \"mu_climate\", priors[\"climate\"][\"normal\"][0], priors[\"climate\"][\"normal\"][1]\n            )\n            sigma_climate = pm.HalfNormal(\"sigma_climate\", priors[\"climate\"][\"normal\"][2])\n            climate_pred = pm.Normal(\n                \"climate_pred\", mu_climate, sigma_climate, observed=data[\"climate\"].values\n            )\n        else:\n            climate_pred = pm.Uniform(\"climate_pred\", 0, 40, observed=data[\"climate\"].values)\n\n        if normal_pred_assumption:\n            mu_lmx = pm.Normal(\"mu_lmx\", priors[\"lmx\"][\"normal\"][0], priors[\"lmx\"][\"normal\"][1])\n            sigma_lmx = pm.HalfNormal(\"sigma_lmx\", priors[\"lmx\"][\"normal\"][2])\n            lmx_pred = pm.Normal(\"lmx_pred\", mu_lmx, sigma_lmx, observed=data[\"lmx\"].values)\n        else:\n            lmx_pred = pm.Uniform(\"lmx_pred\", 0, 40, observed=data[\"lmx\"].values)\n\n        # Likelihood(s)\n        lmx_imputed = pm.Normal(\n            \"lmx_imputed\",\n            alpha[0] + beta[0] * data[\"male\"] + beta[1] * climate_pred,\n            sigma[0],\n            observed=data[\"lmx\"].values,\n        )\n        climate_imputed = pm.Normal(\n            \"climate_imputed\",\n            alpha[1] + beta[2] * data[\"male\"],\n            sigma[1],\n            observed=data[\"climate\"].values,\n        )\n        empower_imputed = pm.Normal(\n            \"emp_imputed\",\n            alpha[2] + beta[3] * data[\"male\"] + beta[4] * climate_pred + beta[5] * lmx_pred,\n            sigma[2],\n            observed=data[\"empower\"].values,\n        )\n\n        idata = pm.sample_prior_predictive()\n        idata.extend(pm.sample(random_seed=120))\n        pm.sample_posterior_predictive(idata, extend_inferencedata=True)\n        return idata, model\n\n\nidata_uniform, model_uniform = make_model(priors, normal_pred_assumption=False)\nidata_normal, model_normal = make_model(priors, normal_pred_assumption=True)"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#bayesian-updating-and-calibration",
    "href": "talks/missing_data/missing_data_causal.html#bayesian-updating-and-calibration",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Bayesian Updating and Calibration",
    "text": "Bayesian Updating and Calibration\nEstimating the Model\nTo evaluate the integrals in the Bayesian model we use MCMC to estimate conditional probabilities of the joint distribution.\n\\[\\underbrace{\\color{blue}{\\beta}}_{\\text{prior draws}} \\sim Normal(0, 1) \\]\n\\[ \\underbrace{p(\\color{blue}{\\beta} | D)}_{\\text{posterior draws}} = \\frac{p(\\mathbb{\\color{blue}{\\beta}})p(D | \\color{blue}{\\beta} )}{\\int_{i}^{n} p(D | \\mathbf{\\color{blue}{\\beta_{i}}})p(\\mathbf{\\color{blue}{\\beta_{i}}}) } \\]\nPriors can be used flexibly to regularise and improve reliability of estimation across structural causal models."
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#the-model-structure",
    "href": "talks/missing_data/missing_data_causal.html#the-model-structure",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "The Model Structure",
    "text": "The Model Structure\nThe PyMC model Graph\n\nChained Equation ModelImputing the values and feeding them forward into the ultimate likelihood terms to estimate the profile of the joint distribution."
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#sensitivty-to-prior-specification",
    "href": "talks/missing_data/missing_data_causal.html#sensitivty-to-prior-specification",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Sensitivty to Prior Specification",
    "text": "Sensitivty to Prior Specification\n\nThe Effect of choosing the right PriorsModel choice using predictive adequacy as a constraint. Bayesian model adequacy workflow applies here too"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#hierarchical-structures-of-non-response",
    "href": "talks/missing_data/missing_data_causal.html#hierarchical-structures-of-non-response",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Hierarchical Structures of Non-response",
    "text": "Hierarchical Structures of Non-response\nTeam dynamics determine probability of response\n\nTeam Empowerment ScoresWe can try to recover ignorable missing-ness i.e moving to MAR from MNAR conditional on group specific random effects."
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#hierarchies-in-code",
    "href": "talks/missing_data/missing_data_causal.html#hierarchies-in-code",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Hierarchies in Code",
    "text": "Hierarchies in Code\nSpecifying the hierarchical model in PyMC\nteam_idx, teams = pd.factorize(df_employee[\"team\"], sort=True)\nemployee_idx, _ = pd.factorize(df_employee[\"employee\"], sort=True)\ncoords = {\"team\": teams, \"employee\": np.arange(len(df_employee))}\n\n\nwith pm.Model(coords=coords) as hierarchical_model:\n    # Priors\n    company_beta_lmx = pm.Normal(\"company_beta_lmx\", 0, 1)\n    company_beta_male = pm.Normal(\"company_beta_male\", 0, 1)\n    company_alpha = pm.Normal(\"company_alpha\", 20, 2)\n    team_alpha = pm.Normal(\"team_alpha\", 0, 1, dims=\"team\")\n    team_beta_lmx = pm.Normal(\"team_beta_lmx\", 0, 1, dims=\"team\")\n    sigma = pm.HalfNormal(\"sigma\", 4, dims=\"employee\")\n\n    # Imputed Predictors\n    mu_lmx = pm.Normal(\"mu_lmx\", 10, 5)\n    sigma_lmx = pm.HalfNormal(\"sigma_lmx\", 5)\n    lmx_pred = pm.Normal(\"lmx_pred\", mu_lmx, sigma_lmx, observed=df_employee[\"lmx\"].values)\n\n    # Combining Levels\n    alpha_global = pm.Deterministic(\"alpha_global\", company_alpha + team_alpha[team_idx])\n    beta_global_lmx = pm.Deterministic(\n        \"beta_global_lmx\", company_beta_lmx + team_beta_lmx[team_idx]\n    )\n    beta_global_male = pm.Deterministic(\"beta_global_male\", company_beta_male)\n\n    # Likelihood\n    mu = pm.Deterministic(\n        \"mu\",\n        alpha_global + beta_global_lmx * lmx_pred + beta_global_male * df_employee[\"male\"].values,\n    )\n\n    empower_imputed = pm.Normal(\n        \"emp_imputed\",\n        mu,\n        sigma,\n        observed=df_employee[\"empower\"].values,\n    )\n\n    idata_hierarchical = pm.sample_prior_predictive()\n    idata_hierarchical.extend(\n        sample_blackjax_nuts(draws=20_000, random_seed=500, target_accept=0.99)\n    )\n    pm.sample_posterior_predictive(idata_hierarchical, extend_inferencedata=True)"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#model-structure",
    "href": "talks/missing_data/missing_data_causal.html#model-structure",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Model Structure",
    "text": "Model Structure\n\nHierarchical Team based Model"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#influence-of-team-membership",
    "href": "talks/missing_data/missing_data_causal.html#influence-of-team-membership",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Influence of Team Membership",
    "text": "Influence of Team Membership\n\nModification of Leader Impact based on Team imputed with Uncertainty"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#imputation-with-team-effects",
    "href": "talks/missing_data/missing_data_causal.html#imputation-with-team-effects",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Imputation with Team Effects",
    "text": "Imputation with Team Effects\n\nImputation patterns under team influence"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#feature-selection-is-not-causal-modelling",
    "href": "talks/missing_data/missing_data_causal.html#feature-selection-is-not-causal-modelling",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Feature Selection is not Causal Modelling",
    "text": "Feature Selection is not Causal Modelling\nOn the Importance of Theory Construction\n\n\n\n\n\nID\n\\(Y_{i}(0)\\)\n\\(Y_{i}(1)\\)\n\n\n\n\n1\n?\n1\n\n\n2\n1\n?\n\n\n3\n?\n0\n\n\n4\n?\n1\n\n\n5\n0\n?\n\n\n\nThe Fundamental problem of Causal Inference as Missing Data\n\n\n\nThe heart of causal inference is understanding the risk of confounding influence.\n\n\n\n\nNaively optimising for some in-sample predictive benchmark does not protect your model from confounding bias.\n\n\n\n\nCausal models with deliberate and careful construction of the dependence mechanism are your best hope for genuine insight and robust predictive performance\n\n\n\n\nThis is crucial for model explainability in the human-centric domains, where the decisions need to be justifiable.\n\n\n\n\nUsed to answer Counterfactuals\n\nHow do patterns of Employee empowerment change if we can intervene on their relationship with their manager?"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#counterfactuals-as-imputation.",
    "href": "talks/missing_data/missing_data_causal.html#counterfactuals-as-imputation.",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Counterfactuals as Imputation.",
    "text": "Counterfactuals as Imputation.\n\nBayesian Imputation can be highly customisable method of imputation in almost any domain.\nImputed treatment effect distributions need not be specified simply as a linear combination of observables.\n\nNon-parametric methods such as BART and Dirichlet Process Mixtures can be used.\n\nIn PyMC you can implement the do-operator to intervene on the graph that represents your data generating process."
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#conclusion",
    "href": "talks/missing_data/missing_data_causal.html#conclusion",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve seen the application of missing data analysis to survey data in the context of People Analytics.\n\nMultivariate approaches are effective but cannot help address confounding bias,\nThe flexibility of the Bayesian approach can be tailored to the appropriate complexity of our theory about why our data is missing.\nHierarchical structures pervade business - conduits for leadership influence/communication channels. Hierarchical modelling can isolate estimates of this impact and control for biases of naive aggregates.\n\nReveal inefficiencies and mismatches between team and management. Imputation gives “voice” to the missing.\n\n\n\n\n\nMissing Data with PyMC"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#bayesian-imputation.",
    "href": "talks/missing_data/missing_data_causal.html#bayesian-imputation.",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Bayesian Imputation.",
    "text": "Bayesian Imputation.\n\nBayesian Imputation can be highly customisable method of imputation in almost any domain.\nImputed treatment effect distributions need not be specified simply as a linear combination of observables.\n\nNon-parametric methods such as BART and Dirichlet Process Mixtures can be used.\n\nIn PyMC you can implement the do-operator to intervene on the graph that represents your data generating process."
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#bayesian-probabilistic-imputation.",
    "href": "talks/missing_data/missing_data_causal.html#bayesian-probabilistic-imputation.",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Bayesian Probabilistic Imputation.",
    "text": "Bayesian Probabilistic Imputation.\n\nBayesian Imputation can be highly customisable method of imputation in almost any domain.\nImputed treatment effect distributions need not be specified simply as a linear combination of observables.\n\nNon-parametric methods such as BART and Dirichlet Process Mixtures can be used.\n\nIn PyMC you can implement the do-operator to intervene on the graph that represents your data generating process.\nHow do patterns of Employee empowerment change if we can intervene on their relationship with their manager?"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#sota-bayesian-probabilistic-imputation.",
    "href": "talks/missing_data/missing_data_causal.html#sota-bayesian-probabilistic-imputation.",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "SOTA: Bayesian Probabilistic Imputation.",
    "text": "SOTA: Bayesian Probabilistic Imputation.\n\nBayesian Imputation can be highly customisable method of imputation in almost any domain.\nImputed treatment effect distributions need not be specified simply as a linear combination of observables.\n\nNon-parametric methods such as BART and Dirichlet Process Mixtures can also be used.\n\nIn PyMC you can implement the do-operator to intervene on the graph that represents your data generating process.\nUsed to answer Counterfactuals\n\nHow do patterns of Employee empowerment change if we can intervene on their relationship with their manager?"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#probabilistic-maps",
    "href": "talks/missing_data/missing_data_causal.html#probabilistic-maps",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Probabilistic Maps",
    "text": "Probabilistic Maps\nSampling the possible spaces over missing data\n\n\n\n\n\nBayes at work busily imputing undiscovered continents\n\n\n\nThe idea\n\nWhen gaps in survey data are not random\nWe need to understand the drivers of missing-ness\nThe “topology” around the gaps gives us clues\nBayesian inference helps to imputes the probable inclines and curves of the space in the gaps conditional on the observed data.\nIn our employee engagement data the question becomes - What are the enviromental influences on the probable responses?"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#agenda",
    "href": "talks/survival_regression/time_to_attrition.html#agenda",
    "title": "Survival Regression Models in PyMC",
    "section": "Agenda",
    "text": "Agenda\n\n\nTime to Event Distributions\n\nHyperObjects and Perspectives on Probability\n\n\n\n\n\nPeople Analytics and Survival Regression\n\nCox Proportional Hazard\nAccelerated Failure Time\n\n\n\n\n\nComparing Model Implications\n\nMarginal Predictions\nAcceleration Factors\n\n\n\n\n\nFrailty Models and Stratified Risk\n\n\n\n\nConclusion"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#layered-abstractions-and-hyperobjects",
    "href": "talks/survival_regression/time_to_attrition.html#layered-abstractions-and-hyperobjects",
    "title": "Survival Regression Models in PyMC",
    "section": "Layered Abstractions and Hyperobjects",
    "text": "Layered Abstractions and Hyperobjects\n\nConcepts that defy easy panoptic survey prohibit easy action e.g. climate change and mathematical structures such as families of probability distributions"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#perspectives-on-probability",
    "href": "talks/survival_regression/time_to_attrition.html#perspectives-on-probability",
    "title": "Survival Regression Models in PyMC",
    "section": "Perspectives on Probability",
    "text": "Perspectives on Probability\nTop-Down and Abstract\n\n\n\n\n\nParametric CDF and Survival functions obscure views of instantaneous hazard\n\n\n\\[ h(t) = \\frac{f(t)}{S(t)} \\\\\nH(t) = -ln(S(t))\\]"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#actuarial-tables-and-survival-curves",
    "href": "talks/survival_regression/time_to_attrition.html#actuarial-tables-and-survival-curves",
    "title": "Survival Regression Models in PyMC",
    "section": "Actuarial Tables and Survival Curves",
    "text": "Actuarial Tables and Survival Curves\nBottom-Up and Concrete\n\nActuarial Tables try to estimate CDF and Survival functionsCalculation of these abstract quantities proceeds from a clear and concrete notion of the risk-set in time."
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#time-to-attrition-data",
    "href": "talks/survival_regression/time_to_attrition.html#time-to-attrition-data",
    "title": "Survival Regression Models in PyMC",
    "section": "Time to Attrition Data",
    "text": "Time to Attrition Data\n\n\n Question: What is the relationship between individual characteristics and their expected survival times?\n\n\nQuestion: How do individual survival times vary as a function of the levels in their covariates."
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#regression-and-censored-data",
    "href": "talks/survival_regression/time_to_attrition.html#regression-and-censored-data",
    "title": "Survival Regression Models in PyMC",
    "section": "Regression and Censored Data",
    "text": "Regression and Censored Data\nCensored Data Biases simple summaries\n\\[\\mathbf{y_{i}} = \\beta \\mathbf{X_{i}} \\]\n\n\\[\\begin{split}\n\\begin{pmatrix}\n\\color{red}{y_{i, c}}  \\\\\n\\color{blue}{y_{i, \\neg c}}  \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n\\beta_{1} \\\\\n\\beta_{2} \\\\\n\\beta_{3}\n\\end{pmatrix} \\begin{pmatrix}\n\\color{red}{x_{1,c}^{i}} & \\color{red}{x_{2, c}^{i}} & \\color{red}{x_{3, c}^{i}}  \\\\\n\\color{blue}{x_{1,\\neg c}^{i}} & \\color{blue}{x_{2, \\neg c}^{i}} & \\color{blue}{x_{3, \\neg c}^{i}}  \\\\\n\\end{pmatrix}\n\\end{split}\n\\]\n\n\n\\[ \\Rightarrow L(\\mathbf{\\beta}, S(\\color{red}{y_{c}}), f(\\color{blue}{y_{\\neg c}}) ) \\]\n\n\n\\[ \\underbrace{p( \\beta | y)}_{\\text{posterior draws}} = \\frac{p(\\mathbb{\\beta})L(\\mathbf{\\beta}, S(\\color{red}{y_{c}}), f(\\color{blue}{y_{\\neg c}}) )}{\\int_{i}^{n} L(\\mathbf{\\beta}_{i}, S(\\color{red}{y_{c}}), f(\\color{blue}{y_{\\neg c}}) )p(\\mathbf{\\beta_{i}}) } \\]"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#regression-for-survival-times",
    "href": "talks/survival_regression/time_to_attrition.html#regression-for-survival-times",
    "title": "Survival Regression Models in PyMC",
    "section": "Regression for Survival Times",
    "text": "Regression for Survival Times\nDistributions for Modelling Attrition\n\nMonotonoc or non-monotonic hazards determined by distribution choice. Risk spikes important in early periods of employment."
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#accelerated-failure-time-models",
    "href": "talks/survival_regression/time_to_attrition.html#accelerated-failure-time-models",
    "title": "Survival Regression Models in PyMC",
    "section": "Accelerated Failure Time Models",
    "text": "Accelerated Failure Time Models\nParametric Models of Survival Distributions\n\\[S_{i}(t) = S_{0}\\Bigg(\\frac{t}{exp(\\mu + \\alpha_{1}x_{1} + \\alpha_{2}x_{2} ... \\alpha_{p}x_{p})} \\Bigg) \\]\nwith pm.Model(coords=coords, check_bounds=False) as aft_model:\n  X_data = pm.MutableData(\"X_data_obs\", X)\n  beta = pm.Normal(\"beta\", 0.0, 1, dims=\"preds\")\n  mu = pm.Normal(\"mu\", 0, 1)\n\n  s = pm.HalfNormal(\"s\", 5.0)\n  eta = pm.Deterministic(\"eta\", pm.math.dot(beta, X_data.T))\n  reg = pm.Deterministic(\"reg\", pt.exp(-(mu + eta) / s))\n  y_obs = pm.Weibull(\"y_obs\", beta=reg[~cens], alpha=s, observed=y[~cens])\n  y_cens = pm.Potential(\"y_cens\", weibull_lccdf(y[cens], alpha=s, beta=reg[cens]))\n  \n  idata = pm.sample_prior_predictive()\n  idata.extend(\n      pm.sample(target_accept=0.95, random_seed=100, idata_kwargs={\"log_likelihood\": True})\n  )\n  idata.extend(pm.sample_posterior_predictive(idata))"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#parametric-model-structure",
    "href": "talks/survival_regression/time_to_attrition.html#parametric-model-structure",
    "title": "Survival Regression Models in PyMC",
    "section": "Parametric Model Structure",
    "text": "Parametric Model Structure\n\n\n\n\nAccelerated Failure time models incorporate the regression component as a weighted sum that enters the parametric probability model\nFor instance: \\[ Weibull(\\alpha, \\beta)\n\\\\ = Weibull(\\alpha, reg)\\]"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#proportional-hazards-cox-regression",
    "href": "talks/survival_regression/time_to_attrition.html#proportional-hazards-cox-regression",
    "title": "Survival Regression Models in PyMC",
    "section": "Proportional Hazards Cox Regression",
    "text": "Proportional Hazards Cox Regression\nFlexible Discrete Intervals Hazards\n\\[ \\text{Baseline Hazard: } \\lambda_{0}(t) \\] \\[ \\lambda_{0}(t) \\cdot exp(\\beta_{1}X_{1} + \\beta_{2}X_{2}... \\beta_{k}X_{k}) \\]\n\nwith pm.Model(coords=coords) as base_model:\n  X_data = pm.MutableData(\"X_data_obs\", retention_df[preds], dims=(\"individuals\", \"preds\"))\n  lambda0 = pm.Gamma(\"lambda0\", 0.01, 0.01, dims=\"intervals\")\n\n  beta = pm.Normal(\"beta\", 0, sigma=1, dims=\"preds\")\n  lambda_ = pm.Deterministic(\n      \"lambda_\",\n      pt.outer(pt.exp(pm.math.dot(beta, X_data.T)), lambda0),\n      dims=(\"individuals\", \"intervals\"),\n  )\n  mu = pm.Deterministic(\"mu\", exposure * lambda_, \n                        dims=(\"individuals\", \"intervals\"))\n\n  obs = pm.Poisson(\"obs\", mu, observed=quit, \n                    dims=(\"individuals\", \"intervals\"))\n  idata = pm.sample(\n      target_accept=0.95, random_seed=100, idata_kwargs={\"log_likelihood\": True}\n  )"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#proportional-hazards-cox-regression-1",
    "href": "talks/survival_regression/time_to_attrition.html#proportional-hazards-cox-regression-1",
    "title": "Survival Regression Models in PyMC",
    "section": "Proportional Hazards Cox Regression",
    "text": "Proportional Hazards Cox Regression\n\n\n\n\n\nSorites Paradoxes and accumulation of a triggering resource\n\n\n\n\n“There’s the ‘mañana paradox’: the unwelcome task which needs to be done, but it’s always a matter of indifference whether it’s done today or tomorrow; the dieter’s paradox: I don’t care at all about the difference to my weight one chocolate will make.” - Dorothy Edgington\n\n\n\n\nBaseline Instantenous Hazard"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#proportional-hazards-cox-regression-2",
    "href": "talks/survival_regression/time_to_attrition.html#proportional-hazards-cox-regression-2",
    "title": "Survival Regression Models in PyMC",
    "section": "Proportional Hazards Cox Regression",
    "text": "Proportional Hazards Cox Regression\n\nCumulative Hazard across Individuals"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#proportional-hazards-cox-regression-3",
    "href": "talks/survival_regression/time_to_attrition.html#proportional-hazards-cox-regression-3",
    "title": "Survival Regression Models in PyMC",
    "section": "Proportional Hazards Cox Regression",
    "text": "Proportional Hazards Cox Regression\nUsing the Poisson Trick\n\n\n\n\n\\[ CoxPH(left, month) \\sim gender + level \\]\nis akin to\n\\[ left \\sim glm(gender + level + (1 | month)) \\\\ \\text{ where link is } Poisson  \\]\napplying an offset to the event rate for each time interval."
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#comparing-models",
    "href": "talks/survival_regression/time_to_attrition.html#comparing-models",
    "title": "Survival Regression Models in PyMC",
    "section": "Comparing Models",
    "text": "Comparing Models\nStated Intention and Sentiment\n\\[ CoxPH(left, month) \\sim gender + field + level + sentiment \\]\n\\[ CoxPH(left, month) \\sim gender + field + level + sentiment + intention \\]"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#comparing-models-1",
    "href": "talks/survival_regression/time_to_attrition.html#comparing-models-1",
    "title": "Survival Regression Models in PyMC",
    "section": "Comparing Models",
    "text": "Comparing Models\nInterpreting Model Coefficients\n\nIf \\(exp(\\beta)\\) > 1: An increase in X is associated with an increased hazard (risk) of the event occurring.\nIf \\(exp(\\beta)\\) < 1: An increase in X is associated with a decreased hazard (lower risk) of the event occurring.\nIf \\(exp(\\beta)\\) = 1: X has no effect on the hazard rate.\n\nPredicting Marginal Effects"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#comparing-models-2",
    "href": "talks/survival_regression/time_to_attrition.html#comparing-models-2",
    "title": "Survival Regression Models in PyMC",
    "section": "Comparing Models",
    "text": "Comparing Models\nPredicting Marginal Effects\n\n\n\n\n\nThe Intention Model absorbs some of the variance in the baseline hazard not accounted for in the sentiment model\n\n\n\n\n\n\n\nThe Sentiment Model"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#comparing-models-3",
    "href": "talks/survival_regression/time_to_attrition.html#comparing-models-3",
    "title": "Survival Regression Models in PyMC",
    "section": "Comparing Models",
    "text": "Comparing Models\n\nAFT models allow us to quantify the acceleration factor due to an individual or group’s risk profile."
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#comparing-models-4",
    "href": "talks/survival_regression/time_to_attrition.html#comparing-models-4",
    "title": "Survival Regression Models in PyMC",
    "section": "Comparing Models",
    "text": "Comparing Models\nMarginal Survival Functions and WAIC\n\n\n\nComparing Marginal Survival Functions"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#frailty-models-and-individual-heterogeneity",
    "href": "talks/survival_regression/time_to_attrition.html#frailty-models-and-individual-heterogeneity",
    "title": "Survival Regression Models in PyMC",
    "section": "Frailty Models and Individual Heterogeneity",
    "text": "Frailty Models and Individual Heterogeneity\nWe want to relax the assumptions of Cox Proportional Hazards model. We introduce (i) frailty terms and (ii) stratified risks\n\\[ \\lambda_{i}(t) = \\color{green}{z_{i}}exp(\\beta X)\\color{red}{\\lambda_{0}^{g}(t)} \\]\nThe multiplicative frailty terms \\(z_{i}\\) can be specified as a gamma distribution centred on unity with stratified risks"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#proportional-hazards-cox-regression-4",
    "href": "talks/survival_regression/time_to_attrition.html#proportional-hazards-cox-regression-4",
    "title": "Survival Regression Models in PyMC",
    "section": "Proportional Hazards Cox Regression",
    "text": "Proportional Hazards Cox Regression\nThe Proportional Hazards Assumption\n\nThe covariates enter once into the weighted sum that modifies the baseline hazard.\nWhile the baseline hazard can change over time the difference in hazard induced by different levels in the covariates remains constant over time.\n\n\\[ \\forall t \\in T:  \\frac{h(t | X_{gender} = 1)}{h(t | X_{gender} = 0)} = constant \\]"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#frailty-model-in-codes",
    "href": "talks/survival_regression/time_to_attrition.html#frailty-model-in-codes",
    "title": "Survival Regression Models in PyMC",
    "section": "Frailty Model in Codes",
    "text": "Frailty Model in Codes\n\nwith pm.Model(coords=coords) as frailty_model:\n        X_data_m = pm.MutableData(\"X_data_m\", df_m[preds], dims=(\"men\", \"preds\"))\n        X_data_f = pm.MutableData(\"X_data_f\", df_f[preds], dims=(\"women\", \"preds\"))\n        lambda0 = pm.Gamma(\"lambda0\", 0.01, 0.01, dims=(\"intervals\", \"gender\"))\n        sigma_frailty = pm.Normal(\"sigma_frailty\", opt_params[\"alpha\"], 1)\n        mu_frailty = pm.Normal(\"mu_frailty\", opt_params[\"beta\"], 1)\n        frailty = pm.Gamma(\"frailty\", mu_frailty, sigma_frailty, dims=\"frailty_id\")\n\n        beta = pm.Normal(\"beta\", 0, sigma=1, dims=\"preds\")\n\n        ## Stratified baseline hazards\n        lambda_m = pm.Deterministic(\n            \"lambda_m\",\n            pt.outer(pt.exp(pm.math.dot(beta, X_data_m.T)), lambda0[:, 0]),\n            dims=(\"men\", \"intervals\"),\n        )\n        lambda_f = pm.Deterministic(\n            \"lambda_f\",\n            pt.outer(pt.exp(pm.math.dot(beta, X_data_f.T)), lambda0[:, 1]),\n            dims=(\"women\", \"intervals\"),\n        )\n        lambda_ = pm.Deterministic(\n            \"lambda_\",\n            frailty[frailty_idx, None] * pt.concatenate([lambda_f, lambda_m], axis=0),\n            dims=(\"obs\", \"intervals\"),\n        )\n\n        mu = pm.Deterministic(\"mu\", exposure * lambda_, dims=(\"obs\", \"intervals\"))\n\n        obs = pm.Poisson(\"outcome\", mu, observed=quit, dims=(\"obs\", \"intervals\"))\n        frailty_idata = pm.sample_prior_predictive()\n        frailty_idata.extend(pm.sample(random_seed=101))"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#frailty-model-structure",
    "href": "talks/survival_regression/time_to_attrition.html#frailty-model-structure",
    "title": "Survival Regression Models in PyMC",
    "section": "Frailty Model Structure",
    "text": "Frailty Model Structure\nIndividual Frailties"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#frailty-model-and-stratified-baseline-risks",
    "href": "talks/survival_regression/time_to_attrition.html#frailty-model-and-stratified-baseline-risks",
    "title": "Survival Regression Models in PyMC",
    "section": "Frailty Model and Stratified Baseline Risks",
    "text": "Frailty Model and Stratified Baseline Risks\n\n\n\n\nSurvival Regression in PyMC"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#frailty-model-structure-1",
    "href": "talks/survival_regression/time_to_attrition.html#frailty-model-structure-1",
    "title": "Survival Regression Models in PyMC",
    "section": "Frailty Model Structure",
    "text": "Frailty Model Structure\nShared Frailties\n\n\n\n\n\n\n\nShared Frailties by Field of Occupation"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#frailty-models-and-stratified-baseline-risks",
    "href": "talks/survival_regression/time_to_attrition.html#frailty-models-and-stratified-baseline-risks",
    "title": "Survival Regression Models in PyMC",
    "section": "Frailty Models and Stratified Baseline Risks",
    "text": "Frailty Models and Stratified Baseline Risks\n\nWe see differences in the risks stratified by gender and additionally how the magnitude of the baseline hazard shrinks with more or less well-specified covariate in the model."
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#individual-frailties-and-marginal-statistics",
    "href": "talks/survival_regression/time_to_attrition.html#individual-frailties-and-marginal-statistics",
    "title": "Survival Regression Models in PyMC",
    "section": "Individual Frailties and Marginal Statistics",
    "text": "Individual Frailties and Marginal Statistics"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#conclusion",
    "href": "talks/survival_regression/time_to_attrition.html#conclusion",
    "title": "Survival Regression Models in PyMC",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\n\nSurvival Analysis is a tool for the expression of probabilities governing state-transitions\n\n\n\n\nImportant everywhere process efficiency and transformative outcomes matter. Corrects for censorship bias of naive summaries.\n\n\n\n\nAllows for sophisticated expression of risk over time and along many dimensions. Variety of hierarchical modelling options\n\n\n\n\nBayesian estimation of these complex model structures is natural and informative. Meaningful across a range of disciplines and domains.\n\n\n\n\nProvides an actionable lens on “actuarial” risk and “diagnostic” causal analysis in time.\n\n\n\n\n\n\nDicing with Death\n\n\n\n\n\nWhen sand becomes a heap? When a heap returns to sand?\n\n\n\n\n\n\n\nSurvival Regression in PyMC"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#actuarial-tables-and-survival-curves-1",
    "href": "talks/survival_regression/time_to_attrition.html#actuarial-tables-and-survival-curves-1",
    "title": "Survival Regression Models in PyMC",
    "section": "Actuarial Tables and Survival Curves",
    "text": "Actuarial Tables and Survival Curves\nBottom-Up and Concrete\ndef make_actuarial_table(actuarial_table):\n    ### Actuarial lifetables are used to describe the nature \n    ### of the risk over time derived from instantaneous hazard\n    actuarial_table[\"p_hat\"] = (actuarial_table[\"failed\"] / \n                                actuarial_table[\"risk_set\"])\n    actuarial_table[\"1-p_hat\"] = 1 - actuarial_table[\"p_hat\"]\n    ### Estimate of Survival function\n    actuarial_table[\"S_hat\"] = actuarial_table[\"1-p_hat\"].cumprod()\n    actuarial_table[\"CH_hat\"] = -np.log(actuarial_table[\"S_hat\"])\n    ### The Estimate of the CDF function\n    actuarial_table[\"F_hat\"] = 1 - actuarial_table[\"S_hat\"]\n    actuarial_table[\"V_hat\"] = greenwood_variance(actuarial_table)\n    return actuarial_table"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html",
    "href": "talks/missing_data/missing_data_causal.html",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "",
    "text": "Background\n\n\n\n\nI’m a data scientist at Personio\nContributor at PyMC and PyMC labs\n\nBayesian statistician,\nReformed philosopher and logician.\n\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\nDisclaimer\n\n\n\nNone of Personio’s data was used in this presentation\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\n\nThe worked examples used here can be found here"
  },
  {
    "objectID": "notes/certain_things/Statistics/PyMC Labs.html",
    "href": "notes/certain_things/Statistics/PyMC Labs.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "Work on discrete choice.\nI’m Nathaniel Forde a Senior data scientist from Dublin. Previously worked in e-commerce for car rentals on pricing where i first started thinking about discrete choice. I’ve been working with Bayesian models for about 5 years. I’ve been a regular contributer to PyMC docs, especially for this project working on discrete choice models which is why Thomas asked me to consult on this project."
  },
  {
    "objectID": "notes/certain_things/Statistics/Introduction - Statistics Topics.html",
    "href": "notes/certain_things/Statistics/Introduction - Statistics Topics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "This section will serve to capture my Zettelkasten style notes on topics in Statistics."
  },
  {
    "objectID": "notes/certain_things/Logic/Introduction - Logic Topics.html",
    "href": "notes/certain_things/Logic/Introduction - Logic Topics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "This section will serve to capture any and all Zettelkasten notes on topics in logic -mathematical, philosophical or any style in between."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Introduction - Philosophy Topics.html",
    "href": "notes/certain_things/Philosophy/Introduction - Philosophy Topics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "This section will serve to capture the philosophical topics in my Zettelkasten notes."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Sorites Paradox and Survival Analysis.html",
    "href": "notes/certain_things/Philosophy/Sorites Paradox and Survival Analysis.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "There seems to be a plausible relationship between (a) the idea of a latent evolving hazard in #survival-analysis and (b) the accumulation effect that drives our intuitions from observations of distinct sand grains to observations of a heap. The classic philosophical puzzle put forward by Sorites.\nI’ve worked on survival analysis in the context of statistics and data science , but I’m putting together this note to arrange my thoughts on the value the perspective has on the classic philosophical puzzle.\nThe first thing to observe is how survival analysis is in general a model of the probabilities of state-transition. Moving between alive-dead, sick-well, subscribed-churned, hired-fired. The Framework is quite abstract and therefore widely applicable to the analysis of all state transitions with both clear, distinct and permeable borders between states.\nTraditionally you might see frequentist elaborations of the mechanics of #survival-analysis , but it becomes more interesting from a philosophical stand point when you phrase the model in a #Bayesian fashion.\nIn the Bayesian setting the uncertainty expressed in the model regarding measures of risk of state-change can be seen to contribute to the semantic ambiguity relevant in the Sorites setting. I will try to bring out this connection in the following."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Sorites Paradox and Survival Analysis.html#the-sorites-paradox",
    "href": "notes/certain_things/Philosophy/Sorites Paradox and Survival Analysis.html#the-sorites-paradox",
    "title": "Examined Algorithms",
    "section": "The Sorites Paradox",
    "text": "The Sorites Paradox\nThe typical presentation of the sorites issue in philosophy has been as a series of conditional predications as follows:\n\nFa_0\nFa_0 -> Fa_1\n.\n.\n.\nFa_{i-1} -> Fa_i\nTherefore: Fa_i\n\nWhere we allow that there is an indifference relation for the predication of F over all elements of the sequence preceding the last entry. Then, it is argued, the last predication fails for some entry (i) in the sequence. The predicate F is said to be suffer from #vagueness .This is purportedly a paradox due to the requirement of logical validity over conditional reasoning caused by the vagueness of F over the course of the sequence.\nOther rephrases of the logical steps truncate the sequence of conditionals by appealing to a Principle of mathematical Induction to arrive at the same conclusion. For out purposes it’s not crucial which phrasing is applied. The point is just that the the vagueness of the predicate F is said to threaten some central tenet of logical validity.\n\nApproaches to the Paradox\nThere have been advocates for acceptance of the Paradox - allowing that there is just a breakdown of logic in the case of these vague predicates. So much the worse for logic. This is quite radical as the prevalence of vague predicates in natural language commits us implicitly to the view that we cannot make distinctions.\nThe more plausible approaches to the Paradox seek to establish a reason for rejecting the validity of the conclusion by denying the premises in some way e.g. claiming that the indifference over the nth predication of F and the n-1 case fails. There is a precise point which is a genuine cut-off between F and not F. This position is called Epistemicism - which locates the causes of vagueness in predication in our degrees of ignorance.\nThe suggestion above is supported somewhat empirically by the reality of borderline cases of predication among reasonable speakers of the same language. People evince hedging behaviour and deliberate vagueness in cases where they avoid commitment to a sharp distinction. “She is sorta cool, nice-ish!”. This is the data the philosophical theory needs to explain. Paradigmatic cases of attributed state-change coupled with paradigmatic cases of hedging.\nThe theoretical question then becomes - what constitutes borderline vagueness? I think this is where we can use survival analysis to elaborate and explain cases of borderline vagueness and empirical cases of disagreement in predication. In particular Bayesian approaches to survival analysis which allow that there is a cut-off point in the sequence, but there is genuine uncertainty where we locate the cut-off point. Seeing this as a problem of Bayesian modelling allows us to locate the sources of hedging in the components and precision of our model terms through which the propagates the uncertainty in our attribution patterns.\n\n\nPerspectives on Probability\nSurvival analysis can seem intimidating because it asks us to understand time-to-event distributions from four distinct perspectives. The first familiar density function, the next cumulative density function and its inverse survival function. Additionally we can view the the cumulative hazard function as a transformation of the survival function, and the instantaneous hazard as a discretisation of over intervals of the temporal sequence ranged over by the cumulative hazard.\nIt’s important to see and understand that these quantities, while appearing to be abstract mathematical objects, can be derived from simple tables which record the subjects in the risk set which experience the event of interest at each interval of time. In other words the set of conditional probabilities instance-by-instance over the range of the temporal sequence. This is how we derive the instantaneous hazard quantity.\nDifferent families of probability distribution allow us to encode different structures in the hazards. For instance if we want hazards to peak early in the sequence and decline later in the sequence non-monotonically we can use the loglogistic distribution. If we want to ensure monotonic hazard sequences we can use Weibull distributions.\n\n\nDistinguishing Risk and Uncertainty\nWe want to explain the semantic ambiguity of Sorites phenomena by the probabilistic nature of state transitions over additive sequences. However, we won’t trade on the uncertainty between distinct models i.e. it’s not merely that your model of the sand-to-heap transition is characterised by one probability distributions and mine by another (although it could be). We are interested in divergences of opinion and semantic uncertainty that arises due to the stochastic nature of the phenomena where we share the same model of the phenomena. This reflects a difference in the view of the risk not the model uncertainty.\n\n\nCox Proportional Hazards Model\nTo make this a bit more concrete consider the cox proportional hazard model. This is a #regression model which aims to characterise the probability of state transition using a statistical model with two components. The baseline hazard:\n\\[ \\lambda_0 (t) \\]\nwhich is combined multiplicatively with an exponentiated weighted linear sum as follows \\[\\lambda_0 (t) \\cdot e^{\\beta_0 X_0 + \\beta_1 X_1 ... \\beta_n X_n}\\] In this model the baseline hazard is a function of the time intervals and we estimate a hazard term for each interval when we fit the model. There is a “free” parameter for the instantaneous hazard at each timepoint. This sequence is the baseline hazard. This latent baseline hazard is akin to an intercept term(s) in more traditional regression models. Individual predictions of the evolving hazard are then determined by how the individuen’s covariate profile modifies the baseline hazard. Estimation procedures for this model find values for the baseline hazard and for the coefficient weights \\(\\beta_i\\) in our equation.\nWith these structures in mind you might be tempted to locate the source of disagreement between people’s judgments as stemming from differences in their covariates \\(X_i\\) , or put another way… we see the probability of transition as a function of the same variables, but disagree on the values of those inputs to the function. The benefit of this perspective is that instead of seeing the Sorites Paradox as an error of logical reasoning that needs to be fixed by one or more adjustments to classical logic, we can instead view the phenomena as reflecting disagreement among latent probabilistic models.\n\n\nComplexity of the Heap\nOne additional perspective on the problem is gained by noting how the Cox proportional model is a prediction model and comes with criteria of model adequacy. How many predictor variables are required to anticipate state transition? How much variance is explained? If we can gauge the complexity of the prediction task, can the complexity itself explain disagreement?\n\n\nHierarchical or Meta Vagueness\nWe’ve seen now a few different sources of divergences. At the highest level we can appeal to the Knightian distinction between risk and uncertainty, then secondarily to differences in the data used to calibrate risk or thirdly in differences due to estimation strategies and finally in pure prediction complexity.\nIf divergences are due to complete uncertainty of the appropriate model, then we concede allot to the sceptic and the quantification of any plausible cut point is hopeless. If differences result from the other candidate sources there remains hope for arriving at intersubjective consensus.\nThis can be seen in some sense in Bayesian model development workflow with hierarchical survival models. Instead of imagining agents reasoning if-then style through a sequence of additional sand grains. Let’s picture the reasoner working with a latent survival model, negotiating a contract between reality and their linguistic usage.\nHierarchical models in the Bayesian setting are typical and interesting in their own right as they allow for the expression of heterogeneity across individuals. Broadly they involve adding one or more parameters that modify the baseline model equation. We saw earlier that the Cox Proportional hazard model is expressed as \\[\\lambda_0 (t) \\cdot e^{\\beta_0 X_0 + \\beta_1 X_1 ... \\beta_n X_n}\\]\nThis can be modified as follows:\n\\[z_{i} \\cdot \\lambda_0 (t) \\cdot e^{\\beta_0 X_0 + \\beta_1 X_1 ... \\beta_n X_n}\\] Where we allow an individual “frailty” term \\(z_{i}\\) is added to the model as a multiplicative factor for each individual in the data set. The terms are drawn from a distribution, often centred on 1, so that the average individual modifies the baseline model not at all… but modifications are expressed as a reduction or increase to the multiplicative speed of state transition. The baseline model can therefore be considered\nRecall that the Bayesian modelling exercise quantifies the probability distribution of all the parameters in the model. A well specified baseline model will mean that less explanatory work needs to be done by the individual frailty terms. A poorly specified model will locate allot of weight in these terms. This is a mechanism which helps quantify the degree of irreducible uncertainty in our attribution patterns derived from our understanding of the paradigmatic cases (our sample).\n\n\nThe Bayesian Reasoner\nAn individual reasoner working with their set of paradigmatic data points y ~ f(X | \\(\\theta\\)) may fit their model to this data. The variance in the distribution of frailty terms \\(z_{i} \\in \\theta\\) estimated represents their view of the remaining uncertainty in cases after controlling for the impact of the covariate profiles across the cases.\nThese quantities represents disagreements regarding the speed up or slow down in the survival curves…but the survival curves quantifies the probability of transition at each point of accumulation. So a survival model allows us to say precisely at each point of accumulation what is the probability of transition. For any given point in series of accumulating instances, the diversity of individual frailty terms needed to account for the predications determine the quantifiable range of uncertainty in the survival probabilities we derive from the paradigmatic cases.\nEpistemicism about existence of a cut point for vague predicates will always assume the existence of threshold. The picture of Sorites Paradox for the Bayesian reasoner sees them go from uncertainty to uncertainty updating the latent model as they go. Maybe the model converges tightly in some cases, maybe not. Incorporating more paradigmatic instances, more covariate indicators as they develop conceptual clarity on what drives the attribution of state-hood under the evolving or growing pressure to change. Finding the threshold would not and could not be a solution to the paradox. Any threshold will be context specific and also learned (with uncertainty) relative to the tolerances of the domain. Understanding that paradox as yet another instance of learning in a multifaceted world at least lets us see the problem without requiring torturous modifications to classical logic."
  },
  {
    "objectID": "oss/pymc/frailty_survival.html",
    "href": "oss/pymc/frailty_survival.html",
    "title": "Frailty Survival Models in PyMC",
    "section": "",
    "text": "In this project I demonstrated how to fit a variety of hierarchical survival models in PyMC. We applied these techniques to a human resources data set attempting to measure time-to-attrition across individuals surveyed. The project is interesting within my work in Personio as example of modelling for process improvements and the measurement of drivers for efficiencies. Additionally, I think there is an interesting connection with the philosophical puzzle of the Sorites Paradox.\nThe project culminated in a publication to the official PyMC documentation that can be found online here or downloaded as notebook here"
  },
  {
    "objectID": "notes/certain_things/Statistics/Non-Parametric Bayesian Causal Inference.html",
    "href": "notes/certain_things/Statistics/Non-Parametric Bayesian Causal Inference.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "The clearest framework for causal inference has a tight relationship with missing data imputation. However, the range of problems addressed seem to require a slew of distinct estimators. It is not always clear in which circumstances we should apply each estimation procedure.\nLike most statistical work the practical details should be worked out in code for the clearest demonstration. However, here we’ll describe (as best we can) the conceptual underpinning of important estimators. Their usage and motivation.\n\nTaxonomies of Missing-ness\nCausal inference can be seen as a species of missing data problem where the missing data is the counterfactual situation(s) of how the world would have been were the course of the world different from the one we know. What if we used this treatment plan rather than another? What it the actors’ behaviour differed from the actions they in fact pursued?\nFrom an estimation perspective there are different species of missing-ness that matter. We won’t here go deep into the distinctions. It is enough to note that they vary in the operative source of the missing-ness: Missing-completely-at-random (MCAR), missing-at-random (MAR) and missing-not-at-random (MNAR).\nVery crudely estimation procedures work reasonably well under (MAR) and (MCAR) but require extra effort when there is assumptions if we hope to account for the (MNAR) cases. This stems largely from successful applications of the law of iterated expectations.\n\nThe various estimation procedures for counterfactual results trade on this property of expectation.\nMore convenient again, when we consider cases of missing data conditional the observed covariate profile, we can derive a propensity score as a one number summary for the conditional probability of missingness. This can be used in imputation techniques where we want to carefully attribute values to the missing data that respect the other observed properties of the individual.\nThe propensity score has a role in a number of reweighting schemes for the estimation of missing data. These rely on the property of expectation under (MAR). So accuracy of the propensity score is itself an important question. The missing-ness variable \\(R\\) is a binary random variable in \\(\\{ 0, 1 \\}\\). Maximum likelihood methods for logistic regression are often used to estimate these terms.\n\\[p_{R}(\\mathbf{x}) \\sim logit(X_{i} \\beta )\\]\nThis score is a summary in some sense of the factors driving missing-ness. In the context of causal inference it is often stated “in reverse” as a probability of being treated. We will keep focus here on the case of missing data, but the generality of the notion shouldn’t be lost. The propensity score is a one number summary of the covariate profile for each individual in the data and under the (MAR) assumption - it alone is sufficient to conditionalise"
  },
  {
    "objectID": "notes/certain_things/Statistics/Analogies between Missing Data and Causal Inference.html",
    "href": "notes/certain_things/Statistics/Analogies between Missing Data and Causal Inference.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "The clearest framework for causal inference has a tight relationship with #missing-data imputation. However, the range of problems addressed seem to require a slew of distinct estimators. It is not always clear in which circumstances we should apply each estimation procedure.\nLike most statistical work the practical details should be worked out in code for the clearest demonstration. However, here we’ll describe (as best we can) the conceptual underpinning of important estimators. Their usage and motivation."
  },
  {
    "objectID": "notes/certain_things/Statistics/Analogies between Missing Data and Causal Inference.html#causal-inference",
    "href": "notes/certain_things/Statistics/Analogies between Missing Data and Causal Inference.html#causal-inference",
    "title": "Examined Algorithms",
    "section": "Causal Inference",
    "text": "Causal Inference\nThe above perspective on missing data has deep analogies with the potential outcomes framework of causal inference. This is well brought out in the beautiful book Foundations of Agnostic Statistics by Aronow and Miller.\n\nPotential Outcomes and SUTVA\nIn the causal context we assume the potential outcomes framework and the notation of \\(Y(1), Y(0)\\) to denote the value of the outcome under the treatment regime \\(D\\).\n\\[ Y_{i} =  \n\\left\\{\\begin{array}{lr}\n        Y_{i}(0), & \\text{for } D = 0\\\\\n        Y_{i}(1), & \\text{for } D = 1\\\\\n        \\end{array}\\right\\}\n\\] where the (S)table (U)nit (T)reatment (V)alue (A)ssumption holds i.e. the observed data under treatment or non-treatment regimes is the potential outcome for that individual. Additionally the counterfactual outcome is assumed to be stable for each individual. It is crucially this assumption that allows for statistical identification of key metrics in causal inference under randomisation.\n\n\nAverage Treatment Effects\nSimilarly, here we rely on the properties of expectation over the observed data to isolate quantities of causal effect. In particular we tend to be interested in the average treatment effects, which we can get by using the following decomposition under random assignment.\n\\[ E[\\tau] = E[Y_{i}(1) - Y_{i}(0)] = E[Y_{i}(1)] - E[Y_{i}(0)] \\] This decomposition is crucial since it allows us to move between the expectations derived from the observed data under each regime towards an estimate of the population treatment effects.\n\n\n\nsubject\n\\(Y_{i}(1)\\)\n\\(Y_{i}(0)\\)\n\\(\\tau\\)\n\n\n\n\nJoe\n?\n115\n?\n\n\nBob\n120\n?\n?\n\n\nJames\n100\n?\n?\n\n\nMary\n115\n?\n?\n\n\nSally\n120\n?\n?\n\n\nLaila\n?\n105\n?\n\n\n\\(E[Y_{i}(D)]\\)\n113.75\n110\n3.75\n\n\n\nThe missing values in this table depict the fundamental problem of causal inference as a missing data issue. So #causal-inference as a strategy is broadly related to finding ways to solve this missing data problem under different regimes of missing-ness. For instance, the reason A/B testing works to isolate the treatment effects is that under randomisation of treatment regime we are implicitly assuming that the reason missing-data is effectively a case of MCAR missing-ness. As such the expectations of the individual columns in the above table are valid estimates which can then be combined using the above decomposition to derived the treatment effect \\(\\tau\\).\nIn this case the pattern of reasoning is akin to performing mean-imputation and then taking the difference of the averages. The imputation step is redundant in A/B testing, but it is highlighted by Aronow and Miller as a useful lens on more complex causal inference tasks on observed data. We are always (under the hood) trying to impute the missing values to gain a better view of the treatment effect distribution. ### Regression Estimators\nAgain we rely on the idea of regression as an approximation to the CEF of the data generating process. The flexibility of regression modelling for automating a host of statistical test should be reasonably familiar. The point here is not to rehash the theory but just to note the similarity with the procedures used above for regression-based imputation. Regression modelling of the treatment effect proceeds on the strong ignorability assumption that - conditional on the observed covariates knowing whether or not an individual received the treatment adds no new information i.e. it is the insistence that assignment might as well be random after accounting for the background characteristics. These assumptions mirror the conditions required for imputation under the MAR regime.\nSo we can derive estimates for the ATE from the data generating model\n\\[ Y \\sim \\beta_0 + \\beta_1 D + ... \\beta_{n} \\cdot X_{n} \\] such that out quantity of interest \\(\\tau\\) is cleanly identified in expectation by the quantity: \\[ E[\\tau] = \\beta_{1}\\] But this result can also be derived by predicting the outcomes under the different treatment regimes, using a fitted regression model, and taking the differences of the averaged predictions over the cases. The equivalence between these perspectives is the insight we want to record here. We drew out this connection in the discussion of poststratification estimators\n\nThis is a neat and beautiful connection between causal-inference and missing data analysis. Simultaneously a reminder of the versatility of regression analysis.\n\n\nPropensity Functions and Reweighting Estimators.\nWe will skip the detailed elaboration of propensity score matching, a technique for creating pseudo treatment and control groups, only noting that there is a rich and detailed literature on the topic for causal inference.\nWe do want to draw out how propensity-scores can be used in the class of reweighting estimators. Where under the strong ignorability assumption we can estimate the treatment effect as a simple expectation:\n\\[E[\\tau] = E [\\dfrac{YD}{p_D(X)} - \\dfrac{Y(1-D)}{(1 - p_D (X))}] \\]\nUsing this formula we can scale each observation by the relative probabilities for the individual falling into each treatment regime. Then the expectation of the scaled differences is an estimate of our ATE. The logic of this inverse probability weighting (IPW) estimator stems from the idea that low propensity individuals are likely underrepresented in the treatment group and over represented in the control. So this estimator down weights and unweights each option accordingly to “balance” the groups before comparison.\nThis balancing operation can work but is dependent on empirical properties of the sample data. Even if the data generating process ensures that strong ignorability holds, if our sample under represents the variety of possible individual in each group then reweighting the remaining individuals is no guarantee for sound inference. This is a small sample problem recurring."
  },
  {
    "objectID": "notes/certain_things/Statistics/Analogies between Missing Data and Causal Inference.html#taxonomies-of-missing-ness",
    "href": "notes/certain_things/Statistics/Analogies between Missing Data and Causal Inference.html#taxonomies-of-missing-ness",
    "title": "Examined Algorithms",
    "section": "Taxonomies of Missing-ness",
    "text": "Taxonomies of Missing-ness\nCausal inference can be seen as a species of missing data problem where the missing data is the counterfactual situation(s) of how the world would have been were the course of the world different from the one we know. What if we used this treatment plan rather than another? What it the actors’ behaviour differed from the actions they in fact pursued?\nFrom an estimation perspective there are different species of missing-ness that matter. We won’t here go deep into the distinctions. It is enough to note that they vary in the operative source of the missing-ness: Missing-completely-at-random (MCAR), missing-at-random (MAR) and missing-not-at-random (MNAR).\n\nThe Stable Outcomes Assumption\nMechanically we can’t work with null values under any of these assumptions. The stable outcomes model is a first step procedure for imputing the missing values. Whether we choose mean imputation or an arbitrary figure - we initially assume missing values at the individual level in a stable fashion by specifying a constant value for the missing cases.\nVery crudely, estimation procedures work reasonably well under (MCAR) but require extra effort when there is assumptions if we hope to account for the (MAR) and (MNAR) cases. Under MAR we are assuming that the values are missing as a function of the observable covariates and can be imputed under proper conditionalisation.\nImputation under MAR and MCAR succeeds largely from successive applications of the law of iterated expectations. In this case the stable outcome assumption encodes the missing data as \\(-99\\) and we then average over the joint distribution of the stable outcomes model and the missingness data.\n\nThe various estimation procedures for counterfactual results trade on this property of expectation that allow for point identification of the expected value for the outcome variable.\nBut we also want to consider individual variation due to the observed covariate profiles. When we consider cases of missing data conditional the observed covariate profile, we can derive a propensity score as a one number summary for the conditional probability of missing-ness. This can be used in imputation techniques where we want to carefully attribute values to the missing data that respect the other observed properties of the individual.\n\n\nPropensity Scores\nThe propensity score has a role in a number of re-weighting schemes for the estimation of missing data. These rely on the property of expectation under (MAR). So accuracy of the propensity score is itself an important question. Because the missing-ness variable \\(R\\) is a binary random variable in \\(\\{ 0, 1 \\}\\) maximum likelihood methods for logistic regression are often used to estimate these terms.\n\\[p_{R}(\\mathbf{x}) \\sim logit(X_{i} \\beta )\\]\nThis score is a summary in some sense of the factors driving missing-ness. In the context of causal inference it is often stated “in reverse” as a probability of being treated. We will keep focus here on the case of missing data, but the generality of the notion shouldn’t be lost. The propensity score is a one number summary of the covariate profile for each individual in the data. Under the (MAR) assumption it is often sufficient to conditionalise on the propensity score for each individual for imputation of their missing data values.\n\n\nRegression Estimators\nWe might want to simply estimate the missing values of our outcome using the conditional expectation function (CEF) property of simple regression. The imputation pattern will work well when the linear properties of the regression model are a good fit for the relationship between the outcome variables and the observed covariates. Hence the estimate for:\n\\[E[Y_{i}] = \\beta_{0} + \\beta_{1}\\cdot X_{1i} ... \\beta_{n} \\cdot X_{ni} \\] where we replace all values to be prediction of our regression model for each individual and then average the predictions.\n\n\nWeighting Estimators\nAnother approach to missing data imputation, which relies on the expectation properties of our outcome variable of interest under (MAR) and the stable outcome model, is the inverse probability weighting approach to imputation.\n\\[ E[Y_{i}] = E[\\dfrac{(YR + (-99) (1-R ))R }{p_{R}(\\mathbf{x})}]\\] With this property we can express estimates of missing values as a function of the individuals’ observed data. There are variations on this theme but sophisticated imputation schemes all rely on functions of the individual’s observed covariate profile. This specificity is important too in the context heterogenous treatment effects in causal inference."
  },
  {
    "objectID": "notes/certain_things/Intro - Zettelkasten.html",
    "href": "notes/certain_things/Intro - Zettelkasten.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "Zettelkasten is a note-taking method that originated from the work of sociologist Niklas Luhmann. The term “Zettelkasten” translates to “slip box” in English. The key idea behind the Zettelkasten method is to create and organise a collection of interconnected notes or “slips” that capture individual ideas, concepts, or pieces of information. Each note is meant to be concise and focused on a single idea. The notes are then linked together through a system of numbered or categorised references, allowing for easy navigation and discovery of connections between different concepts.\nIn this section i will capture my notes using Obsidian and publish them to my website where relevant."
  },
  {
    "objectID": "notes/certain_things/Statistics/Analogies between Missing Data and Causal Inference.html#conclusion",
    "href": "notes/certain_things/Statistics/Analogies between Missing Data and Causal Inference.html#conclusion",
    "title": "Examined Algorithms",
    "section": "Conclusion",
    "text": "Conclusion\nThe sequence of complexity in missing data imputation is as follows:\n\\[ MCAR \\Rightarrow MAR \\Rightarrow MNAR \\]\nwhich mirrors the complexity of cases in causal inference. Here we have:\n\\[ Ignorability \\Rightarrow \\text{Strong Ignorability} \\Rightarrow \\text{Non Ignorability} \\] As we consider circumstances moving up the hierarchy, we require an increase in assumptions or structural commitments to offset the risk of non-identifiability bringing us back down the hierarchy. The emphasis in the book stresses how properties of good experimental design can help recover sound inference by enforcing MAR conditions in MNAR circumstances. But the crucial role of modelling in defending the strong ignorability condition is underplayed.\nYes, we need to justify our estimator but also our model! Are we including the right covariates? Have we an appropriate covariance structure? What is the functional form and why is it reasonable? Are we accounting for heterogeneity of outcome? All such questions centre the importance of domain knowledge for causal inference. This is not a criticism of boon focused on Agnostic statistics. Their focus is appropriately on the design aspects that enable inference. However it should be abundantly clear that you cannot get away with agnostic approaches in the real world. There is no way to justify stepping back down the hierarchy without substantial commitments about the world-model fit. Even if your aesthetic preferences drive you toward design based methods, this only serves to obscure the commitments. Statistics in the real world require real world commitments."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Book - Thinking about Statistics.html",
    "href": "notes/certain_things/Philosophy/Book - Thinking about Statistics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "In this note we’ll capture reflections about Jun Otsuka’s Thinking about statistics."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Book - Thinking about Statistics.html#statistical-models-and-the-problem-of-induction",
    "href": "notes/certain_things/Philosophy/Book - Thinking about Statistics.html#statistical-models-and-the-problem-of-induction",
    "title": "Examined Algorithms",
    "section": "Statistical Models and the Problem of Induction",
    "text": "Statistical Models and the Problem of Induction\nThe book begins by framing the different epistemological projects of both #Bayesian and #Frequentist patterns of inference as approaches to solving the problem of induction expressed by David Hume.\nThis is a nice lens on the development of statistics and the applied work of statistical modelling. The two probabilistic frameworks are contrasted or compared to the more pragmatist position of model selection based on predictive power. We may prefer a model which does not capture the true data generating process just so long as it performs better in prediction tasks."
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html",
    "href": "talks/survival_regression/time_to_attrition.html",
    "title": "Survival Regression Models in PyMC",
    "section": "",
    "text": "Time to Event Distributions\n\nHyperObjects and Perspectives on Probability\n\n\n\n\n\nPeople Analytics and Survival Regression\n\nCox Proportional Hazard\nAccelerated Failure Time\n\n\n\n\n\nComparing Model Implications\n\nMarginal Predictions\nAcceleration Factors\n\n\n\n\n\nFrailty Models and Stratified Risk\n\n\n\n\nConclusion"
  },
  {
    "objectID": "notes/certain_things/Philosophy/Thinking about Statistics.html",
    "href": "notes/certain_things/Philosophy/Thinking about Statistics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "In this note we’ll capture reflections about Jun Otsuka’s Thinking about statistics"
  },
  {
    "objectID": "notes/certain_things/Philosophy/Thinking about Statistics.html#statistical-models-and-the-problem-of-induction",
    "href": "notes/certain_things/Philosophy/Thinking about Statistics.html#statistical-models-and-the-problem-of-induction",
    "title": "Examined Algorithms",
    "section": "Statistical Models and the Problem of Induction",
    "text": "Statistical Models and the Problem of Induction\nThe book begins by framing the different epistemological projects of both #Bayesian and #Frequentist patterns of inference as approaches to solving the problem of induction expressed by David Hume. #book #philosophy #statistics\nThis is a nice lens on the development of statistics and the applied work of statistical modelling. The two probabilistic frameworks are initially contrasted or compared to each other. The distinction is drawn between the epistemological process involved in both approaches. Firstly the notion of Bayesian conditionalisation which incorporates new data to derive new beliefs coherent with the observed facts is spelled out. Then we see how the frequentist approach can be considered as a species of reliablist epistemology, where the focus is on the error control of well defined processes. In both cases the problem of induction is located as one of inference i.e. if we have an appropriate set of i.i.d sample data we can warrant the inference that the future will look like the past\nHe draws out the ontological commitments to probabilistic kinds that appear required to underwrite statistical inference in both paradigms. He supplies a justification for these commitments as being instances of real patterns in the sense of Daniel Dennett’s phrasing. This is a kind of indispensability argument for the deployment of probabilistic kinds in our best science."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Thinking about Statistics.html#the-uniformity-of-nature",
    "href": "notes/certain_things/Philosophy/Thinking about Statistics.html#the-uniformity-of-nature",
    "title": "Examined Algorithms",
    "section": "The Uniformity of Nature",
    "text": "The Uniformity of Nature\nIf probabilistic kinds inhabit the world they can exhibit different characteristics - varying fauna and flora of the natural world. One process might be well described by a Gaussian distribution, another by a Laplace distribution… this diversity is all well and good, but to go beyond descriptive statistics we need to posit more. We need the assumption that there is some stability to the processes we seek to characterise. A uniformity of nature that underwrites statistical inference and probabilistic prediction models.\nOtsuka suggests that this commitment is cashed out in contemporary statistics with the famous i.i.d assumption. This posit argues that for sound inference, we must assume that any sample data is drawn from a probability distribution that each draw is independent and identically distributed.\n\n“The IID condition is a mathematical specification of what Hume called the uniformity of nature. To say that nature is uniform means that whatever circumstances holds for the observed, the same circumstances will continue to hold for the unobserved. This is what Hume required for the possibility of inductive reasoning …” pg 25/26\n\nThis obviously is constraint on sound inference, but also implicitly an ontological assertion regarding distributional drift. This commitment is deeper than when we argue for a particular distributional characterisation of our process of interest. Our target process could be articulated as a mixture distribution or some more complicated beast, but in each case we require that (a) it is well described by some probability model and (b) the world is set up in such a way that more observations enable us to learn which particular statistical model (parametric or non-parametric) fits the data."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Thinking about Statistics.html#approaches-to-learning",
    "href": "notes/certain_things/Philosophy/Thinking about Statistics.html#approaches-to-learning",
    "title": "Examined Algorithms",
    "section": "Approaches to Learning",
    "text": "Approaches to Learning\nWith this background Otsuka goes on to describe the manner in which the Bayesian and frequentist schools approach the task of learning from data.\n\nBayesian Machinery\nThe focus in the Bayesian setting presents conditionalisation as a logic of inductive reasoning, which expands on logical inference. These are frameworks for organising and interrogating our system of beliefs and their relationship to the achievement of knowledge. Otsuka argues that Bayesian inference plays a crucial role in justification. Moving from prior to posterior distribution is seen as a change in the weighting of our beliefs. An internalist species of the justification-relation between beliefs in light of data.\nBut founding a story about epistemological justification on bayesian inference involves a defense of priors and likelihood specifications. Priors are defended using the usual moves: (1) appeal to wash-out theorems of iteratively updating on sufficiently large data. Convergence theorems assuring Bayesian updated belief is truth conducive in the limit regardless of apparently subjective prior specification. (2) Non-informative priors and (3) Objective priors or empirical Bayes.\nWe won’t spend much time on (2) because it’s just kind of silly to justify beliefs and their manipulation by constant appeals to ignorance. On (3) there is a more interesting discussion regarding the relationship between degrees of belief and chance. We want our priors to reflect our background knowledge and update our beliefs in a way that it tracks the actual occurence of the events in question. David Lewis enshrined this requirement as the Principal Principle. But while this is an agreeable sounding tenet it cannot serve as a foundational justification for our priors within an internalist picture of justification without risk of infinite regress. This is problematic for the philosopher that seeks to establish bayesian inference as the sole source of belief generation, but seems less serious if you can tolerate primitive or foundational epistemological commitments outside those justified with inductive inference in the Bayesian loop. Justification must end somewhere (the spade eventually turns), and in-practice arguments and evidential exchanges rarely get anywhere close to an infinite series.\nAdditionally the Bayesian needs to defend the incorporation of different likelihood choices. Their shape and implications. Fortunately this can be more pragmatic in so far likelihood specifications are in effect testable hypotheses about the data generating process. They can be justified by the success of the modelling endeavour and our ability to recover data akin to our observations. The data is a fixed quantity, we use it to update our probabilstic beliefs and commitments. This is to the good because it can be shown (via Dutch book style theorems) that in strategic decision making where your beliefs adhere to the probability calculus they will strictly dominate other strategies.\nThe Bayesian machinery is a set of tools for arranging coherence amongst our beliefs, commitments - tracing out the implications. We move dynamically between prior and posterior by means of the likelihood term. This process cannot serve as as an ultimate court of appeal for basic beliefs that kick off the learning process itself. It is an abstract, highly flexible set of tools applicable to a wide range of questions. It provides a very general model of learning where the concern is justification of our beliefs in the context of what we know.\n\n\nFrequentist Consolidation\nSo far so uncontroversial. The Bayesian perspective is a natural fit for a species of internalist epistemology. The framework is abstract and characterised concisely, so relatively straightforward to incorporate in a general philosophical picture. Otsuka’s synthesis of the “classical” inferential picture is in this way more impressive.\nThe classical frequentist picture has a history of poor pedagogy and can seem disparate and ad-hoc. Jaynes is famously dismissive of the absurdities engendered by the pick-and-mix approach to statistical inference adhered to in the “classical” approach.\nThe frequentist view ties statements of probability to measures of relative frequency within a collection of observations. This makes it impossible to articulate probability statements for specific hypotheses. The epistemological perspective is quite distinct from the Bayesian view of updating individual hypotheses. Instead the focus must lie of testing statements about stochastic processes - processes which are inherently repeatable.\nAs such these processes can be described by probabilistic kinds. The question then becomes - how can the properties of these observable processes feed into knowledge gathering routines. How can we go from a statement about an observed frequency to claims of knowledge or belief regarding the data generating process?\nThe route is to go via the framework of statistical testing which has some relationship to Karl Popper’s falsification. This involves positing a statistical hypothesis with direct implications. These implications can be parsed as an explicit prediction that can be compared to future observations. In this way, the hypothesis is tested against the data. This gives us a means of arguing reductio ad unlikely against the initial hypothesis and turns statistical inference into a “process of systematically plowing out falsehood”. Through iteratively testing and refining more targeted hypotheses. We define and reject the null hypotheses as we go.\nThe constraints on the test design are built to ensure reliability over the course of repeated testing under a known null hypothesis. Sample size considerations, alpha-spending and statistical power are properties of the test. These properties need to be chosen in such a way to ensure reliability of a particular test, but there are also constraints for running repeated tests of multiple hypotheses. The entire testing enterprise is set up to minimise and control errors. The epistemological picture is one of reliablism. Whether a belief is justified is determined by the nature and defensiblity of the process that generated the claim. This approach allows us to reject the Gettier style counter examples to justified true belief analyses of knowledge. If the procedures of knowledge acquisition are not themselves well founded than the coincidence between claim and fact in the Gettier cases cannot be counted as justified. The procedures of knowledge acquisition are fixed, uncertainty stems from how we learn the shape of the data probabilistically.\nFor this method to work the reliabilism the methodology seeks should be clarified. Otsuka suggests that the reliability implicit in statistical testing needs to underwrite truth-tracking counterfactuals. In particular the claims:\n\nif P were not true, S would not believe P\nIf P were true then S would believe P.\n\nWhich is not to say that any particular statistical test will lead to the endorsement/rejection of the hypothesis in question. Statistical tests cannot directly accept the null - just fail to reject. But cumulatively over many tests a reliabilist epistemology should ultimately ascertain the falsity of the null when the null is false. This points to a tension in the focus on individual tests and specific p-values. Statistical testing as an epistemological enterprise is a wholesale endeavour and the overall success or failure of conditions which underwrite the reliability of the process are not easily discerned by mere success in our world. The procedures must be valid and truth tracking in “nearby” counterfactual worlds where the null hypothesis is not how it is in our world. The conditions under which a process is ultimately truth-tracking may depend on contextual factors which cannot be easily turned to an algorithm. This perspective nicely unites all the ad-hoc approaches to defining tests with asymptotic error control properties. The concern is inherently procedural, and procedures can (and perhaps should) vary in the context of the learning. But their adoption is founded on the commitment that they would be reliably truth-tracking in all worlds similar enough to our own."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Thinking about Statistics.html#when-philosophies-conflict",
    "href": "notes/certain_things/Philosophy/Thinking about Statistics.html#when-philosophies-conflict",
    "title": "Examined Algorithms",
    "section": "When Philosophies Conflict",
    "text": "When Philosophies Conflict\nOtsuka’s survey of the two approaches is detailed and comprehensive. It sketches the motivations of each position well, and you might hope to reconcile the two. Apply each in their own domain where appropriate…, but unfortunately the motivating instincts can clash irreconcilably.\nThe Bayesian perspective necessitates the adoption of the Likelihood Principle which can be violated by the classical procedure of sequential testing. Recall how for the Bayesian the data is a fixed quantity, fed forward into the likelihood term to update our beliefs. This works the same whether we update our beliefs at time t1, t2 or tN. All the information is in the likelihood irrespective of how much data has accrued over time.\nWhile under the frequentist model if the data is analysed under different experimental designs (e.g. one with a stopping rule and one without) the results of test for a particular null hypothesis can differ I.e. with the same data and the same null hypothesis, one experiment can reject the null and the other will fail to reject it. This is because the result incorporates information about the design of the experiment that cannot be captured in the simple likelihood. Otsuka makes the point that this is a broad problem for all reliabilist philosophies where the fit between process/test and reality is contestable. There is no perfectly general procedure that returns true results in all circumstances. As such the frequentist methodologist must argue anew in each circumstances for the appropriateness of their methods.\nThis is a keen source of divisiveness between the two schools. Scientists historically cannot be trusted to review their methods with respect to their goals. Instead frequentist statistical methodology has been abused in practice. Adherents use the routine nature of procedure as a rubber stamp without due consideration for the reliability of the methods in the context of the question at hand. This pattern of abuse is rightly seen as a key contributor to the replication crisis in science. Ironically it is this aspect of frequentist methodology that introduces obscure subjective bias into the experimental exercise. The Bayesian (often accused of criminal subjectivity) wears their priors clearly and defends their appropriateness for each analysis.\nNeither school of thought is a self-contained epistemology. Neither method is self-sustaining. Both priors and procedures must be justified with an appeal to their apt fit for the problem at hand. Statistical methodology slots into a broader epistemological endeavour and there is no substitute for the careful and knowing application of inferential machinery if we hope to justify the achievements of science."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Thinking about Statistics.html#bias-and-regularisation",
    "href": "notes/certain_things/Philosophy/Thinking about Statistics.html#bias-and-regularisation",
    "title": "Examined Algorithms",
    "section": "Bias and Regularisation",
    "text": "Bias and Regularisation\nIn later chapters we move towards the more pragmatist position of model selection based on predictive power. This mirrors a move away from understanding of uncertainty due to natural variation towards an model-uncertainty in the analysis.\nWe may prefer a model which does not capture the true data generating process just so long as it performs better in prediction tasks. This effectively invokes the more typical approaches to machine learning model evaluation as driving us toward pragmatic biases that work well to optimise for some measure of out-of-sample prediction. Your inferential framework can be independent of your model selection criteria, but model ranking is still conducted under uncertainty. Each measure of performance is still an estimate.\nOtsuka contrasts the ranking of models based on information criteria with the out of sample predictive performance of deep learning systems. The AIC style methodologies penalises models with too many parameters to optimise for predictive performance. Deep learning methods have explicit methods to induce regularisation like effects with: drop-out, modified loss-functions. What is striking is that while both methods are checks against overfitting of models to the data, both are pragmatic compromises that dispense with the idea of epistemological truth-tracking. They concede that the it is often better to overcome the problem of model uncertainty with suitably practical abstraction. Don’t worry about the world-model fit so long as the outcomes of the model are workable.\nBut this comes with a burden of explainability often demanded of opaque models. Predictive success in one domain does not always translate to success in another. Regulators and stakeholders need to understand when and why the predictive reliability of deep learning systems can be expected to transfer well across tasks. Appeals to the epistemological surety of beliefs derived from predictions in these black-box systems stem from a loosely held belief in the virtues of the deep learning mechanisms. These virtues are less well established than error-control rates of a statistical test and they are somewhat transient across task."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Thinking about Statistics.html#causal-inference-and-statistics",
    "href": "notes/certain_things/Philosophy/Thinking about Statistics.html#causal-inference-and-statistics",
    "title": "Examined Algorithms",
    "section": "Causal Inference and Statistics",
    "text": "Causal Inference and Statistics\nFinally Otsuka pauses to consider the role of causal inference in contemporary statistics and how here optimising for predictive power will often fail when the task requires subtlety or insight into the data generating process. The manner in which models and measurements can be confounded by aspects of the data generating process cannot be detected automatically without knowledge of the relationships in the system. The focus here is on how tools for identification are required when we want to be sure that the causal quantities of interest are soundly derivable from the data we have to hand. Whether we use the potential outcomes framework, the do-calculus or structural estimation. The work of statistical inference using any and all of the above frameworks can only get off the ground when we have enough of a world-picture - a set of commitments or assumptions that allow our inferences to have warrant. Our priors our informed, our inferential procedures suitably well defined to sustain truth-tracking counterfactuals."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Thinking about Statistics.html#conclusion",
    "href": "notes/certain_things/Philosophy/Thinking about Statistics.html#conclusion",
    "title": "Examined Algorithms",
    "section": "Conclusion",
    "text": "Conclusion\nThe broad picture painted in Otsuka’s survey of statistics is the central role of inferential procedures in our broader epistemological landscape. The diversity of roles it can play and different standards of rigour at play in different contexts. The links between the foundational questions and puzzles of epistemology have illuminated the structure of the debates in statistics."
  },
  {
    "objectID": "talks/mister_p/mister_p.html",
    "href": "talks/mister_p/mister_p.html",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "",
    "text": "Intro\n\n\n\n\nI’m a data scientist at Personio\n\nBayesian statistician,\nReformed philosopher and logician.\n\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\nDisclaimer\n\n\n\nNone of Personio’s data was used in this presentation\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\n\nThe worked examples used here can be found here"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#agenda",
    "href": "talks/mister_p/mister_p.html#agenda",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Agenda",
    "text": "Agenda"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#agenda-1",
    "href": "talks/mister_p/mister_p.html#agenda-1",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Agenda",
    "text": "Agenda\n\n\nRegression as Strata specific Summarisation\n\n\n\n\nSampling and Probability Sampling\n\n\n\n\nStratum Specific Modelling\n\n\n\n\nStratum Specific Adjustment\n\n\n\n\nConclusion\n\nWhen to Adjust and Why?"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#regression-what-are-we-even-doing",
    "href": "talks/mister_p/mister_p.html#regression-what-are-we-even-doing",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Regression: What are we even doing?",
    "text": "Regression: What are we even doing?\n\n\n\\[\\hat{y_{i}} = \\alpha + \\beta_{1}X_{1} ... \\beta_{n}X_{n}\\]\n\nAssume \\(y = \\hat{y_{i}} + \\epsilon\\) where \\(E(\\epsilon) = 0\\)\n\n\\[ E[y | X = x] = \\alpha + \\beta_{1}X_{1} ... \\beta_{n}X_{n}\\]\n\\[ y  \\sim Normal(\\hat{y_{i}}, \\sigma) \\]"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#regression-what-are-we-even-doing-1",
    "href": "talks/mister_p/mister_p.html#regression-what-are-we-even-doing-1",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Regression: What are we even doing?",
    "text": "Regression: What are we even doing?\nm0 = smf.ols('np.log(hwage) ~ job +  educ', data=df).fit()\nm1 = smf.ols('np.log(hwage) ~ job + educ + male ', data=df).fit()\npred = m0.predict(['software_engineer', 'college'])\npred1 = m1.predict(['software_engineer', 'college', 1])\ndiff = predc - pred1\n\n\n\nAs we add more covariates we add more combinatorial branches which define the available strata across our population of interest.\nA fitted regression model allows us to explore the conditional branching probabilities."
  },
  {
    "objectID": "talks/mister_p/mister_p.html#regression-as-effect-modification",
    "href": "talks/mister_p/mister_p.html#regression-as-effect-modification",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Regression as Effect Modification",
    "text": "Regression as Effect Modification\nreg = bmb.Model(\"Outcome ~ 1 + Treatment\", df)\nresults = reg.fit()\n\nreg_strata = bmb.Model(\"\"\"Outcome ~ 1 + Treatment + Risk_Strata \n+ Treatment_x_Risk_Strata\"\"\", df)\nresults_strata = reg_strata.fit()\nbmb.interpret.plot_predictions(reg, results, conditional=[\"Treatment\"])\nbmb.interpret.plot_predictions(reg_strata, results_strata, conditional=[\"Treatment\"])\n\n\n\n\n\n\n\n\n\n\n\nBambi’s Marginal Effects Interpretation module automates the implications of each model fit"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#regression-as-weighting-adjustment",
    "href": "talks/mister_p/mister_p.html#regression-as-weighting-adjustment",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Regression as Weighting Adjustment",
    "text": "Regression as Weighting Adjustment\nRegression automates the more manual re-weighting that needs to occur to account for different varieties of risk across the strata of our population"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#the-need-for-re-weighting",
    "href": "talks/mister_p/mister_p.html#the-need-for-re-weighting",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "The Need for Re-Weighting",
    "text": "The Need for Re-Weighting\n\n\n\nCausal Inference\n\nInverse Probability Weighting\nPseudo-Population Imputation\nTreatment effect estimation\n\n\n\n\nSurvey Sample Bias\n\nNon-response\nOpt-out Sampling contracts\nIncomplete coverage\nMultilevel Regression\nPost-stratification Adjustment"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#the-data",
    "href": "talks/mister_p/mister_p.html#the-data",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "The Data",
    "text": "The Data\nWe examine a comprehensive YouGov poll on whether employers should cover abortion in their coverage plans.\nWe select a biased subsample."
  },
  {
    "objectID": "talks/mister_p/mister_p.html#deliberate-bias",
    "href": "talks/mister_p/mister_p.html#deliberate-bias",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Deliberate Bias",
    "text": "Deliberate Bias\n\nIllustrated differences in vote share by demographics"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#prep-data-for-modelling",
    "href": "talks/mister_p/mister_p.html#prep-data-for-modelling",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Prep Data for Modelling",
    "text": "Prep Data for Modelling"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#exploratory-modelling",
    "href": "talks/mister_p/mister_p.html#exploratory-modelling",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Exploratory Modelling",
    "text": "Exploratory Modelling\nWe fit a preliminary model to investigate the interactions across demographic splits. We specify a logit model using the binomial link.\nformula = \"\"\" p(abortion, n) ~ C(state) + C(eth) + C(edu) + male + repvote\"\"\"\n\nbase_model = bmb.Model(formula, model_df, family=\"binomial\")\n\nresult = base_model.fit(\n    random_seed=100,\n    target_accept=0.95,\n    idata_kwargs={\"log_likelihood\": True},\n)\n\nfig, ax = bmb.interpret.plot_comparisons(\n    model=base_model,\n    idata=result,\n    contrast={\"eth\": [\"Black\", \"White\"]},\n    conditional=[\"age\", \"edu\"],\n    comparison_type=\"diff\",\n    subplot_kwargs={\"main\": \"age\", \"group\": \"edu\"},\n    fig_kwargs={\"figsize\": (12, 5), \"sharey\": True},\n    legend=True,\n)\nax[0].set_title(\"Comparison of Difference in Ethnicity \\n within Age and Educational Strata\");"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#plotting-implications",
    "href": "talks/mister_p/mister_p.html#plotting-implications",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Plotting Implications",
    "text": "Plotting Implications"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#the-full-hierarchical-interaction-model",
    "href": "talks/mister_p/mister_p.html#the-full-hierarchical-interaction-model",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "The Full Hierarchical Interaction Model",
    "text": "The Full Hierarchical Interaction Model\n\\[Pr(y_i = 1) = logit^{-1}\\Bigg(\n\\alpha_{\\rm s[i]}^{\\rm state}\n+ \\alpha_{\\rm a[i]}^{\\rm age}\n+ \\alpha_{\\rm r[i]}^{\\rm eth}\n+ \\alpha_{\\rm e[i]}^{\\rm edu} \\\\\n+ \\beta^{\\rm male} \\cdot {\\rm Male}_{\\rm i}\n+ \\alpha_{\\rm g[i], r[i]}^{\\rm male.eth}\n+ \\alpha_{\\rm e[i], a[i]}^{\\rm edu.age}\n+ \\alpha_{\\rm e[i], r[i]}^{\\rm edu.eth}\n\\Bigg)\\]\nAllowing for stratum specific intercept terms for each level of the demographic categories and their interaction effects."
  },
  {
    "objectID": "talks/mister_p/mister_p.html#the-model-in-code",
    "href": "talks/mister_p/mister_p.html#the-model-in-code",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "The Model in Code",
    "text": "The Model in Code\nFitting the model to the biased sample:\nformula = \"\"\" p(abortion, n) ~ (1 | state) + (1 | eth) + (1 | edu)\n + male + repvote  + (1 | male:eth) + (1 | edu:age) + (1 | edu:eth)\"\"\"\n\nmodel_hierarchical = bmb.Model(formula, model_df, family=\"binomial\")"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#learning-the-bias",
    "href": "talks/mister_p/mister_p.html#learning-the-bias",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Learning the Bias",
    "text": "Learning the Bias\nThe Model Derived Coefficients"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#preliminaries",
    "href": "talks/mister_p/mister_p.html#preliminaries",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Preliminaries",
    "text": "Preliminaries\n\n\n\n\n\n\nIntro\n\n\n\nI’m a data scientist at Personio\n\nBayesian statistician,\nReformed philosopher and logician.\n\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\n\nDisclaimer\n\n\nNone of Personio’s data was used in this presentation\n\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\nThe worked examples used here can be found here"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#predicting-vote-share",
    "href": "talks/mister_p/mister_p.html#predicting-vote-share",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Predicting Vote Share",
    "text": "Predicting Vote Share\nUsing the Biased Model\nnew_data = (new_data.merge(\n    new_data.groupby(\"state\").agg({\"n\": \"sum\"})\n    .reset_index()\n    .rename({\"n\": \"state_total\"}, axis=1)\n)\n)\nnew_data[\"state_percent\"] = new_data[\"n\"] / new_data[\"state_total\"]\nnew_data.head()\n\nresult_adjust = model_hierarchical.predict(result, \ndata=new_data, inplace=False, kind=\"pps\")\nresult_adjust"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#adjusting-the-state-level-predictions",
    "href": "talks/mister_p/mister_p.html#adjusting-the-state-level-predictions",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Adjusting the State Level Predictions",
    "text": "Adjusting the State Level Predictions\nReweighting Outcomes by Strata specific Share\n\nestimates = []\n## The base model posterior fitted on biased sample\nabortion_posterior_base = az.extract(result)[\"p(abortion, n)_mean\"]\n\n## The posterior updated with national level figures\nabortion_posterior_mrp = az.extract(result_adjust)[\"p(abortion, n)_mean\"]\n\n## Adjusting the predictions on state level\nfor s in new_data[\"state\"].unique():\n    idx = new_data.index[new_data[\"state\"] == s].tolist()\n    predicted_mrp = (\n        ((abortion_posterior_mrp[idx].mean(dim=\"sample\") * \n        new_data.iloc[idx][\"state_percent\"]))\n        .sum()\n        .item()\n    )\n    predicted_mrp_lb = (\n        (\n            (\n                abortion_posterior_mrp[idx].quantile(0.025, dim=\"sample\")\n                * new_data.iloc[idx][\"state_percent\"]\n            )\n        )\n        .sum()\n        .item()\n    )\n    predicted_mrp_ub = (\n        (\n            (\n                abortion_posterior_mrp[idx].quantile(0.975, dim=\"sample\")\n                * new_data.iloc[idx][\"state_percent\"]\n            )\n        )\n        .sum()\n        .item()\n    )\n    predicted = abortion_posterior_base[idx].mean().item()\n    base_lb = abortion_posterior_base[idx].quantile(0.025).item()\n    base_ub = abortion_posterior_base[idx].quantile(0.975).item()\n\n    estimates.append(\n        [s, predicted, base_lb, base_ub, predicted_mrp, predicted_mrp_ub, predicted_mrp_lb]\n    )\n\n\nstate_predicted = pd.DataFrame(\n    estimates,\n    columns=[\"state\", \"base_expected\", \"base_lb\", \n    \"base_ub\", \"mrp_adjusted\", \"mrp_ub\", \"mrp_lb\"],\n)\n\nstate_predicted = (\n    state_predicted.merge(cces_all_df.groupby(\"state\")[[\"abortion\"]].mean().reset_index())\n    .sort_values(\"mrp_adjusted\")\n    .rename({\"abortion\": \"census_share\"}, axis=1)\n)\nstate_predicted.head()"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#comparing-adjusted-and-raw-predictions",
    "href": "talks/mister_p/mister_p.html#comparing-adjusted-and-raw-predictions",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Comparing Adjusted and Raw Predictions",
    "text": "Comparing Adjusted and Raw Predictions\nDerived state level predictions using the biased sample and the corrected values."
  },
  {
    "objectID": "talks/mister_p/mister_p.html#comparing-adjusted-and-raw-predictions-1",
    "href": "talks/mister_p/mister_p.html#comparing-adjusted-and-raw-predictions-1",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Comparing Adjusted and Raw Predictions",
    "text": "Comparing Adjusted and Raw Predictions"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#conclusion",
    "href": "talks/mister_p/mister_p.html#conclusion",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Conclusion",
    "text": "Conclusion\nThe Need for Reweighting\n\n“The IID condition is a mathematical specification of what Hume called the uniformity of nature. To say that nature is uniform means that whatever circumstances holds for the observed, the same circumstances will continue to hold for the unobserved. This is what Hume required for the possibility of inductive reasoning”\n\n\n\n\nSurvey Bias Breaks the IID condition\nInference falls apart with non-representative samples\nPrediction suffers from wild skew\n\n\n\nKnowledge about demographic representation informs priors\nHistoric rates can be used to improve sample representation\nModel recovers inferential validity. Prediction improves.\n\n\n\n\n\nPost-Stratification Weighting"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#informed-sampling",
    "href": "talks/mister_p/mister_p.html#informed-sampling",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Informed Sampling",
    "text": "Informed Sampling\nModelling of Survey Outcomes with Prior information\n\n“In conventional sampling theory, the only scenario considered is essentially that of ‘drawing from an urn’, and the only probabilities that arise are those that presuppose the contents of the ‘urn’ or the ‘population’ already known, and seek to predict what ‘data’ we are likely to get as a result. Problems of this type can become arbitrarily complicated in the details, and there is a highly developed mathematical literature dealing with them…It was our use of probability theory as logic that has enabled us to do so easily what was impossible for those who thought of probability as a physical phenomenon associated with ‘randomness’. Quite the opposite; we have thought of probability distributions as carriers of information. At the same time, under the protection of Cox’s theorems, we have avoided the inconsistencies and absurdities which are generated inevitably by those who try to deal with the problems of scientific inference by inventing ad hoc devices instead of applying the rules of probability theory” - Edwin Jaynes in Probability: The Logic of Science pg88"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#modelling-of-survey-outcomes-with-prior-information",
    "href": "talks/mister_p/mister_p.html#modelling-of-survey-outcomes-with-prior-information",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Modelling of Survey Outcomes with Prior information",
    "text": "Modelling of Survey Outcomes with Prior information\nBayesian Models incorporate different sources of knowledge\n\n“In conventional sampling theory, the only scenario considered is essentially that of ‘drawing from an urn’, and the only probabilities that arise are those that presuppose the contents of the ‘urn’ or the ‘population’ already known, and seek to predict what ‘data’ we are likely to get as a result. …It was our use of probability theory as logic that has enabled us to do so easily what was impossible for those who thought of probability as a physical phenomenon associated with ‘randomness’. Quite the opposite; we have thought of probability distributions as carriers of information.” - Edwin Jaynes in Probability: The Logic of Science pg88 & p117"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#modelling-of-survey-outcomes",
    "href": "talks/mister_p/mister_p.html#modelling-of-survey-outcomes",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Modelling of Survey Outcomes",
    "text": "Modelling of Survey Outcomes\nBayesian Models incorporate different sources of knowledge\n\n“In conventional sampling theory, the only scenario considered is essentially that of ‘drawing from an urn’, and the only probabilities that arise are those that presuppose the contents of the ‘urn’ or the ‘population’ already known, and seek to predict what ‘data’ we are likely to get as a result. …It was our use of probability theory as logic that has enabled us to do so easily what was impossible for those who thought of probability as a physical phenomenon associated with ‘randomness’. Quite the opposite; we have thought of probability distributions as carriers of information.” - Edwin Jaynes in Probability: The Logic of Science pg88 & p117"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#plotting-implications-1",
    "href": "talks/mister_p/mister_p.html#plotting-implications-1",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Plotting Implications",
    "text": "Plotting Implications"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#investigating-marginal-contrasts",
    "href": "talks/mister_p/mister_p.html#investigating-marginal-contrasts",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Investigating Marginal Contrasts",
    "text": "Investigating Marginal Contrasts"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#preliminaries",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#preliminaries",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Preliminaries",
    "text": "Preliminaries\n\n\n\n\n\n\nProfile\n\n\n\nI’m a data scientist at Personio\n\nBayesian statistician,\nReformed philosopher and logician.\n\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\n\nDisclaimer\n\n\nNone of Personio’s data was used in this presentation\n\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\nThe worked examples used here can be found here"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#agenda",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#agenda",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Agenda",
    "text": "Agenda"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#agenda-1",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#agenda-1",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Agenda",
    "text": "Agenda\n\n\nTypology of Missing-ness\n\n\n\n\nMultivariate imputation using FIML\n\n\n\n\nBayesian Imputation by Chained Equations\n\n\n\n\nHierarchical Structures impacting Missing-ness\n\n\n\n\nImputation and Causal Narratives\n\n\n\n\nConclusion\n\nMissing Data: a Gateway to Causal Inference\n\n\n\n\n\nNon Parametric Causal Inference"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#three-acts",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#three-acts",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Three Acts",
    "text": "Three Acts\n\n\nPropensity Scores and Non-Parametric Causal Inference\n\n\n\n\nConfounding and Debiasing\n\n\n\n\nCausal Structure and Mediation"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#three-acts-1",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#three-acts-1",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Three Acts",
    "text": "Three Acts\n\n\nPropensity Scores and Non-Parametric Causal Inference\n\n\n\n\nConfounding and Debiasing\n\n\n\n\nCausal Structure and Mediation\n\n\n\n\nNon Parametric Causal Inference"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#act-one",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#act-one",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Act One",
    "text": "Act One\nPropensity Scores and Non-Parametric Causal Inference\n\n\nStrong Ignorability and Propensity Scores\n\n\n\n\nBART models and Non-Parametric Estimation\n\n\n\n\nBalance and Inverse Propensity Weighting\n\n\n\n\nRobust and Doubly Robust methods"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#act-two",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#act-two",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Act Two",
    "text": "Act Two\nConfounding and Debiasing\n\n\nPropensity Scores Miscalibrated\n\n\n\n\nBART models and Overfitting\n\n\n\n\nDebiased Machine Learning\n\n\n\n\nCATE estimation"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#act-three",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#act-three",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Act Three",
    "text": "Act Three\nCausal Structure and Mediation\n\n\nParametric Mediation\n\n\n\n\nNon-Parametric Mediation\n\n\n\n\nEscalating Structural Assumptions and Bayesian Inference"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#sources",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#sources",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Sources",
    "text": "Sources\n\n\n\n\n\nFoundations\n\n\n\n\n\n\nFoundations\n\n\n\n\n\n\nNon-Parametric"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#debts-and-sources",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#debts-and-sources",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Debts and Sources",
    "text": "Debts and Sources\n\n\n\n\n\nFoundations\n\n\n\n\n\n\nG-Computation\n\n\n\n\n\n\nNon-Parametric"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#primary-debts-and-sources",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#primary-debts-and-sources",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Primary Debts and Sources",
    "text": "Primary Debts and Sources\n\n\n\n\n\nFoundations\n\n\n\n\n\n\nDebiased ML\n\n\n\n\n\n\nNon-Parametric"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#strong-ignorability-and-propensity-scores",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#strong-ignorability-and-propensity-scores",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Strong Ignorability and Propensity Scores",
    "text": "Strong Ignorability and Propensity Scores\nDefinitions\n\nPotential Outcomes\n\n\\(Y(0)\\) and \\(Y(1)\\) under different treatment regimes \\(T \\in \\{ 0, 1\\}\\)\n\nStrong Ignorability\n\nOutcomes are independent of the treatment assignment given a covariate profile \\(X\\): \\(Y(0), Y(1) \\perp\\!\\!\\!\\perp T | X\\)\n\nPropensity Scores\n\nAn estimate of the probability for a particular treatment status conditional on the covariate profile \\(X\\): \\(0 \\leq p_{t}(X) \\leq 1\\)"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#bart-models-and-non-parametric-estimation",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#bart-models-and-non-parametric-estimation",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "BART Models and Non-Parametric Estimation",
    "text": "BART Models and Non-Parametric Estimation\n\nBART\n\nBayesian Additive Regression Trees\n“[B]lack-box method based on the sum of many trees where priors are used to regularize inference”\n\nNon-Parametric Estimation\n\nOutcomes and Propensity Scores can be estimated using non-parametric methods\nCausal estimands can be estimated using posterior predictive imputation under different treatment regimes\nBenefit of minimalist structural assumptions."
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#inverse-propensity-score-weighting",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#inverse-propensity-score-weighting",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Inverse Propensity Score Weighting",
    "text": "Inverse Propensity Score Weighting\n\nAdjustment by representative Weighting\n\nUsing the propensity scores as a summary metric for group membership, we down-weight and upweight the prevalence of high and low propensity score in each group to induce strong ignorability like conditions."
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#robust-and-doubly-robust-weighting-schemes",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#robust-and-doubly-robust-weighting-schemes",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Robust and Doubly Robust Weighting Schemes",
    "text": "Robust and Doubly Robust Weighting Schemes"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#robust-and-doubly-robust",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#robust-and-doubly-robust",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Robust and Doubly Robust",
    "text": "Robust and Doubly Robust\nWeighting Schemes"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#robust-and-doubly-robust-methods",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#robust-and-doubly-robust-methods",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Robust and Doubly Robust Methods",
    "text": "Robust and Doubly Robust Methods\nDiffering Weighting Schemes\n\nRaw\n\n\\(\\sum\\frac{1}{N}\\Big[ Y(1) \\cdot \\frac{1}{p_{T}(X)} - (Y(0)\\cdot\\frac{1}{1-p_{T}(X)}) \\Big]\\)\n\nDoubly Robust\n\n\\[ \\hat{Y(1)} = \\frac{1}{n} \\sum_{0}^{N} \\Bigg[ \\frac{T(Y - m_{1}(X))}{p_{T}(X)} + m_{1}(X) \\Bigg] \\\\ \\hat{Y(0)} = \\frac{1}{n} \\sum_{0}^{N} \\Bigg[ \\frac{(1-T)(Y - m_{0}(X))}{(1-p_{T}(X))} + m_{0}(X) \\Bigg] \\]"
  },
  {
    "objectID": "oss/pymc/bnp.html",
    "href": "oss/pymc/bnp.html",
    "title": "Bayesian Non Parametric Causal Inference in PyMC",
    "section": "",
    "text": "Bayesian Non Parametric Causal Inference\nIn this project I demonstrated how to fit and use BART propensity score models in the assessment of causal inference questions. We showed the conditions under which these methods work well and an example of where they break down due to overfit. We showed how they could be fixed using debiasing techniques and how appropriate analysis of this case could be achieved using non-parametric mediation analysis.\nThe project culminated in a publication to the official PyMC documentation that can be found online here or downloaded as notebook here\n\n\n\nConditional Average Treatment Effects of Smoking\n\n\nI also recorded a youtube modeling webinair on this topic with Alex Andorra\n\n\n\nYoutube Webinair"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#miscalibrated-propensity-scores",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#miscalibrated-propensity-scores",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Miscalibrated Propensity Scores",
    "text": "Miscalibrated Propensity Scores\nWhat is the Estimand?\n\n“Each theoretical estimand is linked to an empirical estimand involving only observable quantities (e.g. a difference in means in a population) by assumptions about the relationship between the data we observe and the data we do not.” - Lundberg et al in What is your Estimand\n\n\nQ1. What are we aiming at when we estimate propensity scores for highly granular covariate profiles?\nQ2. What happens when the sample data has no treatment data cases for a particular covariate profile?"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#overfitting",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#overfitting",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Overfitting",
    "text": "Overfitting\nBART models can achieve perfect Allocation\n\nOverfitting"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#debiasing-machine-learning",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#debiasing-machine-learning",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Debiasing Machine Learning",
    "text": "Debiasing Machine Learning\n\nK-fold Propensity Estimation"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#cate-estimation",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#cate-estimation",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "CATE Estimation",
    "text": "CATE Estimation\n\nConditional Average Treatment Effect"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#parametric-mediation",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#parametric-mediation",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Parametric Mediation",
    "text": "Parametric Mediation\n\nTraditional Model Based Mediation"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#causal-mediation-analysis",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#causal-mediation-analysis",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Causal Mediation Analysis",
    "text": "Causal Mediation Analysis\nNon-Parametric Estimation\n\nNDE: \\(E[Y(t, M(t^{*})) - Y(t^{*}, M(t^{*}))]\\)\n\nWhich is to say we’re interested in the differences in the imputed outcomes under different treatments, mediated by values for M under a specific treatment regime.\n\nNIE: \\(E[(Y(t, M(t))) - Y(t, M(t^{*}))]\\)\n\nWhich amounts to the imputed differences in the outcome Y due to differences in the treatment regimes which generated the mediation values M.\n\nTE: NDE + NIE"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#conclusion",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#conclusion",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Conclusion",
    "text": "Conclusion\n### Structural Assumption\n\n\nNon Parametric Causal Inference"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#conclusions",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#conclusions",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Conclusions",
    "text": "Conclusions\nStructural Beliefs and Bayesian Inference\n\nPropensity score adjustment reflects the belief in the need for adjustment\n\nIt reflects a belief in the adequacy of the propensity score model for achieving balance\n\nRegression based imputation of treatment effects reflects the belief that covariate controls eliminates selection effects\nDoubly Robust methods reflect the belief that either the propensity or outcome model is mispecified.\n\nBut that one ought to be adequately specified.\n\nDebiased ML methods reflect the belief that mis-specification can be corrected by cross-validation and non-parametric estimation of residuals in FWL theorem\nMediation Analysis reflects the belief that the causal influence must be interepreted with particular causal structure to avoid confounding.\nThere are no truly Agnostic statistics:In each case sound inference proceeds as we take steps to adjust our model or the conditions of its assessment as informed by our best beliefs regarding the problem to hand.\n\n\n\nNon Parametric Causal Inference"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#bayesian-non-parametrics",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#bayesian-non-parametrics",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Bayesian Non-parametrics",
    "text": "Bayesian Non-parametrics\n\nThe Pitch"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#the-pitch-1",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#the-pitch-1",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "The Pitch",
    "text": "The Pitch\n\nThe Pitch"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#obligatory-meme",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#obligatory-meme",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Obligatory Meme",
    "text": "Obligatory Meme\nBayesian Structural Modelling\n\nThe Pitch"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#propaganda",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#propaganda",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Propaganda",
    "text": "Propaganda\nFull Luxury Bayesianism\n\nPosterior Predictive Imputation of Treatment Effects"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#bayes-formula",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#bayes-formula",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Bayes Formula",
    "text": "Bayes Formula\nFull Luxury Bayesianism\n\\[ p(\\theta |D) \\propto p(D | \\theta)p(D) \\]\nwhere \\(\\theta\\) is parameter becomes\n\\[ p(G |D) \\propto p(D | G)p(D) \\]\nwhere G is a general stochastic process"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#non-parametric-bayes-formula",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#non-parametric-bayes-formula",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Non Parametric Bayes Formula",
    "text": "Non Parametric Bayes Formula\nFull Luxury Bayesianism\n\\[ p(\\color{blue}{\\theta} |D) \\propto p(D | \\color{blue}{\\theta})p(D) \\]\nwhere \\(\\color{blue}{\\theta}\\) is an explict model parameter becomes\n\\[ p(\\color{blue}{G} |D) \\propto p(D | \\color{blue}{G})p(D) \\]\nwhere \\(\\color{blue}{G}\\) is a general stochastic process"
  },
  {
    "objectID": "notes/certain_things/Wedding/Ceremony.html",
    "href": "notes/certain_things/Wedding/Ceremony.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "Remind guests to turn off phones, and no social media.\nBridesmaid :Margaret Stanley Best Man : Jamie Forde Readers –  TBD Witnesses : Margaret and Jamie Music : Recorded\nWelcome & Introduction\nA very warm welcome to each and every one here this afternoon to the beautiful Marco Pierre White’s here in the heart of Donnybrook, in Dublin to the wedding of Joanne and Nathaniel\nIntroduction\nMy name is Dave Russell and I am a One Spirit Interfaith Minister & Celebrant and it is an honour and a joy to have been invited here today by Joanne and Nathaniel to celebrate their marriage. Jamie Forde, Nathaniel’s brother is the best man and Margaret Stanley, Joanne’s sister is the maid of honour. Please bring any all issues to their attention and avoid bothering the bride and groom!\nJoanne and Nathaniel are delighted that so many friends and family could join them here today. Those of you have set aside your routines and commitments to be here to celebrate with them this afternoon.\nWelcome guests from UK, America, Netherlands, Germany and Dublin  You are all very welcome."
  },
  {
    "objectID": "notes/certain_things/Wedding/Ceremony.html#opening-the-ceremony-with-lighting-of-a-candle",
    "href": "notes/certain_things/Wedding/Ceremony.html#opening-the-ceremony-with-lighting-of-a-candle",
    "title": "Examined Algorithms",
    "section": "Opening the Ceremony with lighting of a Candle",
    "text": "Opening the Ceremony with lighting of a Candle\nSo as we begin…let’s take a moment of silence as we gather ourselves into this ceremonial space so that we are fully present for Joanne and Nathaniel\nAs I light this candle symbolising the kindling of their feelings for one another, that has grown into the light of love that is present between our beautiful couple.\nWe also acknowledge all the members of the Stanley and Forde families who for whatever reason cannot be with us here today.\nWe especially remember Nathaniel’s Dad Desmond He is very much in our thoughts here today, as he would have quite been annoyed to miss the party.\n\nOpening words for the Couple\nWe are thankful for the gift of Joanne and Nathaniel - the gift they have been to each other and the gift they are to their families & friends.\nToday we surround them both in great love and wonderment and may they always know and feel the strength and power of their love for each other."
  },
  {
    "objectID": "notes/certain_things/Wedding/Ceremony.html#candle-ceremony",
    "href": "notes/certain_things/Wedding/Ceremony.html#candle-ceremony",
    "title": "Examined Algorithms",
    "section": "Candle ceremony",
    "text": "Candle ceremony\nJoanne and Nathaniel will light two candles, acknowledging the support and care offered them by their parents, their family and friends for the love and care they have shown over innumerable instances and interactions - too many to recount.\nYou have all played a role in teaching them about kindness, love and life. By lighting these candles they are now symbolising the light that they are to each other.\n\nSong: Take Care - by Beachhouse\n\nWe are here to witness and celebrate their deep love for one another; and their total acceptance of each other and the connection between life and love.\nThere are moments in our lives that are ruled not so much by time but by the heart.  This is such a moment for Joanne and Nathaniel\nThese two people have fallen in love .. so deeply, so completely … that today they make a bond, a profound covenant whereby their hearts, their bodies and their souls shall be united as one in marriage for the rest of their days.\nToday, before all of you, their most cherished family and friends, they will say the most powerful most loving words two people can say to each other.\n > They will take their wedding vows   And it is our honour and privilege to stand witness.  For this profound act, my friends, is magnificent and so tender to behold.\n\n_This is where I talk about the two of you meeting up etc, I don’t include it here with you as I want to keep it as a bit of a surprise for the day, don’t worry I won’t be adding anything that would embarrass either of you.\nDave will speak to the three things we’ve shared:_\n\nOver the last 10 years, their love for one another has been constant, like the stars above them and the earth beneath them their love had a constant source of light and a firm foundation from which to grow.\n   >Speaking to Joanne and Nathaniel   Today you begin the next stage of your life together. Our wish for you is that you both become: - more magnificent and powerful than you have ever been - healthier than you have ever been - more full of life and love than you have ever been - more at one with yourself and all others as a direct result of the transformational power of your marriage here today.\nJoanne and Nathaniel, I am just putting in some samples of readings. You can let me know the ones which you have chosen."
  },
  {
    "objectID": "notes/certain_things/Wedding/Ceremony.html#readings",
    "href": "notes/certain_things/Wedding/Ceremony.html#readings",
    "title": "Examined Algorithms",
    "section": "Readings:",
    "text": "Readings:\n\nOur first reading is being read for us by Aadil Kurji\n\nOn Exactitude in Science\n“…In that Empire, the Art of Cartography attained such Perfection that the map of a single Province occupied the entirety of a City, and the map of the Empire, the entirety of a Province. In time, those Unconscionable Maps no longer satisfied, and the Cartographers Guilds struck a Map of the Empire whose size was that of the Empire, and which coincided point for point with it. The following Generations, who were not so fond of the Study of Cartography as their Forebears had been, saw that that vast Map was Useless, and not without some Pitilessness was it, that they delivered it up to the Inclemencies of Sun and Winters. In the Deserts of the West, still today, there are Tattered Ruins of that Map, inhabited by Animals and Beggars; in all the Land there is no other Relic of the Disciplines of Geography.” - On exactitude in Science by Borges\n\nOur Second Reading is read for us by John Stanley\n\nSonnet 115\nThose lines that I before have writ do lie, Even those that said I could not love you dearer; Yet then my judgment knew no reason why My most full flame should afterwards burn clearer. But reckoning time, whose millioned accidents Creep in ’twixt vows and change decrees of kings, Tan sacred beauty, blunt the sharp’st intents, Divert strong minds to th’ course of alt’ring things— Alas, why, fearing of time’s tyranny, Might I not then say “Now I love you best,” When I was certain o’er incertainty, Crowning the present, doubting of the rest? Love is a babe. Then might I not say so, To give full growth to that which still doth grow.” - Shakespeare\n\nOur Third Reading is read for us by Ciaran Murray\n\nLetter to Mauro\n“The surest way to avoid a broken heart is to love nothing and no-one — not your partner, your child, your mother or father, your brothers or sisters; not your friends; not your neighbour; not your dog or your cat; not your football team, your garden, your granny or your job. In short, love not the world and love nothing in it. Beware of the things that draw you to love — music, art, literature, cinema, philosophy, nature and religion. Keep your heart narrow, hard, cynical, invulnerable, impenetrable, and shun small acts of kindness; be not merciful, forgiving, generous or charitable — these acts expand the heart and make you susceptible to love — because as Neil Young so plainly and painfully sings, ‘Only love can break your heart.’ In short, resist love, because real love, big love, true love, fierce love, is a perilous thing, and travels surely towards its devastation. A broken heart — that grief of love — is always love’s true destination. This is the covenant of love.\nHowever, Mauro, to resist love and inoculate yourself against heartbreak is to reject life itself, for to love is your primary human function. It is your duty to love in whatever way you can, and to move boldly into that love — deeply, dangerously and recklessly — and restore the world with your awe and wonder. This world is in urgent need — desperate, crucial need — and is crying out for love, your love. It cannot survive without it.\nTo love the world is a participatory and reciprocal action — for what you give to the world, the world returns to you, many fold, and you will live days of love that will make your head spin, that you will treasure for all time. You will discover that love, radical love, is a kind of supercharged aliveness, and all that is of true value in the world is animated by it. And, yes, heartache awaits love’s end, but you find in time that this too is a gift — this little death — from which you are reborn, time and again. I have only one piece of advice for you both, and it is the very best that I can give. Love. The world is waiting.” - Nick Cave \n\nThird Song Here… half a song: Into my Arms - Nick Cave"
  },
  {
    "objectID": "notes/certain_things/Wedding/Ceremony.html#vow-ceremony",
    "href": "notes/certain_things/Wedding/Ceremony.html#vow-ceremony",
    "title": "Examined Algorithms",
    "section": "Vow Ceremony          ",
    "text": "Vow Ceremony          \n\nCivil Vows\nJoanne and Nathaniel you both formally attended in the presence of a civil registrar and on that occasion a declaration was signed by the two of you stating that there was no impediment of kindred or alliance or any other impediment or lawful hindrance to marriage and that the consent of every person whose consent to such marriage  is by law required has been duly obtained.  \nI Nathaniel  do solemnly and sincerely declare……. that I know not….. of any lawful impediment…. why I Nathaniel may not be joined in matrimony to Joanne.\nI.. Joanne…….do solemnly and sincerely declare…. that I know not….. of any lawful impediment why I, Joanne…. may not be joined in matrimony to Nathaniel.\nExpression of intent\nMarriage is a profound and compelling adventure. You are committing to loving each another for ever.\nNathaniel do you take Joanne to be your wife?\nNathaniel: I do\nDo you offer her your heart, mind, body and spirit? \nNathaniel: I Do \nDo you offer her your friendship and your loving care - honouring her growth and freedom as your own, cherishing and respecting her, listening to her, loving and embracing her in times of adversity and in times of joy?\nNathaniel: I Do. \nJoanne,   Do you take Nathaniel to be your husband?\nJoanne : I Do\nDo you offer him your heart, mind, body and spirit? :\nJoanne:  I Do\nDo you offer him your friendship and your loving care - honouring his growth and freedom as your own, cherishing and respecting him, listening to him, loving and embracing him in times of adversity and in times of joy?\nJoanne: I do\nVows\n\nNathaniel  please repeat after me…..\n\nToday I will marry my friend,\nThe one I will live with, ….. dream with and love.\nJoanne  I take you to be my wife.\nFrom this day forward ….. I will cherish you,\nI will look with joy …. down the path of our tomorrow’s\nKnowing we will walk it together … side by side,\nhand in hand …. and heart to heart.\n\nJoanne  please repeat after me…\n\nToday I will marry my friend,\nThe one I will live with, ….. dream with and love.\nNathaniel  I take you to be my husband.\nFrom this day forward ….. I will cherish you,\nI will look with joy …. down the path of our tomorrow’s\nKnowing we will walk it together … side by side,\nhand in hand …. and heart to heart.\nExchanging of Rings.\nThese rings represent circles of love and circles of life.  And as it is with the great mysteries of life, they have no beginning and no end.  \nWe wish for you never ending love and may your life together be strong and radiant. May these rings join you with one another in happiness, joy and fulfilment. May these rings join you with one another in honesty, trust and commitment. May these rings join you with one another in encircling and never-ending love. As you wear them, may they always remind you of this precious moment.\n\nNathaniel, take the ring, put it on Joanne’s finger and repeat after me.\n\nJoanne, with this ring, I commit my heart and my soul to you.   I promise to remain faithful and love you all the days of my life.\n\nJoanne,  take the ring, put it on Nathaniel’s finger and repeat after me.\n\nNathaniel…..With this ring, I commit my heart and my soul to you.   I promise to remain faithful and love you all the days of my life.\n\n\nPronouncement\n\nJoanne and Nathaniel \n\nToday have promised before your family and friends, to give wholly and freely to one another, and to love one another according to your vows,\nIt is my honour and delight to pronounce you husband and wife.\nYou may kiss the Bride.\nCongratulations \n\nFourth song here Sea of Love - Cat Power\n\nLight Marriage candle and the Signing of the Register, \nWitnesses Margaret and Jamie.\nClosing words\nThese two have found each other, and their destinies will now be woven into one and their joys shall not be known apart.   May the nourishment of the earth be yours, May the clarity of the light be yours, May the fluency of the ocean be yours, May the protection of the ancestors be yours. And so may a slow wind …. work these words of love …… around you, Like an invisible cloak ……To mind your lives now & forever……….. Thank our whoever looked after the music.\n\nBride and Groom then walk down the aisle together as a married couple Fifth song Temptation by New Order"
  },
  {
    "objectID": "notes/certain_things/Wedding/Three things.html",
    "href": "notes/certain_things/Wedding/Three things.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "I think the three things should add detail that evokes concrete imagery and raises an element of depth unexplored - hinting at a tapestry of connections between aspects of her character that only we share….\n\nPassion\nCourage\nJoy"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#preliminaries",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#preliminaries",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Preliminaries",
    "text": "Preliminaries\n\n\n\n\n\n\nI am not an Economist\n\n\n\nI’m a data scientist at Personio - where we work on cool problems ranging across themes of:\n\nrevenue optimisation\ncustomer churn\nexperimentation\nsurvey design\nproduct analytics\n\nBayesian statistician, reformed philosopher and logician.\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\n\nDisclaimer\n\n\nNone of Personio’s data was used in this presentation\n\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\nThe worked examples used here can be found here"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#agenda",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#agenda",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Agenda",
    "text": "Agenda\n\n\nHistory and Conceptual Background\n\n\n\n\nA Naive Utilty Model\n\n\n\n\nAn Augmented Model\n\n\n\n\nAdding Correlation Structure\n\n\n\n\nCounterfactual Questions\n\n\n\n\nIndividual Heterogenous Utility\n\n\n\n\nConclusion\n\nThe World in the Model"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#mcfadden-and-bart",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#mcfadden-and-bart",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "McFadden and BART",
    "text": "McFadden and BART\n\n\n\n\n“Transport projects involve sinking money in expensive capital investments, which have a long life and wide repercussions. There is no escape from the attempt both to estimate the demand for their services over twenty or thirty years and to assess their repercussions on the economy as a whole.” - Denys Munby, Transport, 1968 ”\n\n\n\n\n\n\n\nBay Area Rapid Transit\n\n\n\n\n\nDublin Metrolink"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#revealed-preference-and-predicting-demand",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#revealed-preference-and-predicting-demand",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Revealed Preference and Predicting Demand",
    "text": "Revealed Preference and Predicting Demand\nSelf Centred Utility Maximisers?\n\n\n\n\nThe assumption of revealed preference theory is that if a person chooses A over B then their latent subjective utility for A is greater than for B.\n\n\n\n\nSurvey data estimated about 15% of users would adopt the newly introduced BART system. McFadden’s random utility model estimated 6%.\n\n\n\n\nHe was right.\n\n\n\n\n\n\nCopernican Shift: He estimated utility to predict choice, rather than infer utility from stated choice."
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#general-applicability-of-choice-problems",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#general-applicability-of-choice-problems",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "General Applicability of Choice Problems",
    "text": "General Applicability of Choice Problems\n\n\nThese models offer the possibility of predicting choice in diverse domains: policy, brand, school, car and partners.\n\n\n\n\nQuestion: What are the attributes that drive these choices? How well are they measurable?\n\n\n\n\nQuestion: How do changes in these attributes influence the predicted market demand for these choices?"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#note-on-model-evaluation",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#note-on-model-evaluation",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Note on Model Evaluation",
    "text": "Note on Model Evaluation\nReplicating the Super Soldier Program"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#note-on-model-evaluation-1",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#note-on-model-evaluation-1",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Note on Model Evaluation",
    "text": "Note on Model Evaluation\nReplicating the Super Soldier Program\n\n\n\n\n\n\n\n\n\n\nBayesian Models aim to replicate the DGP holistically"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-the-data",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-the-data",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Choice: The Data",
    "text": "Choice: The Data\nGas Central Heating and Electrical Central Heating described by their cost of installation and operation.\n\n\n\nchoice_id\nchosen\nic_gc\noc_gc\n…\noc_ec\n\n\n\n\n1\ngc\n866\n200\n…\n542\n\n\n2\nec\n802\n195\n…\n510\n\n\n3\ner\n759\n203\n…\n495\n\n\n4\ngr\n789\n220\n…\n502"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-a-naive-model",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-a-naive-model",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Choice: A Naive Model",
    "text": "Choice: A Naive Model\nUnderspecified Utilities\nLet there be five goods described by their cost of installation and operation.\n\\[ \\begin{split} \\overbrace{\\begin{pmatrix}\n\\color{green}{u_{gc}}   \\\\\n\\color{green}{u_{gr}}   \\\\\n\\color{green}{u_{ec}}   \\\\\n\\color{green}{u_{er}}   \\\\\n\\color{green}{u_{hp}}   \\\\\n\\end{pmatrix}}^{utility} =  \\begin{pmatrix}\ngc_{ic} & gc_{oc}  \\\\\ngr_{ic} & gr_{oc}  \\\\\nec_{ic} & ec_{oc}  \\\\\ner_{ic} & er_{oc}  \\\\\nhp_{ic} & hp_{oc}  \\\\\n\\end{pmatrix} \\overbrace{\\begin{pmatrix}\n\\color{blue}{\\beta_{ic}}   \\\\\n\\color{blue}{\\beta_{oc}}   \\\\\n\\end{pmatrix}}^{parameters}  \\end{split}\n\\]"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-a-naive-model-1",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-a-naive-model-1",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Choice: A Naive Model",
    "text": "Choice: A Naive Model\nThe utility calculation is fundamentally comparative. \\[ \\begin{split} \\begin{pmatrix}\n\\color{green}{u_{gc}}   \\\\\n\\color{green}{u_{gr}}   \\\\\n\\color{green}{u_{ec}}   \\\\\n\\color{green}{u_{er}}   \\\\\n\\color{red}{\\overbrace{0}^{\\text{outside good}}}   \\\\\n\\end{pmatrix} =  \\begin{pmatrix}\ngc_{ic} & gc_{oc}  \\\\\ngr_{ic} & gr_{oc}  \\\\\nec_{ic} & ec_{oc}  \\\\\ner_{ic} & er_{oc}  \\\\\n\\color{red}{0} & \\color{red}{0} \\\\\n\\end{pmatrix} \\begin{pmatrix}\n\\color{blue}{\\beta_{ic}}   \\\\\n\\color{blue}{\\beta_{oc}}   \\\\\n\\end{pmatrix}  \\end{split}\n\\]\nWe zero out one category in the data set to represent the “outside good” for comparison. Similar to dummy variables in Regression, this is required for the model to be identified."
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-a-naive-model-2",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-a-naive-model-2",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Choice: A Naive Model",
    "text": "Choice: A Naive Model\nUtility determines choice probability of choice:\n\\[\\text{softmax}(\\color{green}{u})_{j} = \\frac{\\exp(\\color{green}{u_{j}})}{\\sum_{q=1}^{J}\\exp(\\color{green}{u_{q}})}\\]\nchoices determine market share where:\n\\[ s_{j}(\\mathbf{\\color{blue}{\\beta}}) = P(\\color{green}{u_{j}} > \\color{green}{u_{k}}; ∀k ̸= j) \\]"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-estimation",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-estimation",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Choice: Estimation",
    "text": "Choice: Estimation\nThe model is traditionally estimated with maximum likelihood caclulations.\n\\[  L(\\color{blue}{\\beta}) = \\prod s_{j}(\\mathbf{\\color{blue}{\\beta}}) \\]\nor taking the log:\n\\[  l(\\color{blue}{\\beta}) = \\sum log(s_{j}(\\mathbf{\\color{blue}{\\beta}})) \\] \\[ \\text{ We find: } \\underset{\\color{blue}{\\beta}}{\\mathrm{argmax}} \\text{ } l(\\color{blue}{\\beta}) \\]\nResults are often brittle!"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-bayesian-estimation",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-bayesian-estimation",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Choice: Bayesian Estimation",
    "text": "Choice: Bayesian Estimation\nTo evaluate the integrals in the Bayesian model we use MCMC to estimate conditional probabilities of the joint distribution.\n\\[\\underbrace{\\color{blue}{\\beta}}_{\\text{prior draws}} \\sim Normal(0, 1) \\]\n\\[ \\underbrace{p(\\color{blue}{\\beta} | D)}_{\\text{posterior draws}} = \\frac{p(\\mathbb{\\color{blue}{\\beta}})p(D | \\color{blue}{\\beta} )}{\\int_{i}^{n} p(D | \\mathbf{\\color{blue}{\\beta_{i}}})p(\\mathbf{\\color{blue}{\\beta_{i}}}) } \\]\nPriors can be used flexibly regularise and improve reliability of estimation across structural causal models."
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#the-naive-model-in-code",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#the-naive-model-in-code",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "The Naive Model in Code",
    "text": "The Naive Model in Code\nwith pm.Model(coords=coords) as model_1:\n    ## Priors for the Beta Coefficients\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n\n    ## Construct Utility matrix and Pivot\n    u0 = beta_ic * wide_heating_df[\"ic.ec\"] + beta_oc * wide_heating_df[\"oc.ec\"]\n    u1 = beta_ic * wide_heating_df[\"ic.er\"] + beta_oc * wide_heating_df[\"oc.er\"]\n    u2 = beta_ic * wide_heating_df[\"ic.gc\"] + beta_oc * wide_heating_df[\"oc.gc\"]\n    u3 = beta_ic * wide_heating_df[\"ic.gr\"] + beta_oc * wide_heating_df[\"oc.gr\"]\n    u4 = np.zeros(N)  # Outside Good\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    ## Apply Softmax Transform\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n\n    ## Likelihood\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#interpreting-the-model-coefficients",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#interpreting-the-model-coefficients",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Interpreting the Model Coefficients",
    "text": "Interpreting the Model Coefficients\nRate of Substitution\n\n\nThe beta coefficients in the model are interpreted as weights of utility. However, the precision in these latent terms is relative to the variance of unobserved factors.\nThe utility scale is not fixed, but the ratio \\(\\frac{\\beta_{ic}}{\\beta_{oc}}\\) is invariant.\n\n\n\n\nRate of Substitution"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#model-posterior-predictive-fits",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#model-posterior-predictive-fits",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Model Posterior Predictive Fits",
    "text": "Model Posterior Predictive Fits\nThe model fit fails to recapture the observed data points\n\nModel Fit"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#augmenting-the-model-1",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#augmenting-the-model-1",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Augmenting the Model:",
    "text": "Augmenting the Model:\nProduct Specific Intercepts\n\\[\n\\begin{split}\n\\begin{pmatrix}\n\\color{red}{u_{gc}} \\\\\n\\color{purple}{u_{gr}} \\\\\n\\color{orange}{u_{ec}} \\\\\n\\color{teal}{u_{er}} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  \\color{red}{\\alpha_{gc}} + \\color{blue}{\\beta_{ic}}gc_{ic} + \\color{blue}{\\beta_{oc}}gc_{oc} \\\\\n  \\color{purple}{\\alpha_{gr}} + \\color{blue}{\\beta_{ic}}gr_{ic} + \\color{blue}{\\beta_{oc}}gr_{oc}  \\\\\n  \\color{orange}{\\alpha_{ec}} + \\color{blue}{\\beta_{ic}}ec_{ic} + \\color{blue}{\\beta_{oc}}ec_{oc}  \\\\\n  \\color{teal}{\\alpha_{er}} + \\color{blue}{\\beta_{ic}}er_{ic} + \\color{blue}{\\beta_{oc}}er_{oc}  \\\\\n   0 + 0 + 0\n\\end{pmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#augmenting-the-model-2",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#augmenting-the-model-2",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Augmenting the Model:",
    "text": "Augmenting the Model:\nwith pm.Model(coords=coords) as model_2:\n    ## Priors for the Beta Coefficients\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n    alphas = pm.Normal(\"alpha\", 0, 1, dims=\"alts_intercepts\")\n\n    ## Construct Utility matrix and Pivot using an intercept per alternative\n    u0 = alphas[0] + beta_ic * wide_heating_df[\"ic.ec\"] + beta_oc * wide_heating_df[\"oc.ec\"]\n    u1 = alphas[1] + beta_ic * wide_heating_df[\"ic.er\"] + beta_oc * wide_heating_df[\"oc.er\"]\n    u2 = alphas[2] + beta_ic * wide_heating_df[\"ic.gc\"] + beta_oc * wide_heating_df[\"oc.gc\"]\n    u3 = alphas[3] + beta_ic * wide_heating_df[\"ic.gr\"] + beta_oc * wide_heating_df[\"oc.gr\"]\n    u4 = np.zeros(N)  # Outside Good\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    ## Apply Softmax Transform\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n\n    ## Likelihood\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#augmenting-the-model-3",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#augmenting-the-model-3",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Augmenting the Model:",
    "text": "Augmenting the Model:\n\nModel Structure"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#augmenting-the-model-4",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#augmenting-the-model-4",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Augmenting the Model",
    "text": "Augmenting the Model\nPosterior Predictions\n\nModel Fit"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#independence-of-irrelevant-alternatives",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#independence-of-irrelevant-alternatives",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Independence of Irrelevant Alternatives",
    "text": "Independence of Irrelevant Alternatives\nNew Products Cannibalise Equally from all Alternatives\n\nSuppose a market choice between transport modes is determined by the above model.\nRed Bus or Car are you initial Options. Assume \\(s_{\\color{red}{bus}}(\\beta) = s_{car}(\\beta)\\). Market Share is 50% to each option.\nIntroduce the Blue Bus Option, then the Independent characteristics of the utility specification implies that \\(s_{\\color{red}{bus}}(\\beta) = s_{car}(\\beta) = s_{\\color{blue}{bus}}(\\beta)\\)\nThis implies an implausible substitution pattern for real markets.1\n\nWhat kind of Monster have we Created!?"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#adding-correlation-structure-1",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#adding-correlation-structure-1",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\nDependence in Market Share\n\\[ \\alpha_{i} \\sim Normal(\\mathbf{0}, \\color{brown}{\\Gamma}) \\]\n\\[\n\\begin{split}\n\\begin{pmatrix}\n\\color{red}{u_{gc}} \\\\\n\\color{purple}{u_{gr}} \\\\\n\\color{orange}{u_{ec}} \\\\\n\\color{teal}{u_{er}} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  \\color{red}{\\alpha_{gc}} + \\color{blue}{\\beta_{ic}}gc_{ic} + \\color{blue}{\\beta_{oc}}gc_{oc} \\\\\n  \\color{purple}{\\alpha_{gr}} + \\color{blue}{\\beta_{ic}}gr_{ic} + \\color{blue}{\\beta_{oc}}gr_{oc}  \\\\\n  \\color{orange}{\\alpha_{ec}} + \\color{blue}{\\beta_{ic}}ec_{ic} + \\color{blue}{\\beta_{oc}}ec_{oc}  \\\\\n  \\color{teal}{\\alpha_{er}} + \\color{blue}{\\beta_{ic}}er_{ic} + \\color{blue}{\\beta_{oc}}er_{oc}  \\\\\n   0 + 0 + 0\n\\end{pmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#adding-correlation-structure-2",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#adding-correlation-structure-2",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\nPriors on Parameters determine Market Structure\n\\[ \\begin{split} \\color{brown}{\\Gamma} =\n\\begin{pmatrix}\n  \\color{red}{1} , \\gamma , \\gamma , \\gamma \\\\\n  \\gamma , \\color{blue}{1} , \\gamma , \\gamma  \\\\\n   \\gamma , \\gamma  , \\color{orange}{1} , \\gamma \\\\\n  \\gamma , \\gamma , \\gamma , \\color{teal}{1}  \n\\end{pmatrix}\n\\end{split} \\]"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#adding-correlation-structure-3",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#adding-correlation-structure-3",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\nwith pm.Model(coords=coords) as model_3:\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n\n    beta_income = pm.Normal(\"beta_income\", 0, 1 dims=\"alts_intercepts\")\n\n    chol, corr, stds = pm.LKJCholeskyCov(\n        \"chol\", n=4, eta=2.0, \n        sd_dist=pm.Exponential.dist(1.0, shape=4)\n    )\n    alphas = pm.MvNormal(\"alpha\", mu=0, chol=chol, dims=\"alts_intercepts\")\n\n    u0 = (\n        alphas[0]\n        + beta_ic * wide_heating_df[\"ic.gc\"]\n        + beta_oc * wide_heating_df[\"oc.gc\"]\n        + beta_income[0] * wide_heating_df[\"income\"]\n    )\n    u1 = (\n        alphas[1]\n        + beta_ic * wide_heating_df[\"ic.gc\"]\n        + beta_oc * wide_heating_df[\"oc.gc\"]\n        + beta_income[1] * wide_heating_df[\"income\"]\n    )\n    u2 = (\n        alphas[2]\n        + beta_ic * wide_heating_df[\"ic.gc\"]\n        + beta_oc * wide_heating_df[\"oc.gc\"]\n        + beta_income[2] * wide_heating_df[\"income\"]\n    )\n    u3 = (\n        alphas[3]\n        + beta_ic * wide_heating_df[\"ic.gr\"]\n        + beta_oc * wide_heating_df[\"oc.gr\"]\n        + beta_income[3] * wide_heating_df[\"income\"]\n    )\n    u4 = np.zeros(N)  # pivot\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#adding-correlation-structure-4",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#adding-correlation-structure-4",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\n\nCorrelation Structure"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#counterfactual-reasoning",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#counterfactual-reasoning",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Counterfactual Reasoning",
    "text": "Counterfactual Reasoning\nFrom Probability to Causation\n\n“[C]ontrary to the views of a number of pessimistic statisticians and philosophers you can get from probabilities to causes after all. Not always, not even ussually - but in just the right circumstances and with just the right kind of starting information, it is in principle possible.” - Nancy Cartwright in Nature’s Capacities and their Measurement\n\n\n“One of the functions of theoretical economics is to provide fully articulated artificial economic systems that can serve as laboratories in which policies that would be prohibitively expensive to experiment with in actual economies can be tested out at a much lower cost” - Mary Morgan quoted in Hunting Causes and Using Them"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#counterfactual-reasoning-1",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#counterfactual-reasoning-1",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Counterfactual Reasoning",
    "text": "Counterfactual Reasoning\nCeteris Paribus Laws\nWith a fitted PyMC model we can counterfactually reset the values for the input data and regenerate the posterior predictive distribution holding else equal in the data generating process.\n\nWhat would the market share be like if prices for electrical systems increased 20%?\n\n # update values of predictors with new 20% \n # price increase in operating costs for electrical options\nwith model_3:\n    pm.set_data({\"oc_ec\": wide_heating_df[\"oc.ec\"] * 1.2, \n                 \"oc_er\": wide_heating_df[\"oc.er\"] * 1.2})\n    # use the updated values and predict outcomes and probabilities:\n    idata_new_policy = pm.sample_posterior_predictive(\n        idata_m3,\n        var_names=[\"p\", \"y_cat\"],\n        return_inferencedata=True,\n        predictions=True,\n        extend_inferencedata=False,\n        random_seed=100,\n    )\n\nidata_new_policy"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#counterfactual-reasoning-2",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#counterfactual-reasoning-2",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Counterfactual Reasoning",
    "text": "Counterfactual Reasoning\n\nCounterfactual Shares"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#counterfactual-reasoning-3",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#counterfactual-reasoning-3",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Counterfactual Reasoning",
    "text": "Counterfactual Reasoning\nInterventions and Conditionalisation\n\n\n\nThere is a sharp distinction between conditional probability distributions and probability under intervention\nIn PyMC you can implement the do-operator to intervene on the graph that represents your data generating process."
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#individual-heterogenous-utility",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#individual-heterogenous-utility",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\nRepeated Choice and Hierarchical Structure\n\n\n\nperson_id\nchoice_id\nchosen\nnabisco_price\nkeebler_price\n\n\n\n\n1\n1\nnabisco\n3.40\n2.00\n\n\n1\n2\nnabisco\n3.45\n2.50\n\n\n1\n3\nkeebler\n3.60\n2.70\n\n\n2\n1\nkeebler\n3.48\n2.20\n\n\n2\n2\nkeebler\n3.30\n2.25"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#individual-heterogenous-utility-1",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#individual-heterogenous-utility-1",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\n\\[\n\\begin{split}\n\\begin{pmatrix}\n\\color{red}{u_{i, nb}} \\\\\n\\color{purple}{u_{i, kb}} \\\\\n\\color{orange}{u_{i, sun}} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  (\\color{red}{\\alpha_{nb}} + \\beta_{i}) + \\color{blue}{\\beta_{p}}p_{nb} + \\color{green}{\\beta_{disp}}d_{nb} \\\\\n  (\\color{purple}{\\alpha_{kb}} + \\beta_{i}) +  \\color{blue}{\\beta_{p}}p_{kb} + \\color{green}{\\beta_{disp}}d_{kb}  \\\\\n  (\\color{orange}{\\alpha_{sun}}  + \\beta_{i})  + \\color{blue}{\\beta_{p}}p_{sun} + \\color{green}{\\beta_{disp}}d_{sun}  \\\\\n   0 + 0 + 0\n\\end{pmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#individual-heterogenous-utility-2",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#individual-heterogenous-utility-2",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\nIn Code\n\nwith pm.Model(coords=coords) as model_4:\n    beta_feat = pm.TruncatedNormal(\"beta_feat\", 0, 1, upper=10, lower=0)\n    beta_disp = pm.TruncatedNormal(\"beta_disp\", 0, 1, upper=10, lower=0)\n    ## Stronger Prior on Price to ensure \n    ## an increase in price negatively impacts utility\n    beta_price = pm.TruncatedNormal(\"beta_price\", 0, 1, upper=0, lower=-10)\n    alphas = pm.Normal(\"alpha\", 0, 1, dims=\"alts_intercepts\")\n    beta_individual = pm.Normal(\"beta_individual\", 0, 0.05,\n     dims=(\"individuals\", \"alts_intercepts\"))\n\n    u0 = (\n        (alphas[0] + beta_individual[person_indx, 0])\n        + beta_disp * c_df[\"disp.sunshine\"]\n        + beta_feat * c_df[\"feat.sunshine\"]\n        + beta_price * c_df[\"price.sunshine\"]\n    )\n    u1 = (\n        (alphas[1] + beta_individual[person_indx, 1])\n        + beta_disp * c_df[\"disp.keebler\"]\n        + beta_feat * c_df[\"feat.keebler\"]\n        + beta_price * c_df[\"price.keebler\"]\n    )\n    u2 = (\n        (alphas[2] + beta_individual[person_indx, 2])\n        + beta_disp * c_df[\"disp.nabisco\"]\n        + beta_feat * c_df[\"feat.nabisco\"]\n        + beta_price * c_df[\"price.nabisco\"]\n    )\n    u3 = np.zeros(N)  # Outside Good\n    s = pm.math.stack([u0, u1, u2, u3]).T\n    # Reconstruct the total data\n\n    ## Apply Softmax Transform\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n\n    ## Likelihood\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#individual-heterogenous-utility-3",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#individual-heterogenous-utility-3",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\nRecovered Posterior Predictive Distribution"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#individual-heterogenous-utility-4",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#individual-heterogenous-utility-4",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\nIndividual Preference\n\n\n\n\n\nIndividual preferences can be derived from the model in this manner.\nThe relationship between preferences over the product offering can be seen too"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#conclusion",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#conclusion",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Conclusion",
    "text": "Conclusion\nThe World in the Model\nWe’ve seen a series of models, each one expanding on the last.\n\nModels should articulate the relevant structure of the world.\nThey serve as microscopes. Simulation systems are tools to interrogate reality.\nBayesian Conditionalisation calibrates the system against the observed facts.\nBayesian Discrete choice models help us interrogate aspects of actors and their motivations under uncertainty.\nPyMC enables us to easily build and experiment with those models.\nCausal inference is plausible to degree that we can defend the structural assumptions. Bayesian models enforce tranparency and justification of structural commitments.\n\n\n“Models… [are] like sonnets for the poet, [a] means to express accounts of life in exact, short form using languages that may easily abstract or analogise, and involve imaginative choices and even a certain degree of playfulness in expression” - Mary Morgan in The World in the Model\n\n\n\n\nDiscrete Choice with PyMC"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "",
    "text": "I am not an Economist\n\n\n\n\nI’m a data scientist at Personio - where we work on cool problems ranging across themes of:\n\nrevenue optimisation\ncustomer churn\nexperimentation\nsurvey design\nproduct analytics\n\nBayesian statistician, reformed philosopher and logician.\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\nDisclaimer\n\n\n\nNone of Personio’s data was used in this presentation\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\n\nThe worked examples used here can be found here"
  },
  {
    "objectID": "notes/certain_things/Wedding/Three things.html#razor-sharp",
    "href": "notes/certain_things/Wedding/Three things.html#razor-sharp",
    "title": "Examined Algorithms",
    "section": "Razor Sharp",
    "text": "Razor Sharp\nMind like a diamond, and her tongue is just as sharp.\n\nJoy\nThe cascade of smiles rising across her cheeks when watching Wyatt burgeoning pride of discovering the next new thing."
  },
  {
    "objectID": "notes/certain_things/Public/Statistics/Analogies between Missing Data and Causal Inference.html",
    "href": "notes/certain_things/Public/Statistics/Analogies between Missing Data and Causal Inference.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "The clearest framework for causal inference has a tight relationship with #missing-data imputation. However, the range of problems addressed seem to require a slew of distinct estimators. It is not always clear in which circumstances we should apply each estimation procedure.\nLike most statistical work the practical details should be worked out in code for the clearest demonstration. However, here we’ll describe (as best we can) the conceptual underpinning of important estimators. Their usage and motivation."
  },
  {
    "objectID": "notes/certain_things/Public/Statistics/Analogies between Missing Data and Causal Inference.html#taxonomies-of-missing-ness",
    "href": "notes/certain_things/Public/Statistics/Analogies between Missing Data and Causal Inference.html#taxonomies-of-missing-ness",
    "title": "Examined Algorithms",
    "section": "Taxonomies of Missing-ness",
    "text": "Taxonomies of Missing-ness\nCausal inference can be seen as a species of missing data problem where the missing data is the counterfactual situation(s) of how the world would have been were the course of the world different from the one we know. What if we used this treatment plan rather than another? What it the actors’ behaviour differed from the actions they in fact pursued?\nFrom an estimation perspective there are different species of missing-ness that matter. We won’t here go deep into the distinctions. It is enough to note that they vary in the operative source of the missing-ness: Missing-completely-at-random (MCAR), missing-at-random (MAR) and missing-not-at-random (MNAR).\n\nThe Stable Outcomes Assumption\nMechanically we can’t work with null values under any of these assumptions. The stable outcomes model is a first step procedure for imputing the missing values. Whether we choose mean imputation or an arbitrary figure - we initially assume missing values at the individual level in a stable fashion by specifying a constant value for the missing cases.\nVery crudely, estimation procedures work reasonably well under (MCAR) but require extra effort when there is assumptions if we hope to account for the (MAR) and (MNAR) cases. Under MAR we are assuming that the values are missing as a function of the observable covariates and can be imputed under proper conditionalisation.\nImputation under MAR and MCAR succeeds largely from successive applications of the law of iterated expectations. In this case the stable outcome assumption encodes the missing data as \\(-99\\) and we then average over the joint distribution of the stable outcomes model and the missingness data.\n\nThe various estimation procedures for counterfactual results trade on this property of expectation that allow for point identification of the expected value for the outcome variable.\nBut we also want to consider individual variation due to the observed covariate profiles. When we consider cases of missing data conditional the observed covariate profile, we can derive a propensity score as a one number summary for the conditional probability of missing-ness. This can be used in imputation techniques where we want to carefully attribute values to the missing data that respect the other observed properties of the individual.\n\n\nPropensity Scores\nThe propensity score has a role in a number of re-weighting schemes for the estimation of missing data. These rely on the property of expectation under (MAR). So accuracy of the propensity score is itself an important question. Because the missing-ness variable \\(R\\) is a binary random variable in \\(\\{ 0, 1 \\}\\) maximum likelihood methods for logistic regression are often used to estimate these terms.\n\\[p_{R}(\\mathbf{x}) \\sim logit(X_{i} \\beta )\\]\nThis score is a summary in some sense of the factors driving missing-ness. In the context of causal inference it is often stated “in reverse” as a probability of being treated. We will keep focus here on the case of missing data, but the generality of the notion shouldn’t be lost. The propensity score is a one number summary of the covariate profile for each individual in the data. Under the (MAR) assumption it is often sufficient to conditionalise on the propensity score for each individual for imputation of their missing data values.\n\n\nRegression Estimators\nWe might want to simply estimate the missing values of our outcome using the conditional expectation function (CEF) property of simple regression. The imputation pattern will work well when the linear properties of the regression model are a good fit for the relationship between the outcome variables and the observed covariates. Hence the estimate for:\n\\[E[Y_{i}] = \\beta_{0} + \\beta_{1}\\cdot X_{1i} ... \\beta_{n} \\cdot X_{ni} \\] where we replace all values to be prediction of our regression model for each individual and then average the predictions.\n\n\nWeighting Estimators\nAnother approach to missing data imputation, which relies on the expectation properties of our outcome variable of interest under (MAR) and the stable outcome model, is the inverse probability weighting approach to imputation.\n\\[ E[Y_{i}] = E[\\dfrac{(YR + (-99) (1-R ))R }{p_{R}(\\mathbf{x})}]\\] With this property we can express estimates of missing values as a function of the individuals’ observed data. There are variations on this theme but sophisticated imputation schemes all rely on functions of the individual’s observed covariate profile. This specificity is important too in the context heterogenous treatment effects in causal inference."
  },
  {
    "objectID": "notes/certain_things/Public/Statistics/Analogies between Missing Data and Causal Inference.html#causal-inference",
    "href": "notes/certain_things/Public/Statistics/Analogies between Missing Data and Causal Inference.html#causal-inference",
    "title": "Examined Algorithms",
    "section": "Causal Inference",
    "text": "Causal Inference\nThe above perspective on missing data has deep analogies with the potential outcomes framework of causal inference. This is well brought out in the beautiful book Foundations of Agnostic Statistics by Aronow and Miller.\n\nPotential Outcomes and SUTVA\nIn the causal context we assume the potential outcomes framework and the notation of \\(Y(1), Y(0)\\) to denote the value of the outcome under the treatment regime \\(D\\).\n\\[ Y_{i} =  \n\\left\\{\\begin{array}{lr}\n        Y_{i}(0), & \\text{for } D = 0\\\\\n        Y_{i}(1), & \\text{for } D = 1\\\\\n        \\end{array}\\right\\}\n\\] where the (S)table (U)nit (T)reatment (V)alue (A)ssumption holds i.e. the observed data under treatment or non-treatment regimes is the potential outcome for that individual. Additionally the counterfactual outcome is assumed to be stable for each individual. It is crucially this assumption that allows for statistical identification of key metrics in causal inference under randomisation.\n\n\nAverage Treatment Effects\nSimilarly, here we rely on the properties of expectation over the observed data to isolate quantities of causal effect. In particular we tend to be interested in the average treatment effects, which we can get by using the following decomposition under random assignment.\n\\[ E[\\tau] = E[Y_{i}(1) - Y_{i}(0)] = E[Y_{i}(1)] - E[Y_{i}(0)] \\] This decomposition is crucial since it allows us to move between the expectations derived from the observed data under each regime towards an estimate of the population treatment effects.\n\n\n\nsubject\n\\(Y_{i}(1)\\)\n\\(Y_{i}(0)\\)\n\\(\\tau\\)\n\n\n\n\nJoe\n?\n115\n?\n\n\nBob\n120\n?\n?\n\n\nJames\n100\n?\n?\n\n\nMary\n115\n?\n?\n\n\nSally\n120\n?\n?\n\n\nLaila\n?\n105\n?\n\n\n\\(E[Y_{i}(D)]\\)\n113.75\n110\n3.75\n\n\n\nThe missing values in this table depict the fundamental problem of causal inference as a missing data issue. So #causal-inference as a strategy is broadly related to finding ways to solve this missing data problem under different regimes of missing-ness. For instance, the reason A/B testing works to isolate the treatment effects is that under randomisation of treatment regime we are implicitly assuming that the reason missing-data is effectively a case of MCAR missing-ness. As such the expectations of the individual columns in the above table are valid estimates which can then be combined using the above decomposition to derived the treatment effect \\(\\tau\\).\nIn this case the pattern of reasoning is akin to performing mean-imputation and then taking the difference of the averages. The imputation step is redundant in A/B testing, but it is highlighted by Aronow and Miller as a useful lens on more complex causal inference tasks on observed data. We are always (under the hood) trying to impute the missing values to gain a better view of the treatment effect distribution. ### Regression Estimators\nAgain we rely on the idea of regression as an approximation to the CEF of the data generating process. The flexibility of regression modelling for automating a host of statistical test should be reasonably familiar. The point here is not to rehash the theory but just to note the similarity with the procedures used above for regression-based imputation. Regression modelling of the treatment effect proceeds on the strong ignorability assumption that - conditional on the observed covariates knowing whether or not an individual received the treatment adds no new information i.e. it is the insistence that assignment might as well be random after accounting for the background characteristics. These assumptions mirror the conditions required for imputation under the MAR regime.\nSo we can derive estimates for the ATE from the data generating model\n\\[ Y \\sim \\beta_0 + \\beta_1 D + ... \\beta_{n} \\cdot X_{n} \\] such that out quantity of interest \\(\\tau\\) is cleanly identified in expectation by the quantity: \\[ E[\\tau] = \\beta_{1}\\] But this result can also be derived by predicting the outcomes under the different treatment regimes, using a fitted regression model, and taking the differences of the averaged predictions over the cases. The equivalence between these perspectives is the insight we want to record here. We drew out this connection in the discussion of poststratification estimators\n\nThis is a neat and beautiful connection between causal-inference and missing data analysis. Simultaneously a reminder of the versatility of regression analysis.\n\n\nPropensity Functions and Reweighting Estimators.\nWe will skip the detailed elaboration of propensity score matching, a technique for creating pseudo treatment and control groups, only noting that there is a rich and detailed literature on the topic for causal inference.\nWe do want to draw out how propensity-scores can be used in the class of reweighting estimators. Where under the strong ignorability assumption we can estimate the treatment effect as a simple expectation:\n\\[E[\\tau] = E [\\dfrac{YD}{p_D(X)} - \\dfrac{Y(1-D)}{(1 - p_D (X))}] \\]\nUsing this formula we can scale each observation by the relative probabilities for the individual falling into each treatment regime. Then the expectation of the scaled differences is an estimate of our ATE. The logic of this inverse probability weighting (IPW) estimator stems from the idea that low propensity individuals are likely underrepresented in the treatment group and over represented in the control. So this estimator down weights and unweights each option accordingly to “balance” the groups before comparison.\nThis balancing operation can work but is dependent on empirical properties of the sample data. Even if the data generating process ensures that strong ignorability holds, if our sample under represents the variety of possible individual in each group then reweighting the remaining individuals is no guarantee for sound inference. This is a small sample problem recurring."
  },
  {
    "objectID": "notes/certain_things/Public/Statistics/Analogies between Missing Data and Causal Inference.html#conclusion",
    "href": "notes/certain_things/Public/Statistics/Analogies between Missing Data and Causal Inference.html#conclusion",
    "title": "Examined Algorithms",
    "section": "Conclusion",
    "text": "Conclusion\nThe sequence of complexity in missing data imputation is as follows:\n\\[ MCAR \\Rightarrow MAR \\Rightarrow MNAR \\]\nwhich mirrors the complexity of cases in causal inference. Here we have:\n\\[ Ignorability \\Rightarrow \\text{Strong Ignorability} \\Rightarrow \\text{Non Ignorability} \\] As we consider circumstances moving up the hierarchy, we require an increase in assumptions or structural commitments to offset the risk of non-identifiability bringing us back down the hierarchy. The emphasis in the book stresses how properties of good experimental design can help recover sound inference by enforcing MAR conditions in MNAR circumstances. But the crucial role of modelling in defending the strong ignorability condition is underplayed.\nYes, we need to justify our estimator but also our model! Are we including the right covariates? Have we an appropriate covariance structure? What is the functional form and why is it reasonable? Are we accounting for heterogeneity of outcome? All such questions centre the importance of domain knowledge for causal inference. This is not a criticism of boon focused on Agnostic statistics. Their focus is appropriately on the design aspects that enable inference. However it should be abundantly clear that you cannot get away with agnostic approaches in the real world. There is no way to justify stepping back down the hierarchy without substantial commitments about the world-model fit. Even if your aesthetic preferences drive you toward design based methods, this only serves to obscure the commitments. Statistics in the real world require real world commitments."
  },
  {
    "objectID": "notes/certain_things/Public/Statistics/Introduction - Statistics Topics.html",
    "href": "notes/certain_things/Public/Statistics/Introduction - Statistics Topics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "This section will serve to capture my Zettelkasten style notes on topics in Statistics."
  },
  {
    "objectID": "notes/certain_things/Public/Logic/Introduction - Logic Topics.html",
    "href": "notes/certain_things/Public/Logic/Introduction - Logic Topics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "This section will serve to capture any and all Zettelkasten notes on topics in logic -mathematical, philosophical or any style in between."
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html",
    "href": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "In this note we’ll capture reflections about Jun Otsuka’s Thinking about statistics"
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#statistical-models-and-the-problem-of-induction",
    "href": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#statistical-models-and-the-problem-of-induction",
    "title": "Examined Algorithms",
    "section": "Statistical Models and the Problem of Induction",
    "text": "Statistical Models and the Problem of Induction\nThe book begins by framing the different epistemological projects of both #Bayesian and #Frequentist patterns of inference as approaches to solving the problem of induction expressed by David Hume. #book #philosophy #statistics\nThis is a nice lens on the development of statistics and the applied work of statistical modelling. The two probabilistic frameworks are initially contrasted or compared to each other. The distinction is drawn between the epistemological process involved in both approaches. Firstly the notion of Bayesian conditionalisation which incorporates new data to derive new beliefs coherent with the observed facts is spelled out. Then we see how the frequentist approach can be considered as a species of reliablist epistemology, where the focus is on the error control of well defined processes. In both cases the problem of induction is located as one of inference i.e. if we have an appropriate set of i.i.d sample data we can warrant the inference that the future will look like the past\nHe draws out the ontological commitments to probabilistic kinds that appear required to underwrite statistical inference in both paradigms. He supplies a justification for these commitments as being instances of real patterns in the sense of Daniel Dennett’s phrasing. This is a kind of indispensability argument for the deployment of probabilistic kinds in our best science."
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#the-uniformity-of-nature",
    "href": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#the-uniformity-of-nature",
    "title": "Examined Algorithms",
    "section": "The Uniformity of Nature",
    "text": "The Uniformity of Nature\nIf probabilistic kinds inhabit the world they can exhibit different characteristics - varying fauna and flora of the natural world. One process might be well described by a Gaussian distribution, another by a Laplace distribution… this diversity is all well and good, but to go beyond descriptive statistics we need to posit more. We need the assumption that there is some stability to the processes we seek to characterise. A uniformity of nature that underwrites statistical inference and probabilistic prediction models.\nOtsuka suggests that this commitment is cashed out in contemporary statistics with the famous i.i.d assumption. This posit argues that for sound inference, we must assume that any sample data is drawn from a probability distribution that each draw is independent and identically distributed.\n\n“The IID condition is a mathematical specification of what Hume called the uniformity of nature. To say that nature is uniform means that whatever circumstances holds for the observed, the same circumstances will continue to hold for the unobserved. This is what Hume required for the possibility of inductive reasoning …” pg 25/26\n\nThis obviously is constraint on sound inference, but also implicitly an ontological assertion regarding distributional drift. This commitment is deeper than when we argue for a particular distributional characterisation of our process of interest. Our target process could be articulated as a mixture distribution or some more complicated beast, but in each case we require that (a) it is well described by some probability model and (b) the world is set up in such a way that more observations enable us to learn which particular statistical model (parametric or non-parametric) fits the data."
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#approaches-to-learning",
    "href": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#approaches-to-learning",
    "title": "Examined Algorithms",
    "section": "Approaches to Learning",
    "text": "Approaches to Learning\nWith this background Otsuka goes on to describe the manner in which the Bayesian and frequentist schools approach the task of learning from data.\n\nBayesian Machinery\nThe focus in the Bayesian setting presents conditionalisation as a logic of inductive reasoning, which expands on logical inference. These are frameworks for organising and interrogating our system of beliefs and their relationship to the achievement of knowledge. Otsuka argues that Bayesian inference plays a crucial role in justification. Moving from prior to posterior distribution is seen as a change in the weighting of our beliefs. An internalist species of the justification-relation between beliefs in light of data.\nBut founding a story about epistemological justification on bayesian inference involves a defense of priors and likelihood specifications. Priors are defended using the usual moves: (1) appeal to wash-out theorems of iteratively updating on sufficiently large data. Convergence theorems assuring Bayesian updated belief is truth conducive in the limit regardless of apparently subjective prior specification. (2) Non-informative priors and (3) Objective priors or empirical Bayes.\nWe won’t spend much time on (2) because it’s just kind of silly to justify beliefs and their manipulation by constant appeals to ignorance. On (3) there is a more interesting discussion regarding the relationship between degrees of belief and chance. We want our priors to reflect our background knowledge and update our beliefs in a way that it tracks the actual occurence of the events in question. David Lewis enshrined this requirement as the Principal Principle. But while this is an agreeable sounding tenet it cannot serve as a foundational justification for our priors within an internalist picture of justification without risk of infinite regress. This is problematic for the philosopher that seeks to establish bayesian inference as the sole source of belief generation, but seems less serious if you can tolerate primitive or foundational epistemological commitments outside those justified with inductive inference in the Bayesian loop. Justification must end somewhere (the spade eventually turns), and in-practice arguments and evidential exchanges rarely get anywhere close to an infinite series.\nAdditionally the Bayesian needs to defend the incorporation of different likelihood choices. Their shape and implications. Fortunately this can be more pragmatic in so far likelihood specifications are in effect testable hypotheses about the data generating process. They can be justified by the success of the modelling endeavour and our ability to recover data akin to our observations. The data is a fixed quantity, we use it to update our probabilstic beliefs and commitments. This is to the good because it can be shown (via Dutch book style theorems) that in strategic decision making where your beliefs adhere to the probability calculus they will strictly dominate other strategies.\nThe Bayesian machinery is a set of tools for arranging coherence amongst our beliefs, commitments - tracing out the implications. We move dynamically between prior and posterior by means of the likelihood term. This process cannot serve as as an ultimate court of appeal for basic beliefs that kick off the learning process itself. It is an abstract, highly flexible set of tools applicable to a wide range of questions. It provides a very general model of learning where the concern is justification of our beliefs in the context of what we know.\n\n\nFrequentist Consolidation\nSo far so uncontroversial. The Bayesian perspective is a natural fit for a species of internalist epistemology. The framework is abstract and characterised concisely, so relatively straightforward to incorporate in a general philosophical picture. Otsuka’s synthesis of the “classical” inferential picture is in this way more impressive.\nThe classical frequentist picture has a history of poor pedagogy and can seem disparate and ad-hoc. Jaynes is famously dismissive of the absurdities engendered by the pick-and-mix approach to statistical inference adhered to in the “classical” approach.\nThe frequentist view ties statements of probability to measures of relative frequency within a collection of observations. This makes it impossible to articulate probability statements for specific hypotheses. The epistemological perspective is quite distinct from the Bayesian view of updating individual hypotheses. Instead the focus must lie of testing statements about stochastic processes - processes which are inherently repeatable.\nAs such these processes can be described by probabilistic kinds. The question then becomes - how can the properties of these observable processes feed into knowledge gathering routines. How can we go from a statement about an observed frequency to claims of knowledge or belief regarding the data generating process?\nThe route is to go via the framework of statistical testing which has some relationship to Karl Popper’s falsification. This involves positing a statistical hypothesis with direct implications. These implications can be parsed as an explicit prediction that can be compared to future observations. In this way, the hypothesis is tested against the data. This gives us a means of arguing reductio ad unlikely against the initial hypothesis and turns statistical inference into a “process of systematically plowing out falsehood”. Through iteratively testing and refining more targeted hypotheses. We define and reject the null hypotheses as we go.\nThe constraints on the test design are built to ensure reliability over the course of repeated testing under a known null hypothesis. Sample size considerations, alpha-spending and statistical power are properties of the test. These properties need to be chosen in such a way to ensure reliability of a particular test, but there are also constraints for running repeated tests of multiple hypotheses. The entire testing enterprise is set up to minimise and control errors. The epistemological picture is one of reliablism. Whether a belief is justified is determined by the nature and defensiblity of the process that generated the claim. This approach allows us to reject the Gettier style counter examples to justified true belief analyses of knowledge. If the procedures of knowledge acquisition are not themselves well founded than the coincidence between claim and fact in the Gettier cases cannot be counted as justified. The procedures of knowledge acquisition are fixed, uncertainty stems from how we learn the shape of the data probabilistically.\nFor this method to work the reliabilism the methodology seeks should be clarified. Otsuka suggests that the reliability implicit in statistical testing needs to underwrite truth-tracking counterfactuals. In particular the claims:\n\nif P were not true, S would not believe P\nIf P were true then S would believe P.\n\nWhich is not to say that any particular statistical test will lead to the endorsement/rejection of the hypothesis in question. Statistical tests cannot directly accept the null - just fail to reject. But cumulatively over many tests a reliabilist epistemology should ultimately ascertain the falsity of the null when the null is false. This points to a tension in the focus on individual tests and specific p-values. Statistical testing as an epistemological enterprise is a wholesale endeavour and the overall success or failure of conditions which underwrite the reliability of the process are not easily discerned by mere success in our world. The procedures must be valid and truth tracking in “nearby” counterfactual worlds where the null hypothesis is not how it is in our world. The conditions under which a process is ultimately truth-tracking may depend on contextual factors which cannot be easily turned to an algorithm. This perspective nicely unites all the ad-hoc approaches to defining tests with asymptotic error control properties. The concern is inherently procedural, and procedures can (and perhaps should) vary in the context of the learning. But their adoption is founded on the commitment that they would be reliably truth-tracking in all worlds similar enough to our own."
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#when-philosophies-conflict",
    "href": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#when-philosophies-conflict",
    "title": "Examined Algorithms",
    "section": "When Philosophies Conflict",
    "text": "When Philosophies Conflict\nOtsuka’s survey of the two approaches is detailed and comprehensive. It sketches the motivations of each position well, and you might hope to reconcile the two. Apply each in their own domain where appropriate…, but unfortunately the motivating instincts can clash irreconcilably.\nThe Bayesian perspective necessitates the adoption of the Likelihood Principle which can be violated by the classical procedure of sequential testing. Recall how for the Bayesian the data is a fixed quantity, fed forward into the likelihood term to update our beliefs. This works the same whether we update our beliefs at time t1, t2 or tN. All the information is in the likelihood irrespective of how much data has accrued over time.\nWhile under the frequentist model if the data is analysed under different experimental designs (e.g. one with a stopping rule and one without) the results of test for a particular null hypothesis can differ I.e. with the same data and the same null hypothesis, one experiment can reject the null and the other will fail to reject it. This is because the result incorporates information about the design of the experiment that cannot be captured in the simple likelihood. Otsuka makes the point that this is a broad problem for all reliabilist philosophies where the fit between process/test and reality is contestable. There is no perfectly general procedure that returns true results in all circumstances. As such the frequentist methodologist must argue anew in each circumstances for the appropriateness of their methods.\nThis is a keen source of divisiveness between the two schools. Scientists historically cannot be trusted to review their methods with respect to their goals. Instead frequentist statistical methodology has been abused in practice. Adherents use the routine nature of procedure as a rubber stamp without due consideration for the reliability of the methods in the context of the question at hand. This pattern of abuse is rightly seen as a key contributor to the replication crisis in science. Ironically it is this aspect of frequentist methodology that introduces obscure subjective bias into the experimental exercise. The Bayesian (often accused of criminal subjectivity) wears their priors clearly and defends their appropriateness for each analysis.\nNeither school of thought is a self-contained epistemology. Neither method is self-sustaining. Both priors and procedures must be justified with an appeal to their apt fit for the problem at hand. Statistical methodology slots into a broader epistemological endeavour and there is no substitute for the careful and knowing application of inferential machinery if we hope to justify the achievements of science."
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#bias-and-regularisation",
    "href": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#bias-and-regularisation",
    "title": "Examined Algorithms",
    "section": "Bias and Regularisation",
    "text": "Bias and Regularisation\nIn later chapters we move towards the more pragmatist position of model selection based on predictive power. This mirrors a move away from understanding of uncertainty due to natural variation towards an model-uncertainty in the analysis.\nWe may prefer a model which does not capture the true data generating process just so long as it performs better in prediction tasks. This effectively invokes the more typical approaches to machine learning model evaluation as driving us toward pragmatic biases that work well to optimise for some measure of out-of-sample prediction. Your inferential framework can be independent of your model selection criteria, but model ranking is still conducted under uncertainty. Each measure of performance is still an estimate.\nOtsuka contrasts the ranking of models based on information criteria with the out of sample predictive performance of deep learning systems. The AIC style methodologies penalises models with too many parameters to optimise for predictive performance. Deep learning methods have explicit methods to induce regularisation like effects with: drop-out, modified loss-functions. What is striking is that while both methods are checks against overfitting of models to the data, both are pragmatic compromises that dispense with the idea of epistemological truth-tracking. They concede that the it is often better to overcome the problem of model uncertainty with suitably practical abstraction. Don’t worry about the world-model fit so long as the outcomes of the model are workable.\nBut this comes with a burden of explainability often demanded of opaque models. Predictive success in one domain does not always translate to success in another. Regulators and stakeholders need to understand when and why the predictive reliability of deep learning systems can be expected to transfer well across tasks. Appeals to the epistemological surety of beliefs derived from predictions in these black-box systems stem from a loosely held belief in the virtues of the deep learning mechanisms. These virtues are less well established than error-control rates of a statistical test and they are somewhat transient across task."
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#causal-inference-and-statistics",
    "href": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#causal-inference-and-statistics",
    "title": "Examined Algorithms",
    "section": "Causal Inference and Statistics",
    "text": "Causal Inference and Statistics\nFinally Otsuka pauses to consider the role of causal inference in contemporary statistics and how here optimising for predictive power will often fail when the task requires subtlety or insight into the data generating process. The manner in which models and measurements can be confounded by aspects of the data generating process cannot be detected automatically without knowledge of the relationships in the system. The focus here is on how tools for identification are required when we want to be sure that the causal quantities of interest are soundly derivable from the data we have to hand. Whether we use the potential outcomes framework, the do-calculus or structural estimation. The work of statistical inference using any and all of the above frameworks can only get off the ground when we have enough of a world-picture - a set of commitments or assumptions that allow our inferences to have warrant. Our priors our informed, our inferential procedures suitably well defined to sustain truth-tracking counterfactuals."
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#conclusion",
    "href": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#conclusion",
    "title": "Examined Algorithms",
    "section": "Conclusion",
    "text": "Conclusion\nThe broad picture painted in Otsuka’s survey of statistics is the central role of inferential procedures in our broader epistemological landscape. The diversity of roles it can play and different standards of rigour at play in different contexts. The links between the foundational questions and puzzles of epistemology have illuminated the structure of the debates in statistics."
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Introduction - Philosophy Topics.html",
    "href": "notes/certain_things/Public/Philosophy/Introduction - Philosophy Topics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "This section will serve to capture the philosophical topics in my Zettelkasten notes."
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Sorites Paradox and Survival Analysis.html",
    "href": "notes/certain_things/Public/Philosophy/Sorites Paradox and Survival Analysis.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "There seems to be a plausible relationship between (a) the idea of a latent evolving hazard in #survival-analysis and (b) the accumulation effect that drives our intuitions from observations of distinct sand grains to observations of a heap. The classic philosophical puzzle put forward by Sorites.\nI’ve worked on survival analysis in the context of statistics and data science , but I’m putting together this note to arrange my thoughts on the value the perspective has on the classic philosophical puzzle.\nThe first thing to observe is how survival analysis is in general a model of the probabilities of state-transition. Moving between alive-dead, sick-well, subscribed-churned, hired-fired. The Framework is quite abstract and therefore widely applicable to the analysis of all state transitions with both clear, distinct and permeable borders between states.\nTraditionally you might see frequentist elaborations of the mechanics of #survival-analysis , but it becomes more interesting from a philosophical stand point when you phrase the model in a #Bayesian fashion.\nIn the Bayesian setting the uncertainty expressed in the model regarding measures of risk of state-change can be seen to contribute to the semantic ambiguity relevant in the Sorites setting. I will try to bring out this connection in the following."
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Sorites Paradox and Survival Analysis.html#the-sorites-paradox",
    "href": "notes/certain_things/Public/Philosophy/Sorites Paradox and Survival Analysis.html#the-sorites-paradox",
    "title": "Examined Algorithms",
    "section": "The Sorites Paradox",
    "text": "The Sorites Paradox\nThe typical presentation of the sorites issue in philosophy has been as a series of conditional predications as follows:\n\nFa_0\nFa_0 -> Fa_1\n.\n.\n.\nFa_{i-1} -> Fa_i\nTherefore: Fa_i\n\nWhere we allow that there is an indifference relation for the predication of F over all elements of the sequence preceding the last entry. Then, it is argued, the last predication fails for some entry (i) in the sequence. The predicate F is said to be suffer from #vagueness .This is purportedly a paradox due to the requirement of logical validity over conditional reasoning caused by the vagueness of F over the course of the sequence.\nOther rephrases of the logical steps truncate the sequence of conditionals by appealing to a Principle of mathematical Induction to arrive at the same conclusion. For out purposes it’s not crucial which phrasing is applied. The point is just that the the vagueness of the predicate F is said to threaten some central tenet of logical validity.\n\nApproaches to the Paradox\nThere have been advocates for acceptance of the Paradox - allowing that there is just a breakdown of logic in the case of these vague predicates. So much the worse for logic. This is quite radical as the prevalence of vague predicates in natural language commits us implicitly to the view that we cannot make distinctions.\nThe more plausible approaches to the Paradox seek to establish a reason for rejecting the validity of the conclusion by denying the premises in some way e.g. claiming that the indifference over the nth predication of F and the n-1 case fails. There is a precise point which is a genuine cut-off between F and not F. This position is called Epistemicism - which locates the causes of vagueness in predication in our degrees of ignorance.\nThe suggestion above is supported somewhat empirically by the reality of borderline cases of predication among reasonable speakers of the same language. People evince hedging behaviour and deliberate vagueness in cases where they avoid commitment to a sharp distinction. “She is sorta cool, nice-ish!”. This is the data the philosophical theory needs to explain. Paradigmatic cases of attributed state-change coupled with paradigmatic cases of hedging.\nThe theoretical question then becomes - what constitutes borderline vagueness? I think this is where we can use survival analysis to elaborate and explain cases of borderline vagueness and empirical cases of disagreement in predication. In particular Bayesian approaches to survival analysis which allow that there is a cut-off point in the sequence, but there is genuine uncertainty where we locate the cut-off point. Seeing this as a problem of Bayesian modelling allows us to locate the sources of hedging in the components and precision of our model terms through which the propagates the uncertainty in our attribution patterns.\n\n\nPerspectives on Probability\nSurvival analysis can seem intimidating because it asks us to understand time-to-event distributions from four distinct perspectives. The first familiar density function, the next cumulative density function and its inverse survival function. Additionally we can view the the cumulative hazard function as a transformation of the survival function, and the instantaneous hazard as a discretisation of over intervals of the temporal sequence ranged over by the cumulative hazard.\nIt’s important to see and understand that these quantities, while appearing to be abstract mathematical objects, can be derived from simple tables which record the subjects in the risk set which experience the event of interest at each interval of time. In other words the set of conditional probabilities instance-by-instance over the range of the temporal sequence. This is how we derive the instantaneous hazard quantity.\nDifferent families of probability distribution allow us to encode different structures in the hazards. For instance if we want hazards to peak early in the sequence and decline later in the sequence non-monotonically we can use the loglogistic distribution. If we want to ensure monotonic hazard sequences we can use Weibull distributions.\n\n\nDistinguishing Risk and Uncertainty\nWe want to explain the semantic ambiguity of Sorites phenomena by the probabilistic nature of state transitions over additive sequences. However, we won’t trade on the uncertainty between distinct models i.e. it’s not merely that your model of the sand-to-heap transition is characterised by one probability distributions and mine by another (although it could be). We are interested in divergences of opinion and semantic uncertainty that arises due to the stochastic nature of the phenomena where we share the same model of the phenomena. This reflects a difference in the view of the risk not the model uncertainty.\n\n\nCox Proportional Hazards Model\nTo make this a bit more concrete consider the cox proportional hazard model. This is a #regression model which aims to characterise the probability of state transition using a statistical model with two components. The baseline hazard:\n\\[ \\lambda_0 (t) \\]\nwhich is combined multiplicatively with an exponentiated weighted linear sum as follows \\[\\lambda_0 (t) \\cdot e^{\\beta_0 X_0 + \\beta_1 X_1 ... \\beta_n X_n}\\] In this model the baseline hazard is a function of the time intervals and we estimate a hazard term for each interval when we fit the model. There is a “free” parameter for the instantaneous hazard at each timepoint. This sequence is the baseline hazard. This latent baseline hazard is akin to an intercept term(s) in more traditional regression models. Individual predictions of the evolving hazard are then determined by how the individuen’s covariate profile modifies the baseline hazard. Estimation procedures for this model find values for the baseline hazard and for the coefficient weights \\(\\beta_i\\) in our equation.\nWith these structures in mind you might be tempted to locate the source of disagreement between people’s judgments as stemming from differences in their covariates \\(X_i\\) , or put another way… we see the probability of transition as a function of the same variables, but disagree on the values of those inputs to the function. The benefit of this perspective is that instead of seeing the Sorites Paradox as an error of logical reasoning that needs to be fixed by one or more adjustments to classical logic, we can instead view the phenomena as reflecting disagreement among latent probabilistic models.\n\n\nComplexity of the Heap\nOne additional perspective on the problem is gained by noting how the Cox proportional model is a prediction model and comes with criteria of model adequacy. How many predictor variables are required to anticipate state transition? How much variance is explained? If we can gauge the complexity of the prediction task, can the complexity itself explain disagreement?\n\n\nHierarchical or Meta Vagueness\nWe’ve seen now a few different sources of divergences. At the highest level we can appeal to the Knightian distinction between risk and uncertainty, then secondarily to differences in the data used to calibrate risk or thirdly in differences due to estimation strategies and finally in pure prediction complexity.\nIf divergences are due to complete uncertainty of the appropriate model, then we concede allot to the sceptic and the quantification of any plausible cut point is hopeless. If differences result from the other candidate sources there remains hope for arriving at intersubjective consensus.\nThis can be seen in some sense in Bayesian model development workflow with hierarchical survival models. Instead of imagining agents reasoning if-then style through a sequence of additional sand grains. Let’s picture the reasoner working with a latent survival model, negotiating a contract between reality and their linguistic usage.\nHierarchical models in the Bayesian setting are typical and interesting in their own right as they allow for the expression of heterogeneity across individuals. Broadly they involve adding one or more parameters that modify the baseline model equation. We saw earlier that the Cox Proportional hazard model is expressed as \\[\\lambda_0 (t) \\cdot e^{\\beta_0 X_0 + \\beta_1 X_1 ... \\beta_n X_n}\\]\nThis can be modified as follows:\n\\[z_{i} \\cdot \\lambda_0 (t) \\cdot e^{\\beta_0 X_0 + \\beta_1 X_1 ... \\beta_n X_n}\\] Where we allow an individual “frailty” term \\(z_{i}\\) is added to the model as a multiplicative factor for each individual in the data set. The terms are drawn from a distribution, often centred on 1, so that the average individual modifies the baseline model not at all… but modifications are expressed as a reduction or increase to the multiplicative speed of state transition. The baseline model can therefore be considered\nRecall that the Bayesian modelling exercise quantifies the probability distribution of all the parameters in the model. A well specified baseline model will mean that less explanatory work needs to be done by the individual frailty terms. A poorly specified model will locate allot of weight in these terms. This is a mechanism which helps quantify the degree of irreducible uncertainty in our attribution patterns derived from our understanding of the paradigmatic cases (our sample).\n\n\nThe Bayesian Reasoner\nAn individual reasoner working with their set of paradigmatic data points y ~ f(X | \\(\\theta\\)) may fit their model to this data. The variance in the distribution of frailty terms \\(z_{i} \\in \\theta\\) estimated represents their view of the remaining uncertainty in cases after controlling for the impact of the covariate profiles across the cases.\nThese quantities represents disagreements regarding the speed up or slow down in the survival curves…but the survival curves quantifies the probability of transition at each point of accumulation. So a survival model allows us to say precisely at each point of accumulation what is the probability of transition. For any given point in series of accumulating instances, the diversity of individual frailty terms needed to account for the predications determine the quantifiable range of uncertainty in the survival probabilities we derive from the paradigmatic cases.\nEpistemicism about existence of a cut point for vague predicates will always assume the existence of threshold. The picture of Sorites Paradox for the Bayesian reasoner sees them go from uncertainty to uncertainty updating the latent model as they go. Maybe the model converges tightly in some cases, maybe not. Incorporating more paradigmatic instances, more covariate indicators as they develop conceptual clarity on what drives the attribution of state-hood under the evolving or growing pressure to change. Finding the threshold would not and could not be a solution to the paradox. Any threshold will be context specific and also learned (with uncertainty) relative to the tolerances of the domain. Understanding that paradox as yet another instance of learning in a multifaceted world at least lets us see the problem without requiring torturous modifications to classical logic."
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport bambi as bmb\nimport seaborn as sns\nfrom pygam.datasets import mcycle\nfrom pygam import LinearGAM, s, f, GAM, l, utils\nimport numpy as np\nimport arviz as az\nimport pymc as pm\nimport scipy as sp\nfrom patsy import bs as bs_patsy, dmatrix\nimport pytensor.tensor as pt\n\n\nrandom_seed = 100\n\nimport warnings\n\nwarnings.simplefilter('ignore')"
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#generalised-additive-models",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#generalised-additive-models",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Generalised Additive models",
    "text": "Generalised Additive models\nNonlinear functions can be approximated with linearly additive combinations of component features. Before delving into the practicalities we’ll quickly note some of the theoretical background. The canonical reference for these additive models is Simon Wood’s “Generalised Additive Models: An Introduction with R” which outlines in some detail the theoretical background of splines and univariate smoothers. The book stresses the trade-offs between the flexibility of splines and the need for cross-validation and penalised estimation methods for spline based modelling. Spline models ape the complexities of esoteric functions by regression-like formulas combining subordinate functions of observed variables to to predict another:\n\\[y \\sim f(x_1) + f(x_2) ...\\]\nThe structure of the individual \\(f\\) functions is approximated. So we are approximating \\(y\\) by adding a series of input approximations \\(\\sum f_{i}\\). In R these penalised models fits can be achieved in mgcv which incorporates a Wilkinson like formula syntax for model specification: y ~ s(x) + s(x1). The closest implementation in python is available in PyGam and we will adopt this package to illustrate the spline based smoothing.\n\nPyGAM and Penalised Fits\nLet’s first look at an example data set on which to demonstrate univariate smoothing patterns using penalised splines. We’ll initially look simply at the function calls before going “under the hood”.\nSpline models can be optimised by fitting differing strength penalities over a varying number of splines. Splines are used to construct non-linear functions of the input variable which are combined additively in our model equation. The penalities used in the optimisation routines constrain how “wiggly” these constructions will be. The number of splines determine how precise our will be.\n\nX, y = mcycle(return_X_y=True)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(X, y)\nax.set_ylabel(\"Acceleration\")\nax.set_xlabel(\"Time Step\")\nax.set_title(\"Crash Test Dummy Acceleration \\n Simulated Motorcycle Crash\", fontsize=20);\n\n\n\n\nNow we fit a number of different models to account for the herky-jerky nature of the data generating processs. We vary the parameterisations to see how the numbers of splines and strength of the penalty help account for the variation in \\(y\\) over the support of \\(X\\).\n\ngam1 = LinearGAM(s(0, n_splines=5)).fit(X, y)\ngam2 = LinearGAM(s(0, n_splines=7)).fit(X, y)\ngam3 = LinearGAM(s(0, n_splines=10)).fit(X, y)\ngam4 = LinearGAM(s(0, n_splines=15)).fit(X, y)\ngam5 = LinearGAM(s(0, lam=.1)).fit(X, y)\ngam6 = LinearGAM(s(0, lam=.5)).fit(X, y)\ngam7 = LinearGAM(s(0, lam=5)).fit(X, y)\ngam8 = LinearGAM(s(0, lam=15)).fit(X, y)\n\n\ndef plot_fit(gam, X, y, ax, t, c1='b', c2='r'):\n    XX = gam.generate_X_grid(term=0, n=500)\n\n    ax.plot(XX, gam.predict(XX), color=c2, linestyle='--')\n    ax.plot(XX, gam.prediction_intervals(XX, width=.95), color=c1, ls='--')\n\n    ax.scatter(X, y, facecolor='gray', edgecolors='none')\n    ax.set_title(f\"\"\"95% prediction interval with {t} \\n LL: {gam.statistics_['loglikelihood']}\"\"\");\n\nfig, axs = plt.subplots(4,2, figsize=(10, 20))\naxs = axs.flatten()\ntitles = ['5_splines', '7_splines', '10_splines', '15_splines',\n'lam=.1', 'lam=.5', 'lam=5', 'lam=15']\ngs = [gam1, gam2, gam3, gam4, gam5, gam6, gam7, gam8]\nfor ax, g, t in zip(axs, gs, titles):\n    plot_fit(g, X, y, ax, t)\n\n\n\n\nHere we’ve seen the PyGAM package applied to fitting our model to the data. In the formula specification we see y ~ s(i) where i denotes the index of the column variable in the X data.\nOver the range of of the x-axis we can see how the conditional expectation is more or less well fit to the data depending on how the penalities and complexity of the model is specified. The art and science of developing a GAM model is finding the constraints on the approximation that help capture the observed patterns but do not excessively overfit to the observed data."
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#optimising-the-parameter-setting",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#optimising-the-parameter-setting",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Optimising The Parameter Setting",
    "text": "Optimising The Parameter Setting\nThe manner of the the linear combination achieved by GAMs is constrained by the optimisation goal in their model fit. We can see from the model summary what is going on under the hood. For a given model specification the summary will report a number of model-fit statistics such as the log-likelihood and the AIC.\n\n## Naive Model manually specified splines\ngam_raw = LinearGAM(s(0, n_splines=10), fit_intercept=False).fit(X, y)\nprint(\"log_likelihood:\", gam_raw.statistics_['loglikelihood'])\nprint(\"AIC:\", gam_raw.statistics_['AIC'])\n\nlog_likelihood: -1042.2807399926558\nAIC: 2097.7407642114244\n\n\nThe question then becomes what changes are induced in the model as we seek to optimise these model fit statistics.\n\n## model optimised\ngam = LinearGAM(s(0),  fit_intercept=False)\ngam.gridsearch(X, y)\ngam.summary()\n\n  0% (0 of 11) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--\n\n\n  9% (1 of 11) |##                       | Elapsed Time: 0:00:00 ETA:   0:00:00\n\n\n100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n\n\n\n\n\nLinearGAM                                                                                                 \n=============================================== ==========================================================\nDistribution:                        NormalDist Effective DoF:                                     11.8135\nLink Function:                     IdentityLink Log Likelihood:                                  -952.2409\nNumber of Samples:                          133 AIC:                                             1930.1088\n                                                AICc:                                            1933.0789\n                                                GCV:                                              609.3811\n                                                Scale:                                            512.7965\n                                                Pseudo R-Squared:                                   0.7984\n==========================================================================================================\nFeature Function                  Lambda               Rank         EDoF         P > x        Sig. Code   \n================================= ==================== ============ ============ ============ ============\ns(0)                              [0.2512]             20           11.8         1.11e-16     ***         \n==========================================================================================================\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n         which can cause p-values to appear significant when they are not.\n\nWARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n         are typically lower than they should be, meaning that the tests reject the null too readily.\n\n\nFortunately, this routine can be performed directly and results in the following differences between the naive and optimised model.\n\nPlot the GAM fits\n\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_fit(gam_raw, X, y, ax, \"Unoptimised Fit\", c1='orange', c2='green')\nplot_fit(gam, X, y, ax, \"Optimised Fit\")\n\n\n\n\nThis is all well and good! We’ve seen an approach to modelling that can capture eccentric patterns in raw data. But how does it work and why should we care? If you’re familiar with the nomenclature of machine learning, you should think of spline modelling as a variety of feature creation. It generates “synthetic” features over the range of the observed variable. These synthetic features are the splines in question. Model building here too then relies on feature selection and principles for model-comparison.\n\n\nDigression on the usage of “Splines”\nThe history of the term “spline” is related to the history of draftmanship. Historically splines were thin strips of flexible wood or plastic that could be bent or shaped around a weight or “knot” points to express a traceable curve over the space of a “numberline”. The elastic nature of the spline material allowed it to be bent around the knot points of curvature expressing a smooth or continuous bend.\nThe mathematical technique apes these properties by defining a curve over in an analogous way. We specify “knots” to carve up the support of the random variable \\(X\\) into portions that require different weighting schemes to represent the outcome \\(y\\) in each partition of the support variable.\n\n\n\nExtracting the Splines\nReturning to our model we can extract the spline features used in the PyGAM by invoking the following commands. We first identify the knot points and create the b-spline basis appropriate for the variable \\(X\\).\n\nknot_edges=utils.gen_edge_knots(X,dtype='numerical')\nknots=np.linspace(knot_edges[0],knot_edges[-1],len(gam.coef_))\n\nsplines = utils.b_spline_basis(X, edge_knots=knot_edges, sparse=False)\n\nsplines_df = pd.DataFrame(splines, columns=[f'basis_{i}' for i in range(len(gam.coef_))])\n\nsplines_df.head(10)\n\n\n\n\n\n  \n    \n      \n      basis_0\n      basis_1\n      basis_2\n      basis_3\n      basis_4\n      basis_5\n      basis_6\n      basis_7\n      basis_8\n      basis_9\n      basis_10\n      basis_11\n      basis_12\n      basis_13\n      basis_14\n      basis_15\n      basis_16\n      basis_17\n      basis_18\n      basis_19\n    \n  \n  \n    \n      0\n      0.166667\n      0.666667\n      0.166667\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      1\n      0.132997\n      0.661606\n      0.205334\n      0.000063\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      2\n      0.059688\n      0.594827\n      0.341426\n      0.004059\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      3\n      0.030095\n      0.518726\n      0.437481\n      0.013698\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      4\n      0.012374\n      0.428013\n      0.527144\n      0.032470\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      5\n      0.000000\n      0.040337\n      0.551431\n      0.399315\n      0.008917\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      6\n      0.000000\n      0.018232\n      0.465467\n      0.492630\n      0.023671\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      7\n      0.000000\n      0.011137\n      0.418489\n      0.535407\n      0.034967\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      8\n      0.000000\n      0.000014\n      0.189310\n      0.664817\n      0.145859\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      9\n      0.000000\n      0.000000\n      0.120914\n      0.656897\n      0.222015\n      0.000174\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n\n\n\nThese spline features range the extent of the of covariate space \\(X\\) defining “partitions” of the space. The model “learns” to capture the shape of the outcome variable \\(y\\) by figuring out how to weight the different portion of this spline basis matrix i.e. the linear combination of this basis matrix with the derived coefficients is a model of our outcome variable.\nNote how each row is 0 everywhere except within the columns that represent a partition of \\(X\\). Additionally each row sums to unity. These properties are important because they ensure that any weighted combination of this basis represents the outcome variable in a controlled and quite granular manner. The more splines we use the more control we have of the representation, but we also risk overfit.\n\n\nPlotting the Weighted Spline\n\nax = splines_df.dot(gam.coef_).plot(title='Weighted Splines', label='Weighted Combination of Spline Basis', figsize=(10, 6))\nax.set_ylabel(\"Acceleration\")\nax.set_xlabel(\"Time Steps\")\nax.legend();\n\n\n\n\nIn this manner we can see how the specification of a spline basis can help us model eccentric curves and waves in an outcome space. Spline features are like ad-hoc joists structuring our ship’s rigging. Features filling the gaps in our theory, they act like duct-tape binding the pieces together. If we hold this part down there, and jiggle another part here, our theory stays afloat. Spline features exemplify the contorted nature of empirically informed theory construction.\nNext we’ll see how to more directly work with the specification of basis splines, passing these feature matrices into Bambi models."
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#bayesian-splines-with-bambi",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#bayesian-splines-with-bambi",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Bayesian Splines with bambi",
    "text": "Bayesian Splines with bambi\nSpline models can be built and assessed using Bayesian approaches to quantify the uncertainty in the constructed function. Under the hood Bambi makes use of the ‘formulae’ package to allow for formula-like syntax to specify spline basis terms and mixed effect model terms. We leverage this to streamline the specification of spline models and demonstrate different kinds of spline-features.\n\nknots_6 = np.linspace(0, np.max(X), 6+2)[1:-1]\nknots_10 = np.linspace(0, np.max(X), 10+2)[1:-1]\nknots_20 = np.linspace(0, np.max(X), 20+2)[1:-1]\n\ndf = pd.DataFrame({'X': X.flatten(), 'y': y})\nformula1 = 'bs(X, degree=0, knots=knots_6)'\nformula2 = 'bs(X, degree=1, knots=knots_6, intercept=False)'\nformula3 = 'bs(X, degree=3, knots=knots_6, intercept=False)'\nformula4 = 'bs(X, degree=3, knots=knots_10, intercept=False)'\nformula5 = 'bs(X, degree=3, knots=knots_20, intercept=False)'\nmodel_spline1 = bmb.Model(f\"y ~ {formula1}\", df)\nmodel_spline2 = bmb.Model(f\"y ~ {formula2}\", df)\nmodel_spline3 = bmb.Model(f\"y ~ {formula3}\", df)\nmodel_spline4 = bmb.Model(f\"y ~ {formula4}\", df)\nmodel_spline5 = bmb.Model(f\"y ~ {formula5}\", df)\nmodel_spline5\n\n       Formula: y ~ bs(X, degree=3, knots=knots_20, intercept=False)\n        Family: gaussian\n          Link: mu = identity\n  Observations: 133\n        Priors: \n    target = mu\n        Common-level effects\n            Intercept ~ Normal(mu: -25.5459, sigma: 222.6623)\n            bs(X, degree=3, knots=knots_20, intercept=False) ~ Normal(mu: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n                0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], sigma: [1407.0172 1448.7263 1394.069   877.5692\n                912.3295  679.8904  507.1974\n              696.2455  860.7155  666.7815  665.863   932.178   883.0419  740.3925\n             1010.3524  914.7406  923.9545 1218.535  1355.2211 1420.6241 1363.5314\n             2403.9764 1393.2356])\n        \n        Auxiliary parameters\n            sigma ~ HalfStudentT(nu: 4.0, sigma: 48.14)\n\n\nAs you can see here we are specifying a range of different degrees of spline basis. The different degrees corresspond to the smoothness of the overlapping splines. The degree=0 splines mean we specify a piecewise constant basis i.e. 0 or 1 within each region of the partition. But we can add more degrees to see more flexible representations of the space.\n\nmodel_spline5.build()\nmodel_spline5.graph()\n\n\n\n\nThe differences here will become clearer as we plot the various spline basis matrices below, but the main thought is that there are different degrees of “smoothness” to the linear basis of the spline features.\n\nPlot the Spline Basis\nThe below functions extract the basis specification from each model and plots the basis design for an increasingly complex series of spline basis matrices.\n\ndef plot_spline_basis(basis, X, ax, title=\"Spline Basis\"):\n    df = (\n        pd.DataFrame(basis)\n        .assign(X=X)\n        .melt(\"X\", var_name=\"basis_idx\", value_name=\"y\")\n    )\n\n\n    for idx in df.basis_idx.unique():\n        d = df[df.basis_idx == idx]\n        ax.plot(d[\"X\"], d[\"y\"])\n    \n    ax.set_title(title)\n    return ax\n\ndef plot_knots(knots, ax):\n    for knot in knots:\n        ax.axvline(knot, color=\"0.1\", alpha=0.4)\n    return ax\n\n\n\nfig, axs = plt.subplots(5, 1, figsize=(9, 20))\naxs = axs.flatten()\naxs.flatten()\nB1 = model_spline1.response_component.design.common[formula1]\nplot_spline_basis(B1, df[\"X\"].values, ax=axs[0], title=\"Piecewise Constant Basis\")\nplot_knots(knots_6, axs[0]);\n\nB2 = model_spline2.response_component.design.common[formula2]\nax = plot_spline_basis(B2, df[\"X\"].values, axs[1], \ntitle=\"Piecewise Linear Basis\")\nplot_knots(knots_6, axs[1]);\n\nB3 = model_spline3.response_component.design.common[formula3]\nax = plot_spline_basis(B3, df[\"X\"].values, axs[2], \ntitle=\"Cubic Spline Basis (6 Knots)\")\nplot_knots(knots_6, axs[2]);\n\nB4 = model_spline4.response_component.design.common[formula4]\nax = plot_spline_basis(B4, df[\"X\"].values, axs[3], \ntitle=\"Cubic Spline Basis (10 Knots)\")\nplot_knots(knots_10, axs[3]);\n\n\nB5 = model_spline5.response_component.design.common[formula5]\nax = plot_spline_basis(B5, df[\"X\"].values, axs[4], \ntitle=\"Cubic Spline Basis (20 Knots)\")\nplot_knots(knots_20, axs[4]);\n\n\n\n\nHere we’ve seen the nature of the modelling splines we’ll fit to the data. By “fit” we mean estimate a set of linear coefficients that we can use to additively combine these spline features creating a smooth linear representation of the outcome.\n\n\nFit the Individual Spline Models\nWe now combine the spline components within a linear model fit, pulling and tying them together in such a way as to ape the shape of the observed sequence.\n\nidata_spline1 = model_spline1.fit(random_seed=random_seed, idata_kwargs={\"log_likelihood\": True})\n\nidata_spline2 = model_spline2.fit(random_seed=random_seed, idata_kwargs={\"log_likelihood\": True})\n\nidata_spline3 = model_spline3.fit(random_seed=random_seed, idata_kwargs={\"log_likelihood\": True})\n\nidata_spline4 = model_spline4.fit(random_seed=random_seed, idata_kwargs={\"log_likelihood\": True})\n\nidata_spline5 = model_spline5.fit(random_seed=random_seed, idata_kwargs={\"log_likelihood\": True})\n\nFinally we can plot fits achieved by each of the models and compare how the different spline smooths contribute to approxiating the outcome variable.\n\n\nPlot the Weighted Mean\n\ndef plot_weighted_splines(B, idata, formula, ax, knots):\n    posterior_stacked = az.extract(idata)\n    wp = posterior_stacked[formula].mean(\"sample\").values\n\n    plot_spline_basis(B * wp.T, df[\"X\"].values, ax)\n    ax.plot(df.X.values, np.dot(B, wp.T), color=\"black\", lw=3, label='Weighted Splines')\n    plot_knots(knots, ax);\n    ax.legend()\n\n\n\nfig, axs = plt.subplots(5, 1, figsize=(10, 20))\naxs = axs.flatten()\naxs.flatten()\n\nplot_weighted_splines(B1, idata_spline1, formula1, axs[0], knots_6)\nplot_weighted_splines(B2, idata_spline2, formula2, axs[1], knots_6)\nplot_weighted_splines(B3, idata_spline3, formula3, axs[2], knots_6)\nplot_weighted_splines(B4, idata_spline4, formula4, axs[3], knots_10)\nplot_weighted_splines(B5, idata_spline5, formula5, axs[4], knots_20)\n\n\n\n\nHere we can see how the models with increasingly complex splines are more exactly able to fit the herky jerky trajectory of the outcome variable in each interval. The fewer the intervals, the less flexibility available to the model.\n\n\nCompare Model Fits\nAs before we can evaluate these model fits and compare them based on leave-one-out cross validation scores and information theoretic complexity measures.\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 7))\naxs = axs.flatten()\nmodels_dict = {\"piecewise_constant\": idata_spline1, \"piecewise_linear\": idata_spline2, \"cubic_bspline\": idata_spline3, \"cubic_bspline_10\": idata_spline4, \n\"cubic_bspline_20\": idata_spline5}\ndf_compare = az.compare(models_dict)\naz.plot_compare(df_compare, ax=axs[0])\naxs[0].get_legend().remove()\naz.plot_compare(az.compare(models_dict, 'waic'), ax=axs[1])\naxs[1].set_yticklabels([])\ndf_compare\n\n\n\n\n\n  \n    \n      \n      rank\n      elpd_loo\n      p_loo\n      elpd_diff\n      weight\n      se\n      dse\n      warning\n      scale\n    \n  \n  \n    \n      cubic_bspline_10\n      0\n      -612.572745\n      11.646898\n      0.000000\n      9.011087e-01\n      9.646678\n      0.000000\n      True\n      log\n    \n    \n      cubic_bspline_20\n      1\n      -620.709337\n      19.669201\n      8.136592\n      1.360171e-13\n      9.600116\n      2.074742\n      True\n      log\n    \n    \n      cubic_bspline\n      2\n      -634.647180\n      8.703519\n      22.074435\n      6.196578e-14\n      8.915728\n      6.642850\n      True\n      log\n    \n    \n      piecewise_constant\n      3\n      -643.781042\n      6.981098\n      31.208296\n      5.299668e-02\n      9.770740\n      8.728072\n      False\n      log\n    \n    \n      piecewise_linear\n      4\n      -647.016885\n      5.987587\n      34.444140\n      4.589467e-02\n      7.914787\n      8.116163\n      False\n      log\n    \n  \n\n\n\n\n\n\n\nHere we see that the extra complexity of using 20 splines leads to slightly worse performance measures than the less complex but seemingly adequate 10 splines. In other words, it’s starting to overfit to the data at 20. This is the price of flexibility and a sign of a model unlikely to generalise well on out-of-sample data.\n\nnew_data = pd.DataFrame({\"X\": np.linspace(df.X.min() - 5, df.X.max() + 5, num=500)})\n    \nmodel_spline4.predict(idata_spline4, data=new_data, \nkind='pps', inplace=True)\n\nidata_spline4\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                                               (chain: 4,\n                                                           draw: 1000,\n                                                           bs(X, degree=3, knots=knots_10, intercept=False)_dim: 13,\n                                                           y_obs: 500)\nCoordinates:\n  * chain                                                 (chain) int64 0 1 2 3\n  * draw                                                  (draw) int64 0 ... 999\n  * bs(X, degree=3, knots=knots_10, intercept=False)_dim  (bs(X, degree=3, knots=knots_10, intercept=False)_dim) int64 ...\n  * y_obs                                                 (y_obs) int64 0 ......\nData variables:\n    Intercept                                             (chain, draw) float64 ...\n    bs(X, degree=3, knots=knots_10, intercept=False)      (chain, draw, bs(X, degree=3, knots=knots_10, intercept=False)_dim) float64 ...\n    y_sigma                                               (chain, draw) float64 ...\n    y_mean                                                (chain, draw, y_obs) float64 ...\nAttributes:\n    created_at:                  2024-05-27T06:00:45.172558\n    arviz_version:               0.17.0\n    inference_library:           pymc\n    inference_library_version:   5.10.3\n    sampling_time:               2.553478956222534\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000bs(X, degree=3, knots=knots_10, intercept=False)_dim: 13y_obs: 500Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])bs(X, degree=3, knots=knots_10, intercept=False)_dim(bs(X, degree=3, knots=knots_10, intercept=False)_dim)int640 1 2 3 4 5 6 7 8 9 10 11 12array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])y_obs(y_obs)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (4)Intercept(chain, draw)float64-30.48 -9.768 17.5 ... -4.816 -3.18array([[-30.47836564,  -9.76829627,  17.49712677, ...,  17.81763441,\n         13.95495119,  -6.10602699],\n       [-16.23316086,   9.49899062,   3.70481638, ..., -25.03974245,\n         -2.91235316, -18.76784286],\n       [ -0.38968653, -23.55404923,  21.85558793, ..., -15.88911032,\n        -12.17693056, -22.799846  ],\n       [-23.78536816, -10.33769596, -18.72531118, ...,  10.30614657,\n         -4.81605641,  -3.18037091]])bs(X, degree=3, knots=knots_10, intercept=False)(chain, draw, bs(X, degree=3, knots=knots_10, intercept=False)_dim)float6460.08 -26.33 72.03 ... 26.62 -19.5array([[[ 60.08361849, -26.32632766,  72.03440963, ..., -24.69505575,\n          41.29966173, 126.0957609 ],\n        [ 42.01923744,  13.97976273, -10.66570712, ...,  37.69872884,\n          32.66180174,  56.71948798],\n        [-20.79003642, -60.86218268,   7.89670524, ..., -79.792871  ,\n          26.13817326,   5.2715798 ],\n        ...,\n        [-53.05009482, -44.90737225,  -0.53936709, ...,  11.86630968,\n         -30.56679357, -47.68641912],\n        [-31.19449725, -22.73967732,  20.06162131, ..., -28.10157326,\n           4.07136337, -21.05300466],\n        [ 35.06057706,  -9.59093606,  11.09479783, ...,  -3.88987727,\n          -8.987879  ,  27.57356096]],\n\n       [[ 33.08556413,  -1.11856386,  24.73543073, ...,  73.28686802,\n         -24.37794241,  33.15302761],\n        [-28.57730471,  -3.73441648,  -2.57506729, ..., -18.45867357,\n          -4.60293539,  -5.79920437],\n        [ -4.42415991, -12.80936972,  20.31341056, ...,   1.64870928,\n          19.07410069,  26.02601531],\n...\n        [  9.81825793,  16.07126062,  40.10272142, ..., -21.65327653,\n          34.82747494,  28.59603009],\n        [ 50.49534197, -16.63350377,  36.81576572, ...,  36.77027802,\n           8.30573837,  15.51060296],\n        [ 33.01164624,  -1.33302557,  38.41471255, ...,   3.94749231,\n          46.54871666,  43.92361804]],\n\n       [[ 18.45099645, -28.24611558,  49.26005839, ...,   4.79065054,\n          50.07280024, -17.17531704],\n        [ 24.27302084,  12.70125656,  13.48983162, ...,  23.33510231,\n         -38.49057179, 101.69703639],\n        [ 22.36020074, -13.16116681,  32.08258294, ...,  56.15103319,\n           4.60000597,  72.46523374],\n        ...,\n        [-71.35771382,  25.42919025, -19.3976876 , ..., -30.2130114 ,\n           5.95767059, -25.33043675],\n        [ 10.9586426 , -46.45830404,  45.31732601, ...,  47.8378147 ,\n         -28.12282789,  41.36265809],\n        [-31.53039023,  20.83808587, -18.46035112, ...,  -4.39658487,\n          26.61753145, -19.50485034]]])y_sigma(chain, draw)float6422.77 23.59 21.69 ... 20.98 25.07array([[22.77247029, 23.5905779 , 21.68882473, ..., 21.91856404,\n        23.1234507 , 20.69603421],\n       [23.30953138, 21.78668016, 23.3377476 , ..., 23.554608  ,\n        22.30139546, 23.25599855],\n       [25.0826106 , 24.14700624, 23.17634169, ..., 22.55796555,\n        22.00396287, 23.83528365],\n       [22.97336709, 22.34507905, 24.09241758, ..., 23.84889601,\n        20.97955574, 25.07045989]])y_mean(chain, draw, y_obs)float64-1.786e+03 -1.688e+03 ... -384.5array([[[-1785.63598357, -1688.46091242, -1594.80861303, ...,\n           471.11786457,   486.11321041,   501.38658015],\n        [-1008.56095247,  -955.43500036,  -904.13867636, ...,\n           209.04228219,   217.17041391,   225.5454136 ],\n        [  171.46343997,   166.77923669,   162.10649158, ...,\n          -298.9913081 ,  -317.81225915,  -337.30145789],\n        ...,\n        [ 1113.19763849,  1056.55963212,  1001.80503673, ...,\n           -46.36650365,   -45.39216019,   -44.30916241],\n        [  672.75237366,   638.6237798 ,   605.62977443, ...,\n          -210.41648402,  -220.83665251,  -231.5695011 ],\n        [ -981.31984966,  -927.88664463,  -876.36327872, ...,\n           245.7530471 ,   256.32233432,   267.17337954]],\n\n       [[ -892.49690627,  -844.79097031,  -798.77984615, ...,\n           543.89918243,   572.12293189,   601.25719405],\n        [  729.10432472,   690.36248691,   652.97626248, ...,\n           -29.69218582,   -31.76353623,   -33.91297753],\n        [   29.72027438,    29.25039646,    28.75480424, ...,\n            45.18641388,    45.54217006,    45.89560459],\n...\n        [ -186.24078895,  -177.60095429,  -169.24721219, ...,\n          -129.71594267,  -138.38733622,  -147.37882584],\n        [-1442.9878318 , -1364.25391604, -1288.35121975, ...,\n           100.15273049,   105.7273578 ,   111.49734282],\n        [ -904.10355066,  -855.98742694,  -809.58838617, ...,\n           -72.7318446 ,   -78.59078957,   -84.67012043]],\n\n       [[ -700.6554728 ,  -661.92143273,  -624.64606141, ...,\n          -521.29646421,  -545.21735853,  -569.83464216],\n        [ -566.28543846,  -536.82610575,  -508.37957578, ...,\n          1032.55566447,  1078.47471162,  1125.69033827],\n        [ -695.22027979,  -657.54938003,  -621.25358394, ...,\n           556.45783286,   581.91710165,   608.14063371],\n        ...,\n        [ 2030.9713329 ,  1919.9629329 ,  1812.93379418, ...,\n          -265.67005783,  -278.5688838 ,  -291.86028771],\n        [ -600.01947051,  -564.67068253,  -530.70395972, ...,\n           590.9906349 ,   619.60614144,   649.1001913 ],\n        [  954.09836044,   900.90776771,   849.64997795, ...,\n          -351.31548764,  -367.65439889,  -384.46730166]]])Indexes: (4)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))bs(X, degree=3, knots=knots_10, intercept=False)_dimPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], dtype='int64', name='bs(X, degree=3, knots=knots_10, intercept=False)_dim'))y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='y_obs', length=500))Attributes: (8)created_at :2024-05-27T06:00:45.172558arviz_version :0.17.0inference_library :pymcinference_library_version :5.10.3sampling_time :2.553478956222534tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 4, draw: 1000, y_obs: 500)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * y_obs    (y_obs) int64 0 1 2 3 4 5 6 7 8 ... 492 493 494 495 496 497 498 499\nData variables:\n    y        (chain, draw, y_obs) float64 -1.769e+03 -1.707e+03 ... -438.7\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000y_obs: 500Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])y_obs(y_obs)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (1)y(chain, draw, y_obs)float64-1.769e+03 -1.707e+03 ... -438.7array([[[-1.76850707e+03, -1.70683103e+03, -1.58503478e+03, ...,\n          4.75269803e+02,  4.71717129e+02,  4.93095980e+02],\n        [-9.78963706e+02, -9.57350338e+02, -8.94242702e+02, ...,\n          2.23703058e+02,  1.99989491e+02,  2.36217010e+02],\n        [ 1.51236792e+02,  1.44573452e+02,  1.59874803e+02, ...,\n         -2.92521983e+02, -3.39277905e+02, -3.39765298e+02],\n        ...,\n        [ 1.12245700e+03,  1.02901445e+03,  1.00163629e+03, ...,\n         -5.76818185e+01, -5.79655659e+01, -6.95232453e+01],\n        [ 7.03262674e+02,  6.89295681e+02,  5.72987411e+02, ...,\n         -2.19231936e+02, -1.91846651e+02, -1.77780985e+02],\n        [-1.00564975e+03, -9.26117757e+02, -8.32114624e+02, ...,\n          2.43447636e+02,  2.46073703e+02,  2.61526238e+02]],\n\n       [[-8.78027516e+02, -8.39940972e+02, -7.70858431e+02, ...,\n          5.62919028e+02,  5.81119362e+02,  5.98368471e+02],\n        [ 7.59822216e+02,  7.35672407e+02,  6.66036776e+02, ...,\n         -3.86148939e+01, -2.62397986e+01, -7.83702403e+01],\n        [ 6.01153527e+01, -8.09278342e-01, -4.14103639e+01, ...,\n          1.77331558e+01,  3.35647548e+01,  8.67001318e+01],\n...\n        [-2.03929257e+02, -1.59688749e+02, -1.67829907e+02, ...,\n         -1.26352656e+02, -1.58838619e+02, -1.33905356e+02],\n        [-1.42981773e+03, -1.35031319e+03, -1.30844300e+03, ...,\n          1.11580849e+02,  8.39822162e+01,  1.55903514e+02],\n        [-9.65920008e+02, -8.62270119e+02, -7.92953633e+02, ...,\n         -5.96365921e+01, -5.50070341e+01, -9.95204835e+01]],\n\n       [[-7.28155125e+02, -6.77887003e+02, -5.89951035e+02, ...,\n         -5.12375162e+02, -5.30806452e+02, -6.00515410e+02],\n        [-5.86770932e+02, -5.53033193e+02, -4.89166246e+02, ...,\n          9.92634196e+02,  1.04493361e+03,  1.12704971e+03],\n        [-7.01773152e+02, -6.43685059e+02, -5.89255386e+02, ...,\n          5.27179378e+02,  5.58208456e+02,  5.88341537e+02],\n        ...,\n        [ 2.02804805e+03,  1.90391514e+03,  1.80455354e+03, ...,\n         -2.68343325e+02, -2.78799372e+02, -2.96707748e+02],\n        [-5.94745406e+02, -5.62236707e+02, -5.62459190e+02, ...,\n          6.09916668e+02,  6.51977577e+02,  6.51319039e+02],\n        [ 9.61991994e+02,  9.05536001e+02,  8.37728249e+02, ...,\n         -3.91949049e+02, -3.49475144e+02, -4.38689017e+02]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='y_obs', length=500))Attributes: (2)modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 4, draw: 1000, y_obs: 133)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * y_obs    (y_obs) int64 0 1 2 3 4 5 6 7 8 ... 125 126 127 128 129 130 131 132\nData variables:\n    y        (chain, draw, y_obs) float64 -4.94 -4.349 -4.059 ... -4.232 -5.027\nAttributes:\n    created_at:                  2024-05-27T06:00:45.279798\n    arviz_version:               0.17.0\n    inference_library:           pymc\n    inference_library_version:   5.10.3\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000y_obs: 133Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])y_obs(y_obs)int640 1 2 3 4 5 ... 128 129 130 131 132array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132])Data variables: (1)y(chain, draw, y_obs)float64-4.94 -4.349 ... -4.232 -5.027array([[[ -4.94013004,  -4.34926962,  -4.05923397, ...,  -4.07989994,\n          -4.32526297, -10.99702367],\n        [ -4.16551546,  -4.0798865 ,  -4.34890397, ...,  -4.27847476,\n          -4.90199179,  -5.2604801 ],\n        [ -4.32114608,  -4.21922923,  -4.01523436, ...,  -4.00171403,\n          -4.46666592,  -4.15055295],\n        ...,\n        [ -4.33667694,  -4.08524041,  -4.16193486, ...,  -4.25075843,\n          -4.0367804 ,  -5.71916378],\n        [ -4.2418906 ,  -4.13815308,  -4.06475279, ...,  -4.08566333,\n          -4.13276703,  -4.35600265],\n        [ -3.99240307,  -3.95297982,  -4.22738498, ...,  -4.39489511,\n          -3.96850328,  -4.08422128]],\n\n       [[ -4.31029941,  -4.1356895 ,  -4.08943162, ...,  -4.39621813,\n          -4.11988518,  -4.10340212],\n        [ -4.09528528,  -4.02978374,  -4.0169432 , ...,  -4.09620393,\n          -4.02276603,  -4.05185642],\n        [ -4.08161109,  -4.08438013,  -4.07760521, ...,  -4.11690648,\n          -4.53977755,  -4.40149229],\n...\n        [ -4.28309395,  -4.19158751,  -4.07661987, ...,  -4.08978805,\n          -4.11622924,  -4.03898436],\n        [ -4.16328531,  -4.01182659,  -4.32670151, ...,  -4.05121559,\n          -4.0454269 ,  -4.0661975 ],\n        [ -4.54760772,  -4.2926589 ,  -4.09280683, ...,  -4.09149036,\n          -4.35227017,  -4.18573222]],\n\n       [[ -4.58924409,  -4.39837658,  -4.20729252, ...,  -4.07955443,\n          -4.09450053,  -6.58164643],\n        [ -4.13256189,  -4.0441922 ,  -4.07194408, ...,  -4.64029899,\n          -4.06235348, -10.54056155],\n        [ -4.40287769,  -4.25092797,  -4.11800136, ...,  -4.11077154,\n          -4.21757925,  -5.69653922],\n        ...,\n        [ -4.18405048,  -4.09405162,  -4.6372543 , ...,  -4.15126206,\n          -4.11639733,  -4.67240483],\n        [ -3.98883574,  -3.9655664 ,  -3.96310644, ...,  -4.21756776,\n          -3.96543135,  -4.72138657],\n        [ -4.14867518,  -4.1893545 ,  -4.35918185, ...,  -4.14286099,\n          -4.23197849,  -5.02728226]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       123, 124, 125, 126, 127, 128, 129, 130, 131, 132],\n      dtype='int64', name='y_obs', length=133))Attributes: (6)created_at :2024-05-27T06:00:45.279798arviz_version :0.17.0inference_library :pymcinference_library_version :5.10.3modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                (chain: 4, draw: 1000)\nCoordinates:\n  * chain                  (chain) int64 0 1 2 3\n  * draw                   (draw) int64 0 1 2 3 4 5 ... 994 995 996 997 998 999\nData variables: (12/17)\n    perf_counter_diff      (chain, draw) float64 0.0007404 ... 0.0007515\n    lp                     (chain, draw) float64 -721.3 -715.6 ... -710.3 -711.0\n    step_size_bar          (chain, draw) float64 0.3142 0.3142 ... 0.3369 0.3369\n    smallest_eigval        (chain, draw) float64 nan nan nan nan ... nan nan nan\n    acceptance_rate        (chain, draw) float64 0.9519 0.8589 ... 0.7224 0.9861\n    reached_max_treedepth  (chain, draw) bool False False False ... False False\n    ...                     ...\n    largest_eigval         (chain, draw) float64 nan nan nan nan ... nan nan nan\n    energy                 (chain, draw) float64 728.6 729.2 ... 725.7 714.8\n    tree_depth             (chain, draw) int64 4 4 4 4 3 4 4 5 ... 4 5 4 3 4 4 4\n    diverging              (chain, draw) bool False False False ... False False\n    index_in_trajectory    (chain, draw) int64 -7 -13 -14 -3 6 ... -3 4 -4 7 8\n    process_time_diff      (chain, draw) float64 0.000741 0.000737 ... 0.000751\nAttributes:\n    created_at:                  2024-05-27T06:00:45.180249\n    arviz_version:               0.17.0\n    inference_library:           pymc\n    inference_library_version:   5.10.3\n    sampling_time:               2.553478956222534\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (17)perf_counter_diff(chain, draw)float640.0007404 0.0007361 ... 0.0007515array([[0.00074042, 0.00073613, 0.00073075, ..., 0.00072508, 0.00071846,\n        0.00071829],\n       [0.00072588, 0.00143679, 0.00037763, ..., 0.00073321, 0.00073304,\n        0.00073225],\n       [0.00073129, 0.00073483, 0.00108604, ..., 0.00142812, 0.00037529,\n        0.00073333],\n       [0.00144608, 0.00037754, 0.0003765 , ..., 0.00075521, 0.00076121,\n        0.0007515 ]])lp(chain, draw)float64-721.3 -715.6 ... -710.3 -711.0array([[-721.32325798, -715.60693507, -712.81600649, ..., -711.63567075,\n        -711.86251963, -711.25215936],\n       [-711.83173082, -714.18292872, -708.88038704, ..., -710.81095225,\n        -707.61049434, -710.43240125],\n       [-712.7395976 , -713.40853693, -713.62383765, ..., -712.97340089,\n        -709.41442383, -710.14460595],\n       [-714.37055597, -716.70641237, -710.51320632, ..., -713.81412225,\n        -710.25741961, -710.97040762]])step_size_bar(chain, draw)float640.3142 0.3142 ... 0.3369 0.3369array([[0.31415288, 0.31415288, 0.31415288, ..., 0.31415288, 0.31415288,\n        0.31415288],\n       [0.33735905, 0.33735905, 0.33735905, ..., 0.33735905, 0.33735905,\n        0.33735905],\n       [0.35905043, 0.35905043, 0.35905043, ..., 0.35905043, 0.35905043,\n        0.35905043],\n       [0.33686986, 0.33686986, 0.33686986, ..., 0.33686986, 0.33686986,\n        0.33686986]])smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])acceptance_rate(chain, draw)float640.9519 0.8589 ... 0.7224 0.9861array([[0.95188821, 0.85891772, 0.71451549, ..., 0.8702077 , 0.50215875,\n        0.97026438],\n       [0.71509145, 0.51444105, 0.90341721, ..., 0.79993904, 0.57204968,\n        0.66470717],\n       [0.53066916, 0.66611802, 0.72800025, ..., 0.61955158, 0.92591962,\n        0.77854416],\n       [0.99982809, 0.97668472, 0.87164137, ..., 0.74461051, 0.72243678,\n        0.98614764]])reached_max_treedepth(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])step_size(chain, draw)float640.2833 0.2833 ... 0.3204 0.3204array([[0.28329617, 0.28329617, 0.28329617, ..., 0.28329617, 0.28329617,\n        0.28329617],\n       [0.23729018, 0.23729018, 0.23729018, ..., 0.23729018, 0.23729018,\n        0.23729018],\n       [0.2420878 , 0.2420878 , 0.2420878 , ..., 0.2420878 , 0.2420878 ,\n        0.2420878 ],\n       [0.32038673, 0.32038673, 0.32038673, ..., 0.32038673, 0.32038673,\n        0.32038673]])perf_counter_start(chain, draw)float642.361e+06 2.361e+06 ... 2.361e+06array([[2360978.51092729, 2360978.51171625, 2360978.51249971, ...,\n        2360979.30091483, 2360979.30168592, 2360979.30245238],\n       [2360978.47167667, 2360978.47245279, 2360978.47393825, ...,\n        2360979.25252446, 2360979.2533045 , 2360979.2540865 ],\n       [2360978.51288592, 2360978.51366746, 2360978.51445129, ...,\n        2360979.26986125, 2360979.27133717, 2360979.27175967],\n       [2360978.46373525, 2360978.46523   , 2360978.46565383, ...,\n        2360979.27246896, 2360979.27327358, 2360979.27464221]])n_steps(chain, draw)float6415.0 15.0 15.0 ... 15.0 15.0 15.0array([[15., 15., 15., ..., 15., 15., 15.],\n       [15., 31.,  7., ..., 15., 15., 15.],\n       [15., 15., 23., ..., 31.,  7., 15.],\n       [31.,  7.,  7., ..., 15., 15., 15.]])max_energy_error(chain, draw)float64-1.01 0.7776 ... 0.8487 -0.3605array([[-1.00971736,  0.77757398,  1.28280502, ...,  0.37495702,\n         1.17970654, -0.34836573],\n       [ 1.3345751 ,  2.3915387 , -1.06730429, ...,  1.15497563,\n         1.75653514,  0.89915678],\n       [ 1.40682187,  1.52508573,  1.19349047, ...,  1.84259665,\n        -1.22585309,  0.93739617],\n       [-1.54429096, -0.75846954,  0.55295383, ...,  0.76575828,\n         0.84873093, -0.36049323]])energy_error(chain, draw)float64-0.2612 -0.1061 ... -0.1596 -0.1877array([[-0.26121615, -0.10611572, -0.69771087, ...,  0.06849994,\n         0.4074744 , -0.15667011],\n       [-0.29476717,  0.28462281, -1.06730429, ..., -0.313449  ,\n        -0.58684002,  0.26092216],\n       [ 0.37035487,  0.68575505,  0.05343487, ...,  0.74127564,\n        -1.20632225,  0.29989649],\n       [-0.85058098, -0.46592943, -0.24132289, ..., -0.12983655,\n        -0.15964365, -0.18767355]])largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])energy(chain, draw)float64728.6 729.2 721.0 ... 725.7 714.8array([[728.6255778 , 729.24609198, 720.95261511, ..., 718.23864589,\n        723.51162522, 718.37013269],\n       [725.96636541, 725.37893664, 719.42266114, ..., 721.44682066,\n        717.94143164, 714.93926686],\n       [724.0278209 , 722.19477874, 721.36794365, ..., 720.83274011,\n        716.33821598, 715.0968274 ],\n       [723.0827726 , 724.98007703, 723.47496067, ..., 724.58398944,\n        725.68406617, 714.81290627]])tree_depth(chain, draw)int644 4 4 4 3 4 4 5 ... 5 4 5 4 3 4 4 4array([[4, 4, 4, ..., 4, 4, 4],\n       [4, 5, 3, ..., 4, 4, 4],\n       [4, 4, 5, ..., 5, 3, 4],\n       [5, 3, 3, ..., 4, 4, 4]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])index_in_trajectory(chain, draw)int64-7 -13 -14 -3 6 -8 ... -3 4 -4 7 8array([[ -7, -13, -14, ...,   5,   3,   5],\n       [  6,  -4,  -4, ...,  -8,  14, -13],\n       [ -7, -13, -14, ...,  10,  -5,   8],\n       [ -8,  -5,   4, ...,  -4,   7,   8]])process_time_diff(chain, draw)float640.000741 0.000737 ... 0.000751array([[0.000741, 0.000737, 0.000731, ..., 0.000724, 0.000719, 0.000718],\n       [0.000725, 0.001437, 0.000377, ..., 0.000734, 0.000732, 0.000732],\n       [0.000731, 0.000735, 0.001086, ..., 0.001429, 0.000375, 0.000734],\n       [0.001446, 0.000378, 0.000377, ..., 0.000756, 0.000762, 0.000751]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (8)created_at :2024-05-27T06:00:45.180249arviz_version :0.17.0inference_library :pymcinference_library_version :5.10.3sampling_time :2.553478956222534tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (y_obs: 133)\nCoordinates:\n  * y_obs    (y_obs) int64 0 1 2 3 4 5 6 7 8 ... 125 126 127 128 129 130 131 132\nData variables:\n    y        (y_obs) float64 0.0 -1.3 -2.7 0.0 -2.7 ... -2.7 10.7 -2.7 10.7\nAttributes:\n    created_at:                  2024-05-27T06:00:45.182737\n    arviz_version:               0.17.0\n    inference_library:           pymc\n    inference_library_version:   5.10.3\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:y_obs: 133Coordinates: (1)y_obs(y_obs)int640 1 2 3 4 5 ... 128 129 130 131 132array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132])Data variables: (1)y(y_obs)float640.0 -1.3 -2.7 ... 10.7 -2.7 10.7array([   0. ,   -1.3,   -2.7,    0. ,   -2.7,   -2.7,   -2.7,   -1.3,\n         -2.7,   -2.7,   -1.3,   -2.7,   -2.7,   -2.7,   -5.4,   -2.7,\n         -5.4,    0. ,   -2.7,   -2.7,    0. ,  -13.3,   -5.4,   -5.4,\n         -9.3,  -16. ,  -22.8,   -2.7,  -22.8,  -32.1,  -53.5,  -54.9,\n        -40.2,  -21.5,  -21.5,  -50.8,  -42.9,  -26.8,  -21.5,  -50.8,\n        -61.7,   -5.4,  -80.4,  -59. ,  -71. ,  -91.1,  -77.7,  -37.5,\n        -85.6, -123.1, -101.9,  -99.1, -104.4, -112.5,  -50.8, -123.1,\n        -85.6,  -72.3, -127.2, -123.1, -117.9, -134. , -101.9, -108.4,\n       -123.1, -123.1, -128.5, -112.5,  -95.1,  -81.8,  -53.5,  -64.4,\n        -57.6,  -72.3,  -44.3,  -26.8,   -5.4, -107.1,  -21.5,  -65.6,\n        -16. ,  -45.6,  -24.2,    9.5,    4. ,   12. ,  -21.5,   37.5,\n         46.9,  -17.4,   36.2,   75. ,    8.1,   54.9,   48.2,   46.9,\n         16. ,   45.6,    1.3,   75. ,  -16. ,  -54.9,   69.6,   34.8,\n         32.1,  -37.5,   22.8,   46.9,   10.7,    5.4,   -1.3,  -21.5,\n        -13.3,   30.8,  -10.7,   29.4,    0. ,  -10.7,   14.7,   -1.3,\n          0. ,   10.7,   10.7,  -26.8,  -14.7,  -13.3,    0. ,   10.7,\n        -14.7,   -2.7,   10.7,   -2.7,   10.7])Indexes: (1)y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       123, 124, 125, 126, 127, 128, 129, 130, 131, 132],\n      dtype='int64', name='y_obs', length=133))Attributes: (6)created_at :2024-05-27T06:00:45.182737arviz_version :0.17.0inference_library :pymcinference_library_version :5.10.3modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n              \n            \n            \n\n\nNext we plot the posterior predictive distribution of our observed variable and compare against the observed data. Additionally we plot the 89th and 50% HDI.\n\nax = az.plot_hdi(new_data['X'], idata_spline4['posterior_predictive']['y'], fill_kwargs={'alpha': 0.2, 'color':'firebrick'}, hdi_prob=0.89, figsize=(10, 8))\n\naz.plot_hdi(new_data['X'], idata_spline4['posterior_predictive']['y'], fill_kwargs={'alpha': 0.8, 'color':'firebrick'}, hdi_prob=0.5)\n\ny_mean = idata_spline4['posterior_predictive']['y'].mean(dim=('chain', 'draw'))\n\nax.plot(new_data['X'], y_mean, label='Expected posterior predictive', color='k')\nax.set_xlabel(\"Time Point\")\nax.set_ylabel(\"Acceleration\")\n\nax.scatter(df['X'], df['y'], label='Observed Datapoints')\n\nax.legend()\n\nax.set_title(\"Posterior Predictive Distribution \\n Based on 10 Knots\");\n\n\n\n\nThis represents a good a clean model fit to the observed data using univariate spline smoothers. However, it’s clear that the uncertainty spikes massively with data points even slightly out of the training data. Next we’ll see another alternative approach to model this outcome variable using approximate gaussian processes."
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#gaussian-processes",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#gaussian-processes",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Gaussian processes",
    "text": "Gaussian processes\nThe topic of gaussian processes is rich and detailed. Too rich to be fairly covered in this blog post, so we’ll just say that we’re using a method designed for function approximation that makes use of drawing samples from a multivariate normal distribution under a range of different covariance relationships.\nThese relationships can be somewhat intuitively interrogated by defining different combinations of covariance relationships with priors over the parameters governing the covariance of a sequence of points. For example consider the following parameterisations.\n\nlengthscale = 3\nsigma = 13\ncov = sigma**2 * pm.gp.cov.ExpQuad(1, lengthscale)\n\nX = np.linspace(0, 60, 200)[:, None]\nK = cov(X).eval()\n\nfig, ax = plt.subplots(figsize=(9, 7))\nax.plot(\n    X,\n    pm.draw(\n        pm.MvNormal.dist(mu=np.zeros(len(K)), cov=K, shape=K.shape[0]), draws=10, random_seed=random_seed\n    ).T,\n)\nplt.title(f\"Samples from the GP prior \\n lengthscale: {3}, sigma: {13}\")\nplt.ylabel(\"y\")\nplt.xlabel(\"X\");\n\n\n\n\nWe’ve specified the range of X to reflect the support of the acceleration example and allowed the draws to be informed by a covariance function we have parameterised using the Exponentiated Quadratic kernel:\n\\[k(x, x') = \\mathrm{exp}\\left[ -\\frac{(x - x')^2}{2 \\ell^2} \\right]\\]\nThe patterns exhibited show a good range of “wiggliness” that they should be flexible enough to capture the shape of the acceleration, if we can calibrate the posterior of parameters against the observed data.\n\nPriors on Gaussian Priors\nConsider the following specification for the priors\n\nfig, axs = plt.subplots(1, 2, figsize=(9, 6))\naxs = axs.flatten()\naxs[0].hist(pm.draw(pm.InverseGamma.dist(mu=1, sigma=1), 1000), ec='black', bins=30);\naxs[0].set_title(\"Priors for Lengthscale \\n in ExpQuad Kernel\")\naxs[1].hist(pm.draw(pm.Exponential.dist(lam=1), 1000), ec='black', bins=30);\naxs[1].set_title(\"Priors for Amplitude \\n in ExpQuad Kernel\")\n\nText(0.5, 1.0, 'Priors for Amplitude \\n in ExpQuad Kernel')\n\n\n\n\n\nWe use these to specify priors on the Hilbert space approximation of gaussian priors available in the Bambi package.\n\nprior_hsgp = {\n    \"sigma\": bmb.Prior(\"Exponential\", lam=1), # amplitude\n    \"ell\": bmb.Prior(\"InverseGamma\", mu=1, sigma=1) # lengthscale\n}\n\n# This is the dictionary we pass to Bambi\npriors = {\n    \"hsgp(X, m=10, c=1)\": prior_hsgp,\n    \"sigma\": bmb.Prior(\"HalfNormal\", sigma=4)\n}\nmodel_hsgp = bmb.Model(\"y ~ 0 + hsgp(X, m=10, c=1)\", df, priors=priors)\nmodel_hsgp\n\n       Formula: y ~ 0 + hsgp(X, m=10, c=1)\n        Family: gaussian\n          Link: mu = identity\n  Observations: 133\n        Priors: \n    target = mu\n        HSGP contributions\n            hsgp(X, m=10, c=1)\n                cov: ExpQuad\n                sigma ~ Exponential(lam: 1.0)\n                ell ~ InverseGamma(mu: 1.0, sigma: 1.0)\n        \n        Auxiliary parameters\n            sigma ~ HalfNormal(sigma: 4.0)\n\n\nHere we’ve set the m=10 to determine the number of basis vectors used in the Hilbert space approximation. The idea differs in detail from the spline based approximations we’ve seen, but it’s perhaps useful to think of the process in the same vein.\n\nidata = model_hsgp.fit(inference_method=\"nuts_numpyro\",target_accept=0.95, random_seed=121195, \nidata_kwargs={\"log_likelihood\": True})\nprint(idata.sample_stats[\"diverging\"].sum().to_numpy())\n\nCompiling...\n\n\nCompilation time = 0:00:01.030799\n\n\nSampling...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling time = 0:00:01.825560\n\n\nTransforming variables...\n\n\nTransformation time = 0:00:00.177224\n\n\nComputing Log Likelihood...\n\n\nLog Likelihood time = 0:00:00.144392\n\n\n0\n\n\nThis model fits and the sampling seems to have worked well.\n\naz.plot_trace(idata, backend_kwargs={\"layout\": \"constrained\"}, figsize=(9, 15));\n\n\n\n\nThe lengthscale and sigma parameters we have learned by calibrating our priors against the data. The degree to which these parameters are meaningful depend a little on how familar you are with covariance matrix kernels and their properties, so we won’t dwell on the point here.\n\naz.summary(idata, var_names=['hsgp(X, m=10, c=1)_ell', 'hsgp(X, m=10, c=1)_sigma', 'y_sigma', 'hsgp(X, m=10, c=1)_weights'])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      hsgp(X, m=10, c=1)_ell\n      3.301\n      0.666\n      2.099\n      4.567\n      0.016\n      0.011\n      1763.0\n      1815.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_sigma\n      23.675\n      2.679\n      18.925\n      28.847\n      0.069\n      0.049\n      1497.0\n      2167.0\n      1.0\n    \n    \n      y_sigma\n      20.589\n      1.121\n      18.590\n      22.751\n      0.019\n      0.014\n      3440.0\n      2485.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[0]\n      -131.588\n      13.784\n      -157.019\n      -105.189\n      0.216\n      0.154\n      4071.0\n      3020.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[1]\n      -94.391\n      18.010\n      -125.921\n      -58.020\n      0.363\n      0.256\n      2500.0\n      2876.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[2]\n      106.121\n      20.717\n      69.627\n      147.247\n      0.458\n      0.324\n      2046.0\n      2563.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[3]\n      149.289\n      22.154\n      106.508\n      188.107\n      0.521\n      0.368\n      1807.0\n      2467.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[4]\n      -83.399\n      22.728\n      -124.812\n      -40.406\n      0.539\n      0.383\n      1776.0\n      2517.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[5]\n      -125.322\n      23.273\n      -169.346\n      -82.187\n      0.531\n      0.377\n      1917.0\n      2940.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[6]\n      37.835\n      21.153\n      -1.712\n      77.574\n      0.512\n      0.362\n      1713.0\n      2577.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[7]\n      94.574\n      20.745\n      53.167\n      132.182\n      0.423\n      0.299\n      2393.0\n      3105.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[8]\n      -1.494\n      17.184\n      -31.835\n      33.063\n      0.361\n      0.255\n      2264.0\n      2853.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[9]\n      -45.442\n      15.955\n      -76.649\n      -16.636\n      0.289\n      0.208\n      3041.0\n      3183.0\n      1.0\n    \n  \n\n\n\n\nBut again we can sample from the posterior predictive distribution of the outcome variable\n\nmodel_hsgp.predict(idata, data=new_data, \nkind='pps', inplace=True)\n\nidata\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                         (chain: 4, draw: 1000,\n                                     hsgp(X, m=10, c=1)_weights_dim: 10,\n                                     y_obs: 500)\nCoordinates:\n  * chain                           (chain) int64 0 1 2 3\n  * draw                            (draw) int64 0 1 2 3 4 ... 996 997 998 999\n  * hsgp(X, m=10, c=1)_weights_dim  (hsgp(X, m=10, c=1)_weights_dim) int64 0 ...\n  * y_obs                           (y_obs) int64 0 1 2 3 4 ... 496 497 498 499\nData variables:\n    hsgp(X, m=10, c=1)_weights_raw  (chain, draw, hsgp(X, m=10, c=1)_weights_dim) float64 ...\n    y_sigma                         (chain, draw) float64 20.86 19.86 ... 21.17\n    hsgp(X, m=10, c=1)_sigma        (chain, draw) float64 26.85 24.0 ... 18.06\n    hsgp(X, m=10, c=1)_ell          (chain, draw) float64 3.861 3.509 ... 3.76\n    hsgp(X, m=10, c=1)_weights      (chain, draw, hsgp(X, m=10, c=1)_weights_dim) float64 ...\n    y_mean                          (chain, draw, y_obs) float64 -0.1748 ... ...\n    hsgp(X, m=10, c=1)              (chain, draw, y_obs) float64 -0.1748 ... ...\nAttributes:\n    created_at:                  2024-04-16T21:48:50.636485\n    arviz_version:               0.17.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000hsgp(X, m=10, c=1)_weights_dim: 10y_obs: 500Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])hsgp(X, m=10, c=1)_weights_dim(hsgp(X, m=10, c=1)_weights_dim)int640 1 2 3 4 5 6 7 8 9array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])y_obs(y_obs)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (7)hsgp(X, m=10, c=1)_weights_raw(chain, draw, hsgp(X, m=10, c=1)_weights_dim)float64-1.452 -0.974 ... 0.9659 -0.2453array([[[-1.45186697, -0.97401433,  1.51626692, ...,  1.84709969,\n          0.92584892, -1.19616826],\n        [-1.73799921, -1.55920285,  1.42827017, ...,  1.78787921,\n         -1.14159607, -1.66042935],\n        [-1.3059213 , -1.22687096,  2.0214052 , ...,  2.80079646,\n          0.17462599, -0.06073935],\n        ...,\n        [-2.26781832, -1.21871881,  1.80249133, ...,  2.65551316,\n          0.30040289, -0.7085128 ],\n        [-2.23372877, -1.1832454 ,  1.74365597, ...,  2.71376469,\n          0.41123845, -0.60671544],\n        [-1.96117304, -1.13093178,  2.24600086, ...,  2.3750183 ,\n         -0.00901474, -0.75147244]],\n\n       [[-1.95999601, -1.27507311,  1.52467611, ...,  3.28813978,\n         -0.1598232 , -1.10988783],\n        [-1.59314189, -1.44864255,  1.95956808, ...,  2.01431167,\n          1.0199955 , -0.8962025 ],\n        [-1.8432561 , -1.32415496,  1.80874966, ...,  3.05464118,\n          0.81105708, -1.02872434],\n...\n        [-1.94277245, -1.5433581 ,  1.78771407, ...,  2.11464836,\n         -0.62470516, -1.01229181],\n        [-1.96747626, -1.39055532,  1.24775496, ...,  2.93940091,\n          0.46989555, -1.57735663],\n        [-2.20654554, -1.042289  ,  1.46402154, ...,  3.37106106,\n         -0.11061588, -2.15605928]],\n\n       [[-1.68378706, -0.84035677,  1.41475357, ...,  2.46320953,\n          0.88793346, -0.56144583],\n        [-1.66959934, -1.01014124,  1.6874097 , ...,  2.71030445,\n          0.68597924, -0.76198643],\n        [-1.54563186, -0.89279988,  1.77115866, ...,  2.3842957 ,\n          0.08392828, -0.78058783],\n        ...,\n        [-2.17148384, -1.59167361,  1.34960308, ...,  2.34802173,\n          0.44912308, -1.0698665 ],\n        [-1.31376138, -0.63355604,  1.97759132, ...,  1.8841587 ,\n         -0.13453184, -1.36605679],\n        [-2.0018976 , -1.36470426,  2.15352253, ...,  3.17100177,\n          0.96590611, -0.24527631]]])y_sigma(chain, draw)float6420.86 19.86 21.17 ... 20.12 21.17array([[20.85613959, 19.86298062, 21.16886643, ..., 19.48940449,\n        19.31511941, 19.44304302],\n       [18.5908299 , 21.71605063, 21.22255223, ..., 19.44547467,\n        19.21042952, 22.0099345 ],\n       [20.48425625, 21.17420065, 22.11318822, ..., 18.40825711,\n        21.65013089, 22.05150211],\n       [21.08143229, 20.7215663 , 22.60954219, ..., 21.15149371,\n        20.11622356, 21.16568101]])hsgp(X, m=10, c=1)_sigma(chain, draw)float6426.85 24.0 27.81 ... 26.09 18.06array([[26.84572148, 24.00472166, 27.80754218, ..., 23.34108117,\n        23.72941745, 22.60241785],\n       [22.01917097, 24.17709343, 26.16315265, ..., 22.03189338,\n        22.17588023, 24.00832607],\n       [24.12239909, 24.36393141, 25.79692715, ..., 24.33866729,\n        20.63877848, 22.26540398],\n       [25.41092492, 25.05093181, 28.3790263 , ..., 22.30157727,\n        26.0901197 , 18.06420147]])hsgp(X, m=10, c=1)_ell(chain, draw)float643.861 3.509 3.695 ... 3.679 3.76array([[3.86057038, 3.50871714, 3.69475981, ..., 3.20961859, 3.07237787,\n        2.99194079],\n       [4.21093385, 4.11858175, 3.55744155, ..., 3.54737759, 2.43665447,\n        3.67009957],\n       [3.38790729, 3.56906294, 3.07026931, ..., 3.17401967, 4.77493695,\n        4.1757221 ],\n       [3.73124202, 3.62463311, 4.06803959, ..., 3.06030257, 3.67928011,\n        3.75995625]])hsgp(X, m=10, c=1)_weights(chain, draw, hsgp(X, m=10, c=1)_weights_dim)float64-120.2 -78.54 ... 27.36 -5.933array([[[-120.19159663,  -78.54474016,  117.04015633, ...,\n           88.13161831,   38.07210878,  -41.65695052],\n        [-122.83658383, -107.83699596,   95.27680851, ...,\n           80.15736607,  -45.26655282,  -57.3944273 ],\n        [-109.63228325, -100.55013819,  159.16260856, ...,\n          141.94233647,    7.72311493,   -2.30700494],\n        ...,\n        [-149.23661164,  -78.75788623,  113.01499747, ...,\n          119.40081633,   12.18790241,  -25.62639515],\n        [-146.28304701,  -76.21158457,  109.23906639, ...,\n          125.36332205,   17.28993874,  -22.96014529],\n        [-120.75657952,  -68.54675039,  132.60289356, ...,\n          105.03375274,   -0.36461133,  -27.50691292]],\n\n       [[-138.76216692,  -87.49701007,   99.32062774, ...,\n          120.85182044,   -4.92170357,  -28.04716014],\n        [-122.53330814, -108.14123925,  139.17924148, ...,\n           82.74939574,   35.37871319,  -25.72818479],\n        [-142.94342858, -100.42482592,  132.17632445, ...,\n          148.36712045,   34.72137014,  -38.24384896],\n...\n        [-132.58698442, -103.47676242,  116.36868708, ...,\n           99.43921353,  -26.56716678,  -38.47612061],\n        [-138.61535955,  -94.11467693,   78.98484564, ...,\n           89.13997713,   11.35087487,  -29.54951483],\n        [-157.32953055,  -72.06983662,   96.1818003 , ...,\n          126.1515611 ,   -3.47852942,  -55.82170316]],\n\n       [[-129.78695588,  -63.20664967,  102.15003445, ...,\n          113.4760498 ,   35.60103971,  -19.27405014],\n        [-125.10213961,  -73.9587159 ,  118.87369923, ...,\n          124.94562328,   27.73901676,  -26.61384578],\n        [-138.71497899,  -77.82483202,  147.07295446, ...,\n          116.05385128,    3.46342745,  -26.78443203],\n        ...,\n        [-133.39280566,  -96.17651826,   79.33895456, ...,\n          102.02385007,   17.77408526,  -38.1416893 ],\n        [-103.26872735,  -48.62806342,  145.87733997, ...,\n           89.78661037,   -5.60101795,  -48.90554932],\n        [-110.1016679 ,  -73.21183754,  110.83494684, ...,\n          103.40789151,   27.35517734,   -5.93337047]]])y_mean(chain, draw, y_obs)float64-0.1748 -0.6437 ... 4.329e-15array([[[-1.74825831e-01, -6.43722945e-01, -1.10198757e+00, ...,\n          8.81438724e-01,  4.41420533e-01,  1.00916482e-14],\n        [ 3.99710952e+00,  4.36245618e+00,  4.70113794e+00, ...,\n         -3.68931837e-01, -1.84461976e-01, -4.21486081e-15],\n        [ 1.12371919e+01,  9.85417314e+00,  8.49086985e+00, ...,\n         -4.14235314e-01, -2.07469138e-01, -4.74327547e-15],\n        ...,\n        [ 5.14480643e-01, -3.24968824e-01, -1.15248120e+00, ...,\n          2.85647398e-02,  1.43964056e-02,  3.29822057e-16],\n        [-1.15486831e+00, -2.03017733e+00, -2.88893220e+00, ...,\n         -1.21523155e-01, -6.06829841e-02, -1.38598577e-15],\n        [ 1.21777447e+01,  1.16287837e+01,  1.10821348e+01, ...,\n          1.95250577e-01,  9.77499077e-02,  2.23450097e-15]],\n\n       [[-2.44479507e+00, -2.94292536e+00, -3.43456273e+00, ...,\n         -7.42363606e-01, -3.71378424e-01, -8.48736477e-15],\n        [-3.36694245e+00, -3.93597519e+00, -4.48819398e+00, ...,\n          1.45535040e+00,  7.28337111e-01,  1.66472919e-14],\n        [ 1.86823417e-01, -1.03465399e+00, -2.23999952e+00, ...,\n          3.24627418e-01,  1.62576930e-01,  3.71683234e-15],\n...\n        [-3.54206467e+00, -3.29770384e+00, -3.05890542e+00, ...,\n         -8.07266002e-01, -4.03861987e-01, -9.22984926e-15],\n        [-8.28095921e+00, -8.58481681e+00, -8.88022655e+00, ...,\n         -1.37004285e-01, -6.83094783e-02, -1.55937910e-15],\n        [ 1.05497489e+00,  4.74317876e-01, -1.10703434e-01, ...,\n         -4.72148575e-01, -2.35887059e-01, -5.38850915e-15]],\n\n       [[-4.02175043e+00, -4.75614076e+00, -5.46523621e+00, ...,\n         -1.18443060e-01, -5.89840516e-02, -1.34595499e-15],\n        [ 3.62393761e+00,  2.72714782e+00,  1.84762910e+00, ...,\n         -9.97541423e-02, -4.96901944e-02, -1.13397909e-15],\n        [ 1.18490633e+01,  1.10655715e+01,  1.02860360e+01, ...,\n         -5.74333102e-01, -2.87269966e-01, -6.56480568e-15],\n        ...,\n        [-9.39146019e+00, -9.93167964e+00, -1.04627004e+01, ...,\n          1.23581084e-01,  6.21200724e-02,  1.42193352e-15],\n        [ 2.48719138e+01,  2.46289484e+01,  2.43707844e+01, ...,\n         -9.35532279e-02, -4.65445693e-02, -1.06175818e-15],\n        [-5.90472295e-01, -1.42675597e+00, -2.23855095e+00, ...,\n          3.78505074e-01,  1.89406640e-01,  4.32904960e-15]]])hsgp(X, m=10, c=1)(chain, draw, y_obs)float64-0.1748 -0.6437 ... 4.329e-15array([[[-1.74825831e-01, -6.43722945e-01, -1.10198757e+00, ...,\n          8.81438724e-01,  4.41420533e-01,  1.00916482e-14],\n        [ 3.99710952e+00,  4.36245618e+00,  4.70113794e+00, ...,\n         -3.68931837e-01, -1.84461976e-01, -4.21486081e-15],\n        [ 1.12371919e+01,  9.85417314e+00,  8.49086985e+00, ...,\n         -4.14235314e-01, -2.07469138e-01, -4.74327547e-15],\n        ...,\n        [ 5.14480643e-01, -3.24968824e-01, -1.15248120e+00, ...,\n          2.85647398e-02,  1.43964056e-02,  3.29822057e-16],\n        [-1.15486831e+00, -2.03017733e+00, -2.88893220e+00, ...,\n         -1.21523155e-01, -6.06829841e-02, -1.38598577e-15],\n        [ 1.21777447e+01,  1.16287837e+01,  1.10821348e+01, ...,\n          1.95250577e-01,  9.77499077e-02,  2.23450097e-15]],\n\n       [[-2.44479507e+00, -2.94292536e+00, -3.43456273e+00, ...,\n         -7.42363606e-01, -3.71378424e-01, -8.48736477e-15],\n        [-3.36694245e+00, -3.93597519e+00, -4.48819398e+00, ...,\n          1.45535040e+00,  7.28337111e-01,  1.66472919e-14],\n        [ 1.86823417e-01, -1.03465399e+00, -2.23999952e+00, ...,\n          3.24627418e-01,  1.62576930e-01,  3.71683234e-15],\n...\n        [-3.54206467e+00, -3.29770384e+00, -3.05890542e+00, ...,\n         -8.07266002e-01, -4.03861987e-01, -9.22984926e-15],\n        [-8.28095921e+00, -8.58481681e+00, -8.88022655e+00, ...,\n         -1.37004285e-01, -6.83094783e-02, -1.55937910e-15],\n        [ 1.05497489e+00,  4.74317876e-01, -1.10703434e-01, ...,\n         -4.72148575e-01, -2.35887059e-01, -5.38850915e-15]],\n\n       [[-4.02175043e+00, -4.75614076e+00, -5.46523621e+00, ...,\n         -1.18443060e-01, -5.89840516e-02, -1.34595499e-15],\n        [ 3.62393761e+00,  2.72714782e+00,  1.84762910e+00, ...,\n         -9.97541423e-02, -4.96901944e-02, -1.13397909e-15],\n        [ 1.18490633e+01,  1.10655715e+01,  1.02860360e+01, ...,\n         -5.74333102e-01, -2.87269966e-01, -6.56480568e-15],\n        ...,\n        [-9.39146019e+00, -9.93167964e+00, -1.04627004e+01, ...,\n          1.23581084e-01,  6.21200724e-02,  1.42193352e-15],\n        [ 2.48719138e+01,  2.46289484e+01,  2.43707844e+01, ...,\n         -9.35532279e-02, -4.65445693e-02, -1.06175818e-15],\n        [-5.90472295e-01, -1.42675597e+00, -2.23855095e+00, ...,\n          3.78505074e-01,  1.89406640e-01,  4.32904960e-15]]])Indexes: (4)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))hsgp(X, m=10, c=1)_weights_dimPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64', name='hsgp(X, m=10, c=1)_weights_dim'))y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='y_obs', length=500))Attributes: (4)created_at :2024-04-16T21:48:50.636485arviz_version :0.17.0modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 4, draw: 1000, y_obs: 500)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * y_obs    (y_obs) int64 0 1 2 3 4 5 6 7 8 ... 492 493 494 495 496 497 498 499\nData variables:\n    y        (chain, draw, y_obs) float64 52.29 12.0 -15.8 ... -7.277 -0.6193\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000y_obs: 500Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])y_obs(y_obs)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (1)y(chain, draw, y_obs)float6452.29 12.0 -15.8 ... -7.277 -0.6193array([[[ 5.22936097e+01,  1.19978202e+01, -1.57982224e+01, ...,\n         -2.94686487e+01, -1.15648426e+00,  3.05766096e+01],\n        [-4.76584635e+01,  1.27246989e+01,  1.32011413e+01, ...,\n         -1.46264536e+01, -3.40966683e+01,  4.27381431e+00],\n        [ 6.57604280e+01, -1.58375968e+00,  3.04687510e+01, ...,\n          9.17079025e+00, -1.31545895e+01, -3.53084183e+00],\n        ...,\n        [ 1.12540415e+01, -1.03612151e+01,  5.16925300e+00, ...,\n         -8.80372304e+00,  4.23855827e+01,  3.86672697e+01],\n        [ 1.58179199e+01,  1.81354709e+00, -1.61407921e+01, ...,\n          2.99057653e+01, -2.81156162e+01,  3.01082768e+01],\n        [ 5.00906194e+01,  1.72989035e+01,  8.72112249e+00, ...,\n         -2.18459942e+01, -1.22332010e+01,  3.47301391e+01]],\n\n       [[ 4.98456573e+01, -2.15902881e+01, -1.01091765e+01, ...,\n          2.77829573e+01, -8.31966503e+00,  4.05382474e+01],\n        [-6.18665801e+01, -1.89799539e+01, -2.18600703e+01, ...,\n         -2.46085346e+00,  3.79662766e+01,  2.12367943e+01],\n        [ 2.82531065e+00, -2.66180770e+01, -3.09302003e+00, ...,\n         -1.47884850e+01, -1.35979869e+01,  1.66919798e+01],\n...\n        [-2.78424147e+01, -2.76704484e+01, -3.40801106e+01, ...,\n         -3.34053498e-01, -3.04495384e+01,  1.28357634e+01],\n        [-7.72085676e+00, -2.97495829e+01, -2.74093551e+01, ...,\n          3.75289441e+01,  1.50098225e+01,  1.27876358e+00],\n        [ 3.90403640e+01,  1.92823746e+01, -5.97898444e+00, ...,\n         -8.27064422e-01, -8.40095524e+00,  3.04210549e+01]],\n\n       [[ 9.80420140e-01,  2.62293682e+00, -2.41407897e+00, ...,\n         -1.36930920e-02,  9.47470172e+00,  3.82376791e+00],\n        [ 2.02564956e+01, -7.67608538e+00,  2.91352132e+01, ...,\n         -1.45786727e+01, -1.40717625e+01,  1.21951915e+01],\n        [ 3.34246765e+01,  3.06645368e+01,  2.89732275e+01, ...,\n         -5.44012965e+00,  5.39713832e+00,  2.42196230e+00],\n        ...,\n        [-3.82283445e+00, -7.38678013e+00, -3.83033130e+01, ...,\n         -1.07484233e+01,  4.01568557e+00,  1.61832716e+01],\n        [ 4.19341088e+00, -1.44430520e+01,  4.93944933e+01, ...,\n          2.17531046e+00,  1.78504565e+01,  1.26495414e+01],\n        [ 3.57253266e+01, -8.19945322e+00, -2.48977977e+01, ...,\n          2.75647652e+00, -7.27697932e+00, -6.19334674e-01]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='y_obs', length=500))Attributes: (2)modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 4, draw: 1000, y_obs: 133)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * y_obs    (y_obs) int64 0 1 2 3 4 5 6 7 8 ... 125 126 127 128 129 130 131 132\nData variables:\n    y        (chain, draw, y_obs) float64 -3.957 -3.957 -3.957 ... -4.013 -4.099\nAttributes:\n    created_at:                  2024-04-16T21:48:50.640043\n    arviz_version:               0.17.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000y_obs: 133Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])y_obs(y_obs)int640 1 2 3 4 5 ... 128 129 130 131 132array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132])Data variables: (1)y(chain, draw, y_obs)float64-3.957 -3.957 ... -4.013 -4.099array([[[-3.95662204, -3.95668036, -3.95697947, ..., -3.96786393,\n         -4.06567925, -4.08819109],\n        [-3.92804385, -3.95248402, -4.00438808, ..., -4.19279911,\n         -3.90894979, -4.05289002],\n        [-4.11236339, -4.08418887, -3.99366348, ..., -4.1948162 ,\n         -3.97180293, -4.09921461],\n        ...,\n        [-3.88915792, -3.88893229, -3.89724674, ..., -4.04248536,\n         -3.89847518, -4.03951892],\n        [-3.88161419, -3.88254965, -3.9052254 , ..., -4.08668059,\n         -3.88211318, -4.03326819],\n        [-4.08257196, -4.09265666, -4.04648571, ..., -3.99108035,\n         -3.91131912, -4.03785686]],\n\n       [[-3.85025382, -3.84763163, -3.85608195, ..., -4.34165517,\n         -3.86684608, -4.00723736],\n        [-4.0090095 , -4.00707108, -4.017144  , ..., -4.01175855,\n         -4.25268066, -4.11837821],\n        [-3.97404168, -3.97456237, -4.00764182, ..., -4.0433394 ,\n         -4.00511976, -4.10110199],\n...\n        [-3.85025004, -3.83654156, -3.832685  , ..., -4.37637177,\n         -3.86484545, -4.00066997],\n        [-4.06709934, -4.05434047, -4.05493257, ..., -4.17570336,\n         -3.99476606, -4.11607839],\n        [-4.01346366, -4.01406212, -4.01257167, ..., -4.30726434,\n         -4.01839009, -4.13004222]],\n\n       [[-3.98552821, -3.98561292, -4.0079376 , ..., -4.15815157,\n         -3.96833515, -4.09613757],\n        [-3.96540632, -3.96291088, -3.95021137, ..., -4.13476712,\n         -3.95195981, -4.08343265],\n        [-4.17463712, -4.17201421, -4.11695573, ..., -4.3197009 ,\n         -4.04463544, -4.14929407],\n        ...,\n        [-4.0692212 , -4.06241625, -4.09034963, ..., -4.09469991,\n         -3.98159516, -4.09860353],\n        [-4.68482086, -4.73793167, -4.72387693, ..., -4.11913623,\n         -3.92229916, -4.06192875],\n        [-3.97170872, -3.97200671, -3.98370832, ..., -4.02417539,\n         -4.01254991, -4.09910258]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       123, 124, 125, 126, 127, 128, 129, 130, 131, 132],\n      dtype='int64', name='y_obs', length=133))Attributes: (4)created_at :2024-04-16T21:48:50.640043arviz_version :0.17.0modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:          (chain: 4, draw: 1000)\nCoordinates:\n  * chain            (chain) int64 0 1 2 3\n  * draw             (draw) int64 0 1 2 3 4 5 6 ... 993 994 995 996 997 998 999\nData variables:\n    acceptance_rate  (chain, draw) float64 0.9484 0.989 0.9772 ... 0.9905 0.9283\n    step_size        (chain, draw) float64 0.1631 0.1631 ... 0.1827 0.1827\n    diverging        (chain, draw) bool False False False ... False False False\n    energy           (chain, draw) float64 671.4 665.4 671.0 ... 670.8 674.0\n    n_steps          (chain, draw) int64 31 31 31 15 15 31 ... 15 31 31 15 15 31\n    tree_depth       (chain, draw) int64 5 5 5 4 4 5 5 4 4 ... 4 5 5 4 5 5 4 4 5\n    lp               (chain, draw) float64 661.7 661.3 665.7 ... 665.6 668.8\nAttributes:\n    created_at:                  2024-04-16T21:48:50.639029\n    arviz_version:               0.17.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (7)acceptance_rate(chain, draw)float640.9484 0.989 ... 0.9905 0.9283array([[0.94837217, 0.98900712, 0.97718268, ..., 0.99599719, 0.97974622,\n        0.93727528],\n       [0.9824305 , 0.87426419, 0.99008681, ..., 0.99942845, 0.97008375,\n        0.99555447],\n       [0.99910745, 0.98130743, 0.95550076, ..., 0.97069333, 0.9342073 ,\n        0.95033605],\n       [0.97363799, 0.97895034, 0.93808177, ..., 0.9514047 , 0.99054107,\n        0.92830969]])step_size(chain, draw)float640.1631 0.1631 ... 0.1827 0.1827array([[0.16306409, 0.16306409, 0.16306409, ..., 0.16306409, 0.16306409,\n        0.16306409],\n       [0.15671706, 0.15671706, 0.15671706, ..., 0.15671706, 0.15671706,\n        0.15671706],\n       [0.15280013, 0.15280013, 0.15280013, ..., 0.15280013, 0.15280013,\n        0.15280013],\n       [0.1826574 , 0.1826574 , 0.1826574 , ..., 0.1826574 , 0.1826574 ,\n        0.1826574 ]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])energy(chain, draw)float64671.4 665.4 671.0 ... 670.8 674.0array([[671.39512608, 665.35858794, 670.96295141, ..., 670.72923684,\n        668.42111758, 669.24601286],\n       [665.04518693, 671.40450254, 672.47055359, ..., 665.8504822 ,\n        674.75907256, 672.99992931],\n       [672.00375668, 668.16520377, 666.3166769 , ..., 671.79404347,\n        671.22701287, 671.74308362],\n       [669.46327702, 671.11966704, 667.28825365, ..., 665.11423361,\n        670.79366043, 673.9890605 ]])n_steps(chain, draw)int6431 31 31 15 15 ... 31 31 15 15 31array([[31, 31, 31, ..., 31, 15, 15],\n       [15, 31, 31, ..., 31, 31, 31],\n       [31, 31, 15, ..., 15, 31, 15],\n       [31, 15, 15, ..., 15, 15, 31]])tree_depth(chain, draw)int645 5 5 4 4 5 5 4 ... 5 5 4 5 5 4 4 5array([[5, 5, 5, ..., 5, 4, 4],\n       [4, 5, 5, ..., 5, 5, 5],\n       [5, 5, 4, ..., 4, 5, 4],\n       [5, 4, 4, ..., 4, 4, 5]])lp(chain, draw)float64661.7 661.3 665.7 ... 665.6 668.8array([[661.70967519, 661.2875458 , 665.70729556, ..., 661.40453042,\n        661.53660449, 661.51768248],\n       [662.47186408, 665.25230302, 666.28391286, ..., 662.10849958,\n        668.1822538 , 664.18500466],\n       [662.60487008, 660.69369885, 662.99303071, ..., 663.00720061,\n        664.60205279, 667.39059102],\n       [664.1010232 , 660.27378818, 664.08879552, ..., 661.26338997,\n        665.58839085, 668.77321037]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (4)created_at :2024-04-16T21:48:50.639029arviz_version :0.17.0modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (y_obs: 133)\nCoordinates:\n  * y_obs    (y_obs) int64 0 1 2 3 4 5 6 7 8 ... 125 126 127 128 129 130 131 132\nData variables:\n    y        (y_obs) float64 0.0 -1.3 -2.7 0.0 -2.7 ... -2.7 10.7 -2.7 10.7\nAttributes:\n    created_at:                  2024-04-16T21:48:50.640324\n    arviz_version:               0.17.0\n    inference_library:           numpyro\n    inference_library_version:   0.13.2\n    sampling_time:               1.82556\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:y_obs: 133Coordinates: (1)y_obs(y_obs)int640 1 2 3 4 5 ... 128 129 130 131 132array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132])Data variables: (1)y(y_obs)float640.0 -1.3 -2.7 ... 10.7 -2.7 10.7array([   0. ,   -1.3,   -2.7,    0. ,   -2.7,   -2.7,   -2.7,   -1.3,\n         -2.7,   -2.7,   -1.3,   -2.7,   -2.7,   -2.7,   -5.4,   -2.7,\n         -5.4,    0. ,   -2.7,   -2.7,    0. ,  -13.3,   -5.4,   -5.4,\n         -9.3,  -16. ,  -22.8,   -2.7,  -22.8,  -32.1,  -53.5,  -54.9,\n        -40.2,  -21.5,  -21.5,  -50.8,  -42.9,  -26.8,  -21.5,  -50.8,\n        -61.7,   -5.4,  -80.4,  -59. ,  -71. ,  -91.1,  -77.7,  -37.5,\n        -85.6, -123.1, -101.9,  -99.1, -104.4, -112.5,  -50.8, -123.1,\n        -85.6,  -72.3, -127.2, -123.1, -117.9, -134. , -101.9, -108.4,\n       -123.1, -123.1, -128.5, -112.5,  -95.1,  -81.8,  -53.5,  -64.4,\n        -57.6,  -72.3,  -44.3,  -26.8,   -5.4, -107.1,  -21.5,  -65.6,\n        -16. ,  -45.6,  -24.2,    9.5,    4. ,   12. ,  -21.5,   37.5,\n         46.9,  -17.4,   36.2,   75. ,    8.1,   54.9,   48.2,   46.9,\n         16. ,   45.6,    1.3,   75. ,  -16. ,  -54.9,   69.6,   34.8,\n         32.1,  -37.5,   22.8,   46.9,   10.7,    5.4,   -1.3,  -21.5,\n        -13.3,   30.8,  -10.7,   29.4,    0. ,  -10.7,   14.7,   -1.3,\n          0. ,   10.7,   10.7,  -26.8,  -14.7,  -13.3,    0. ,   10.7,\n        -14.7,   -2.7,   10.7,   -2.7,   10.7])Indexes: (1)y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       123, 124, 125, 126, 127, 128, 129, 130, 131, 132],\n      dtype='int64', name='y_obs', length=133))Attributes: (7)created_at :2024-04-16T21:48:50.640324arviz_version :0.17.0inference_library :numpyroinference_library_version :0.13.2sampling_time :1.82556modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n              \n            \n            \n\n\nand plot the model fit to see if it can recover the observed data.\n\nax = az.plot_hdi(new_data['X'], idata['posterior_predictive']['y'], fill_kwargs={'alpha': 0.2, 'color':'firebrick'}, figsize=(9, 8))\n\naz.plot_hdi(new_data['X'], idata['posterior_predictive']['y'], fill_kwargs={'alpha': 0.8, 'color':'firebrick'}, hdi_prob=0.5)\n\ny_mean = idata['posterior_predictive']['y'].mean(dim=('chain', 'draw'))\n\nax.plot(new_data['X'], y_mean, label='Expected posterior predictive', color='k')\n\nax.scatter(df['X'], df['y'], label='Observed Datapoints')\n\nax.legend()\n\nax.set_title(\"Posterior Predictive Distribution \\n Based on HSGP approximation\")\n\nText(0.5, 1.0, 'Posterior Predictive Distribution \\n Based on HSGP approximation')\n\n\n\n\n\nAnd we can compare versus the spline models to see that by the aggregate performance measures our HSGP model seems to come out on top.\n\nmodels_dict = {\"piecewise_constant\": idata_spline1, \"piecewise_linear\": idata_spline2, \"cubic_bspline\": idata_spline3, \"cubic_bspline_10\": idata_spline4, \n\"cubic_bspline_15\": idata_spline5, 'hsgp': idata}\ndf_compare = az.compare(models_dict)\ndf_compare\n\n\n\n\n\n  \n    \n      \n      rank\n      elpd_loo\n      p_loo\n      elpd_diff\n      weight\n      se\n      dse\n      warning\n      scale\n    \n  \n  \n    \n      hsgp\n      0\n      -608.819770\n      9.958827\n      0.000000\n      0.873213\n      12.009698\n      0.000000\n      False\n      log\n    \n    \n      cubic_bspline_10\n      1\n      -612.278059\n      11.465651\n      3.458289\n      0.000000\n      9.688433\n      2.962396\n      True\n      log\n    \n    \n      cubic_bspline_15\n      2\n      -616.252712\n      15.566839\n      7.432943\n      0.000000\n      9.833120\n      3.457579\n      True\n      log\n    \n    \n      cubic_bspline\n      3\n      -634.729956\n      8.698337\n      25.910186\n      0.000000\n      8.897139\n      7.686648\n      True\n      log\n    \n    \n      piecewise_constant\n      4\n      -643.781042\n      6.981098\n      34.961272\n      0.069506\n      9.770740\n      9.490817\n      False\n      log\n    \n    \n      piecewise_linear\n      5\n      -647.267410\n      6.170543\n      38.447641\n      0.057282\n      7.902853\n      9.200728\n      False\n      log\n    \n  \n\n\n\n\n\n\nRecap\nSo far we’ve seen how we can use splines and gauassian processes to model highly eccentric functional relationships where the function could be approximated with univariate smoothing routine. These are two distinct abstractions which seem adequately fit to the world, but demand very different ways of thinking about the underlying reality. This is no fundamental contradiction in so far as the world admits many descriptions.\n\n[K]nowledge develops in a multiplicity of theories, each with it’s limited utility … These theories overlap very considerable in their logical laws and in much else, but that they add up to an integrated and consistent whole is only a worthy ideal and not a pre-requistite of scientific progress … Let reconciliations proceed. - W.V.O Quine in Word and Object\n\nAnother observation in a similar vein is that penalised spline models are provably equivalent to hierarchical regression (random effects) models. This is striking because the character of these types of model seems diametrically opposed. With spline models you jerry-rig your features to an optimisation goal, with hierarchical model you tend to impose theoretical structure to induce shrinkage. It’s hard to see how this works - with a penalised spline model are you inducing a hierarchy of latent features you can’t name? Should you even try to translate between the two!? The abstract components of our model is less graspable than the predictive performance.\nNext we’ll step up our abstractions and show how to use hierarchical modelling over spline fits to extract insight into the data generating process over a family of curves. In particular we’ll focus on the development of insurance loss curves."
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#insurance-loss-curves-hierarchical-spline-models",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#insurance-loss-curves-hierarchical-spline-models",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Insurance Loss Curves: Hierarchical Spline Models",
    "text": "Insurance Loss Curves: Hierarchical Spline Models\nHierarchical models of spline fits allow us to characterise and sample from fits over a family of curves while improving out of sample predictive accuracy. We draw on car insurance losses data set discussed in Mick Cooney’s Stan case-study, but we simplify things for ourselves considerably by focusing on two types of loss and ensuring that each year under consideration has equal observations of the accruing losses.\n\nloss_df = pd.read_csv('ppauto_pos.csv')\nloss_df = loss_df[(loss_df['GRCODE'].isin([43, 353])) & (loss_df['DevelopmentYear'] < 1998)]\n\nloss_df = loss_df[['GRCODE', 'AccidentYear', 'DevelopmentYear', 'DevelopmentLag', 'EarnedPremDIR_B', 'CumPaidLoss_B']]\n\n\nloss_df.columns = ['grcode', 'acc_year', 'dev_year', 'dev_lag', 'premium', 'cum_loss']\nloss_df['lr'] = loss_df['cum_loss'] / loss_df['premium']\nloss_df = loss_df[(loss_df['acc_year'] <= 1992) & (loss_df['dev_lag'] <= 6)].reset_index(drop=True)\n\nloss_df['year_code'] = loss_df['acc_year'].astype(str) + '_' + loss_df['grcode'].astype(str)\nloss_df.sort_values(by=['year_code', 'acc_year', 'dev_lag'], inplace=True)\nloss_df['standardised_premium'] = (loss_df.assign(mean_premium = np.mean(loss_df['premium']))\n.assign(std_premium = np.std(loss_df['premium']))\n.apply(lambda x: (x['mean_premium'] - x['premium']) /x['std_premium'], axis=1)\n)\n\nloss_df.head(12)\n\n\n\n\n\n  \n    \n      \n      grcode\n      acc_year\n      dev_year\n      dev_lag\n      premium\n      cum_loss\n      lr\n      year_code\n      standardised_premium\n    \n  \n  \n    \n      30\n      353\n      1988\n      1988\n      1\n      18793\n      4339\n      0.230884\n      1988_353\n      -0.347453\n    \n    \n      31\n      353\n      1988\n      1989\n      2\n      18793\n      9617\n      0.511733\n      1988_353\n      -0.347453\n    \n    \n      32\n      353\n      1988\n      1990\n      3\n      18793\n      11584\n      0.616400\n      1988_353\n      -0.347453\n    \n    \n      33\n      353\n      1988\n      1991\n      4\n      18793\n      12001\n      0.638589\n      1988_353\n      -0.347453\n    \n    \n      34\n      353\n      1988\n      1992\n      5\n      18793\n      12640\n      0.672591\n      1988_353\n      -0.347453\n    \n    \n      35\n      353\n      1988\n      1993\n      6\n      18793\n      12966\n      0.689938\n      1988_353\n      -0.347453\n    \n    \n      0\n      43\n      1988\n      1988\n      1\n      957\n      133\n      0.138976\n      1988_43\n      1.722344\n    \n    \n      1\n      43\n      1988\n      1989\n      2\n      957\n      333\n      0.347962\n      1988_43\n      1.722344\n    \n    \n      2\n      43\n      1988\n      1990\n      3\n      957\n      431\n      0.450366\n      1988_43\n      1.722344\n    \n    \n      3\n      43\n      1988\n      1991\n      4\n      957\n      570\n      0.595611\n      1988_43\n      1.722344\n    \n    \n      4\n      43\n      1988\n      1992\n      5\n      957\n      615\n      0.642633\n      1988_43\n      1.722344\n    \n    \n      5\n      43\n      1988\n      1993\n      6\n      957\n      615\n      0.642633\n      1988_43\n      1.722344\n    \n  \n\n\n\n\n\nPlot the Loss Curves\nHere we have plotted the developing loss curves from two different coded insurance products.\n\npivot = loss_df.pivot(index=['dev_lag'], columns=['grcode', 'acc_year'], values='lr')\nfig, axs = plt.subplots(1, 2, figsize=(9, 7))\npivot.plot(figsize=(10, 6), ax=axs[0])\naxs[0].set_title(\"Loss Ratios by Year\");\nfor c in pivot.columns:\n    if 43 in c:\n        color='red'\n    else: \n        color='grey'\n    axs[1].plot(pivot[c], color=color, label=c)\naxs[1].legend()\naxs[1].set_title(\"Loss Ratio by Group\");\n\n\n\n\nWe want to model these curves collectively as instances of draws from a distribution of loss curves. To do so we will specify a PyMC hierarchical (mixed) spline model. To do so we will have a spline basis for the global hyper parameters beta_g and the individual parameters for each curve. Here we define a convenience function to generate the basis splines.\n\ndef make_basis_splines(num_knots=3, max_dev=7):\n    knot_list = np.linspace(0, max_dev, num_knots+2)[1:-1]\n    dev_periods = np.arange(1, max_dev, 1)\n\n    Bi = dmatrix(\n        \"bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=True) - 1\",\n        {\"dev_periods\": dev_periods, \"knots\": knot_list},\n    )\n\n    Bg = dmatrix(\n        \"bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=True) - 1\",\n        {\"dev_periods\": dev_periods, \"knots\": knot_list})\n\n\n    return Bi, Bg\n\nBi, Bg = make_basis_splines()\nBg\n\nDesignMatrix with shape (6, 7)\n  Columns:\n    ['bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=True)[0]',\n     'bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=True)[1]',\n     'bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=True)[2]',\n     'bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=True)[3]',\n     'bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=True)[4]',\n     'bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=True)[5]',\n     'bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=True)[6]']\n  Terms:\n    'bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=True)' (columns 0:7)\n  (to view full data, use np.asarray(this_obj))\n\n\nNext we specify a model maker function to create the various pooled, unpooled and hierarhical (mixed) models of the insurance curve data. Note that even though we’re specifying a hierarhical model we have not specified a hierarchy over the insurance codes, instead we have added this as a “fixed” effect feature into our regression model. The idea is that this fixed effect will capture the differences in expected baseline per group code of insurance product.\n\ndef make_model(loss_df, num_knots=3, max_dev=7, model_type='mixed'):\n    Bi, Bg = make_basis_splines(num_knots, max_dev)\n    observed = loss_df['lr'].values\n    uniques, unique_codes = pd.factorize(loss_df['year_code'])\n    coords= {'years': unique_codes, \n            'splines': list(range(Bi.shape[1])) ,\n            'measurement': list(range(6)), \n            'obs': uniques\n            }\n\n    with pm.Model(coords=coords) as sp_insur:\n        basis_g = pm.MutableData('Bg', np.asfortranarray(Bg))\n\n        tau = pm.HalfCauchy('tau', 1)\n        ## Global Hierarchical Spline Terms\n        beta_g = pm.Normal(\"beta_g\", mu=0, sigma=tau, \n        dims='splines')\n        mu_g = pm.Deterministic(\"mu_g\", pm.math.dot(basis_g, beta_g), dims='measurement')\n\n        ## Individual or Year Specific Spline Modifications\n        if model_type in ['mixed', 'unpooled']:\n            sigma = pm.HalfCauchy('sigma_i', 1)\n            basis_i = pm.MutableData('Bi', np.asfortranarray(Bi))\n            beta = pm.Normal(\"beta\", mu=0, sigma=sigma, dims=('splines', 'years'))\n            mui = pm.Deterministic(\"mui\", pm.math.dot(basis_i, beta), dims=('measurement', 'years'))\n        \n        ## Features\n        prem = pm.MutableData('prem', loss_df['standardised_premium'].values)\n        grcode = pm.MutableData('grcode', loss_df['grcode'] == 43)\n\n        beta_prem = pm.Normal('beta_prem', 0, 1)\n        beta_grcode = pm.Normal('beta_grcode', 0, 1)\n        mu_prem = beta_prem*prem\n        mu_grcode = beta_grcode*grcode\n\n        ## Likelihood\n        sigma = pm.TruncatedNormal(\"sigma\", 1, lower=0.005)\n        if model_type == 'mixed':\n            mu = pm.Deterministic('mu',  mu_grcode + mu_prem + (mu_g.T + mui.T).ravel(), dims='obs')\n            lr_likelihood = pm.Normal(\"lr\", mu, sigma, observed=observed, dims=('obs'))\n        elif model_type == 'pooled': \n             lr_likelihood = pm.Normal(\"lr\",  np.repeat(mu_g, len(unique_codes)), sigma, observed=observed, dims='obs')\n        elif model_type == 'unpooled':\n            lr_likelihood = pm.Normal(\"lr\",  mui.T.ravel(), sigma, observed=observed, dims=('obs'))\n\n\n        ## Sampling\n        idata_sp_insur = pm.sample(2000, return_inferencedata=True, target_accept=.99,\n        idata_kwargs={\"log_likelihood\": True})\n        idata_sp_insur = pm.sample_posterior_predictive(\n            idata_sp_insur,extend_inferencedata=True)\n\n    return idata_sp_insur, sp_insur\n\n\nidata_sp_insur_unpooled, sp_insur_unpooled = make_model(loss_df, model_type='unpooled')\nidata_sp_insur_pooled, sp_insur_pooled = make_model(loss_df, model_type='pooled')\nidata_sp_insur_mixed, sp_insur_mixed = make_model(loss_df, model_type='mixed')\n\nThe model structure can be seen more clearly in this graph. The important features to note are the spline components and the manner of their combination. We are postulating a kind of hierarchy of development curves. We specify the “global” curve in 7 development steps and allow that each distinct year is a realisation of a curve that can be thought of as modifying the profile of “global” curve as mui is combined with mu_g. These structures are then combined into a more traditional regression and fed into our likelihood term.\n\npm.model_to_graphviz(sp_insur_mixed)\n\n\n\n\nWe can extract the effect of the differences grcodes and examine the baseline and annual spline related coefficients.\n\nsummary = az.summary(idata_sp_insur_mixed, var_names=['sigma', 'beta_grcode', 'beta_prem', 'beta_g', 'beta'])\n\nsummary\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      sigma\n      0.014\n      0.007\n      0.005\n      0.027\n      0.001\n      0.000\n      153.0\n      348.0\n      1.04\n    \n    \n      beta_grcode\n      0.231\n      0.057\n      0.123\n      0.337\n      0.001\n      0.001\n      4479.0\n      4712.0\n      1.00\n    \n    \n      beta_prem\n      -0.031\n      0.029\n      -0.086\n      0.023\n      0.000\n      0.000\n      4884.0\n      5002.0\n      1.00\n    \n    \n      beta_g[0]\n      0.132\n      0.069\n      0.006\n      0.265\n      0.001\n      0.001\n      3185.0\n      3812.0\n      1.00\n    \n    \n      beta_g[1]\n      0.070\n      0.395\n      -0.651\n      0.841\n      0.006\n      0.006\n      4464.0\n      3851.0\n      1.00\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      beta[6, 1990_43]\n      0.547\n      0.073\n      0.412\n      0.686\n      0.001\n      0.001\n      2884.0\n      4276.0\n      1.00\n    \n    \n      beta[6, 1991_353]\n      -0.098\n      0.070\n      -0.225\n      0.034\n      0.001\n      0.001\n      2873.0\n      4230.0\n      1.00\n    \n    \n      beta[6, 1991_43]\n      0.115\n      0.073\n      -0.016\n      0.257\n      0.001\n      0.001\n      3073.0\n      4496.0\n      1.00\n    \n    \n      beta[6, 1992_353]\n      -0.115\n      0.070\n      -0.250\n      0.012\n      0.001\n      0.001\n      2861.0\n      4547.0\n      1.00\n    \n    \n      beta[6, 1992_43]\n      -0.033\n      0.091\n      -0.212\n      0.135\n      0.002\n      0.001\n      3726.0\n      4461.0\n      1.00\n    \n  \n\n80 rows × 9 columns\n\n\n\nAgain we can compare the performance metrics of the various models.\n\ncompare_df = az.compare({'unpooled': idata_sp_insur_unpooled, \n            'pooled': idata_sp_insur_pooled, \n            'mixed': idata_sp_insur_mixed})\n\naz.plot_compare(compare_df)\ncompare_df\n\n\n\n\n\n  \n    \n      \n      rank\n      elpd_loo\n      p_loo\n      elpd_diff\n      weight\n      se\n      dse\n      warning\n      scale\n    \n  \n  \n    \n      mixed\n      0\n      134.697068\n      61.393405\n      0.000000\n      1.000000e+00\n      1.382933\n      0.000000\n      True\n      log\n    \n    \n      unpooled\n      1\n      88.378689\n      73.735810\n      46.318379\n      0.000000e+00\n      1.267429\n      1.520694\n      True\n      log\n    \n    \n      pooled\n      2\n      -9.288780\n      5.995182\n      143.985848\n      6.050367e-10\n      5.414548\n      5.020442\n      False\n      log\n    \n  \n\n\n\n\n\n\n\nThe hierarchical splne model is by far the best fit to our data. What does it represent? Yes, splines are function approximation tools. Sure, extrapolation is dangerous, but in this domain where the range of the development curves is constrained they seem to offer an elegant way of articulating the structure of the risk profiles for various products.\n\n\nPlot the Posterior Predictive Checks\nWe can check how well the model can recapture the observed data.\n\ndef plot_ppc_splines(idata):\n    fig, axs = plt.subplots(2, 5, figsize=(20, 10), sharey=True)\n    axs = axs.flatten()\n    dev_periods = np.arange(1, 7, 1)\n    uniques, unique_codes = pd.factorize(loss_df['year_code'])\n    for y, c in zip(unique_codes, range(10)):\n        az.plot_hdi(dev_periods, idata['posterior_predictive']['lr'].sel(obs=c), color='firebrick', ax=axs[c], fill_kwargs={'alpha': 0.2}, hdi_prob=.89)\n        az.plot_hdi(dev_periods, idata['posterior_predictive']['lr'].sel(obs=c), color='firebrick', ax=axs[c], hdi_prob=0.5)\n        axs[c].scatter(dev_periods, loss_df[(loss_df['year_code'] == y)]['lr'], color='k', label='Actual Loss Ratio')\n        axs[c].plot(dev_periods, loss_df[(loss_df['year_code'] == y)]['lr'], color='k')\n        axs[c].set_title(f\"PPC 89% and 50% HDI: {y}\")\n        axs[c].set_ylabel(\"Loss Ratio\")\n        axs[c].legend();\n\nplot_ppc_splines(idata_sp_insur_mixed);\n\n\n\n\n\n\nPlot the Hierarchical Components\nIn the following plot we show similarly how to recapture the observed data, but additionally we can decompose the structure of the model in each case and extract baseline forecasts which would be our guide to future loss-ratio development curves in lieu of any other information.\n\nmu_g = idata_sp_insur_mixed.posterior.stack(draws=(\"chain\", \"draw\"))[\"mu_g\"]\n\nmu_i = idata_sp_insur_mixed.posterior.stack(draws=(\"chain\", \"draw\"))[\"mui\"]\n\nmu  = idata_sp_insur_mixed.posterior.stack(draws=(\"chain\", \"draw\"))['mu']\n\nbeta_grcode = idata_sp_insur_mixed.posterior.stack(draws=(\"chain\", \"draw\"))['beta_grcode']\n\ndev_periods = np.arange(1, 7, 1)\nuniques, unique_codes = pd.factorize(loss_df['year_code'])\n\nmosaic = \"\"\"\n         ABCDE\n         FGHIJ\n         KKKKK\n\"\"\"\nfig, axs = plt.subplot_mosaic(mosaic, sharey=True, \nfigsize=(20, 15))\naxs = [axs[k] for k in axs.keys()] \n\nmu_g_mean = mu_g.mean(dim='draws')\nfor y, c in zip(unique_codes, range(10)):\n    group_effect = 0\n    if '43' in y: \n        group_effect = beta_grcode.mean().item()\n    mu_i_mean = mu_i.sel(years=y).mean(dim='draws')\n    axs[c].plot(dev_periods, group_effect + mu_g_mean.values + mu_i_mean.values, label='Combined + E(grp_effect)', color='purple', linewidth=3.5)\n    axs[c].plot(dev_periods, group_effect + mu_g_mean.values, label='E(Hierarchical Baseline)', color='red', linestyle='--')\n    axs[c].plot(dev_periods,  group_effect + mu_i_mean.values, label='E(Year Specific Adjustment)', color='blue', linestyle='--')\n    axs[c].scatter(dev_periods, loss_df[(loss_df['year_code'] == y)]['lr'], color='k', label='Actual Loss Ratio')\n    az.plot_hdi(dev_periods,mu.sel(obs=c).T  , ax=axs[c], color='firebrick', fill_kwargs={'alpha': 0.2})\n    az.plot_hdi(dev_periods, mu.sel(obs=c).T , ax=axs[c], color='firebrick', fill_kwargs={'alpha': 0.5}, hdi_prob=.50)\n    axs[c].set_title(f\"Components for Year {y}\")\n    axs[c].set_ylabel(\"Loss Ratio\")\n    if (c == 0):\n         axs[c].legend()\n\naxs[10].plot(dev_periods, mu_g_mean.values, label='E(Hierarchical Baseline)', color='black')\naxs[10].plot(dev_periods, mu_g_mean.values + group_effect, label='E(Hierarchical Baseline) + E(grp_effect)', color='black', linestyle='--')\naz.plot_hdi(dev_periods, mu_g.T.values, color='slateblue', ax=axs[10], fill_kwargs={'alpha': 0.2})\naz.plot_hdi(dev_periods, mu_g.T.values + group_effect, color='magenta', ax=axs[10], fill_kwargs={'alpha': 0.2})\naz.plot_hdi(dev_periods, mu_g.T.values, color='slateblue', ax=axs[10], hdi_prob=.5)\naz.plot_hdi(dev_periods, mu_g.T.values  + group_effect, color='magenta', ax=axs[10], hdi_prob=.5)\naxs[10].set_title(\"Baseline Forecast Loss Ratio\")\naxs[10].legend();\n\n\n\n\nThis plot is a bit clunky, because we’re mixing expectations and posterior distributions over the parameters. The point is just to highlight the “compositional” structure of our model.\n\n\nPredicting Next Year’s Losses\nA better way to interrogate the implications of the model is to “push” forward different data through the posterior predictive distribution and derive a kind of ceteris paribus rule for accrual of losses.\n\nwith sp_insur_mixed: \n    pm.set_data({'grcode': np.ones(len(loss_df)), \n    })\n    idata_43 = pm.sample_posterior_predictive(idata_sp_insur_mixed, var_names=['lr'], extend_inferencedata =True)\n\nwith sp_insur_mixed: \n    pm.set_data({'grcode': np.zeros(len(loss_df))})\n    idata_353 = pm.sample_posterior_predictive(idata_sp_insur_mixed, var_names=['lr'], extend_inferencedata=True)\n\nidata_353\n\nSampling: [lr]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00<00:00]\n    \n    \n\n\nSampling: [lr]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00<00:00]\n    \n    \n\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:      (chain: 4, draw: 2000, splines: 7, years: 10, measurement: 6,\n                  obs: 60)\nCoordinates:\n  * chain        (chain) int64 0 1 2 3\n  * draw         (draw) int64 0 1 2 3 4 5 6 ... 1994 1995 1996 1997 1998 1999\n  * splines      (splines) int64 0 1 2 3 4 5 6\n  * years        (years) <U8 '1988_353' '1988_43' ... '1992_353' '1992_43'\n  * measurement  (measurement) int64 0 1 2 3 4 5\n  * obs          (obs) int64 0 0 0 0 0 0 1 1 1 1 1 1 ... 8 8 8 8 8 8 9 9 9 9 9 9\nData variables:\n    beta_g       (chain, draw, splines) float64 0.08213 0.08078 ... 0.6575\n    beta         (chain, draw, splines, years) float64 0.1395 ... -0.1751\n    beta_prem    (chain, draw) float64 -0.0156 -0.03719 ... -0.0874 -0.06236\n    beta_grcode  (chain, draw) float64 0.2377 0.2905 0.2288 ... 0.3751 0.3465\n    tau          (chain, draw) float64 0.5886 0.5384 0.4783 ... 0.6427 0.4663\n    sigma_i      (chain, draw) float64 0.2091 0.171 0.2062 ... 0.2064 0.1871\n    sigma        (chain, draw) float64 0.01148 0.009831 ... 0.005943 0.005919\n    mu_g         (chain, draw, measurement) float64 0.08213 0.3377 ... 0.6575\n    mui          (chain, draw, measurement, years) float64 0.1395 ... -0.1751\n    mu           (chain, draw, obs) float64 0.2271 0.509 ... 0.9015 0.9269\nAttributes:\n    created_at:                 2024-05-27T06:09:05.927942\n    arviz_version:              0.17.0\n    inference_library:          pymc\n    inference_library_version:  5.10.3\n    sampling_time:              227.7068202495575\n    tuning_steps:               1000xarray.DatasetDimensions:chain: 4draw: 2000splines: 7years: 10measurement: 6obs: 60Coordinates: (6)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999array([   0,    1,    2, ..., 1997, 1998, 1999])splines(splines)int640 1 2 3 4 5 6array([0, 1, 2, 3, 4, 5, 6])years(years)<U8'1988_353' '1988_43' ... '1992_43'array(['1988_353', '1988_43', '1989_353', '1989_43', '1990_353', '1990_43',\n       '1991_353', '1991_43', '1992_353', '1992_43'], dtype='<U8')measurement(measurement)int640 1 2 3 4 5array([0, 1, 2, 3, 4, 5])obs(obs)int640 0 0 0 0 0 1 1 ... 8 8 9 9 9 9 9 9array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n       4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n       8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9])Data variables: (10)beta_g(chain, draw, splines)float640.08213 0.08078 ... 0.1945 0.6575array([[[ 0.08213429,  0.08078411,  0.43921067, ...,  0.68826351,\n          0.76152486,  0.57747165],\n        [ 0.0729024 ,  0.34594954,  0.30222266, ...,  0.58894912,\n          0.70131994,  0.56389405],\n        [ 0.1296795 , -0.07301492,  0.59161604, ...,  0.89261377,\n          0.32944684,  0.70257189],\n        ...,\n        [-0.04196952,  0.17650917,  0.4217089 , ...,  0.62258926,\n          0.59316501,  0.79250849],\n        [-0.00829219, -0.1684174 ,  0.68416978, ...,  0.81418464,\n          0.38975666,  0.74581672],\n        [ 0.0817568 ,  0.21642253,  0.4364298 , ...,  0.70190257,\n          0.48913041,  0.62736766]],\n\n       [[ 0.1372587 ,  0.23309973,  0.49454757, ...,  0.65137438,\n          0.75772987,  0.61066381],\n        [ 0.01118497, -0.501921  ,  0.70257164, ...,  0.86052648,\n         -0.11635177,  0.5797538 ],\n        [ 0.19565787,  0.27753886,  0.68404036, ...,  0.96958051,\n          0.72825027,  0.73195019],\n...\n        [ 0.13938073, -0.02592998,  0.73699995, ...,  0.88935344,\n          0.379609  ,  0.81252247],\n        [ 0.11689267,  0.04623414,  0.67605048, ...,  0.90426575,\n          0.4922117 ,  0.70663987],\n        [ 0.12929436,  0.12568724,  0.61095241, ...,  0.75010102,\n          0.58792975,  0.74250617]],\n\n       [[ 0.1464027 ,  0.76694943, -0.06153482, ...,  0.15534104,\n          1.32746341,  0.66072813],\n        [ 0.14358004,  0.68940626, -0.11149118, ...,  0.17124399,\n          1.18011474,  0.71314634],\n        [ 0.05419155, -0.42883811,  1.14901708, ...,  1.26919344,\n          0.01586078,  0.63687761],\n        ...,\n        [ 0.18723226,  0.48566063,  0.23265438, ...,  0.5794988 ,\n          0.85015745,  0.86002011],\n        [ 0.03751469,  0.0086789 ,  0.55346192, ...,  0.8412314 ,\n          0.43493969,  0.65331407],\n        [ 0.05265733, -0.19628186,  0.64580651, ...,  0.90007346,\n          0.19448681,  0.65747667]]])beta(chain, draw, splines, years)float640.1395 -0.1399 ... -0.1164 -0.1751array([[[[ 1.39520480e-01, -1.39920512e-01,  1.59795716e-01, ...,\n          -4.88250518e-02,  1.49757586e-01, -7.87841938e-02],\n         [ 1.57822205e-01,  1.57221882e-01,  8.80230108e-02, ...,\n           1.84740484e-01,  1.40654109e-01,  1.05066007e-01],\n         [ 1.71863836e-01, -3.89547722e-01,  2.10933769e-01, ...,\n          -1.10459344e-02,  2.27594707e-02, -1.97138584e-01],\n         ...,\n         [-1.36108960e-01, -1.82625112e-01,  4.75130784e-02, ...,\n           4.45905407e-02, -3.94624785e-02, -1.40864962e-01],\n         [ 6.71214512e-02, -3.95470614e-01, -1.96252840e-01, ...,\n          -2.64734451e-02, -3.31353605e-01,  2.14460499e-02],\n         [ 1.11116707e-01, -1.45940962e-01,  3.79732609e-02, ...,\n           1.98087329e-01, -8.13679795e-03,  8.28432092e-02]],\n\n        [[ 1.46217147e-01, -1.45987241e-01,  1.33636350e-01, ...,\n          -1.04141339e-01,  1.56999588e-01, -1.73189166e-01],\n         [ 1.77012604e-01,  1.41028626e-02,  7.00230739e-02, ...,\n          -6.63265264e-02,  5.91449369e-02, -2.69751242e-01],\n         [ 1.41749001e-01, -3.79009870e-01,  1.25803026e-01, ...,\n           2.14529318e-02,  5.38979285e-02, -1.33607062e-01],\n...\n         [ 3.45479799e-02, -3.58625458e-01, -2.18827643e-01, ...,\n          -1.90020093e-01, -2.01458145e-01, -3.92093686e-01],\n         [-1.46373265e-01, -5.99317086e-02,  1.30108020e-01, ...,\n           9.35956673e-02, -9.62921164e-02, -1.04324354e-01],\n         [ 1.15647935e-02, -2.22568566e-01, -4.62102677e-02, ...,\n          -9.27173884e-03, -1.22732622e-01, -2.49213829e-01]],\n\n        [[ 1.63621902e-01, -1.51455454e-01,  1.65518502e-01, ...,\n          -1.56291791e-01,  1.56621714e-01, -2.49188309e-01],\n         [-1.31496227e-02,  1.33838754e-01, -1.48588957e-01, ...,\n          -8.37641665e-02,  9.22397066e-02, -3.70879910e-01],\n         [ 2.55943473e-01, -4.43680242e-01,  2.87918409e-01, ...,\n          -6.67540255e-02, -1.11330929e-03, -1.87938351e-01],\n         ...,\n         [ 1.05025544e-01, -2.90129799e-01,  1.05270857e-01, ...,\n          -4.30757899e-02, -1.14967846e-01, -1.59326571e-01],\n         [-3.26459908e-02, -8.45911318e-02, -1.55795510e-01, ...,\n           1.08681450e-01, -7.15263058e-02, -1.45134288e-01],\n         [ 8.68094467e-03, -2.62496397e-01, -4.86758721e-02, ...,\n           9.04772502e-03, -1.16437070e-01, -1.75097569e-01]]]])beta_prem(chain, draw)float64-0.0156 -0.03719 ... -0.06236array([[-0.01560439, -0.03719103, -0.04590153, ..., -0.04335828,\n        -0.06037419,  0.00715623],\n       [-0.08052324, -0.01663825, -0.07448925, ..., -0.04856727,\n        -0.00828554, -0.03699675],\n       [-0.01992275, -0.04479077, -0.05495835, ...,  0.03346062,\n        -0.00432382, -0.03984384],\n       [-0.06903017, -0.03561944, -0.06378486, ..., -0.04559814,\n        -0.08739882, -0.06236288]])beta_grcode(chain, draw)float640.2377 0.2905 ... 0.3751 0.3465array([[0.23773592, 0.29052938, 0.22882446, ..., 0.22874581, 0.27521708,\n        0.20095271],\n       [0.31476531, 0.33120918, 0.10765162, ..., 0.31997823, 0.19133539,\n        0.27806806],\n       [0.21980155, 0.17918741, 0.25618125, ..., 0.16353416, 0.13860823,\n        0.19789005],\n       [0.26856768, 0.26101595, 0.17110103, ..., 0.2398666 , 0.37505705,\n        0.34646743]])tau(chain, draw)float640.5886 0.5384 ... 0.6427 0.4663array([[0.58855469, 0.5383688 , 0.47833408, ..., 0.35456915, 1.00747773,\n        0.63732983],\n       [0.37739955, 0.53458212, 0.66920479, ..., 0.48638153, 0.59084796,\n        0.76687933],\n       [0.79058091, 0.78629887, 0.47156563, ..., 0.6155269 , 0.84141259,\n        0.90084556],\n       [0.74634296, 0.56718238, 0.63635355, ..., 0.53907184, 0.64268993,\n        0.46630978]])sigma_i(chain, draw)float640.2091 0.171 ... 0.2064 0.1871array([[0.20909747, 0.17103069, 0.20618882, ..., 0.211108  , 0.19893185,\n        0.21397901],\n       [0.19959592, 0.21750556, 0.23931261, ..., 0.20211561, 0.18453507,\n        0.20412235],\n       [0.19160775, 0.19335863, 0.1767455 , ..., 0.19974999, 0.19592067,\n        0.2032291 ],\n       [0.20263706, 0.19317583, 0.18770624, ..., 0.19822884, 0.20642422,\n        0.18713983]])sigma(chain, draw)float640.01148 0.009831 ... 0.005919array([[0.0114768 , 0.0098313 , 0.01093488, ..., 0.00679955, 0.00660215,\n        0.00630935],\n       [0.00763042, 0.00790286, 0.00785762, ..., 0.01620113, 0.01155728,\n        0.01278793],\n       [0.01091791, 0.01269563, 0.0112426 , ..., 0.0160019 , 0.00890353,\n        0.00816588],\n       [0.01126429, 0.01476696, 0.01066087, ..., 0.00525937, 0.00594278,\n        0.00591885]])mu_g(chain, draw, measurement)float640.08213 0.3377 ... 0.6124 0.6575array([[[ 0.08213429,  0.33768236,  0.49079526,  0.58195553,\n          0.6893857 ,  0.57747165],\n        [ 0.0729024 ,  0.3523693 ,  0.47549742,  0.57555192,\n          0.62509911,  0.56389405],\n        [ 0.1296795 ,  0.36973404,  0.53621521,  0.64158577,\n          0.66511154,  0.70257189],\n        ...,\n        [-0.04196952,  0.37081521,  0.53617268,  0.60818718,\n          0.61363189,  0.79250849],\n        [-0.00829219,  0.38796381,  0.54652576,  0.59643256,\n          0.63414497,  0.74581672],\n        [ 0.0817568 ,  0.38371482,  0.51608403,  0.6080783 ,\n          0.61884953,  0.62736766]],\n\n       [[ 0.1372587 ,  0.40331188,  0.4591502 ,  0.51765001,\n          0.65439136,  0.61066381],\n        [ 0.01118497,  0.26250702,  0.41669178,  0.47397997,\n          0.47131783,  0.5797538 ],\n        [ 0.19565787,  0.55795127,  0.69739504,  0.79814199,\n          0.8594259 ,  0.73195019],\n...\n        [ 0.13938073,  0.47041618,  0.61011331,  0.66575445,\n          0.68220337,  0.81252247],\n        [ 0.11689267,  0.45871305,  0.59006843,  0.67156949,\n          0.72612402,  0.70663987],\n        [ 0.12929436,  0.45995572,  0.6102857 ,  0.66215402,\n          0.68172986,  0.74250617]],\n\n       [[ 0.1464027 ,  0.32512369,  0.50687362,  0.58588734,\n          0.62142815,  0.66072813],\n        [ 0.14358004,  0.26546548,  0.4564482 ,  0.55671318,\n          0.57768676,  0.71314634],\n        [ 0.05419155,  0.53789895,  0.62458472,  0.66990082,\n          0.74346936,  0.63687761],\n        ...,\n        [ 0.18723226,  0.35784665,  0.45608484,  0.57637053,\n          0.66758408,  0.86002011],\n        [ 0.03751469,  0.38496915,  0.56650101,  0.66771488,\n          0.68016691,  0.65331407],\n        [ 0.05265733,  0.34718215,  0.49383633,  0.58342827,\n          0.61237385,  0.65747667]]])mui(chain, draw, measurement, years)float640.1395 -0.1399 ... -0.1164 -0.1751array([[[[ 1.39520480e-01, -1.39920512e-01,  1.59795716e-01, ...,\n          -4.88250518e-02,  1.49757586e-01, -7.87841938e-02],\n         [ 1.65873605e-01, -1.93497148e-01,  1.47566232e-01, ...,\n           7.87630258e-02,  5.26650556e-02, -5.69859310e-02],\n         [ 1.42490699e-01, -2.54958783e-01,  9.21905855e-02, ...,\n           1.17105881e-01, -7.24684266e-03,  4.86017675e-03],\n         [ 3.83479319e-02, -1.91999665e-01,  3.42301192e-02, ...,\n           1.33318539e-01, -3.34724977e-02,  2.27443612e-02],\n         [-3.72703490e-02, -2.46357200e-01, -3.30969747e-02, ...,\n           4.52311029e-02, -1.28371119e-01, -5.07836293e-02],\n         [ 1.11116707e-01, -1.45940962e-01,  3.79732609e-02, ...,\n           1.98087329e-01, -8.13679795e-03,  8.28432092e-02]],\n\n        [[ 1.46217147e-01, -1.45987241e-01,  1.33636350e-01, ...,\n          -1.04141339e-01,  1.56999588e-01, -1.73189166e-01],\n         [ 1.50806947e-01, -2.30291464e-01,  1.09203466e-01, ...,\n           9.20703581e-03,  4.54642291e-02, -1.56726472e-01],\n         [ 1.22668583e-01, -2.49806973e-01,  1.13781261e-01, ...,\n           8.35633149e-02,  1.22118628e-03, -5.88513761e-02],\n         [ 6.62855094e-02, -2.01179440e-01,  4.96226930e-02, ...,\n...\n          -9.33073690e-02, -1.27232209e-01, -3.13057795e-01],\n         [-5.68682820e-02, -2.92825044e-01, -7.16499541e-02, ...,\n          -9.83126153e-02, -1.67003142e-01, -3.19813169e-01],\n         [-4.65304179e-02, -2.48490531e-01, -8.00679145e-02, ...,\n          -8.15331960e-02, -1.62924280e-01, -2.87153932e-01],\n         [ 1.15647935e-02, -2.22568566e-01, -4.62102677e-02, ...,\n          -9.27173884e-03, -1.22732622e-01, -2.49213829e-01]],\n\n        [[ 1.63621902e-01, -1.51455454e-01,  1.65518502e-01, ...,\n          -1.56291791e-01,  1.56621714e-01, -2.49188309e-01],\n         [ 1.36287416e-01, -2.29995142e-01,  1.06986574e-01, ...,\n          -5.57859181e-02,  2.19595602e-02, -2.41298686e-01],\n         [ 9.00781502e-02, -2.78835663e-01,  7.79656367e-02, ...,\n           2.00867494e-03, -3.20768389e-02, -1.75140835e-01],\n         [ 3.93295341e-02, -2.29924865e-01,  1.67347175e-02, ...,\n           1.21352879e-02, -7.20658012e-02, -1.62981950e-01],\n         [ 4.44384071e-02, -2.10066162e-01,  8.83961075e-04, ...,\n           1.70640485e-02, -9.33275239e-02, -1.55367387e-01],\n         [ 8.68094467e-03, -2.62496397e-01, -4.86758721e-02, ...,\n           9.04772502e-03, -1.16437070e-01, -1.75097569e-01]]]])mu(chain, draw, obs)float640.2271 0.509 ... 0.9015 0.9269array([[[0.22707658, 0.50897777, 0.63870776, ..., 0.86695823,\n         0.9008604 , 0.9225732 ],\n        [0.2320417 , 0.5160984 , 0.61108815, ..., 0.85957629,\n         0.88786053, 0.91962846],\n        [0.22591164, 0.49834019, 0.63366749, ..., 0.87302862,\n         0.8952962 , 0.91718248],\n        ...,\n        [0.21752702, 0.50863783, 0.62085732, ..., 0.85667073,\n         0.90717039, 0.91667823],\n        [0.24668158, 0.50946944, 0.61496311, ..., 0.85950557,\n         0.91499729, 0.93394897],\n        [0.22803856, 0.51225596, 0.60431136, ..., 0.85384508,\n         0.91546043, 0.92367727]],\n\n       [[0.20798215, 0.49653525, 0.61163146, ..., 0.84536501,\n         0.90679466, 0.9304019 ],\n        [0.21296999, 0.52098307, 0.6079544 , ..., 0.85788457,\n         0.90174194, 0.91528221],\n        [0.21005812, 0.50465123, 0.59585638, ..., 0.85754494,\n         0.90510048, 0.92520922],\n...\n        [0.23575948, 0.52517243, 0.61730376, ..., 0.86419416,\n         0.88562497, 0.92776999],\n        [0.23289817, 0.51858843, 0.61101066, ..., 0.86935524,\n         0.89006191, 0.92026702],\n        [0.24012795, 0.50146092, 0.611026  , ..., 0.85900662,\n         0.9094241 , 0.916668  ]],\n\n       [[0.24430336, 0.49530459, 0.60862876, ..., 0.86711487,\n         0.89899689, 0.89876914],\n        [0.23968997, 0.50137434, 0.61292425, ..., 0.87145524,\n         0.90762533, 0.91330743],\n        [0.22874435, 0.5038266 , 0.63214802, ..., 0.86415242,\n         0.900888  , 0.92221532],\n        ...,\n        [0.22737739, 0.50688812, 0.61785909, ..., 0.86241171,\n         0.90291879, 0.93063882],\n        [0.2253554 , 0.52076927, 0.61201075, ..., 0.86030662,\n         0.90541789, 0.91650515],\n        [0.23794743, 0.50513776, 0.60558268, ..., 0.86491745,\n         0.9014776 , 0.92685024]]])Indexes: (6)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999],\n      dtype='int64', name='draw', length=2000))splinesPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6], dtype='int64', name='splines'))yearsPandasIndexPandasIndex(Index(['1988_353', '1988_43', '1989_353', '1989_43', '1990_353', '1990_43',\n       '1991_353', '1991_43', '1992_353', '1992_43'],\n      dtype='object', name='years'))measurementPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5], dtype='int64', name='measurement'))obsPandasIndexPandasIndex(Index([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n       4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n       8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9],\n      dtype='int64', name='obs'))Attributes: (6)created_at :2024-05-27T06:09:05.927942arviz_version :0.17.0inference_library :pymcinference_library_version :5.10.3sampling_time :227.7068202495575tuning_steps :1000\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 4, draw: 2000, obs: 60)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 ... 1993 1994 1995 1996 1997 1998 1999\n  * obs      (obs) int64 0 0 0 0 0 0 1 1 1 1 1 1 2 ... 7 8 8 8 8 8 8 9 9 9 9 9 9\nData variables:\n    lr       (chain, draw, obs) float64 0.233 0.5237 0.6346 ... 0.8981 0.9297\nAttributes:\n    created_at:                 2024-05-27T06:09:08.031307\n    arviz_version:              0.17.0\n    inference_library:          pymc\n    inference_library_version:  5.10.3xarray.DatasetDimensions:chain: 4draw: 2000obs: 60Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999array([   0,    1,    2, ..., 1997, 1998, 1999])obs(obs)int640 0 0 0 0 0 1 1 ... 8 8 9 9 9 9 9 9array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n       4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n       8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9])Data variables: (1)lr(chain, draw, obs)float640.233 0.5237 ... 0.8981 0.9297array([[[0.2329779 , 0.52369396, 0.63459651, ..., 0.88727935,\n         0.89708185, 0.92159448],\n        [0.22917617, 0.52664668, 0.59261549, ..., 0.86452905,\n         0.90716367, 0.92848945],\n        [0.21781397, 0.4993653 , 0.62570146, ..., 0.87851744,\n         0.91024927, 0.90860314],\n        ...,\n        [0.21017767, 0.50642437, 0.61274687, ..., 0.86717561,\n         0.90019066, 0.92583528],\n        [0.25375665, 0.50843402, 0.60506852, ..., 0.85534182,\n         0.91866865, 0.94337612],\n        [0.22315311, 0.50562829, 0.60910443, ..., 0.85065712,\n         0.91298724, 0.92813007]],\n\n       [[0.21910589, 0.51068439, 0.60243355, ..., 0.82541821,\n         0.91008109, 0.93325851],\n        [0.20831853, 0.51436674, 0.61926284, ..., 0.85784094,\n         0.91192378, 0.91751401],\n        [0.211794  , 0.50545555, 0.59703336, ..., 0.84964635,\n         0.90139699, 0.93754076],\n...\n        [0.20342139, 0.53073673, 0.58714448, ..., 0.85629361,\n         0.86931632, 0.93170111],\n        [0.22936355, 0.51296337, 0.62169933, ..., 0.86757595,\n         0.87222132, 0.91036641],\n        [0.23170526, 0.50702719, 0.615165  , ..., 0.86935216,\n         0.89592565, 0.9289633 ]],\n\n       [[0.23720474, 0.49880944, 0.60052934, ..., 0.86819616,\n         0.89829526, 0.89245385],\n        [0.21610787, 0.49419633, 0.59675283, ..., 0.87135881,\n         0.91189684, 0.92522446],\n        [0.21011217, 0.48862193, 0.62340207, ..., 0.85734297,\n         0.90236241, 0.90941639],\n        ...,\n        [0.22319361, 0.50040966, 0.62049363, ..., 0.85878946,\n         0.90237754, 0.93678927],\n        [0.21677194, 0.51807386, 0.61236904, ..., 0.85692388,\n         0.90624593, 0.91623522],\n        [0.23988418, 0.5116527 , 0.59962956, ..., 0.86614767,\n         0.89813962, 0.92968554]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999],\n      dtype='int64', name='draw', length=2000))obsPandasIndexPandasIndex(Index([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n       4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n       8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9],\n      dtype='int64', name='obs'))Attributes: (4)created_at :2024-05-27T06:09:08.031307arviz_version :0.17.0inference_library :pymcinference_library_version :5.10.3\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 4, draw: 2000, obs: 60)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 ... 1993 1994 1995 1996 1997 1998 1999\n  * obs      (obs) int64 0 0 0 0 0 0 1 1 1 1 1 1 2 ... 7 8 8 8 8 8 8 9 9 9 9 9 9\nData variables:\n    lr       (chain, draw, obs) float64 3.493 3.52 1.659 ... 3.711 4.137 4.128\nAttributes:\n    created_at:                 2024-05-27T06:09:06.204725\n    arviz_version:              0.17.0\n    inference_library:          pymc\n    inference_library_version:  5.10.3xarray.DatasetDimensions:chain: 4draw: 2000obs: 60Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999array([   0,    1,    2, ..., 1997, 1998, 1999])obs(obs)int640 0 0 0 0 0 1 1 ... 8 8 9 9 9 9 9 9array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n       4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n       8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9])Data variables: (1)lr(chain, draw, obs)float643.493 3.52 1.659 ... 4.137 4.128array([[[ 3.49346525,  3.51967093,  1.65940483, ...,  3.3082412 ,\n          3.51674037,  3.53525774],\n        [ 3.69631071,  3.6046685 ,  3.55729917, ...,  3.70154431,\n          2.3967742 ,  3.58347456],\n        [ 3.4934784 ,  2.84680694,  2.35000913, ...,  2.77423593,\n          3.29784217,  3.37659525],\n        ...,\n        [ 2.14258977,  3.96834899,  3.85707249, ...,  4.01314636,\n          3.94561888,  3.42040059],\n        [ 1.23863774,  4.04264235,  4.07774621, ...,  4.09852065,\n          2.65095731,  3.06425839],\n        [ 4.04510066,  4.14335   ,  2.31136224, ...,  3.81265832,\n          2.42504945,  4.13947279]],\n\n       [[-0.54742785,  1.97315188,  3.76142259, ...,  2.35947554,\n          3.87719424,  3.6514542 ],\n        [ 1.35250133,  3.23660373,  3.35059476, ...,  3.91158231,\n          3.88923252,  3.25015959],\n        [ 0.41505901,  3.52118601,  0.50965773, ...,  3.91012279,\n          3.91261702,  3.9225444 ],\n...\n        [ 3.16969071,  2.86342788,  3.21451326, ...,  3.16348367,\n          2.57445527,  3.19445904],\n        [ 3.77677685,  3.50595189,  3.61919133, ...,  3.12638761,\n          2.62018425,  3.69252388],\n        [ 3.24809316,  3.09764755,  3.67232438, ...,  3.88885252,\n          3.64764734,  3.43590068]],\n\n       [[ 2.85754207,  2.50362844,  3.32921447, ...,  3.30786291,\n          3.47806162,  0.97031193],\n        [ 3.11861345,  3.05038605,  3.26872831, ...,  2.94087519,\n          3.26203195,  3.01224386],\n        [ 3.60209919,  3.34722452,  2.5311683 , ...,  3.50557154,\n          3.58614045,  3.60045959],\n        ...,\n        [ 4.10655831,  3.90449352,  4.29030831, ...,  4.11874516,\n          4.31624387,  3.63427395],\n        [ 3.77393089,  3.05063185,  3.93392071, ...,  4.18257176,\n          4.16737028,  3.31520253],\n        [ 3.49856554,  3.58985088,  2.54068726, ...,  3.71137697,\n          4.136818  ,  4.12777749]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999],\n      dtype='int64', name='draw', length=2000))obsPandasIndexPandasIndex(Index([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n       4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n       8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9],\n      dtype='int64', name='obs'))Attributes: (4)created_at :2024-05-27T06:09:06.204725arviz_version :0.17.0inference_library :pymcinference_library_version :5.10.3\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                (chain: 4, draw: 2000)\nCoordinates:\n  * chain                  (chain) int64 0 1 2 3\n  * draw                   (draw) int64 0 1 2 3 4 5 ... 1995 1996 1997 1998 1999\nData variables: (12/17)\n    perf_counter_diff      (chain, draw) float64 0.08216 0.0801 ... 0.08181\n    lp                     (chain, draw) float64 184.4 196.5 ... 216.8 224.4\n    step_size_bar          (chain, draw) float64 0.005908 0.005908 ... 0.005495\n    smallest_eigval        (chain, draw) float64 nan nan nan nan ... nan nan nan\n    acceptance_rate        (chain, draw) float64 0.9998 0.9855 ... 0.944 0.9537\n    reached_max_treedepth  (chain, draw) bool False False False ... True False\n    ...                     ...\n    largest_eigval         (chain, draw) float64 nan nan nan nan ... nan nan nan\n    energy                 (chain, draw) float64 -141.4 -154.8 ... -177.6 -175.2\n    tree_depth             (chain, draw) int64 10 10 10 10 10 ... 10 10 10 10 10\n    diverging              (chain, draw) bool False False False ... False False\n    index_in_trajectory    (chain, draw) int64 -352 387 -740 ... 790 521 -683\n    process_time_diff      (chain, draw) float64 0.08216 0.08007 ... 0.0818\nAttributes:\n    created_at:                 2024-05-27T06:09:05.940582\n    arviz_version:              0.17.0\n    inference_library:          pymc\n    inference_library_version:  5.10.3\n    sampling_time:              227.7068202495575\n    tuning_steps:               1000xarray.DatasetDimensions:chain: 4draw: 2000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999array([   0,    1,    2, ..., 1997, 1998, 1999])Data variables: (17)perf_counter_diff(chain, draw)float640.08216 0.0801 ... 0.08166 0.08181array([[0.08215638, 0.08010133, 0.08184954, ..., 0.08156783, 0.08167758,\n        0.08158583],\n       [0.08136029, 0.08029138, 0.08061071, ..., 0.08166729, 0.08090975,\n        0.08182967],\n       [0.08155483, 0.08204283, 0.08018262, ..., 0.08148587, 0.08159787,\n        0.08199237],\n       [0.080971  , 0.08061054, 0.07909571, ..., 0.08166879, 0.08165904,\n        0.08180558]])lp(chain, draw)float64184.4 196.5 191.3 ... 216.8 224.4array([[184.36486414, 196.53933231, 191.27157196, ..., 200.00169387,\n        204.82818005, 214.33525947],\n       [192.7863688 , 187.12040812, 189.2459854 , ..., 155.91231505,\n        178.6049379 , 187.65005752],\n       [188.86778753, 183.74271399, 179.99640544, ..., 167.53100342,\n        193.17434736, 201.62651467],\n       [178.50910416, 177.79847515, 184.59234725, ..., 220.75603875,\n        216.79913757, 224.43676879]])step_size_bar(chain, draw)float640.005908 0.005908 ... 0.005495array([[0.0059085 , 0.0059085 , 0.0059085 , ..., 0.0059085 , 0.0059085 ,\n        0.0059085 ],\n       [0.00804049, 0.00804049, 0.00804049, ..., 0.00804049, 0.00804049,\n        0.00804049],\n       [0.00597803, 0.00597803, 0.00597803, ..., 0.00597803, 0.00597803,\n        0.00597803],\n       [0.00549452, 0.00549452, 0.00549452, ..., 0.00549452, 0.00549452,\n        0.00549452]])smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])acceptance_rate(chain, draw)float640.9998 0.9855 ... 0.944 0.9537array([[0.99979105, 0.98550281, 0.97845882, ..., 0.98798177, 0.99585789,\n        0.91300196],\n       [0.88604944, 0.92966456, 0.9978866 , ..., 0.97677582, 0.99334874,\n        0.9967715 ],\n       [0.99347033, 0.99333916, 0.95811576, ..., 0.97819394, 0.99938068,\n        0.99843286],\n       [0.99895433, 0.99905734, 0.97817478, ..., 0.94058163, 0.94399245,\n        0.95368363]])reached_max_treedepth(chain, draw)boolFalse False False ... True Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [ True, False, False, ...,  True, False,  True],\n       [False, False, False, ..., False,  True, False]])step_size(chain, draw)float640.004065 0.004065 ... 0.007016array([[0.00406536, 0.00406536, 0.00406536, ..., 0.00406536, 0.00406536,\n        0.00406536],\n       [0.00551699, 0.00551699, 0.00551699, ..., 0.00551699, 0.00551699,\n        0.00551699],\n       [0.00537033, 0.00537033, 0.00537033, ..., 0.00537033, 0.00537033,\n        0.00537033],\n       [0.00701583, 0.00701583, 0.00701583, ..., 0.00701583, 0.00701583,\n        0.00701583]])perf_counter_start(chain, draw)float642.361e+06 2.361e+06 ... 2.361e+06array([[2361318.79279567, 2361318.87508063, 2361318.95529054, ...,\n        2361479.04572258, 2361479.12740954, 2361479.20918579],\n       [2361312.089392  , 2361312.17086767, 2361312.25125425, ...,\n        2361457.72276992, 2361457.80452825, 2361457.88554038],\n       [2361312.71388937, 2361312.79556071, 2361312.87769787, ...,\n        2361473.83453225, 2361473.91610921, 2361473.99781033],\n       [2361315.01769779, 2361315.098796  , 2361315.17949475, ...,\n        2361475.50722583, 2361475.58900379, 2361475.67075933]])n_steps(chain, draw)float641.023e+03 1.023e+03 ... 1.023e+03array([[1023., 1023., 1023., ..., 1023., 1023., 1023.],\n       [1023., 1023., 1023., ..., 1023., 1023., 1023.],\n       [1023., 1023., 1023., ..., 1023., 1023., 1023.],\n       [1023., 1023., 1023., ..., 1023., 1023., 1023.]])max_energy_error(chain, draw)float64-0.03173 -0.06478 ... 0.1507 0.15array([[-0.03173012, -0.06477837, -0.2075868 , ..., -0.24468182,\n        -0.11603018,  0.29430899],\n       [-0.84910791, -0.52014748, -0.76125186, ..., -0.32416101,\n        -0.36622319, -0.5667363 ],\n       [-0.15657836, -0.25783916, -0.18139475, ...,  0.06635242,\n        -0.05819053, -0.06931609],\n       [-0.13684815, -0.08643352, -0.15030015, ...,  0.17884379,\n         0.15065315,  0.14999317]])energy_error(chain, draw)float64-0.02417 0.02804 ... -0.06934array([[-0.02417412,  0.02804133, -0.01108127, ..., -0.03346141,\n        -0.00129006,  0.24695244],\n       [ 0.05038906,  0.22945603, -0.36579941, ...,  0.04978283,\n         0.07012965, -0.27605762],\n       [ 0.00575441, -0.04686598, -0.03784106, ...,  0.00566893,\n         0.00287121, -0.0143361 ],\n       [-0.00074252, -0.00241762, -0.06416166, ..., -0.02266332,\n         0.07892408, -0.06933777]])largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])energy(chain, draw)float64-141.4 -154.8 ... -177.6 -175.2array([[-141.35781495, -154.82042184, -158.50830829, ..., -169.51286638,\n        -161.92045136, -167.16629184],\n       [-161.91301831, -144.81534601, -149.29314566, ..., -124.68284739,\n        -126.4725391 , -140.03003424],\n       [-142.80921945, -146.5387865 , -146.8058751 , ..., -129.37532231,\n        -140.67959809, -156.69836175],\n       [-139.25837416, -135.14965861, -136.52022668, ..., -188.92859374,\n        -177.63265783, -175.21581418]])tree_depth(chain, draw)int6410 10 10 10 10 ... 10 10 10 10 10array([[10, 10, 10, ..., 10, 10, 10],\n       [10, 10, 10, ..., 10, 10, 10],\n       [10, 10, 10, ..., 10, 10, 10],\n       [10, 10, 10, ..., 10, 10, 10]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])index_in_trajectory(chain, draw)int64-352 387 -740 273 ... 790 521 -683array([[-352,  387, -740, ...,  643,  428, -650],\n       [ 453,  487,  609, ...,  749, -425,  216],\n       [-596,  472,  590, ..., -161,  452,  263],\n       [-220,  272, -830, ...,  790,  521, -683]])process_time_diff(chain, draw)float640.08216 0.08007 ... 0.08164 0.0818array([[0.082157, 0.080075, 0.081789, ..., 0.081559, 0.081672, 0.08158 ],\n       [0.08135 , 0.080263, 0.080593, ..., 0.081654, 0.080874, 0.081816],\n       [0.081545, 0.082021, 0.080146, ..., 0.081464, 0.081571, 0.081977],\n       [0.080926, 0.080607, 0.079093, ..., 0.081597, 0.081638, 0.0818  ]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999],\n      dtype='int64', name='draw', length=2000))Attributes: (6)created_at :2024-05-27T06:09:05.940582arviz_version :0.17.0inference_library :pymcinference_library_version :5.10.3sampling_time :227.7068202495575tuning_steps :1000\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (obs: 60)\nCoordinates:\n  * obs      (obs) int64 0 0 0 0 0 0 1 1 1 1 1 1 2 ... 7 8 8 8 8 8 8 9 9 9 9 9 9\nData variables:\n    lr       (obs) float64 0.2309 0.5117 0.6164 0.6386 ... 0.859 0.9038 0.9244\nAttributes:\n    created_at:                 2024-05-27T06:09:05.943015\n    arviz_version:              0.17.0\n    inference_library:          pymc\n    inference_library_version:  5.10.3xarray.DatasetDimensions:obs: 60Coordinates: (1)obs(obs)int640 0 0 0 0 0 1 1 ... 8 8 9 9 9 9 9 9array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n       4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n       8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9])Data variables: (1)lr(obs)float640.2309 0.5117 ... 0.9038 0.9244array([0.23088384, 0.51173309, 0.61639972, 0.63858884, 0.67259086,\n       0.68993774, 0.13897597, 0.34796238, 0.45036573, 0.59561129,\n       0.64263323, 0.64263323, 0.2345894 , 0.47973401, 0.58713321,\n       0.62354866, 0.63473718, 0.63742875, 0.25277402, 0.47253045,\n       0.64005413, 0.69797023, 0.74776725, 0.80270636, 0.29829006,\n       0.51463926, 0.59872363, 0.66151898, 0.68509768, 0.70209967,\n       0.33072662, 0.79244053, 1.12088628, 1.31753014, 1.40029326,\n       1.42440534, 0.25679105, 0.44820942, 0.53957139, 0.57970674,\n       0.59681361, 0.59784754, 0.25876918, 0.65744596, 0.86254492,\n       0.94998004, 0.98790852, 1.03097017, 0.24606574, 0.41036624,\n       0.49781029, 0.54771644, 0.56508975, 0.57923865, 0.25779626,\n       0.54739102, 0.76565216, 0.85900276, 0.90375243, 0.9244402 ])Indexes: (1)obsPandasIndexPandasIndex(Index([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n       4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n       8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9],\n      dtype='int64', name='obs'))Attributes: (4)created_at :2024-05-27T06:09:05.943015arviz_version :0.17.0inference_library :pymcinference_library_version :5.10.3\n                      \n                  \n            \n            \n            \n                  \n                  constant_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:       (Bg_dim_0: 6, Bg_dim_1: 7, Bi_dim_0: 6, Bi_dim_1: 7,\n                   prem_dim_0: 60, grcode_dim_0: 60)\nCoordinates:\n  * Bg_dim_0      (Bg_dim_0) int64 0 1 2 3 4 5\n  * Bg_dim_1      (Bg_dim_1) int64 0 1 2 3 4 5 6\n  * Bi_dim_0      (Bi_dim_0) int64 0 1 2 3 4 5\n  * Bi_dim_1      (Bi_dim_1) int64 0 1 2 3 4 5 6\n  * prem_dim_0    (prem_dim_0) int64 0 1 2 3 4 5 6 7 ... 52 53 54 55 56 57 58 59\n  * grcode_dim_0  (grcode_dim_0) int64 0 1 2 3 4 5 6 7 ... 53 54 55 56 57 58 59\nData variables:\n    Bg            (Bg_dim_0, Bg_dim_1) float64 1.0 0.0 0.0 0.0 ... 0.0 0.0 1.0\n    Bi            (Bi_dim_0, Bi_dim_1) float64 1.0 0.0 0.0 0.0 ... 0.0 0.0 1.0\n    prem          (prem_dim_0) float64 -0.3475 -0.3475 -0.3475 ... -1.572 -1.572\n    grcode        (grcode_dim_0) float64 0.0 0.0 0.0 0.0 0.0 ... 1.0 1.0 1.0 1.0\nAttributes:\n    created_at:                 2024-05-27T06:09:05.943907\n    arviz_version:              0.17.0\n    inference_library:          pymc\n    inference_library_version:  5.10.3xarray.DatasetDimensions:Bg_dim_0: 6Bg_dim_1: 7Bi_dim_0: 6Bi_dim_1: 7prem_dim_0: 60grcode_dim_0: 60Coordinates: (6)Bg_dim_0(Bg_dim_0)int640 1 2 3 4 5array([0, 1, 2, 3, 4, 5])Bg_dim_1(Bg_dim_1)int640 1 2 3 4 5 6array([0, 1, 2, 3, 4, 5, 6])Bi_dim_0(Bi_dim_0)int640 1 2 3 4 5array([0, 1, 2, 3, 4, 5])Bi_dim_1(Bi_dim_1)int640 1 2 3 4 5 6array([0, 1, 2, 3, 4, 5, 6])prem_dim_0(prem_dim_0)int640 1 2 3 4 5 6 ... 54 55 56 57 58 59array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59])grcode_dim_0(grcode_dim_0)int640 1 2 3 4 5 6 ... 54 55 56 57 58 59array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59])Data variables: (4)Bg(Bg_dim_0, Bg_dim_1)float641.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 1.0array([[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 3.08571429e-01, 5.69339736e-01, 1.21488595e-01,\n        6.00240096e-04, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.14285714e-02, 4.09819928e-01, 5.03721489e-01,\n        7.50300120e-02, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 7.50300120e-02, 5.03721489e-01,\n        4.09819928e-01, 1.14285714e-02, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 6.00240096e-04, 1.21488595e-01,\n        5.69339736e-01, 3.08571429e-01, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n        0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])Bi(Bi_dim_0, Bi_dim_1)float641.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 1.0array([[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 3.08571429e-01, 5.69339736e-01, 1.21488595e-01,\n        6.00240096e-04, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.14285714e-02, 4.09819928e-01, 5.03721489e-01,\n        7.50300120e-02, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 7.50300120e-02, 5.03721489e-01,\n        4.09819928e-01, 1.14285714e-02, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 6.00240096e-04, 1.21488595e-01,\n        5.69339736e-01, 3.08571429e-01, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n        0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])prem(prem_dim_0)float64-0.3475 -0.3475 ... -1.572 -1.572array([-0.34745346, -0.34745346, -0.34745346, -0.34745346, -0.34745346,\n       -0.34745346,  1.72234378,  1.72234378,  1.72234378,  1.72234378,\n        1.72234378,  1.72234378, -0.3654406 , -0.3654406 , -0.3654406 ,\n       -0.3654406 , -0.3654406 , -0.3654406 ,  1.40460971,  1.40460971,\n        1.40460971,  1.40460971,  1.40460971,  1.40460971, -0.5486773 ,\n       -0.5486773 , -0.5486773 , -0.5486773 , -0.5486773 , -0.5486773 ,\n        1.12110923,  1.12110923,  1.12110923,  1.12110923,  1.12110923,\n        1.12110923, -0.63582788, -0.63582788, -0.63582788, -0.63582788,\n       -0.63582788, -0.63582788, -0.20123544, -0.20123544, -0.20123544,\n       -0.20123544, -0.20123544, -0.20123544, -0.5779209 , -0.5779209 ,\n       -0.5779209 , -0.5779209 , -0.5779209 , -0.5779209 , -1.57150713,\n       -1.57150713, -1.57150713, -1.57150713, -1.57150713, -1.57150713])grcode(grcode_dim_0)float640.0 0.0 0.0 0.0 ... 1.0 1.0 1.0 1.0array([0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n       0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n       1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n       0., 0., 0., 1., 1., 1., 1., 1., 1.])Indexes: (6)Bg_dim_0PandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5], dtype='int64', name='Bg_dim_0'))Bg_dim_1PandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6], dtype='int64', name='Bg_dim_1'))Bi_dim_0PandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5], dtype='int64', name='Bi_dim_0'))Bi_dim_1PandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6], dtype='int64', name='Bi_dim_1'))prem_dim_0PandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59],\n      dtype='int64', name='prem_dim_0'))grcode_dim_0PandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59],\n      dtype='int64', name='grcode_dim_0'))Attributes: (4)created_at :2024-05-27T06:09:05.943907arviz_version :0.17.0inference_library :pymcinference_library_version :5.10.3\n                      \n                  \n            \n            \n              \n            \n            \n\n\nEven here though we want to average the curves over the specific years in the data and abstract a view of the model implications under different counterfactual settings. Or put another way, we want a view of the average development curve between years, not within a year. If we want to predict the novel insurance loss for next year we need some way to aggregate between the annual trajectories. Here we define a helper function to effect this step.\n\ndef get_posterior_predictive_curve(idata, prem=2, grcode=1):\n    weighted_splines = [np.dot(np.asfortranarray(Bi), az.extract(idata['posterior']['beta'])['beta'].values[:, :, i]) for i in range(2000)]\n\n    weighted_splines_1 = [np.dot(np.asfortranarray(Bg), az.extract(idata['posterior']['beta_g'])['beta_g'].values[:, i]) for i in range(2000)]\n\n    beta_grcode = az.extract(idata['posterior']['beta_grcode'])['beta_grcode']\n\n    beta_prem = az.extract(idata['posterior']['beta_prem'])['beta_prem']\n    df1 = pd.DataFrame([beta_prem.values[i]*prem + beta_grcode.values[i]*grcode for i in range(2000)]).T\n\n\n    ## How do we averaging over the years to get\n    ## a view of a new development period?\n    #df = pd.concat([pd.DataFrame(weighted_splines_1[i].T + weighted_splines[i].T).mean() for i in range(1000)], axis=1)\n\n    ## Sample random group each draw from posterior\n    df = pd.concat([pd.DataFrame((weighted_splines_1[i].T + weighted_splines[i].T)[np.random.choice(list(range(10))), :]) for i in range(2000)], axis=1)\n\n    ## Average random subset of of groups\n    #df = pd.concat([pd.DataFrame((weighted_splines_1[i].T + weighted_splines[i].T)[np.random.choice(list(range(10)), 5), :]).mean() for i in range(2000)], axis=1)\n\n    df = df1.iloc[0].values + df\n\n    return df\n\npred_df_1 = get_posterior_predictive_curve(idata_43, prem=1, grcode=1)\npred_df_0 = get_posterior_predictive_curve(idata_353, prem=1, grcode=0) \n\nfig, ax = plt.subplots(figsize=(9, 7), sharey=True)\n\naz.plot_hdi(range(6), pred_df_0.values.T, ax=ax, color='slateblue', smooth=False, fill_kwargs={'alpha': 0.2})\naz.plot_hdi(range(6), pred_df_1.values.T, ax=ax, color='firebrick', smooth=False, fill_kwargs={'alpha': 0.2})\naz.plot_hdi(range(6), pred_df_0.values.T, ax=ax, color='slateblue', smooth=False, fill_kwargs={'alpha': 0.5}, hdi_prob=.5)\naz.plot_hdi(range(6), pred_df_1.values.T, ax=ax, color='firebrick', smooth=False, fill_kwargs={'alpha': 0.5}, hdi_prob=.5)\nax.plot(pred_df_0.mean(axis=1), linestyle='-', color='k', linewidth=4, label='grcode 353 prem 1')\n\nax.plot(pred_df_1.mean(axis=1), linestyle='--', color='grey', linewidth=4, label='grcode 43 prem 1')\n\nax.set_title(\"Posterior Samples of the Trajectories \\n Under different Counterfactual settings\")\nax.set_ylabel(\"Loss Ratio\")\nax.set_xlabel(\"Development Period\")\nax.legend();\n\n\n\n\nNote how we’ve varied the predictor inputs to showcase the expected realisation under different product codes. In this manner we can construct a hierarchical spline model with informed by the historic trajectories of the development curves but with predictions modulated by more recent information."
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#conclusion",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#conclusion",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve seen the application of splines as univariate smoothers to approximate wiggly curves of arbitrary shape. We’ve also tried gaussian process approximations of the same univariate functions. The suggested flexibility of both methods is a strength, but we need to be careful where splines have a tendency to over-fit to individual curves. As such we have tried to show that we can incorporate spline basis modelling in a hierarchical bayesian model and recover more compelling posterior predictive checks and additionally derive predictiions from the mixed variant of the hierarhical model which helps us understand the implications of the data for generic forecasts of insurance loss curves. Finally we showed how splines can be used additively to model non-linear functions of multiple covariates. These are a powerful tool to interrogate complex non-linear relationships, but they offer interpolation of functions within a well understood domain. Applealing to these models for extrapolation needs to be done carefully.\nWe can, I think, draw a broad moral from this presentation. Observed phenomena admit description with a wide variety of mathematical abstraction. The “correctness” of the abstraction is never the right question. The manner in which one abstraction distils the structure of reality is not fundamentally any better than the manner in which an alternative scheme will do so. Predictive success achieved applying one abstraction can be traded off for interpretive tractability in another. What matters is the task at hand - solving the problem. Sometimes problems can be solved with new information, but mostly they’re resolved by re-arranging assumptions and phrasing the problem anew - seeing through a different lens, pulling the door handle where we previously pushed. Splines, hierarhical splines and gaussian process models all attempt to approximate an answer by rephrasing the question. The more of these tools we have, the better we are prepared to tackle novel problems and discern the pertinent structure in the world."
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#multiple-smoother-regression-models",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#multiple-smoother-regression-models",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Multiple Smoother Regression Models",
    "text": "Multiple Smoother Regression Models\nNon-linear spline relationships can be additively combined across multiple variables in simple and hierarchical regressions. To demonstrate how spline modelling can be further adapted to the multiple regression like cases we use the PISA data set discussed in this case study from Michael Clark. We’ll see how crucial it is to carefully assess the implications of these model fits.\nThe data set has been constructed using average Science scores by country from the Programme for International Student Assessment (PISA) 2006, along with GNI per capita (Purchasing Power Parity, 2005 dollars), Educational Index, Health Index, and Human Development Index from UN data. We want to model the overall outcome score as a function of these broad demographic features.\n\npisa_df = pd.read_csv(\"https://raw.githubusercontent.com/m-clark/generalized-additive-models/master/data/pisasci2006.csv\")\n\npisa_df.head()\n\n\n\n\n\n  \n    \n      \n      Country\n      Overall\n      Issues\n      Explain\n      Evidence\n      Interest\n      Support\n      Income\n      Health\n      Edu\n      HDI\n    \n  \n  \n    \n      0\n      Albania\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.599\n      0.886\n      0.716\n      0.724\n    \n    \n      1\n      Argentina\n      391.0\n      395.0\n      386.0\n      385.0\n      567.0\n      506.0\n      0.678\n      0.868\n      0.786\n      0.773\n    \n    \n      2\n      Australia\n      527.0\n      535.0\n      520.0\n      531.0\n      465.0\n      487.0\n      0.826\n      0.965\n      0.978\n      0.920\n    \n    \n      3\n      Austria\n      511.0\n      505.0\n      516.0\n      505.0\n      507.0\n      515.0\n      0.835\n      0.944\n      0.824\n      0.866\n    \n    \n      4\n      Azerbaijan\n      382.0\n      353.0\n      412.0\n      344.0\n      612.0\n      542.0\n      0.566\n      0.780\n      NaN\n      NaN\n    \n  \n\n\n\n\nThe relationships displayed between each of these measures is not obviously linear, and as such could plausibly benefit from being modelled with splines.\n\ng = sns.pairplot(pisa_df[['Overall', 'Income', 'Support', 'Health', 'Edu']],  kind=\"reg\", height=1.5)\ng.fig.suptitle(\"Pair Plot of Complex Relations\", y=1.05);\n\n\n\n\nWe define three models for contrasting the implications. Note here how we have to define a seperate spline basis for each of the covariates. Here we create the knots for defining our basis on each covariate.\n\nknots_income = np.linspace(np.min(pisa_df['Income']), np.max(pisa_df['Income']), 5+2)[1:-1]\n\nknots_edu = np.linspace(np.min(pisa_df['Edu']), np.max(pisa_df['Edu']), 5+2)[1:-1]\n\nknots_health = np.linspace(np.min(pisa_df['Health']), np.max(pisa_df['Health']), 5+2)[1:-1]\n\nknots_income1 = np.linspace(np.min(pisa_df['Income']), np.max(pisa_df['Income']), 3+2)[1:-1]\n\nknots_edu1 = np.linspace(np.min(pisa_df['Edu']), np.max(pisa_df['Edu']), 3+2)[1:-1]\n\nknots_health1 = np.linspace(np.min(pisa_df['Health']), np.max(pisa_df['Health']), 3+2)[1:-1]\n\nNow we initialise these models\n\nformula = \"Overall ~ Income + Edu + Health\"\nbase_model = bmb.Model(formula, pisa_df, dropna=True)\n\nformula_spline = \"\"\"Overall ~ bs(Income, degree=3, knots=knots_income) + bs(Edu, degree=3, knots=knots_edu) + bs(Health, degree=3, knots=knots_health) \"\"\"\n\nformula_spline1 = \"\"\"Overall ~ (1 | Country) + bs(Income, degree=3, knots=knots_income1) + bs(Edu, degree=3, knots=knots_edu1) + bs(Health, degree=3, knots=knots_health1) \"\"\"\n\nspline_model = bmb.Model(formula_spline, pisa_df, dropna=True)\n\nspline_model1 = bmb.Model(formula_spline1, pisa_df, dropna=True)\n\nspline_model1\n\nAutomatically removing 13/65 rows from the dataset.\n\n\nAutomatically removing 13/65 rows from the dataset.\n\n\nAutomatically removing 13/65 rows from the dataset.\n\n\n       Formula: Overall ~ (1 | Country) + bs(Income, degree=3, knots=knots_income1) + bs(Edu, degree=3, knots=knots_edu1) + bs(Health, degree=3, knots=knots_health1) \n        Family: gaussian\n          Link: mu = identity\n  Observations: 52\n        Priors: \n    target = mu\n        Common-level effects\n            Intercept ~ Normal(mu: 471.1538, sigma: 517.9979)\n            bs(Income, degree=3, knots=knots_income1) ~ Normal(mu: [0. 0. 0. 0. 0. 0.], sigma: [1630.0781\n                872.587   532.485   674.4864  570.5563  779.4582])\n            bs(Edu, degree=3, knots=knots_edu1) ~ Normal(mu: [0. 0. 0. 0. 0. 0.], sigma: [965.2606 734.261\n                559.679  590.0573 684.2515 649.6795])\n            bs(Health, degree=3, knots=knots_health1) ~ Normal(mu: [0. 0. 0. 0. 0. 0.], sigma: [1842.3891\n                815.7797  534.7546  671.1905  544.161   802.929 ])\n        \n        \n        Group-level effects\n            1|Country ~ Normal(mu: 0.0, sigma: HalfNormal(sigma: 517.9979))\n        \n        Auxiliary parameters\n            sigma ~ HalfStudentT(nu: 4.0, sigma: 53.4654)\n\n\n\nspline_model.build()\nspline_model.graph()\n\n\n\n\n\nbase_idata = base_model.fit(random_seed=random_seed, idata_kwargs={\"log_likelihood\": True})\nspline_idata = spline_model.fit(random_seed=random_seed, idata_kwargs={\"log_likelihood\": True}, target_accept=.95)\nspline_idata1 = spline_model1.fit(random_seed=random_seed, idata_kwargs={\"log_likelihood\": True}, target_accept=.99)\n\nWe can compare the simple regression approach to the spline based regression in the now usual way.\n\ncompare_df = az.compare({'spline': spline_idata, 'raw': base_idata, 'spline_hierarchy':spline_idata1})\n\ncompare_df\n\n\n\n\n\n  \n    \n      \n      rank\n      elpd_loo\n      p_loo\n      elpd_diff\n      weight\n      se\n      dse\n      warning\n      scale\n    \n  \n  \n    \n      spline_hierarchy\n      0\n      -244.691895\n      38.487159\n      0.000000\n      1.000000e+00\n      3.901855\n      0.000000\n      True\n      log\n    \n    \n      spline\n      1\n      -254.101306\n      20.218465\n      9.409411\n      0.000000e+00\n      4.724572\n      2.840914\n      True\n      log\n    \n    \n      raw\n      2\n      -263.061455\n      9.617888\n      18.369559\n      1.054712e-13\n      10.083921\n      9.376156\n      True\n      log\n    \n  \n\n\n\n\nThe coefficients comparisons are harder\n\naz.summary(base_idata)\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      118.438\n      81.996\n      -41.166\n      266.033\n      1.522\n      1.086\n      2915.0\n      2986.0\n      1.0\n    \n    \n      Income\n      181.955\n      89.547\n      17.413\n      353.598\n      1.913\n      1.353\n      2196.0\n      2432.0\n      1.0\n    \n    \n      Edu\n      234.125\n      59.153\n      120.739\n      340.343\n      1.017\n      0.740\n      3414.0\n      2518.0\n      1.0\n    \n    \n      Health\n      30.395\n      140.966\n      -223.699\n      298.258\n      2.852\n      2.153\n      2450.0\n      2598.0\n      1.0\n    \n    \n      Overall_sigma\n      34.234\n      3.620\n      27.882\n      41.175\n      0.060\n      0.043\n      3757.0\n      2879.0\n      1.0\n    \n  \n\n\n\n\nsince it’s less clear what the spline coefficient terms mean.\n\naz.summary(spline_idata)\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      225.689\n      205.515\n      -133.222\n      630.021\n      6.036\n      4.292\n      1163.0\n      1653.0\n      1.0\n    \n    \n      bs(Income, degree=3, knots=knots_income)[0]\n      -355.159\n      1613.343\n      -3460.788\n      2642.535\n      56.357\n      39.865\n      819.0\n      1504.0\n      1.0\n    \n    \n      bs(Income, degree=3, knots=knots_income)[1]\n      194.905\n      122.719\n      -42.478\n      423.960\n      2.343\n      1.728\n      2748.0\n      2719.0\n      1.0\n    \n    \n      bs(Income, degree=3, knots=knots_income)[2]\n      48.748\n      123.638\n      -182.012\n      272.863\n      4.065\n      2.875\n      923.0\n      1431.0\n      1.0\n    \n    \n      bs(Income, degree=3, knots=knots_income)[3]\n      68.533\n      98.242\n      -118.858\n      250.496\n      3.369\n      2.383\n      853.0\n      1416.0\n      1.0\n    \n    \n      bs(Income, degree=3, knots=knots_income)[4]\n      210.290\n      103.892\n      17.347\n      406.753\n      3.626\n      2.565\n      825.0\n      1404.0\n      1.0\n    \n    \n      bs(Income, degree=3, knots=knots_income)[5]\n      222.546\n      105.553\n      31.163\n      427.878\n      3.589\n      2.539\n      866.0\n      1442.0\n      1.0\n    \n    \n      bs(Income, degree=3, knots=knots_income)[6]\n      202.395\n      105.402\n      0.453\n      394.110\n      3.629\n      2.567\n      846.0\n      1507.0\n      1.0\n    \n    \n      bs(Income, degree=3, knots=knots_income)[7]\n      81.592\n      103.313\n      -106.154\n      279.937\n      3.641\n      2.575\n      805.0\n      1390.0\n      1.0\n    \n    \n      bs(Edu, degree=3, knots=knots_edu)[0]\n      145.558\n      287.518\n      -410.399\n      654.146\n      8.525\n      6.029\n      1142.0\n      1509.0\n      1.0\n    \n    \n      bs(Edu, degree=3, knots=knots_edu)[1]\n      47.574\n      193.029\n      -323.504\n      392.406\n      5.595\n      3.957\n      1197.0\n      1656.0\n      1.0\n    \n    \n      bs(Edu, degree=3, knots=knots_edu)[2]\n      71.806\n      211.848\n      -312.889\n      469.050\n      6.246\n      4.417\n      1156.0\n      1647.0\n      1.0\n    \n    \n      bs(Edu, degree=3, knots=knots_edu)[3]\n      147.390\n      200.622\n      -224.182\n      518.214\n      5.972\n      4.224\n      1134.0\n      1603.0\n      1.0\n    \n    \n      bs(Edu, degree=3, knots=knots_edu)[4]\n      70.472\n      203.119\n      -314.815\n      427.662\n      6.018\n      4.256\n      1144.0\n      1535.0\n      1.0\n    \n    \n      bs(Edu, degree=3, knots=knots_edu)[5]\n      121.297\n      202.750\n      -264.164\n      485.547\n      5.998\n      4.243\n      1147.0\n      1575.0\n      1.0\n    \n    \n      bs(Edu, degree=3, knots=knots_edu)[6]\n      78.372\n      206.089\n      -296.582\n      457.669\n      6.012\n      4.252\n      1178.0\n      1713.0\n      1.0\n    \n    \n      bs(Edu, degree=3, knots=knots_edu)[7]\n      124.157\n      203.070\n      -258.062\n      490.727\n      6.008\n      4.249\n      1148.0\n      1532.0\n      1.0\n    \n    \n      bs(Health, degree=3, knots=knots_health)[0]\n      235.831\n      885.656\n      -1490.479\n      1862.011\n      31.104\n      22.002\n      813.0\n      1482.0\n      1.0\n    \n    \n      bs(Health, degree=3, knots=knots_health)[1]\n      162.805\n      389.636\n      -552.198\n      913.839\n      7.756\n      6.545\n      2529.0\n      2422.0\n      1.0\n    \n    \n      bs(Health, degree=3, knots=knots_health)[2]\n      14.967\n      120.574\n      -202.130\n      248.327\n      3.854\n      2.726\n      979.0\n      1657.0\n      1.0\n    \n    \n      bs(Health, degree=3, knots=knots_health)[3]\n      25.213\n      100.498\n      -170.867\n      204.686\n      3.411\n      2.413\n      871.0\n      1555.0\n      1.0\n    \n    \n      bs(Health, degree=3, knots=knots_health)[4]\n      -21.315\n      102.473\n      -223.005\n      158.665\n      3.577\n      2.530\n      822.0\n      1468.0\n      1.0\n    \n    \n      bs(Health, degree=3, knots=knots_health)[5]\n      -19.449\n      101.418\n      -213.704\n      165.809\n      3.541\n      2.505\n      823.0\n      1444.0\n      1.0\n    \n    \n      bs(Health, degree=3, knots=knots_health)[6]\n      -55.820\n      100.895\n      -252.216\n      126.193\n      3.547\n      2.509\n      811.0\n      1347.0\n      1.0\n    \n    \n      bs(Health, degree=3, knots=knots_health)[7]\n      0.840\n      101.892\n      -196.002\n      186.216\n      3.543\n      2.506\n      828.0\n      1469.0\n      1.0\n    \n    \n      Overall_sigma\n      24.766\n      3.303\n      18.816\n      30.884\n      0.085\n      0.060\n      1491.0\n      2068.0\n      1.0\n    \n  \n\n\n\n\nWe can also check that the model seems to recover the observed data well.\n\nPlotting Posterior Predictive Checks\n\nbase_model.predict(base_idata, kind='pps')\nspline_model.predict(spline_idata, kind='pps')\nspline_model1.predict(spline_idata1, kind='pps')\n\n\nfig, axs = plt.subplots(1, 3, figsize=(20, 6))\naxs = axs.flatten()\naz.plot_ppc(base_idata, ax=axs[0])\naz.plot_ppc(spline_idata, ax=axs[1]);\naz.plot_ppc(spline_idata1, ax=axs[2]);\naxs[0].set_xlabel('')\naxs[1].set_title(\"PPC: Spline Model\");\naxs[0].set_title(\"PPC: Regression Model\");\naxs[2].set_title(\"PPC: Hierarchical Spline Model\");\n\n\n\n\nNext we’ll dig into the spline basis features and decompose the predicted outcome and show how the outcome varies for levels in the inputs variables. The Bambi interpret package offers some functionality to assess the conditional predictions for varying values of the input variables.\n\n\nModel Interpretability Plots in Bambi\nWe want to highlight the differences between the hierarchical and non-hierarchical multivariable spline models here. The shrinkage effects of hierarchically modelling the country intercepts is clearly evident. They helpfully constrain the poor extrapolation effects of the simpler spline model.\n\nfig, axs = plt.subplots(3, 2, figsize=(9, 14))\naxs = axs.flatten()\nbmb.interpret.plot_predictions(\n    spline_model, spline_idata, \"Income\", ax=axs[0]\n)\n\naxs[0].set_title(\"Non-Hierarchical Income\")\naxs[2].set_title(\"Non-Hierarchical Edu\")\naxs[4].set_title(\"Non-Hierarchical Health\")\n\naxs[1].set_title(\"Hierarchical Income\")\naxs[3].set_title(\"Hierarchical Edu\")\naxs[5].set_title(\"Hierarchical Health\")\n\n\nbmb.interpret.plot_predictions(\n    spline_model, spline_idata, \"Edu\", ax=axs[2]\n)\n\nbmb.interpret.plot_predictions(\n    spline_model, spline_idata, \"Health\", ax=axs[4]\n);\n\nbmb.interpret.plot_predictions(\n    spline_model1, spline_idata1, \"Income\", \n    sample_new_groups=True, ax=axs[1]\n)\n\nbmb.interpret.plot_predictions(\n    spline_model1, spline_idata1, \"Edu\", sample_new_groups=True, ax=axs[3]\n)\n\nbmb.interpret.plot_predictions(\n    spline_model1, spline_idata1, \"Health\",sample_new_groups=True, ax=axs[5]\n);\n\n\n\n\nWe can pull these types of values out into a table. Note here how we ask for predictions based on varying values of the Edu and Income variables where we keep the Health variable fixed at the mean value.\n\nsummary_df = bmb.interpret.predictions(\n    spline_model,\n    spline_idata,\n    conditional={\n        \"Edu\": list(np.linspace(0.5, .95, 10)),\n        \"Income\": list(np.linspace(0.4, .95, 10)),\n        },\n)\nsummary_df\n\n\nsummary_df1 = bmb.interpret.predictions(\n    spline_model1,\n    spline_idata1,\n    conditional={\n        \"Edu\": list(np.linspace(0.5, .95, 10)),\n        \"Income\": list(np.linspace(0.4, .95, 10)),\n        },\n    sample_new_groups=True\n)\nsummary_df1\n\n\n\n\n\n  \n    \n      \n      Edu\n      Income\n      Country\n      Health\n      estimate\n      lower_3.0%\n      upper_97.0%\n    \n  \n  \n    \n      0\n      0.50\n      0.400000\n      Albania\n      0.885672\n      102.686176\n      -587.761967\n      750.373783\n    \n    \n      1\n      0.50\n      0.461111\n      Albania\n      0.885672\n      451.070276\n      -37.688251\n      905.442110\n    \n    \n      2\n      0.50\n      0.522222\n      Albania\n      0.885672\n      376.852378\n      -161.138067\n      945.914437\n    \n    \n      3\n      0.50\n      0.583333\n      Albania\n      0.885672\n      283.859330\n      -357.146695\n      933.659941\n    \n    \n      4\n      0.50\n      0.644444\n      Albania\n      0.885672\n      288.608418\n      -345.493652\n      920.643943\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      0.95\n      0.705556\n      Albania\n      0.885672\n      460.351293\n      413.197965\n      508.270287\n    \n    \n      96\n      0.95\n      0.766667\n      Albania\n      0.885672\n      516.885593\n      474.706704\n      561.845324\n    \n    \n      97\n      0.95\n      0.827778\n      Albania\n      0.885672\n      552.375825\n      506.285465\n      603.102537\n    \n    \n      98\n      0.95\n      0.888889\n      Albania\n      0.885672\n      499.921356\n      455.457677\n      550.896029\n    \n    \n      99\n      0.95\n      0.950000\n      Albania\n      0.885672\n      229.655207\n      49.386349\n      412.317948\n    \n  \n\n100 rows × 7 columns\n\n\n\nWe can then plot these results based on the conditional realisations and see some interesting behaviour at the implausilble reaslisations. This should make us somewhat wary of using this model to extrapolate too far beyond the observable range of data\n\ng = sns.relplot(data=summary_df, x=\"Income\", y=\"estimate\", hue=\"Edu\")\n\ng.fig.suptitle(\"Marginal Predictions of the Outcome Variable \\n conditional on counterfactual values for Edu and Income\", y=1.05);\n\n\ng = sns.relplot(data=summary_df1, x=\"Income\", y=\"estimate\", hue=\"Edu\")\n\ng.fig.suptitle(\"Hierarchical Marginal Predictions of the Outcome Variable \\n conditional on counterfactual values for Edu and Income\", y=1.05);\n\n\n\n\n\n\n\nHowever, we can see here how the hierarchical component shrinks the predicted values back towards a reasonable range! Constraining the poor extrapolation of the more naive spline model. This demonstrates something of the interplay between in-sample approximation and out of sample generalisation we saw above in the case of the insurance curve development. The additional structure insists on commensurate realisations under different counterfactual settings reflective of the fact that countries do not vary so radically.\n\n\nThe Spline Component Contributions\nFinally, we’ll pull out the component contributions of each variable and see how they combine additively. This will also serve as a kind of posterior predictive check for each country as we can show the degree which posterior draws from each component sum to achieve a plausible mirror of the observed data.\n\nBincome = spline_model.response_component.design.common['bs(Income, degree=3, knots=knots_income)']\n\nincome_coefs = az.extract(spline_idata['posterior']['bs(Income, degree=3, knots=knots_income)'])['bs(Income, degree=3, knots=knots_income)']\n\nBedu = spline_model.response_component.design.common['bs(Edu, degree=3, knots=knots_edu)']\n\nedu_coefs = az.extract(spline_idata['posterior']['bs(Edu, degree=3, knots=knots_edu)'])['bs(Edu, degree=3, knots=knots_edu)']\n\n\nBhealth = spline_model.response_component.design.common['bs(Health, degree=3, knots=knots_health)']\n\nhealth_coefs = az.extract(spline_idata['posterior']['bs(Health, degree=3, knots=knots_health)'])['bs(Health, degree=3, knots=knots_health)']\n\nincome = np.dot(Bincome, income_coefs).T \nedu = np.dot(Bedu, edu_coefs).T\nhealth = np.dot(Bhealth, health_coefs).T\n\nintercept = az.extract(spline_idata['posterior']['Intercept'])['Intercept'].values\n\nfig, ax = plt.subplots(figsize=(10, 7))\nfor i in range(100):\n    if i == 1:\n        ax.plot(income[i], label='Income Component', color='red')\n        ax.plot(edu[i], label='Edu Component', color='blue')\n        ax.plot(health[i], label='Health Component', color='darkgreen')\n        ax.plot(intercept[i] + income[i] + edu[i] + health[i], label='Combined Components', color='purple')\n    else: \n        ax.plot(income[i], alpha=0.1, color='red')\n        ax.plot(edu[i], alpha=0.1, color='blue')\n        ax.plot(health[i], alpha=0.1, color='darkgreen')\n        ax.plot(intercept[i] + income[i] + edu[i] + health[i], color='purple', alpha=0.3)\n\nax.scatter(range(len(spline_idata['observed_data']['Overall'])), spline_idata['observed_data']['Overall'], label='Observed', color='grey', s=56, ec='black')\nax.set_title(\"PISA Outcomes \\n Additive Spline Components\", fontsize=20)\nax.legend();\nax.set_ylabel(\"Overall Score\", fontsize=12)\nax.set_xticklabels(pisa_df.dropna(axis=0).reset_index()['Country'], fontsize=12);\n\n\n\n\nWe can see here how the combined components borrow the structure of the outcome variable primarily from the income variable component. The health measures have closer to zero additive contribution while the uncertainty in the educational component varies wildly. But the blue educational component here is used primarily as a scaling contribution which adds to the level of the outcome variable rather than distorting the shape. These initially opaque synthetic features of splines can offer deep insight into the structure of our data generating process when seen correctly."
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#theoretical-posits-and-science",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#theoretical-posits-and-science",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Theoretical Posits and Science",
    "text": "Theoretical Posits and Science\n\nThere is no way to establish fully secured, neat protocol statements as starting points of the sciences. There is no tabula rasa. We are like sailors who have to rebuild their ship on the open sea, without ever being able to dismantle it in dry-dock and reconstruct it from its best components. - Otto Neurath\n\nIn this blog post we’re going to dive into modelling of non-linear functions and explore some of the tooling available in the python eco-system. We’ll start by looking into Generalised Additive Models with splines in pyGAM before preceding to look Bayesian versions of spline modelling comparing the splines to Gaussian processes in Bambi and PyMC. Before showing how hierarchical bayesian models avoid some of the issues of overfit in simpler spline models.\nOur interest in these models stems from their flexibility to approximate functions of arbitrary complexity. We’ll see how the methods work in the case of relatively straightforward toy example and then we’ll apply each of the methods to deriving insights into the functional form of insurance loss curves. In this application we adapt a data set discussed in Mick Cooney’s Stan case study to demonstrate the power of hierarchical spline models. Throughout we’ll draw on the discussion of these methods in Osvaldo Martin’s “Bayesian Analysis with Python” for practical details implementing these models.\nAll of these methods need to be assessed with respect to their in-sample model fit and their out of sample performance. How can we best calibrate the model fits to perform reasonably well out of sample?"
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#theoretical-posits-and-scientific-commitments",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#theoretical-posits-and-scientific-commitments",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Theoretical Posits and Scientific Commitments",
    "text": "Theoretical Posits and Scientific Commitments\nThe scientific enterprise must abstract away details of reality to provide insight into the structure of reality. Statistical models of real mechanisms are cobbled together from a variety of abstractions; probability distributions and functional relations cojoined, are then calibrated against observed data. We are building on disparate foundations. Good abstractions aim at modelling the structures or real patterns that matter. It doesn’t matter if those abstractions are themselves reflective of reality.\n\n[T]here is no way to establish [a] fully secured, … starting point of the sciences. There is no tabula rasa. We are like sailors who have to rebuild their ship on the open sea, without ever being able to dismantle it in dry-dock and reconstruct it from its best components. - Otto Neurath\n\nThis is an apt metaphor for spline models in particular - like these ships built at sea, spline models are constructed from ad-hoc materials fished from the vast ocean of possible models. They are linear approxiations invoked to ride the next wave form.\nIn this blog post we’re going to dive into modelling of non-linear functions and explore some of the tooling available in the python eco-system. We’ll start by looking into Generalised Additive Models with splines in pyGAM before proceeding to look at how Bayesian versions of spline modelling compares to Gaussian processes in Bambi and PyMC. Finally we will show how hierarchical bayesian models avoid some of the issues of overfit in simpler spline models.\nOur interest in these models stems from their flexibility to approximate functions of arbitrary complexity. We’ll see how the methods work in the case of relatively straightforward toy example and then we’ll apply each of the methods to deriving insights into the functional form of insurance loss curves. In this application we adapt a data set discussed in Mick Cooney’s Stan case study to demonstrate the power of hierarchical spline models. Throughout we’ll draw on the discussion of these methods in Osvaldo Martin’s “Bayesian Analysis with Python” for practical details implementing these models."
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#approximate-gaussian-processes",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#approximate-gaussian-processes",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Approximate Gaussian processes",
    "text": "Approximate Gaussian processes\nGaussian processes models allow us to express our beliefs about the structure of a function and calibrate these beliefs against the observed data. The topic of gaussian processes is rich and complex. Too rich to be fairly covered in this blog post, so we’ll just say that we’re using a method designed for function approximation that makes use of drawing samples from a multivariate normal distribution under a range of different covariance relationships. The properties of these covariance relations determine the shape and fluidity of the function realisations. We will use the HSGP approximation of gaussian processes models.\nThe gaussian process models can be somewhat interrogated by examining the different combinations of covariance relationships with priors over the parameters governing the covariance of a sequence of points. For example consider the following parameterisations.\n\nfig, axs = plt.subplots(2, 2, figsize=(9, 10))\naxs = axs.flatten()\n\ndef plot_cov_draws(ax1, ax2, lengthscale=3, sigma=13):\n    cov = sigma**2 * pm.gp.cov.ExpQuad(1, lengthscale)\n    X = np.linspace(0, 60, 200)[:, None]\n    K = cov(X).eval()\n\n    sns.heatmap(pd.DataFrame(K), center=0, xticklabels=[], yticklabels=[], ax=ax1, cmap='crest');\n    ax1.set_title(f\"Covariance Length Scale {lengthscale}\")\n    ax2.plot(\n        X,\n        pm.draw(\n            pm.MvNormal.dist(mu=np.zeros(len(K)), cov=K, shape=K.shape[0]), draws=10, random_seed=random_seed\n        ).T, color='blue', alpha=0.5\n    )\n    ax2.set_title(f\"Samples from the GP prior \\n lengthscale: {lengthscale}, sigma: {sigma}\")\n    plt.ylabel(\"y\")\n    plt.xlabel(\"X\");\n\nplot_cov_draws(axs[0], axs[1])\nplot_cov_draws(axs[2], axs[3], lengthscale=10, sigma=13)\n\n\n\n\nWe’ve specified the range of X to reflect the support of the acceleration example and allowed the draws to be informed by a covariance function we have parameterised using the Exponentiated Quadratic kernel. We’ve additionally tweaked the lengthscale to demonstrate how proximity in the sequence informs the kernel and determines the shape of the covariance structure.\n\\[k(x, x') = \\mathrm{exp}\\left[ -\\frac{(x - x')^2}{2 \\ell^2} \\right]\\]\nThe plot on the left highlights the importance of the lengthscale parameter over the sequence. The wider the central shading is the more pronounced is the correlation among more points in the sequence. The patterns to the right show a good range of “wiggliness” that they should be flexible enough to capture the shape of the acceleration. These are the constraints of structure we can impose of the theoretical realisations of our model. If we can calibrate the model and derive posterior parameters against the observed data, we can learn the probable shape of our functional relationship.\n\nPriors on Gaussian Processes\nConsider the following specification for the priors\n\nfig, axs = plt.subplots(1, 2, figsize=(9, 6))\naxs = axs.flatten()\naxs[0].hist(pm.draw(pm.InverseGamma.dist(mu=1, sigma=1), 1000), ec='black', bins=30);\naxs[0].set_title(\"Priors for Lengthscale \\n in ExpQuad Kernel\")\naxs[1].hist(pm.draw(pm.Exponential.dist(lam=1), 1000), ec='black', bins=30);\naxs[1].set_title(\"Priors for Amplitude \\n in ExpQuad Kernel\")\n\nText(0.5, 1.0, 'Priors for Amplitude \\n in ExpQuad Kernel')\n\n\n\n\n\nWe use these to specify priors on the Hilbert space approximation of gaussian priors available in the Bambi package.\n\nprior_hsgp = {\n    \"sigma\": bmb.Prior(\"Exponential\", lam=1), # amplitude\n    \"ell\": bmb.Prior(\"InverseGamma\", mu=1, sigma=1) # lengthscale\n}\n\n# This is the dictionary we pass to Bambi\npriors = {\n    \"hsgp(X, m=10, c=1)\": prior_hsgp,\n    \"sigma\": bmb.Prior(\"HalfNormal\", sigma=4)\n}\nmodel_hsgp = bmb.Model(\"y ~ 0 + hsgp(X, m=10, c=1)\", df, priors=priors)\nmodel_hsgp\n\n       Formula: y ~ 0 + hsgp(X, m=10, c=1)\n        Family: gaussian\n          Link: mu = identity\n  Observations: 133\n        Priors: \n    target = mu\n        HSGP contributions\n            hsgp(X, m=10, c=1)\n                cov: ExpQuad\n                sigma ~ Exponential(lam: 1.0)\n                ell ~ InverseGamma(mu: 1.0, sigma: 1.0)\n        \n        Auxiliary parameters\n            sigma ~ HalfNormal(sigma: 4.0)\n\n\nHere we’ve set the m=10 to determine the number of basis vectors used in the Hilbert space approximation. The idea differs in detail from the spline based approximations we’ve seen, but it’s perhaps useful to think of the process in the same vein. Here again we have a theory of the world expressed as a function of opaque components jerry-rigged for modelling some phenomena.\n\nidata_hsgp = model_hsgp.fit(inference_method=\"nuts_numpyro\",target_accept=0.95, random_seed=121195, \nidata_kwargs={\"log_likelihood\": True})\nprint(idata_hsgp.sample_stats[\"diverging\"].sum().to_numpy())\n\nCompiling...\n\n\nCompilation time = 0:00:01.138923\n\n\nSampling...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling time = 0:00:01.870798\n\n\nTransforming variables...\n\n\nTransformation time = 0:00:00.176689\n\n\nComputing Log Likelihood...\n\n\nLog Likelihood time = 0:00:00.144039\n\n\n0\n\n\nThis model fits and the sampling seems to have worked well.\n\naz.plot_trace(idata_hsgp, backend_kwargs={\"layout\": \"constrained\"}, figsize=(9, 15));\n\n\n\n\nThe lengthscale and sigma parameters we have learned by calibrating our priors against the data. The degree to which these parameters are meaningful depend a little on how familar you are with covariance matrix kernels and their properties, so we won’t dwell on the point here.\n\naz.summary(idata_hsgp, var_names=['hsgp(X, m=10, c=1)_ell', 'hsgp(X, m=10, c=1)_sigma', 'y_sigma', 'hsgp(X, m=10, c=1)_weights'])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      hsgp(X, m=10, c=1)_ell\n      3.301\n      0.666\n      2.099\n      4.567\n      0.016\n      0.011\n      1763.0\n      1815.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_sigma\n      23.675\n      2.679\n      18.925\n      28.847\n      0.069\n      0.049\n      1497.0\n      2167.0\n      1.0\n    \n    \n      y_sigma\n      20.589\n      1.121\n      18.590\n      22.751\n      0.019\n      0.014\n      3440.0\n      2485.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[0]\n      -131.588\n      13.784\n      -157.019\n      -105.189\n      0.216\n      0.154\n      4071.0\n      3020.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[1]\n      -94.391\n      18.010\n      -125.921\n      -58.020\n      0.363\n      0.256\n      2500.0\n      2876.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[2]\n      106.121\n      20.717\n      69.627\n      147.247\n      0.458\n      0.324\n      2046.0\n      2563.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[3]\n      149.289\n      22.154\n      106.508\n      188.107\n      0.521\n      0.368\n      1807.0\n      2467.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[4]\n      -83.399\n      22.728\n      -124.812\n      -40.406\n      0.539\n      0.383\n      1776.0\n      2517.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[5]\n      -125.322\n      23.273\n      -169.346\n      -82.187\n      0.531\n      0.377\n      1917.0\n      2940.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[6]\n      37.835\n      21.153\n      -1.712\n      77.574\n      0.512\n      0.362\n      1713.0\n      2577.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[7]\n      94.574\n      20.745\n      53.167\n      132.182\n      0.423\n      0.299\n      2393.0\n      3105.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[8]\n      -1.494\n      17.184\n      -31.835\n      33.063\n      0.361\n      0.255\n      2264.0\n      2853.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[9]\n      -45.442\n      15.955\n      -76.649\n      -16.636\n      0.289\n      0.208\n      3041.0\n      3183.0\n      1.0\n    \n  \n\n\n\n\nBut again we can sample from the posterior predictive distribution of the outcome variable\n\nmodel_hsgp.predict(idata_hsgp, data=new_data, \nkind='pps', inplace=True)\n\nidata_hsgp\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                         (chain: 4, draw: 1000,\n                                     hsgp(X, m=10, c=1)_weights_dim: 10,\n                                     y_obs: 500)\nCoordinates:\n  * chain                           (chain) int64 0 1 2 3\n  * draw                            (draw) int64 0 1 2 3 4 ... 996 997 998 999\n  * hsgp(X, m=10, c=1)_weights_dim  (hsgp(X, m=10, c=1)_weights_dim) int64 0 ...\n  * y_obs                           (y_obs) int64 0 1 2 3 4 ... 496 497 498 499\nData variables:\n    hsgp(X, m=10, c=1)_weights_raw  (chain, draw, hsgp(X, m=10, c=1)_weights_dim) float64 ...\n    y_sigma                         (chain, draw) float64 20.86 19.86 ... 21.17\n    hsgp(X, m=10, c=1)_sigma        (chain, draw) float64 26.85 24.0 ... 18.06\n    hsgp(X, m=10, c=1)_ell          (chain, draw) float64 3.861 3.509 ... 3.76\n    hsgp(X, m=10, c=1)_weights      (chain, draw, hsgp(X, m=10, c=1)_weights_dim) float64 ...\n    y_mean                          (chain, draw, y_obs) float64 15.74 ... -4...\n    hsgp(X, m=10, c=1)              (chain, draw, y_obs) float64 15.74 ... -4...\nAttributes:\n    created_at:                  2024-05-27T06:01:00.623094\n    arviz_version:               0.17.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000hsgp(X, m=10, c=1)_weights_dim: 10y_obs: 500Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])hsgp(X, m=10, c=1)_weights_dim(hsgp(X, m=10, c=1)_weights_dim)int640 1 2 3 4 5 6 7 8 9array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])y_obs(y_obs)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (7)hsgp(X, m=10, c=1)_weights_raw(chain, draw, hsgp(X, m=10, c=1)_weights_dim)float64-1.452 -0.974 ... 0.9659 -0.2453array([[[-1.45186697, -0.97401433,  1.51626692, ...,  1.84709969,\n          0.92584892, -1.19616826],\n        [-1.73799921, -1.55920285,  1.42827017, ...,  1.78787921,\n         -1.14159607, -1.66042935],\n        [-1.3059213 , -1.22687096,  2.0214052 , ...,  2.80079646,\n          0.17462599, -0.06073935],\n        ...,\n        [-2.26781832, -1.21871881,  1.80249133, ...,  2.65551316,\n          0.30040289, -0.7085128 ],\n        [-2.23372877, -1.1832454 ,  1.74365597, ...,  2.71376469,\n          0.41123845, -0.60671544],\n        [-1.96117304, -1.13093178,  2.24600086, ...,  2.3750183 ,\n         -0.00901474, -0.75147244]],\n\n       [[-1.95999601, -1.27507311,  1.52467611, ...,  3.28813978,\n         -0.1598232 , -1.10988783],\n        [-1.59314189, -1.44864255,  1.95956808, ...,  2.01431167,\n          1.0199955 , -0.8962025 ],\n        [-1.8432561 , -1.32415496,  1.80874966, ...,  3.05464118,\n          0.81105708, -1.02872434],\n...\n        [-1.94277245, -1.5433581 ,  1.78771407, ...,  2.11464836,\n         -0.62470516, -1.01229181],\n        [-1.96747626, -1.39055532,  1.24775496, ...,  2.93940091,\n          0.46989555, -1.57735663],\n        [-2.20654554, -1.042289  ,  1.46402154, ...,  3.37106106,\n         -0.11061588, -2.15605928]],\n\n       [[-1.68378706, -0.84035677,  1.41475357, ...,  2.46320953,\n          0.88793346, -0.56144583],\n        [-1.66959934, -1.01014124,  1.6874097 , ...,  2.71030445,\n          0.68597924, -0.76198643],\n        [-1.54563186, -0.89279988,  1.77115866, ...,  2.3842957 ,\n          0.08392828, -0.78058783],\n        ...,\n        [-2.17148384, -1.59167361,  1.34960308, ...,  2.34802173,\n          0.44912308, -1.0698665 ],\n        [-1.31376138, -0.63355604,  1.97759132, ...,  1.8841587 ,\n         -0.13453184, -1.36605679],\n        [-2.0018976 , -1.36470426,  2.15352253, ...,  3.17100177,\n          0.96590611, -0.24527631]]])y_sigma(chain, draw)float6420.86 19.86 21.17 ... 20.12 21.17array([[20.85613959, 19.86298062, 21.16886643, ..., 19.48940449,\n        19.31511941, 19.44304302],\n       [18.5908299 , 21.71605063, 21.22255223, ..., 19.44547467,\n        19.21042952, 22.0099345 ],\n       [20.48425625, 21.17420065, 22.11318822, ..., 18.40825711,\n        21.65013089, 22.05150211],\n       [21.08143229, 20.7215663 , 22.60954219, ..., 21.15149371,\n        20.11622356, 21.16568101]])hsgp(X, m=10, c=1)_sigma(chain, draw)float6426.85 24.0 27.81 ... 26.09 18.06array([[26.84572148, 24.00472166, 27.80754218, ..., 23.34108117,\n        23.72941745, 22.60241785],\n       [22.01917097, 24.17709343, 26.16315265, ..., 22.03189338,\n        22.17588023, 24.00832607],\n       [24.12239909, 24.36393141, 25.79692715, ..., 24.33866729,\n        20.63877848, 22.26540398],\n       [25.41092492, 25.05093181, 28.3790263 , ..., 22.30157727,\n        26.0901197 , 18.06420147]])hsgp(X, m=10, c=1)_ell(chain, draw)float643.861 3.509 3.695 ... 3.679 3.76array([[3.86057038, 3.50871714, 3.69475981, ..., 3.20961859, 3.07237787,\n        2.99194079],\n       [4.21093385, 4.11858175, 3.55744155, ..., 3.54737759, 2.43665447,\n        3.67009957],\n       [3.38790729, 3.56906294, 3.07026931, ..., 3.17401967, 4.77493695,\n        4.1757221 ],\n       [3.73124202, 3.62463311, 4.06803959, ..., 3.06030257, 3.67928011,\n        3.75995625]])hsgp(X, m=10, c=1)_weights(chain, draw, hsgp(X, m=10, c=1)_weights_dim)float64-120.2 -78.54 ... 27.36 -5.933array([[[-120.19159663,  -78.54474016,  117.04015633, ...,\n           88.13161831,   38.07210878,  -41.65695052],\n        [-122.83658383, -107.83699596,   95.27680851, ...,\n           80.15736607,  -45.26655282,  -57.3944273 ],\n        [-109.63228325, -100.55013819,  159.16260856, ...,\n          141.94233647,    7.72311493,   -2.30700494],\n        ...,\n        [-149.23661164,  -78.75788623,  113.01499747, ...,\n          119.40081633,   12.18790241,  -25.62639515],\n        [-146.28304701,  -76.21158457,  109.23906639, ...,\n          125.36332205,   17.28993874,  -22.96014529],\n        [-120.75657952,  -68.54675039,  132.60289356, ...,\n          105.03375274,   -0.36461133,  -27.50691292]],\n\n       [[-138.76216692,  -87.49701007,   99.32062774, ...,\n          120.85182044,   -4.92170357,  -28.04716014],\n        [-122.53330814, -108.14123925,  139.17924148, ...,\n           82.74939574,   35.37871319,  -25.72818479],\n        [-142.94342858, -100.42482592,  132.17632445, ...,\n          148.36712045,   34.72137014,  -38.24384896],\n...\n        [-132.58698442, -103.47676242,  116.36868708, ...,\n           99.43921353,  -26.56716678,  -38.47612061],\n        [-138.61535955,  -94.11467693,   78.98484564, ...,\n           89.13997713,   11.35087487,  -29.54951483],\n        [-157.32953055,  -72.06983662,   96.1818003 , ...,\n          126.1515611 ,   -3.47852942,  -55.82170316]],\n\n       [[-129.78695588,  -63.20664967,  102.15003445, ...,\n          113.4760498 ,   35.60103971,  -19.27405014],\n        [-125.10213961,  -73.9587159 ,  118.87369923, ...,\n          124.94562328,   27.73901676,  -26.61384578],\n        [-138.71497899,  -77.82483202,  147.07295446, ...,\n          116.05385128,    3.46342745,  -26.78443203],\n        ...,\n        [-133.39280566,  -96.17651826,   79.33895456, ...,\n          102.02385007,   17.77408526,  -38.1416893 ],\n        [-103.26872735,  -48.62806342,  145.87733997, ...,\n           89.78661037,   -5.60101795,  -48.90554932],\n        [-110.1016679 ,  -73.21183754,  110.83494684, ...,\n          103.40789151,   27.35517734,   -5.93337047]]])y_mean(chain, draw, y_obs)float6415.74 15.74 15.71 ... -4.651 -4.606array([[[ 15.73717823,  15.74095015,  15.71442577, ...,  -4.67876513,\n          -4.20969038,  -3.71327448],\n        [-22.08564087, -21.76667383, -21.39772149, ...,   7.03009177,\n           7.12845194,   7.21535709],\n        [ 56.02093901,  56.10232981,  56.08787761, ...,   1.40193661,\n           1.10510341,   0.79052751],\n        ...,\n        [ 27.64916801,  27.65115973,  27.60142293, ...,   1.4750735 ,\n           1.59074503,   1.70839567],\n        [ 29.49505409,  29.44086994,  29.3291777 , ...,   3.89804528,\n           4.04221731,   4.18596665],\n        [ 24.70348988,  24.9191706 ,  25.10055256, ...,  -1.75167732,\n          -1.70197532,  -1.64814065]],\n\n       [[ 13.09479226,  13.08797832,  13.05446772, ...,  11.0508337 ,\n          11.0972292 ,  11.12522822],\n        [ 19.45360075,  19.35303968,  19.21116795, ..., -17.11306852,\n         -16.95029643, -16.75310125],\n        [ 39.40925935,  39.40667898,  39.33002811, ...,  -2.12983751,\n          -2.01625076,  -1.89843614],\n...\n        [-14.4322126 , -14.38059677, -14.30401941, ...,  11.55147703,\n          11.55940569,  11.5447161 ],\n        [  3.99064388,   3.88036701,   3.75439771, ...,   6.42065763,\n           6.71826106,   7.01812801],\n        [ 11.85726202,  12.01889355,  12.16180986, ...,  12.90196389,\n          13.3121751 ,  13.717131  ]],\n\n       [[ 27.47986715,  27.31475262,  27.09142621, ...,   7.10292311,\n           7.47525977,   7.85365034],\n        [ 34.24033269,  34.24902789,  34.19561616, ...,   5.51087296,\n           5.77912687,   6.04952041],\n        [ 31.73947173,  31.94613931,  32.10509971, ...,   9.39499668,\n           9.48056436,   9.55191877],\n        ...,\n        [  9.46004312,   9.35384251,   9.22108586, ...,   3.61236621,\n           3.94961437,   4.29454081],\n        [ 17.75725777,  18.27924573,  18.79439922, ...,   5.95699839,\n           6.24094799,   6.5246295 ],\n        [ 33.41206521,  33.28138834,  33.08465457, ...,  -4.68577545,\n          -4.65087068,  -4.60586526]]])hsgp(X, m=10, c=1)(chain, draw, y_obs)float6415.74 15.74 15.71 ... -4.651 -4.606array([[[ 15.73717823,  15.74095015,  15.71442577, ...,  -4.67876513,\n          -4.20969038,  -3.71327448],\n        [-22.08564087, -21.76667383, -21.39772149, ...,   7.03009177,\n           7.12845194,   7.21535709],\n        [ 56.02093901,  56.10232981,  56.08787761, ...,   1.40193661,\n           1.10510341,   0.79052751],\n        ...,\n        [ 27.64916801,  27.65115973,  27.60142293, ...,   1.4750735 ,\n           1.59074503,   1.70839567],\n        [ 29.49505409,  29.44086994,  29.3291777 , ...,   3.89804528,\n           4.04221731,   4.18596665],\n        [ 24.70348988,  24.9191706 ,  25.10055256, ...,  -1.75167732,\n          -1.70197532,  -1.64814065]],\n\n       [[ 13.09479226,  13.08797832,  13.05446772, ...,  11.0508337 ,\n          11.0972292 ,  11.12522822],\n        [ 19.45360075,  19.35303968,  19.21116795, ..., -17.11306852,\n         -16.95029643, -16.75310125],\n        [ 39.40925935,  39.40667898,  39.33002811, ...,  -2.12983751,\n          -2.01625076,  -1.89843614],\n...\n        [-14.4322126 , -14.38059677, -14.30401941, ...,  11.55147703,\n          11.55940569,  11.5447161 ],\n        [  3.99064388,   3.88036701,   3.75439771, ...,   6.42065763,\n           6.71826106,   7.01812801],\n        [ 11.85726202,  12.01889355,  12.16180986, ...,  12.90196389,\n          13.3121751 ,  13.717131  ]],\n\n       [[ 27.47986715,  27.31475262,  27.09142621, ...,   7.10292311,\n           7.47525977,   7.85365034],\n        [ 34.24033269,  34.24902789,  34.19561616, ...,   5.51087296,\n           5.77912687,   6.04952041],\n        [ 31.73947173,  31.94613931,  32.10509971, ...,   9.39499668,\n           9.48056436,   9.55191877],\n        ...,\n        [  9.46004312,   9.35384251,   9.22108586, ...,   3.61236621,\n           3.94961437,   4.29454081],\n        [ 17.75725777,  18.27924573,  18.79439922, ...,   5.95699839,\n           6.24094799,   6.5246295 ],\n        [ 33.41206521,  33.28138834,  33.08465457, ...,  -4.68577545,\n          -4.65087068,  -4.60586526]]])Indexes: (4)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))hsgp(X, m=10, c=1)_weights_dimPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64', name='hsgp(X, m=10, c=1)_weights_dim'))y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='y_obs', length=500))Attributes: (4)created_at :2024-05-27T06:01:00.623094arviz_version :0.17.0modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 4, draw: 1000, y_obs: 500)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * y_obs    (y_obs) int64 0 1 2 3 4 5 6 7 8 ... 492 493 494 495 496 497 498 499\nData variables:\n    y        (chain, draw, y_obs) float64 76.04 21.77 64.12 ... -41.12 -26.02\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000y_obs: 500Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])y_obs(y_obs)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (1)y(chain, draw, y_obs)float6476.04 21.77 64.12 ... -41.12 -26.02array([[[ 7.60446220e+01,  2.17746429e+01,  6.41213079e+01, ...,\n          2.88184766e+01,  5.35607407e+00,  8.28161473e+00],\n        [-3.71916430e+01, -1.02105981e+01, -3.75556355e+01, ...,\n          6.13483186e+00,  5.17152558e+00, -7.14534907e+00],\n        [ 4.18037444e+01,  8.94677926e+01,  6.01499750e+01, ...,\n          5.15362586e+00,  1.50220973e+01,  5.10623210e+00],\n        ...,\n        [ 1.17455216e+01,  7.18088614e+00,  2.27295340e+01, ...,\n         -1.40562367e+01, -1.55645500e+01, -1.59305317e+01],\n        [ 4.57996940e+01,  4.09450669e+01,  6.17203837e+01, ...,\n          5.43347241e+00,  2.16723522e+01, -1.34623303e+01],\n        [-1.99298911e+01, -7.31053097e+00,  2.33680338e+01, ...,\n         -1.86693562e+01, -3.45100068e+01, -4.37085484e-02]],\n\n       [[ 1.68744925e+01,  4.86269717e+01, -3.54535158e-01, ...,\n          6.97092074e+00,  1.38401427e+01,  4.41897478e+01],\n        [ 3.80673611e+01,  5.08088452e+01,  3.24135895e+01, ...,\n         -1.64036397e+00,  9.20252673e-01, -2.28191503e+01],\n        [ 2.45419863e+01,  3.67156591e+01,  8.06594166e+01, ...,\n          2.35165419e+01, -2.27769611e+00,  3.19786177e+01],\n...\n        [ 7.65492507e+00, -2.52651612e+01, -1.14319746e+01, ...,\n          2.10783876e+00,  6.51461474e+01,  1.39449220e+01],\n        [-1.52884615e+01, -3.81858568e+01,  3.83976350e+01, ...,\n          6.70649060e+00, -4.19015833e+00,  3.10102382e+01],\n        [ 3.22122816e+01,  1.40541501e-01,  1.39868866e+01, ...,\n          1.39995757e+00,  1.40413480e+01,  2.15327900e+01]],\n\n       [[ 6.45006934e+01,  1.89982303e+01,  4.92943955e+01, ...,\n          4.48989355e+01,  6.00626370e+00, -4.11335459e+00],\n        [ 7.69567142e+01,  3.85217722e+01,  6.00470265e+01, ...,\n         -2.85540970e+01,  1.17829549e+00,  2.88735960e+01],\n        [ 3.82736929e+01,  1.35754696e+01,  3.18636391e+01, ...,\n          1.30573340e+01,  2.58695675e+01,  3.63775721e+00],\n        ...,\n        [ 2.64333429e+01,  2.96956055e+01,  9.65696351e+00, ...,\n         -8.66072179e+00, -1.22879325e+00, -1.74010423e+01],\n        [-3.67585411e+01, -3.45711874e+00,  1.22087765e+01, ...,\n          2.78805963e+01, -2.67630198e+01, -1.17549578e+01],\n        [ 6.74946544e+01,  2.73016469e+01,  1.89342532e+01, ...,\n         -1.80490133e+01, -4.11156817e+01, -2.60158793e+01]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='y_obs', length=500))Attributes: (2)modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 4, draw: 1000, y_obs: 133)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * y_obs    (y_obs) int64 0 1 2 3 4 5 6 7 8 ... 125 126 127 128 129 130 131 132\nData variables:\n    y        (chain, draw, y_obs) float64 -3.957 -3.957 -3.957 ... -4.013 -4.099\nAttributes:\n    created_at:                  2024-05-27T06:01:00.626582\n    arviz_version:               0.17.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000y_obs: 133Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])y_obs(y_obs)int640 1 2 3 4 5 ... 128 129 130 131 132array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132])Data variables: (1)y(chain, draw, y_obs)float64-3.957 -3.957 ... -4.013 -4.099array([[[-3.95662204, -3.95668036, -3.95697947, ..., -3.96786393,\n         -4.06567925, -4.08819109],\n        [-3.92804385, -3.95248402, -4.00438808, ..., -4.19279911,\n         -3.90894979, -4.05289002],\n        [-4.11236339, -4.08418887, -3.99366348, ..., -4.1948162 ,\n         -3.97180293, -4.09921461],\n        ...,\n        [-3.88915792, -3.88893229, -3.89724674, ..., -4.04248536,\n         -3.89847518, -4.03951892],\n        [-3.88161419, -3.88254965, -3.9052254 , ..., -4.08668059,\n         -3.88211318, -4.03326819],\n        [-4.08257196, -4.09265666, -4.04648571, ..., -3.99108035,\n         -3.91131912, -4.03785686]],\n\n       [[-3.85025382, -3.84763163, -3.85608195, ..., -4.34165517,\n         -3.86684608, -4.00723736],\n        [-4.0090095 , -4.00707108, -4.017144  , ..., -4.01175855,\n         -4.25268066, -4.11837821],\n        [-3.97404168, -3.97456237, -4.00764182, ..., -4.0433394 ,\n         -4.00511976, -4.10110199],\n...\n        [-3.85025004, -3.83654156, -3.832685  , ..., -4.37637177,\n         -3.86484545, -4.00066997],\n        [-4.06709934, -4.05434047, -4.05493257, ..., -4.17570336,\n         -3.99476606, -4.11607839],\n        [-4.01346366, -4.01406212, -4.01257167, ..., -4.30726434,\n         -4.01839009, -4.13004222]],\n\n       [[-3.98552821, -3.98561292, -4.0079376 , ..., -4.15815157,\n         -3.96833515, -4.09613757],\n        [-3.96540632, -3.96291088, -3.95021137, ..., -4.13476712,\n         -3.95195981, -4.08343265],\n        [-4.17463712, -4.17201421, -4.11695573, ..., -4.3197009 ,\n         -4.04463544, -4.14929407],\n        ...,\n        [-4.0692212 , -4.06241625, -4.09034963, ..., -4.09469991,\n         -3.98159516, -4.09860353],\n        [-4.68482086, -4.73793167, -4.72387693, ..., -4.11913623,\n         -3.92229916, -4.06192875],\n        [-3.97170872, -3.97200671, -3.98370832, ..., -4.02417539,\n         -4.01254991, -4.09910258]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       123, 124, 125, 126, 127, 128, 129, 130, 131, 132],\n      dtype='int64', name='y_obs', length=133))Attributes: (4)created_at :2024-05-27T06:01:00.626582arviz_version :0.17.0modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:          (chain: 4, draw: 1000)\nCoordinates:\n  * chain            (chain) int64 0 1 2 3\n  * draw             (draw) int64 0 1 2 3 4 5 6 ... 993 994 995 996 997 998 999\nData variables:\n    acceptance_rate  (chain, draw) float64 0.9484 0.989 0.9772 ... 0.9905 0.9283\n    step_size        (chain, draw) float64 0.1631 0.1631 ... 0.1827 0.1827\n    diverging        (chain, draw) bool False False False ... False False False\n    energy           (chain, draw) float64 671.4 665.4 671.0 ... 670.8 674.0\n    n_steps          (chain, draw) int64 31 31 31 15 15 31 ... 15 31 31 15 15 31\n    tree_depth       (chain, draw) int64 5 5 5 4 4 5 5 4 4 ... 4 5 5 4 5 5 4 4 5\n    lp               (chain, draw) float64 661.7 661.3 665.7 ... 665.6 668.8\nAttributes:\n    created_at:                  2024-05-27T06:01:00.625568\n    arviz_version:               0.17.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (7)acceptance_rate(chain, draw)float640.9484 0.989 ... 0.9905 0.9283array([[0.94837217, 0.98900712, 0.97718268, ..., 0.99599719, 0.97974622,\n        0.93727528],\n       [0.9824305 , 0.87426419, 0.99008681, ..., 0.99942845, 0.97008375,\n        0.99555447],\n       [0.99910745, 0.98130743, 0.95550076, ..., 0.97069333, 0.9342073 ,\n        0.95033605],\n       [0.97363799, 0.97895034, 0.93808177, ..., 0.9514047 , 0.99054107,\n        0.92830969]])step_size(chain, draw)float640.1631 0.1631 ... 0.1827 0.1827array([[0.16306409, 0.16306409, 0.16306409, ..., 0.16306409, 0.16306409,\n        0.16306409],\n       [0.15671706, 0.15671706, 0.15671706, ..., 0.15671706, 0.15671706,\n        0.15671706],\n       [0.15280013, 0.15280013, 0.15280013, ..., 0.15280013, 0.15280013,\n        0.15280013],\n       [0.1826574 , 0.1826574 , 0.1826574 , ..., 0.1826574 , 0.1826574 ,\n        0.1826574 ]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])energy(chain, draw)float64671.4 665.4 671.0 ... 670.8 674.0array([[671.39512608, 665.35858794, 670.96295141, ..., 670.72923684,\n        668.42111758, 669.24601286],\n       [665.04518693, 671.40450254, 672.47055359, ..., 665.8504822 ,\n        674.75907256, 672.99992931],\n       [672.00375668, 668.16520377, 666.3166769 , ..., 671.79404347,\n        671.22701287, 671.74308362],\n       [669.46327702, 671.11966704, 667.28825365, ..., 665.11423361,\n        670.79366043, 673.9890605 ]])n_steps(chain, draw)int6431 31 31 15 15 ... 31 31 15 15 31array([[31, 31, 31, ..., 31, 15, 15],\n       [15, 31, 31, ..., 31, 31, 31],\n       [31, 31, 15, ..., 15, 31, 15],\n       [31, 15, 15, ..., 15, 15, 31]])tree_depth(chain, draw)int645 5 5 4 4 5 5 4 ... 5 5 4 5 5 4 4 5array([[5, 5, 5, ..., 5, 4, 4],\n       [4, 5, 5, ..., 5, 5, 5],\n       [5, 5, 4, ..., 4, 5, 4],\n       [5, 4, 4, ..., 4, 4, 5]])lp(chain, draw)float64661.7 661.3 665.7 ... 665.6 668.8array([[661.70967519, 661.2875458 , 665.70729556, ..., 661.40453042,\n        661.53660449, 661.51768248],\n       [662.47186408, 665.25230302, 666.28391286, ..., 662.10849958,\n        668.1822538 , 664.18500466],\n       [662.60487008, 660.69369885, 662.99303071, ..., 663.00720061,\n        664.60205279, 667.39059102],\n       [664.1010232 , 660.27378818, 664.08879552, ..., 661.26338997,\n        665.58839085, 668.77321037]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (4)created_at :2024-05-27T06:01:00.625568arviz_version :0.17.0modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (y_obs: 133)\nCoordinates:\n  * y_obs    (y_obs) int64 0 1 2 3 4 5 6 7 8 ... 125 126 127 128 129 130 131 132\nData variables:\n    y        (y_obs) float64 0.0 -1.3 -2.7 0.0 -2.7 ... -2.7 10.7 -2.7 10.7\nAttributes:\n    created_at:                  2024-05-27T06:01:00.626851\n    arviz_version:               0.17.0\n    inference_library:           numpyro\n    inference_library_version:   0.13.2\n    sampling_time:               1.870798\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:y_obs: 133Coordinates: (1)y_obs(y_obs)int640 1 2 3 4 5 ... 128 129 130 131 132array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132])Data variables: (1)y(y_obs)float640.0 -1.3 -2.7 ... 10.7 -2.7 10.7array([   0. ,   -1.3,   -2.7,    0. ,   -2.7,   -2.7,   -2.7,   -1.3,\n         -2.7,   -2.7,   -1.3,   -2.7,   -2.7,   -2.7,   -5.4,   -2.7,\n         -5.4,    0. ,   -2.7,   -2.7,    0. ,  -13.3,   -5.4,   -5.4,\n         -9.3,  -16. ,  -22.8,   -2.7,  -22.8,  -32.1,  -53.5,  -54.9,\n        -40.2,  -21.5,  -21.5,  -50.8,  -42.9,  -26.8,  -21.5,  -50.8,\n        -61.7,   -5.4,  -80.4,  -59. ,  -71. ,  -91.1,  -77.7,  -37.5,\n        -85.6, -123.1, -101.9,  -99.1, -104.4, -112.5,  -50.8, -123.1,\n        -85.6,  -72.3, -127.2, -123.1, -117.9, -134. , -101.9, -108.4,\n       -123.1, -123.1, -128.5, -112.5,  -95.1,  -81.8,  -53.5,  -64.4,\n        -57.6,  -72.3,  -44.3,  -26.8,   -5.4, -107.1,  -21.5,  -65.6,\n        -16. ,  -45.6,  -24.2,    9.5,    4. ,   12. ,  -21.5,   37.5,\n         46.9,  -17.4,   36.2,   75. ,    8.1,   54.9,   48.2,   46.9,\n         16. ,   45.6,    1.3,   75. ,  -16. ,  -54.9,   69.6,   34.8,\n         32.1,  -37.5,   22.8,   46.9,   10.7,    5.4,   -1.3,  -21.5,\n        -13.3,   30.8,  -10.7,   29.4,    0. ,  -10.7,   14.7,   -1.3,\n          0. ,   10.7,   10.7,  -26.8,  -14.7,  -13.3,    0. ,   10.7,\n        -14.7,   -2.7,   10.7,   -2.7,   10.7])Indexes: (1)y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       123, 124, 125, 126, 127, 128, 129, 130, 131, 132],\n      dtype='int64', name='y_obs', length=133))Attributes: (7)created_at :2024-05-27T06:01:00.626851arviz_version :0.17.0inference_library :numpyroinference_library_version :0.13.2sampling_time :1.870798modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n              \n            \n            \n\n\nand plot the model fit to see if it can recover the observed data.\n\nax = az.plot_hdi(new_data['X'], idata_hsgp['posterior_predictive']['y'], fill_kwargs={'alpha': 0.2, 'color':'firebrick'}, figsize=(9, 8))\n\naz.plot_hdi(new_data['X'], idata_hsgp['posterior_predictive']['y'], fill_kwargs={'alpha': 0.8, 'color':'firebrick'}, hdi_prob=0.5)\n\ny_mean = idata_hsgp['posterior_predictive']['y'].mean(dim=('chain', 'draw'))\n\nax.plot(new_data['X'], y_mean, label='Expected posterior predictive', color='k')\n\nax.scatter(df['X'], df['y'], label='Observed Datapoints')\n\nax.legend()\n\nax.set_title(\"Posterior Predictive Distribution \\n Based on HSGP approximation\");\n\n\n\n\nHere again we’ve allowed the model to extrapolate beyond the observed data but we achieve far more reasonable extrapolation than with the Spline model. And we can compare other performance metrics versus the spline models to see that by the aggregate performance measures our HSGP model seems to come out on top too.\n\nmodels_dict = {\"piecewise_constant\": idata_spline1, \"piecewise_linear\": idata_spline2, \"cubic_bspline\": idata_spline3, \"cubic_bspline_10\": idata_spline4, \n\"cubic_bspline_15\": idata_spline5, 'hsgp': idata_hsgp}\ndf_compare = az.compare(models_dict)\ndf_compare\n\n\n\n\n\n  \n    \n      \n      rank\n      elpd_loo\n      p_loo\n      elpd_diff\n      weight\n      se\n      dse\n      warning\n      scale\n    \n  \n  \n    \n      hsgp\n      0\n      -608.819807\n      9.958864\n      0.000000\n      0.872617\n      12.009745\n      0.000000\n      False\n      log\n    \n    \n      cubic_bspline_10\n      1\n      -612.578311\n      11.652463\n      3.758504\n      0.000000\n      9.646544\n      3.125929\n      True\n      log\n    \n    \n      cubic_bspline_15\n      2\n      -620.709337\n      19.669201\n      11.889531\n      0.000000\n      9.600116\n      3.836813\n      True\n      log\n    \n    \n      cubic_bspline\n      3\n      -634.647180\n      8.703519\n      25.827373\n      0.000000\n      8.915728\n      7.666315\n      True\n      log\n    \n    \n      piecewise_constant\n      4\n      -643.781042\n      6.981098\n      34.961235\n      0.069309\n      9.770740\n      9.490854\n      False\n      log\n    \n    \n      piecewise_linear\n      5\n      -647.016885\n      5.987587\n      38.197078\n      0.058073\n      7.914787\n      9.204140\n      False\n      log\n    \n  \n\n\n\n\nFor a deeper dive on HSGP you might consult Juan Orduz’s work here"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#industry-questions-are-causal",
    "href": "talks/missing_data/missing_data_causal.html#industry-questions-are-causal",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Industry Questions are Causal",
    "text": "Industry Questions are Causal\nOn the Importance of Theory Construction\n\n\n\n\n\nID\n\\(Y_{i}(0)\\)\n\\(Y_{i}(1)\\)\n\n\n\n\n1\n?\n1\n\n\n2\n1\n?\n\n\n3\n?\n0\n\n\n4\n?\n1\n\n\n5\n0\n?\n\n\n\nThe Fundamental problem of Causal Inference as Missing Data\n\n\n\nThe heart of causal inference is understanding the risk of confounding influence.\n\n\n\n\nNaively optimising for some in-sample predictive benchmark does not protect your model from confounding bias.\n\n\n\n\nCausal models with deliberate and careful construction of the dependence mechanism are your best hope for genuine insight and robust predictive performance\n\n\n\n\nThis is crucial for model explainability in the human-centric domains, where the decisions need to be justifiable.\n\n\n\n\nUsed to answer Counterfactuals\n\nHow do patterns of Employee empowerment change if we can intervene on their relationship with their manager?"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#hierarchies-and-human-relations",
    "href": "talks/missing_data/missing_data_causal.html#hierarchies-and-human-relations",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Hierarchies and Human Relations",
    "text": "Hierarchies and Human Relations\nPower Structures Interactions\n\n\n\n\n\nMilitary Hierarchy\n\n\n\n\n\n\nCapitalism"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#recap-and-conclusion",
    "href": "talks/missing_data/missing_data_causal.html#recap-and-conclusion",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Recap and Conclusion",
    "text": "Recap and Conclusion\nWe’ve seen the application of missing data analysis to survey data in the context of People Analytics.\n\nMultivariate approaches are effective but cannot help address confounding bias,\nThe flexibility of the Bayesian approach can be tailored to the appropriate complexity of our theory about why our data is missing.\nHierarchical structures pervade business - conduits for leadership influence/communication channels. Hierarchical modelling can isolate estimates of this impact and control for biases of naive aggregates.\n\nReveal inefficiencies and mismatches between team and management.\n\nImputation gives “voice” to the missing. Inverse Propensity weighting corrects mis-representative samples. Both are correctives for selection effect bias.\n\n\n\n\nMissing Data with PyMC"
  },
  {
    "objectID": "notes/certain_things/Wedding/Speech.html",
    "href": "notes/certain_things/Wedding/Speech.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "You will have to forgive me, but I’m going to try and say something about love.\nAbout loving Joanne.\nMany of you probably love her too, but I suspect we got to the same destination by different routes.\nLoving Joanne has crystallised a few things for me. Cleared up some utterly naive confusions of mine. I don’t think these confusions were mine alone.\nLove is an overloaded concept. Obscured by layers of cliche. Efforts at defining the thing tend to be premature. Good definitions are battle tested, cover edge cases and disbar near imposters. I’m not going to offer you a definition. But I want to say why it’s important to reckon with the demands of love.\n\nShow , don’t tell.\nThere is a clarity that comes with absence. Travelling reveals this. (Prison too I imagine.) When you travel, you know what you miss at home. Sure, there is a mundane sense of loss, you don’t know where the teabags are, or how to navigate public transport… but the profound, deep loss is the absence of her. When you find someone whose absence is a glaring abyss in every room, you’ve found a good one.\nThere were other signs too though… Less apocalyptic signs. Hints, that maybe we were a good fit. Hints that we’d complement one another. Hints that she’d put up with me. That last one being about as rare a trait as you’d guess.\nI first emailed Joanne about the usefulness of baby wipes in 2014. I could not over-emphasise how central they were for my cleanliness while traveling in Mongolia. In a follow up series of densely overwritten emails about baked beans and desert landscapes, I wooed this woman. That is a statement as much about her character as it is about mine.\nWe have been together now for more than 10 years. We bought a house, we had a son. We lived through a pandemic. In sickness and health, good humour and fierce tempers. We have shared more than I imagined possible at the beginning. There was no good map, no rote formula to trace out this particular trajectory.\nThat was a problem for me. By temperament I’m the type of person who will use Wikipedia to spoil the ending of a thriller, if I think I’ve guessed where the plot is going. And I had some inclination that we were trending towards “Happily ever after.” The urge to extrapolate is overwhelming."
  },
  {
    "objectID": "oss/causalpy/inv_prop_weights.html",
    "href": "oss/causalpy/inv_prop_weights.html",
    "title": "Inverse Propensity Score Weighting in CausalPy",
    "section": "",
    "text": "Inverse Propensity Score Weighting\nIn this project I sought to add the functionality for bayesian inverse propensity weighting functionality to the CausalPy package. I adapted earlier work with PyMC to contribute the base classes to the package and demonstrated how these classes can be used to esitmate various treatment effect estimands. The demonstration can be seen here"
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#bayesian-penalised-splines-smoothing-priors",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#bayesian-penalised-splines-smoothing-priors",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Bayesian Penalised Splines (Smoothing Priors)",
    "text": "Bayesian Penalised Splines (Smoothing Priors)\nPriors can be used in the same way to that regularisation techniques are deployed in machine learning. They serve as a kind of extra model parameter used to effect a variable selection routine. In the case of splines we are creating these synthetic features and this can be seen as a generating a feature selection problem. So far we have discussed priors as a means of imposing structure based on our knowledge of the problem but we can also use priors to automate feature selection over spline features.\nHere we fit a two spline models to the speed-test data set but we use 60 knot points to generate our spline features. This is wildy excessive for the data set in question. We show how to use non-centered parameterisation with a “smoothing” trick to induce a good fit despite excessive features. In this way we’re removing the burden of manually specifying the spline structure. This is akin to penalisation methods used in PyGam but with a Bayesian flavour.\n\nmax_dev= len(y)\nnum_knots=60\nknot_list = np.linspace(0, np.max(X), num_knots)[2:-2]\ndev_periods = np.linspace(0, np.max(X), len(y))\n\nBx = dmatrix(\n    \"bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=False)\",\n    {\"dev_periods\": dev_periods, \"knots\": knot_list},\n)\nBx\n\ndef penalised_model(Bx, penalised=True):\n    with pm.Model() as model:\n        basis = pm.MutableData('basis', np.asfortranarray(Bx))\n\n        if penalised:\n            sigma_a = pm.HalfCauchy('sigma_a', 5.)\n            a0 = pm.Normal('a0', 0., 10.)\n            delta_a = pm.Normal('delta_a', 0., 1.,shape=num_knots)\n            a = pm.Deterministic('a', a0 + (sigma_a * delta_a).cumsum())\n        else: \n            b = pm.Normal('delta_a', 0., 25,\n            shape=num_knots)\n            a0 = pm.Normal('a0', 0, 10)\n            a = pm.Deterministic('a', a0 + b)\n        \n        sigma = pm.HalfCauchy('sigma', 5.)\n        obs = pm.Normal('obs', pm.math.dot(basis, a), sigma, observed=y)\n        idata = pm.sample(target_accept=.95, idata_kwargs={\"log_likelihood\": True})\n    \n    return model, idata\n\nmodel_raw, idata_raw = penalised_model(Bx, penalised=False)\nmodel_penalised, idata_penalised = penalised_model(Bx, penalised=True)\n\nThe model structure is as follows:\n\npm.model_to_graphviz(model_penalised)\n\n\n\n\nThis parameterisation trick is owed to work by Austin Rochford and Adrian Seyboldt discussed here. We can see how the two parameterisations induce quite distinct values on the coefficients for splines.\n\nfig, ax = plt.subplots(figsize=(9, 20))\naz.plot_forest([idata_raw, idata_penalised], var_names=['a'], combined=True, ax=ax, model_names=['raw', 'penalised'])\nax.fill_betweenx(range(160), -25, 25, alpha=0.2, color='red');\n\n\n\n\nThis results in much different posterior predictive fits. The un-smoothed spline fit overfits to the small partitions of the spline features, whereas the smoothing spline model achieves a reasonable fit recovering the smooth structure of the data generating process.\n\nB_df = pd.DataFrame(Bx)\nweights = az.summary(idata_penalised, var_names='a')['mean']\nfig, ax = plt.subplots(figsize=(8, 7))\nprocessed = np.array([np.dot(B_df, az.extract(idata_penalised['posterior']['a'])['a'].to_numpy()[:, i]) for i in range(4000)])\naz.plot_hdi(range(len(y)), processed, ax=ax,  fill_kwargs={'alpha': 0.4, 'color':'firebrick'})\naz.plot_hdi(range(len(y)), processed, ax=ax,  fill_kwargs={'alpha': 0.8, 'color':'firebrick'}, hdi_prob=.50)\nweights_raw = az.summary(idata_raw, var_names='a')['mean']\nax.plot(range(len(y)), np.dot(B_df, weights), color='black', label='Cubic_Bsplines_60_Penalised')\nax.plot(range(len(y)), np.dot(B_df, weights_raw), color='blue', linestyle='--', label='Cubic_Bsplines_60_unpenalised')\nax.set_title(\"Bayesian Automatic Regularisation \\n Smoothing Priors over 60 Knots\")\nax.legend();\n\n\n\n\nThis is reflected in the model performance measures as well.\n\nmodels_dict = {\"piecewise_constant\": idata_spline1, \"piecewise_linear\": idata_spline2, \"cubic_bspline\": idata_spline3, \"cubic_bspline_10\": idata_spline4, \n\"cubic_bspline_20\": idata_spline5, 'hsgp': idata_hsgp, 'spline_penalised_bayes': idata_penalised, 'cubic_bspline_60': idata_raw}\ndf_compare = az.compare(models_dict)\ndf_compare\n\n\n\n\n\n  \n    \n      \n      rank\n      elpd_loo\n      p_loo\n      elpd_diff\n      weight\n      se\n      dse\n      warning\n      scale\n    \n  \n  \n    \n      hsgp\n      0\n      -608.819807\n      9.958864\n      0.000000\n      0.764385\n      12.009745\n      0.000000\n      False\n      log\n    \n    \n      spline_penalised_bayes\n      1\n      -610.258217\n      19.573938\n      1.438411\n      0.115567\n      11.444307\n      2.822797\n      True\n      log\n    \n    \n      cubic_bspline_10\n      2\n      -612.578311\n      11.652463\n      3.758504\n      0.000000\n      9.646544\n      3.125929\n      True\n      log\n    \n    \n      cubic_bspline_20\n      3\n      -620.709337\n      19.669201\n      11.889531\n      0.000000\n      9.600116\n      3.836813\n      True\n      log\n    \n    \n      cubic_bspline\n      4\n      -634.647180\n      8.703519\n      25.827373\n      0.000000\n      8.915728\n      7.666315\n      True\n      log\n    \n    \n      cubic_bspline_60\n      5\n      -642.070674\n      24.951194\n      33.250867\n      0.000667\n      9.135971\n      8.313174\n      False\n      log\n    \n    \n      piecewise_constant\n      6\n      -643.781042\n      6.981098\n      34.961235\n      0.061876\n      9.770740\n      9.490854\n      False\n      log\n    \n    \n      piecewise_linear\n      7\n      -647.016885\n      5.987587\n      38.197078\n      0.057506\n      7.914787\n      9.204140\n      False\n      log\n    \n  \n\n\n\n\nThis functionality of Bayesian priors is not unique to spline models. We can mirror Lasso regression models with double-exponential prior specifications. Ridge regrssion is provably a species of regression models with Normal priors pulled toward 0. Other alternatives such horse-shoe and slab and spike specifications offer a rich range of bayesian approaches to variable selection methods with sparse data.\n\nRecap\nSo far we’ve seen how we can use splines and gauassian processes to model highly eccentric functional relationships where the function could be approximated with univariate smoothing routine. These are two distinct abstractions which seem adequately fit to the world, but demand very different ways of thinking about the underlying reality. This is no fundamental contradiction in so far as the world admits many descriptions.\n\n[K]nowledge develops in a multiplicity of theories, each with it’s limited utility … These theories overlap very considerable in their logical laws and in much else, but that they add up to an integrated and consistent whole is only a worthy ideal and not a pre-requistite of scientific progress … Let reconciliations proceed. - W.V.O Quine in Word and Object\n\nAnother observation in a similar vein is that penalised spline models are provably equivalent to hierarchical regression (random effects) models. This is striking because the character of these types of model seems diametrically opposed. With spline models you jerry-rig your features to an optimisation goal, with hierarchical model you tend to impose theoretical structure to induce shrinkage. It’s hard to see how this would work? With a penalised spline model are you inducing a hierarchy of latent features you can’t name? Should you even try to translate between the two!? How can we leverage our prior knowledge of the data generating process to add structure to spline model predictions?\nThe abstract components of our model are less graspable than their predictive performance yet the qualitative character of theory buiding differs markedly between these abstractions and aesthetics and explainability feature heavily in model preference. Next we’ll add another layer to our abstractions and show how to use hierarchical modelling over spline fits can be used to extract insight into the data generating process over a family of curves. In particular we’ll focus on the development of insurance loss curves and how to constrain our predictions within reasonable bounds. This is an apt problem for spline modelling in that the process is repeatable annually, but the domain of the support i.e. development in time is fixed year-on-year so the extrapolation problem is muted."
  },
  {
    "objectID": "oss/causalpy/justifying_instruments.html",
    "href": "oss/causalpy/justifying_instruments.html",
    "title": "Justifying Instruments in CausalPy",
    "section": "",
    "text": "The Strength of Instruments\nIn this project I sought to outline the process of justifying instrumental variable regressions in CausalPy using the Bayesian modelling workflow. We emphasised the role of exploratory data analysis and plotting in how we come to understand the causal dynamics at play in our data generating process. The focus was on the manner of justification and role of comparative assessment in credible inference. The demonstration can be seen here.\n\n\n\nComparison with OLS"
  },
  {
    "objectID": "posts/post-with-code/multilevel_confounding/multilevel_models.html",
    "href": "posts/post-with-code/multilevel_confounding/multilevel_models.html",
    "title": "Freedom, Hierarchies and Confounded Estimates",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport bambi as bmb\nimport pymc as pm\nimport arviz as az\nimport matplotlib.pyplot as plt\nfrom matplotlib import transforms\nfrom itertools import product\nimport pyfixest as pf\nimport statsmodels.formula.api as smf\nfrom patsy import dmatrices, dmatrix\nimport seaborn as sns\nimport networkx as nx\nfrom pyfixest.did.estimation import did2s, lpdid\nfrom matplotlib.lines import Line2D\nfrom matplotlib.patches import Patch\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\npd.options.mode.chained_assignment = None \n\nnp.random.seed(100)"
  },
  {
    "objectID": "posts/post-with-code/multilevel_confounding/multilevel_models.html#estimation-and-group-effects",
    "href": "posts/post-with-code/multilevel_confounding/multilevel_models.html#estimation-and-group-effects",
    "title": "Freedom, Hierarchies and Saturated Regression (WIP)",
    "section": "Estimation and Group Effects",
    "text": "Estimation and Group Effects\nFirst consider an estimation example due to Richard McElreath’s lecture series where we examine the various parameter recovery options available in the case of group level confounding. Define a data generating process determined by group level effects:\n\ndef inv_logit(p):\n    return np.exp(p) / (1 + np.exp(p))\n\nN_groups = 30\nN_id = 3000\na0 = -2\nbXY = 1\nbZY = -0.5\ng = np.random.choice(range(N_groups), size=N_id, replace=True)\nUg = np.random.normal(1.5, size=N_groups)\n\nX = np.random.normal(Ug[g], size=N_id)\nZ = np.random.normal(size=N_groups)\n\ns = a0 + bXY*X + Ug[g] + bZY*Z[g]\np = inv_logit(s)\nY = np.random.binomial(n=1, p=p)\n\n\nsim_df = pd.DataFrame({'Y': Y, 'p': p, 's': s, 'g': g, 'X': X, 'Z': Z[g]})\nsim_df\n\nThis is a bernoulli outcome with group level confounds. If we model this relationship the confounding effects will bias naive parameter estimates on the covariates \\(X\\), \\(Z\\). Seeing different results as we explore different ways of parameterising the relationships.\n\n\n\n\n\n\nWarning\n\n\n\nThere is a huge degree of confusion over the meaing of the terms “Fixed Effects” and “Random Effects”. Within this blog post we will mean the population level parameters. \\[\\beta X\\]\nfor an individual variable \\(X\\) when we refer to fixed effects. Corresspondingly we will refer to group-level parameters \\(\\beta_{g}\\)\n\\[\\Big(\\underbrace{\\beta}_{pop} + \\underbrace{\\beta_{g}}_{group}\\Big)X\\]\nwhich are incorporated into our model equation modifying population level parameters as random effects. We will generally use Wilkison notation to specify these choices where random effects for modifying a population are denoted with a conditional bar over the variable ( X | group) and fixed effects are specified by just including the variable in the equation i.e. y ~ X + (Z | group) where X has a fixed effect parameterisation and Z a random effects parameterisation. We can also create indicator variables for group membership using this syntax with y ~ C(group) + X + Z where under the hood we pivot the group category into a zero-one variables indicating group membership. This parameterisation means each indicator variable (one for each level of the grouping variable) will receive a fixed effects population parameter.\n\n\n\nNaive Model\nThe formula syntax for Bambi also enables us to specify generalised linear models with bernoulli outcomes.\n\nnaive_model = bmb.Model(f\"Y['>.5'] ~ X + Z \", sim_df, \nfamily=\"bernoulli\")\nnaive_idata = naive_model.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True},)\n\n\naz.summary(naive_idata, var_names=['Intercept', 'X', 'Z'])\n\nHere we see that all three parameter estimates are biased away from their true values. Let’s try a simple fixed effects approach that adds indicator variables for all but one of the group levels.\n\n\nFixed Effects Model\nThe additional syntax for creating the group level indicator variables is specified here.\n\nfixed_effects_model = bmb.Model(f\"Y['>.5'] ~ C(g) + X + Z\", sim_df, \nfamily=\"bernoulli\")\nfixed_effects_idata = fixed_effects_model.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True},)\n\n\naz.summary(fixed_effects_idata, var_names=['Intercept', 'X', 'Z'])\n\nNow we see that the coefficient on the \\(X\\) variable seem correct, but the coefficient on \\(Z\\) is wildly wrong. Indeed the uncertainty interval on the \\(Z\\) coefficient is huge. The fixed effect model was unable to learn anything about the correct parameter. Whereas the naive model seems to learn the correct \\(Z\\) parameter but over estimates the \\(X\\) coefficient. These are the kinds of trade-offs we need to be wary of as we account for the complexities of extensive group interactions in our model’s functional form.\n\n\nCode\nfig, axs = plt.subplots(2, 1, figsize=(10, 9))\naxs = axs.flatten()\n\naz.plot_posterior(naive_idata, var_names=['X'], ax=axs[0], \npoint_estimate=None, color='red', label='Naive Model')\naxs[0].axvline(1)\n\naz.plot_posterior(fixed_effects_idata , var_names=['X'], ax=axs[0], point_estimate=None, hdi_prob='hide', label='Fixed Effect Models')\naxs[0].set_title(\"Naive/Fixed Model X Coefficient\")\n\naz.plot_posterior(naive_idata, var_names=['Z'], ax=axs[1], point_estimate=None,  color='red', ref_val_color='black')\n\n\naz.plot_posterior(fixed_effects_idata , var_names=['Z'], ax=axs[1], point_estimate=None, hdi_prob='hide')\n\naxs[1].set_title(\"Naive/Fixed Effect Model Z Coefficient\")\naxs[1].axvline(-0.5);\n\n\nWe now want to try another approach to handle to the group confounding that involves a hierarchical approach to add group level effects to the intercept term.\n\n\nMultilevel Model\nThe syntax for the random effects model is similar.\n\nmultilevel_model = bmb.Model(f\"Y['>.5'] ~ (1 | g) + X + Z\", sim_df, \nfamily=\"bernoulli\")\nmultilevel_model_idata = multilevel_model.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True},)\n\nThis method posits a view that there is a shared underlying process generating each instance of group-behaviour i.e. the reliased values of the outcome within each group. We replace the indivdual fixed effects indicator columns with additional parameters modifying the intercept.\n\naz.summary(multilevel_model_idata, var_names=['X', 'Z', 'Intercept'])\n\nNext we’ll apply the Mundlak device method which adds the group mean back to each observation as a covariate.\n\n\nMundlak Model\nFor this technique we supply group aggregate mean values as additional columns. This somewhat absolves of the requirement to include an extra multiplicity of parameters. It’s more akin to feature creation than model specification, but it serves the same purpose - it accounts for group level variation and provides a mechanism for the model to learn the appropriate weights to accord each group in the final calculation.\n\nsim_df['group_mean'] = sim_df.groupby('g')['X'].transform(np.mean)\n\nsim_df['group_mean_Z'] = sim_df.groupby('g')['Z'].transform(np.mean)\n\nmundlak_model = bmb.Model(f\"Y['>.5'] ~ (1 | g) + X + Z + group_mean\", sim_df, \nfamily=\"bernoulli\")\nmundlak_idata = mundlak_model.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True},)\n\n\naz.summary(mundlak_idata, var_names=['Intercept', 'X', 'Z'])\n\nWe can now plot all the parameter recovery models together and we’ll see that there are some trade-offs between the fixed effects and random effects varieties of the modelling.\n\n\nPlotting the Comparisons\nThe parameter recovery exercise shows striking differences across each of the models\n\n\nCode\nfig, axs = plt.subplots(3, 1, figsize=(10, 11))\naxs = axs.flatten()\n\naz.plot_posterior(naive_idata, var_names=['X'], ax=axs[0], \npoint_estimate=None, color='red', label='Naive', hdi_prob='hide')\naxs[0].axvline(1, color='k', linestyle='--', label='True value')\n\naz.plot_posterior(fixed_effects_idata , var_names=['X'], ax=axs[0], point_estimate=None, hdi_prob='hide', label='Fixed')\n\naz.plot_posterior(multilevel_model_idata, var_names=['X'], ax=axs[0], point_estimate=None, hdi_prob='hide', color='green', label='Hierarchical')\n\naz.plot_posterior(mundlak_idata, var_names=['X'], ax=axs[0], point_estimate=None, hdi_prob='hide', color='purple', label='Mundlak')\n\n\naxs[0].set_title(\"X Coefficient\")\n\naz.plot_posterior(naive_idata, var_names=['Z'], ax=axs[1], point_estimate=None,  color='red', ref_val_color='black', hdi_prob='hide')\n\n\naz.plot_posterior(fixed_effects_idata , var_names=['Z'], ax=axs[1], point_estimate=None, hdi_prob='hide')\n\naz.plot_posterior(multilevel_model_idata, var_names=['Z'], ax=axs[1], point_estimate=None, hdi_prob='hide', color='green')\n\naz.plot_posterior(mundlak_idata, var_names=['Z'], ax=axs[1], point_estimate=None, hdi_prob='hide', color='purple')\n\naxs[1].set_title(\"Z Coefficient\")\naxs[1].axvline(-0.5, color='k', linestyle='--');\n\naz.plot_posterior(naive_idata, var_names=['Intercept'], ax=axs[2], point_estimate=None,  color='red', ref_val_color='black', hdi_prob='hide')\n\n\naz.plot_posterior(fixed_effects_idata , var_names=['Intercept'], ax=axs[2], point_estimate=None, hdi_prob='hide')\n\naz.plot_posterior(multilevel_model_idata, var_names=['Intercept'], ax=axs[2], point_estimate=None, hdi_prob='hide', color='green')\n\naz.plot_posterior(mundlak_idata, var_names=['Intercept'], ax=axs[2], point_estimate=None, hdi_prob='hide', color='purple')\n\naxs[2].axvline(-2, color='k', linestyle='--');\n\n\nImportantly, we see that while the fixed effects model is focused on recovering the treatment effect on the \\(X\\) covariate, it does so somewhat at the expense of accuracy on the other systematic components of the model. This focus renders the model less predictively accurate. Compare the models on the cross-validation score and we see how the hierarchical mundlak model is to be preferred.\n\ncompare_df = az.compare({'naive': naive_idata, 'fixed': fixed_effects_idata, 'hierarchical': multilevel_model_idata, \n'mundlak': mundlak_idata})\ncompare_df\n\n\naz.plot_compare(compare_df);\n\nThis is not the only way to assess viability of the model’s functional form but it’s not a bad way.\n\n\nFull Luxury Bayesian Mundlak Machine\nAs good Bayesians we might be worry about the false precision of adding simple point estimates for the group mean covariates in the Mundlak model. We can remedy this by explicitly incorporating these values as an extra parameter and adding uncertainty to the draws on these parameters.\n\nid_indx, unique_ids = pd.factorize(sim_df[\"g\"])\n\ncoords = {'ids': list(range(N_groups))}\nwith pm.Model(coords=coords) as model: \n\n    x_data = pm.Data('X_data', sim_df['X'])\n    z_data = pm.Data('Z_data', sim_df['Z'])\n    y_data = pm.Data('Y_data', sim_df['Y'])\n\n    alpha0 = pm.Normal('Intercept', 0, 1)\n    alpha_j = pm.Normal('alpha_j', 0, 1, dims='ids')\n    beta_xy = pm.Normal('X', 0, 1)\n    beta_zy = pm.Normal('Z', 0, 1)\n\n    group_means = pm.Normal('group_means', sim_df.groupby('g')['X'].mean().values, .1, dims='ids')\n\n    mu = pm.Deterministic('mu', (alpha0 + alpha_j[id_indx]) + beta_xy*x_data + beta_zy*z_data + group_means[id_indx])\n    p = pm.Deterministic(\"p\", pm.math.invlogit(mu))\n    # likelihood\n    pm.Binomial(\"y\", n=1, p=p, observed=y_data)\n\n    idata = pm.sample(idata_kwargs={\"log_likelihood\": True})\n\n\naz.summary(idata, var_names=['Intercept', 'X', 'Z'])\n\nThis model bakes more uncertainty into the process assuming a kind of measurement-error model which may be more or less apt depending on how much data you’ve acquired and your view of the underlying process. We’ll now examine how these considerations play out when there are multiple group-level influences."
  },
  {
    "objectID": "posts/post-with-code/multilevel_confounding/multilevel_models.html#nested-groups-and-fixed-effects",
    "href": "posts/post-with-code/multilevel_confounding/multilevel_models.html#nested-groups-and-fixed-effects",
    "title": "Freedom, Hierarchies and Saturated Regression (WIP)",
    "section": "Nested Groups and Fixed Effects",
    "text": "Nested Groups and Fixed Effects\nWe’ve seen how various attempts to account for the group effects can more or less recover the parameters of a complex data generating process with group confounding. Now we want to look at a case where we can have interacting group effects at multiple levels.\n\nPupils within Class Rooms within Schools\nA natural three level group hierarchy occurs in the context of educational organisations and business org-charts. We can use this fact to interrogate briefly how inferential statements about treatment effects vary as a function of what and how we control for group level variation. We draw the following data set from Linear Mixed Models: A Practical Guide Using Statistical Software.\n\ndf = pd.read_csv('classroom.csv')\ndf['class_mean'] = df.groupby(['classid'])['mathprep'].transform(np.mean)\ndf['school_mean'] = df.groupby(['schoolid'])['mathprep'].transform(np.mean)\ndf.head()\n\nThe data has three distinct levels: (1) the child or pupil and their demographic attributes and outcome variable mathgain, (2) the classroom and the teacher level attributes such as their experience yearstea and a record of their mathematics courses taken mathprep, (3) school and neighbourhood level with features describing poverty measures in the vicinity housepov.\nWe’ll plot the child’s outcome mathgain against the mathprep and distinguish the patterns by school.\n\n\nCode\ndef rand_jitter(arr):\n    stdev = .01 * (max(arr) - min(arr))\n    return arr + np.random.randn(len(arr)) * stdev\n\n\nschools = df['schoolid'].unique()\nschools_10 = [schools[i:i+10] for i in range(0, len(schools), 10)]\nfig, axs = plt.subplots(3,4, figsize=(20, 10), \nsharey=True, sharex=True)\naxs = axs.flatten()\nfor s, ax in zip(schools_10, axs):\n    temp = df[df['schoolid'].isin(s)]\n    ax.scatter(rand_jitter(temp['mathprep']), temp['mathgain'], \n    c=temp['schoolid'], cmap='tab10')\n    ax.set_title(f\"Schools \\n {s}\");\n\n\nThere is a small number of observed students per school so the individual school level distributions show some extreme outliers but the overall distribution nicely converges to an approximately normal symmetric shape.\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 6))\naxs = axs.flatten()\nfor school in schools:\n    temp = df[df['schoolid'] ==school]\n    axs[0].hist(temp['mathgain'], color='grey', alpha=0.3, density=True, histtype='step', cumulative=True)\n\naxs[0].hist(df['mathgain'], bins=30, ec='black', density=True, cumulative=True, histtype='step')\naxs[1].hist(df['mathgain'], bins=30, ec='black', density=True, cumulative=False)\naxs[0].set_title(\"Cumulative Distribution Function by School\")\naxs[1].set_title(\"Overall Distribution\");\n\nWith these kinds of structures we need to be careful in how we evaluate any treatment effects when there are reasons to believe in group-level effects that impact the outcome variable. Imagine the true treatment effect is a sprinter running too fast to cleanly measure - each group interaction effect is added to his load as a weight. Enough weights absorb enough of the variation in the treatment that he is dragged to a crawl. Whatever movement he can make while dragging this burden is the effect we attribute to the treatment. Or put another way - the effects of various group interactions will modify the treatment effectiveness in some way and unless we account for this impact in our model, then the inferences regarding the treatment will be clouded.\n\n\nInteraction Effects and Nuisance Parameters\nNot all possible interactions will be present in the data. But if we specify the model to account for various interactions we may explode the number of parameters beyond the number of data points. This can cause issues in estimation routines and requires consideration about what the group parameters are aimed at capturing.\nWhy add covariates for group-membership when no observation reflects outcomes in that group? We cannot learn anything about these cases. At least if we add the group effects as a hierarchical random effect we induce shrinkage on the parameter estimates towards the mean of the hierarchical parameter in the model. This means that when predicting on “new” data with examples of these missing cases we can predict a sensible default. The distinction rests in the role we have in mind for these tools. If we seriously commit to the idea that group variation reflects a real “common” process in the larger population and we want to learn about that over-arching process then we deploy a random effects model. But if we only see these as tools for accounting to variation in the sample, allowing us to pin down an alternative focal estimate then the group indicator covariates are just “nuisance” parameters and missing cases are irrelevant.\n\n\n\n\n\n\nPhilosophical Digression\n\n\n\nThis last point skips a little quickly over a fundamental feature of interpreting these models. If we aim to interpret these models as reflecting a common process across these groups that exists in a “population”, then we’re endorsing an inferential view that extends beyond the sample. We’re actively seeking to learn a general truth about the data generating process which we deem to be adequately expressed in our model. If we seek to “soak up” the variation due to group effects, we’re treating these group effects as noise in the sample data and making inferential commitments only about the focal parameter in the model. This approach to learning differentiates approaches to credible causal inference. On the one hand, fixing your estimand and designing estimators to specifically capture that estimate seems like a modest and compelling strategy. On the other hand if your model ignores aspects of underlying phenomena or fails to retrodict the observable data, it’s dubious as to why anyone would trust its output.\n\n\nTo see the extent of redundany we can examine the dimensions of the covariate matrices that result from including more or less interaction terms.\n\ny, X = dmatrices(\"mathgain ~ mathprep + C(schoolid)+ C(classid)\", df, return_type=\"dataframe\")\nprint(X.shape)\n\ny, X1 = dmatrices(\"mathgain ~ mathprep + C(schoolid)/C(classid)\", df, return_type=\"dataframe\")\nprint(X1.shape)\n\n\ny, X2 = dmatrices(\"mathgain ~ mathprep + C(schoolid):C(childid)\", df, return_type=\"dataframe\")\nprint(X2.shape)\n\nWe see here how different ways in which to account for group level variation and interaction effects lead to vastly inflated feature matrices. However not all interaction terms matter, or put another way… nor all the possible interactions feature in the data. So we have likely inflated the data matrix beyond necessity.\nHere we define a helper function to parse a complex interaction formula, remove the columns entirely composed of zeros and return a new formula and dataframe which has a suitable range of features to capture the variation structures in the data.\n\ndef make_interactions_df(formula, df):\n    y, X = dmatrices(formula, df, return_type=\"dataframe\")\n    n = X.shape[1]\n    X = X[X.columns[~(np.abs(X) < 1e-12).all()]]\n    n1 = X.shape[1]\n    target_name = y.columns[0]\n    d = pd.concat([y, X], axis=1)\n    d.drop(['Intercept'], axis=1, inplace=True)\n    d.columns = [c.replace('[', '').replace(']','').replace('C(', '').replace(')', '').replace('.', '_').replace(':', '_') for c in d.columns]\n    cols = ' + '.join([col for col in d.columns if col != target_name])\n    formula = f\"{target_name} ~ {cols}\"\n    print(f\"\"\"Size of original interaction features: {n} \\nSize of reduced feature set: {n1}\"\"\")\n    return formula, d\n\nformula, interaction_df = make_interactions_df(\"mathgain ~ mathprep + C(schoolid):C(childid)\", df)\n\ninteraction_df.head()\n\nWe have reduced the number of interactions by an order of magnitude! We can now fit a regression model to the revised feature matrix.\n\n\nComparing Interaction Models\nConsider the variation in the coefficient values estimated for mathprep as we add more and more interaction effects. The addition of interaction effects generates a large number of completely 0 interaction terms which we remove here.\n\nformulas = [\"\"\"mathgain ~ mathprep + C(schoolid)\"\"\",\n\"\"\" mathgain ~ mathprep + school_mean*class_mean\"\"\" , \n\"\"\"mathgain ~ mathprep + C(schoolid) + C(classid)\"\"\", \n\"\"\"mathgain ~ mathprep + C(schoolid)*C(classid)\"\"\",\n\"\"\"mathgain ~ mathprep + C(classid):C(childid)\"\"\", \n]\n\nestimates_df = []\nfor f in formulas:\n    formula, interaction_df = make_interactions_df(f, df)\n    result = smf.ols(formula, interaction_df).fit()\n    estimates = [[result.params['mathprep']], list(result.conf_int().loc['mathprep', :]), [formula]]\n    estimates = [e for est in estimates for e in est]\n    estimates_df.append(estimates)\n\nestimates_df = pd.DataFrame(estimates_df, columns=['mathprep_estimate', 'lower bound', 'upper bound', 'formula'])\n\nestimates_df\n\nThe point here (perhap obvious) is that the estimate of treatment effects due to some policy or programme can be differently understood when the regression model is able to account for increasing aspects of individual variation. Choice of the right way to “saturate” your regression specification are at the heart of causal inference. We will consider a number of specifications below that incorporate these group effects in a hierarchical model which nests the effect of class-membership within school membership. This choice allows us to control for group specific interactions without worrying about over-indexing on the observed interaction effects in the sample data requiring that we handle more fixed effects parameters than we have data points.\nIn what follows we’ll specify a nested approach to the parameter specifcation using a random effects model. The idea here is that classes are already implicitly nested in schools and so we don’t need to add parameters for classes at multiple schools. Additionally we’re positing that there is independent interest in the effectiveness school/class effects i.e. the degree to which variation in a school/class nest can account for variation in the outcome.\n\nMinimal Model\n\nmodel = bmb.Model(f\"mathgain ~ mathprep + (1 | schoolid / classid)\", df)\nidata = model.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\nThe model specification here is deliberately minimalist we want to observe how much of the variation in the outcome can be accounted for by solely adding extensive controls for interactions of group level effects and the treatment but ignoring all else.\n\nmodel.graph()\n\nWe can see the derived sigma parameters here which can be understood as partialling out the variance of the outcome into components due to those group level effects and the unexplained residuals.\n\naz.summary(idata, var_names=['Intercept', '1|schoolid_sigma', '1|schoolid:classid_sigma', 'mathgain_sigma', 'mathprep'])\n\nNote here the relative proportion of the school specifc variances 1|schoolid_sigma to the overall variance of the residuals mathgain_sigma.\n\n\n\nCalculating the IntraClass Correlation Coefficient\nThese models faciliate the calculation of the ICC statistics which is a measure of “explained variance”. The thought is to gauge the proportion of variance ascribed to one set of random effects over and above the total estimated variance in the baseline model, including the residuals mathgain_sigma.\n\na = idata['posterior']['1|schoolid_sigma']**2\n\nb = (idata['posterior']['1|schoolid:classid_sigma']**2 + idata['posterior']['1|schoolid_sigma']**2)\n\nc = (idata['posterior']['1|schoolid:classid_sigma']**2 + idata['posterior']['1|schoolid_sigma']**2 + idata['posterior']['mathgain_sigma']**2)\n\n(a / c).mean().item() \n\n\n((a + b) / c).mean().item()\n\nWe can see here that the interaction terms do seem to account for a goodly portion of the variance in the outcome and we ought to consider retaining their inclusion in our modelling work. The structure of the problem drives us towards their inclusion. Class/school effects are going to absorb a sufficient portion of the variation. So they merit study in their own right, lest the individual class/school dynamics obscure the effectiveness of the mathprep treatment. Similarly, it’s likely valuable to consider the efficacy of the average class/school in a wider policy conversation.\n\n\nAugmenting the Models\nNext we augment our model with more pupil level control variables aiming to pin down some of the aspects of the variation in the outcome.\n\nAdding Pupil Fixed Effects\nHere we add these fixed effects population parameters. But note they are not merely devices for controlling variance in the outcome, they’re interpretation is likely of independent interest.\n\nmodel_fixed = bmb.Model(f\"mathgain ~ mathkind + sex + minority + ses + mathprep + (1 | schoolid / classid)\", df)\nidata_fixed = model_fixed.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\n\naz.summary(idata_fixed, var_names=['Intercept', \n'mathkind', 'sex', 'minority', 'ses', 'mathprep',\n'1|schoolid_sigma', '1|schoolid:classid_sigma', 'mathgain_sigma'])\n\nNow we add a further class level control.\n\n\nAdding Class Level Fixed Effects\n\nmodel_fixed_1 = bmb.Model(f\"mathgain ~ mathkind + sex + minority + ses + yearstea + mathknow + mathprep + (1 | schoolid / classid)\", df.dropna())\nidata_fixed_1 = model_fixed_1.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\n\naz.summary(idata_fixed_1, var_names=['Intercept', \n'mathkind', 'sex', 'minority', 'ses', 'yearstea', 'mathknow', 'mathprep','1|schoolid_sigma', '1|schoolid:classid_sigma', 'mathgain_sigma'])\n\n\nfig, ax = plt.subplots(figsize=(10, 7))\naz.plot_forest([idata, idata_fixed, idata_fixed_1], combined=True, var_names=['mathprep', '1|schoolid_sigma', '1|schoolid:classid_sigma', 'mathgain_sigma'], ax=ax)\nax.axvline(1)\n\nWe now make use of bambis model interpretation module to plot the marginal effect on the outcome due to changes in the treatment intensity.\n\n\nCode\nfig, axs = plt.subplots(3, 1, figsize=(10, 14), \ndpi=120, sharey=True, sharex=True)\naxs = axs.flatten()\naxs[0].axhline(50, color='red', label='Reference Line', \nlinestyle='--')\naxs[1].axhline(50, color='red', label='Reference Line', \nlinestyle='--')\naxs[2].axhline(50, color='red', label='Reference Line', \nlinestyle='--')\nbmb.interpret.plot_predictions(model, idata, \"mathprep\", ax=axs[0]);\nbmb.interpret.plot_predictions(model_fixed, idata_fixed, \"mathprep\", ax=axs[1]);\nbmb.interpret.plot_predictions(model_fixed_1, idata_fixed_1, \"mathprep\", ax=axs[2]);\naxs[0].set_title(\"Variation in Implied Outcome by Treatment \\n mathgain ~ mathprep + (1 | schoolid / classid) \")\naxs[1].set_title(\"Variation in Implied Outcome by Treatment \\n mathgain ~ mathkind + sex + minority + ses + mathprep + (1 | schoolid / classid) \")\naxs[2].set_title(\"Variation in Implied Outcome by Treatment \\n mathgain ~ mathkind + sex + minority + ses + yearstea + \\n mathknow + mathprep + (1 | schoolid / classid\")\naxs[0].set_xlabel('')\naxs[1].set_xlabel('')\naxs[0].legend();\n\n\nAs we can see here across all the different model specifications we see quite modest effects of treatment with very wide uncertainty. So far, so what?! You might be sceptical that teacher training has any real discernible impact on child outcomes? Maybe you believe other interventions are more important to fund? These kinds of questions determine policy. So misguided policy interventions on child-hood education can have radical consequences. It’s, therefore, vital that we have robust and justifiable approaches to the analysis of these policy questions in the face of group level confounding. This last point is crucial - when model complexity balloons due extensive interaction effects, then we need efficient tools to interrogate the outcome level differences implied by the model choices."
  },
  {
    "objectID": "posts/post-with-code/multilevel_confounding/multilevel_models.html#two-way-fixed-effects-and-temporal-confounding",
    "href": "posts/post-with-code/multilevel_confounding/multilevel_models.html#two-way-fixed-effects-and-temporal-confounding",
    "title": "Freedom, Hierarchies and Confounded Estimates",
    "section": "Two Way Fixed Effects and Temporal Confounding",
    "text": "Two Way Fixed Effects and Temporal Confounding\nDifference in Differences designs are the overworked donkeys of social science. Many, many studies stand or fall by the assumptions baked into DiD designs. There are at least two aspects to these assumptions (i) the substantive commitments about the data generating process (e.g. parrallel trends) and (ii) the appropriateness of the functional form used to model (i). We will look first at a case where all the assumptions can be met, and then examine how things break-down under challenging staggered treatment regimes.\n\nEvent Studies and Change in Time\nEvent studies or dynamic treatment effects are an approach to measuring the gradual treatment effect as it evolves in time. We take this panel data set from the pyfixest package to demonstrate.\n\nurl = \"https://raw.githubusercontent.com/py-econometrics/pyfixest/master/pyfixest/did/data/df_het.csv\"\ndf_het = pd.read_csv(url)\ndf_het.head()\n\n\n\n\n\n  \n    \n      \n      unit\n      state\n      group\n      unit_fe\n      g\n      year\n      year_fe\n      treat\n      rel_year\n      rel_year_binned\n      error\n      te\n      te_dynamic\n      dep_var\n    \n  \n  \n    \n      0\n      1\n      33\n      Group 2\n      7.043016\n      2010\n      1990\n      0.066159\n      False\n      -20.0\n      -6\n      -0.086466\n      0\n      0.0\n      7.022709\n    \n    \n      1\n      1\n      33\n      Group 2\n      7.043016\n      2010\n      1991\n      -0.030980\n      False\n      -19.0\n      -6\n      0.766593\n      0\n      0.0\n      7.778628\n    \n    \n      2\n      1\n      33\n      Group 2\n      7.043016\n      2010\n      1992\n      -0.119607\n      False\n      -18.0\n      -6\n      1.512968\n      0\n      0.0\n      8.436377\n    \n    \n      3\n      1\n      33\n      Group 2\n      7.043016\n      2010\n      1993\n      0.126321\n      False\n      -17.0\n      -6\n      0.021870\n      0\n      0.0\n      7.191207\n    \n    \n      4\n      1\n      33\n      Group 2\n      7.043016\n      2010\n      1994\n      -0.106921\n      False\n      -16.0\n      -6\n      -0.017603\n      0\n      0.0\n      6.918492\n    \n  \n\n\n\n\nPanel data of this kind of structure is difficult to intuit unless visualised. The key structures on which this DiD estimator works are the averages across state and time. We depict these here. Note how we define a causal contrast as the difference between the treated and control groups averages over time. In particular our estimator has to account for the fact that we have a “dynamic” control group. As each cohort is treated our treatment group expands and the causal contrast of interest is differently defined. To gain a view of the time-evolution of treatment we want to marginalise over the specifc cohorts and extract a view of the how the effects evolve after the introduction of the treatment in expectation.\n\n\nCode\nfig, axs = plt.subplots(2, 1, figsize=(10, 12))\naxs = axs.flatten()\nfor u in df_het['unit'].unique():\n    temp = df_het[df_het['unit']==u]\n    axs[0].plot(temp['year'], temp['dep_var'], color='grey', alpha=0.01)\n    axs[1].plot(temp['year'], temp['dep_var'], color='grey', alpha=0.01)\ndf_het.groupby(['year', 'state'])[['dep_var']].mean().reset_index().pivot(index='year', columns='state', values='dep_var').plot(ax=axs[0], legend=False, color='blue', \nalpha=0.4)\n\ndf_het.groupby(['year', 'g'])[['dep_var']].mean().reset_index().pivot(index='year', columns='g', values='dep_var').plot(ax=axs[1], legend=False)\n\naxs[0].set_title(\"Difference in Differences \\n State Mean Change an Individual Trajectories\")\naxs[1].set_title(\"Difference in Differences \\n Mean Change an Individual Trajectories\");\n\n\n\n\n\nNote how the blue line represents a cohort that never undergoes a treatment and is maintained as a coherent control group throughout the sequence even though we have two other cohorts. This ensures we have a constant contrast case allowing us to identify the causal estimand of interest. It’s also worth calling out that we have heterogenous patterns in the treatment effects across the cohorts i.e. the the intensity of the treatment effect differs across the cohorts. We’ll apply the vanilla TWFE estimator and then dig into how it works.\n\n\nTWFE in pyfixest\nA natural question is to wonder how the treatment effect evolves over time? How does policy shift behaviours? Is it initially impactful converging to a quick plateau or a slowly building pattern of consistent growth?\n\nfit_twfe_event = pf.feols(\n    \"dep_var ~ i(rel_year, ref=-1.0) | state + year \",\n    df_het,\n    vcov={\"CRV1\": \"state\"},\n)\n\nfit_twfe_event.tidy()\n\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n\n\n\n\n\n\n  \n    \n      \n      Estimate\n      Std. Error\n      t value\n      Pr(>|t|)\n      2.5%\n      97.5%\n    \n    \n      Coefficient\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-20.0]\n      -0.099445\n      0.081699\n      -1.217202\n      0.230842\n      -0.264697\n      0.065808\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-19.0]\n      0.000624\n      0.083213\n      0.007493\n      0.994060\n      -0.167690\n      0.168937\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-18.0]\n      0.004125\n      0.089719\n      0.045974\n      0.963565\n      -0.177349\n      0.185599\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-17.0]\n      0.021899\n      0.085686\n      0.255573\n      0.799624\n      -0.151418\n      0.195216\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-16.0]\n      -0.036933\n      0.096921\n      -0.381067\n      0.705221\n      -0.232975\n      0.159108\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-15.0]\n      0.069578\n      0.081289\n      0.855936\n      0.397262\n      -0.094844\n      0.234000\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-14.0]\n      0.037734\n      0.086618\n      0.435641\n      0.665499\n      -0.137467\n      0.212936\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-13.0]\n      0.061779\n      0.083362\n      0.741098\n      0.463073\n      -0.106836\n      0.230394\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-12.0]\n      0.089913\n      0.084900\n      1.059044\n      0.296095\n      -0.081814\n      0.261639\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-11.0]\n      0.000982\n      0.079104\n      0.012417\n      0.990156\n      -0.159021\n      0.160986\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-10.0]\n      -0.113033\n      0.064922\n      -1.741047\n      0.089560\n      -0.244351\n      0.018285\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-9.0]\n      -0.069225\n      0.057046\n      -1.213481\n      0.232244\n      -0.184612\n      0.046162\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-8.0]\n      -0.061290\n      0.060362\n      -1.015370\n      0.316188\n      -0.183383\n      0.060804\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-7.0]\n      -0.002022\n      0.064602\n      -0.031306\n      0.975185\n      -0.132693\n      0.128648\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-6.0]\n      -0.055810\n      0.064674\n      -0.862938\n      0.393447\n      -0.186626\n      0.075006\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-5.0]\n      -0.065009\n      0.064810\n      -1.003066\n      0.322012\n      -0.196099\n      0.066082\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-4.0]\n      -0.009850\n      0.053098\n      -0.185505\n      0.853794\n      -0.117251\n      0.097551\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-3.0]\n      0.046338\n      0.062950\n      0.736104\n      0.466072\n      -0.080991\n      0.173667\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-2.0]\n      0.015947\n      0.065442\n      0.243687\n      0.808751\n      -0.116421\n      0.148316\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.0.0]\n      1.406404\n      0.055641\n      25.276422\n      0.000000\n      1.293860\n      1.518949\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.1.0]\n      1.660552\n      0.065033\n      25.533859\n      0.000000\n      1.529010\n      1.792094\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.2.0]\n      1.728790\n      0.056793\n      30.440100\n      0.000000\n      1.613915\n      1.843665\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.3.0]\n      1.854839\n      0.058315\n      31.807288\n      0.000000\n      1.736886\n      1.972792\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.4.0]\n      1.958676\n      0.071144\n      27.531232\n      0.000000\n      1.814774\n      2.102578\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.5.0]\n      2.082161\n      0.063855\n      32.607651\n      0.000000\n      1.953002\n      2.211320\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.6.0]\n      2.191062\n      0.068510\n      31.981462\n      0.000000\n      2.052487\n      2.329637\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.7.0]\n      2.279073\n      0.075014\n      30.381921\n      0.000000\n      2.127342\n      2.430803\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.8.0]\n      2.364593\n      0.058598\n      40.352477\n      0.000000\n      2.246067\n      2.483120\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.9.0]\n      2.372163\n      0.056560\n      41.940696\n      0.000000\n      2.257759\n      2.486566\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.10.0]\n      2.649271\n      0.056177\n      47.158942\n      0.000000\n      2.535641\n      2.762901\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.11.0]\n      2.753591\n      0.075526\n      36.458951\n      0.000000\n      2.600826\n      2.906356\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.12.0]\n      2.813935\n      0.079320\n      35.475900\n      0.000000\n      2.653496\n      2.974375\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.13.0]\n      2.756070\n      0.078368\n      35.168251\n      0.000000\n      2.597555\n      2.914584\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.14.0]\n      2.863427\n      0.098389\n      29.103072\n      0.000000\n      2.664416\n      3.062438\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.15.0]\n      2.986652\n      0.093309\n      32.008066\n      0.000000\n      2.797916\n      3.175388\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.16.0]\n      2.963032\n      0.085427\n      34.684870\n      0.000000\n      2.790239\n      3.135825\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.17.0]\n      2.972596\n      0.092390\n      32.174553\n      0.000000\n      2.785721\n      3.159472\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.18.0]\n      2.935051\n      0.094126\n      31.182203\n      0.000000\n      2.744664\n      3.125439\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.19.0]\n      2.918707\n      0.084193\n      34.667036\n      0.000000\n      2.748411\n      3.089002\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.20.0]\n      2.979701\n      0.087777\n      33.946394\n      0.000000\n      2.802156\n      3.157246\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.inf]\n      0.128053\n      0.078513\n      1.630976\n      0.110946\n      -0.030755\n      0.286861\n    \n  \n\n\n\n\nThe model specification defines indicator variables for the relative years before and after the penultimate year before treatment is applied, it then incorporates the fixed effects for state and year. The coefficient values ascribed the the relative year indicators are used to plot the event-study trajectories. This is a two-way fixed effects estimation routine where the fixed effects for state and year indicators absorb the variance due to those groupings. It is often estimated using a de-meaning technique which we will demonstrate below.\n\nfigsize = [1200, 600]\nfit_twfe_event.iplot(\n    coord_flip=False,\n    title=\"TWFE-Estimator\",\n    figsize=figsize,\n    xintercept=18.5,\n    yintercept=0,\n).show()\n\n   \n   \n\n\nWe can also aim to marginalise over the details of the temporal trajectories by defining the similar estimation routine on the individuals and their treatment indicator.\n\nfit_twfe = pf.feols(\n    \"dep_var ~ i(treat) | unit + year\",\n    df_het,\n    vcov={\"CRV1\": \"state\"},\n)\n\nfit_twfe.tidy()\n\n\n\n\n\n  \n    \n      \n      Estimate\n      Std. Error\n      t value\n      Pr(>|t|)\n      2.5%\n      97.5%\n    \n    \n      Coefficient\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      C(treat)[T.True]\n      1.98254\n      0.019331\n      102.55618\n      0.0\n      1.943439\n      2.021642\n    \n  \n\n\n\n\n\n\nDe-meaning and TWFEs\nWe’ve seen above that the fixed-effect estimators in these DiD designs involve a lot of indicator variables. These are largely not the focus on the question at hand but are used exlusively to absorb the noise that takes away from our understanding of the treatment effect. We can achieve similar results with less parameters required if we “de-mean” the focus variables by the group averages of the control factors of state and year or unit. This operation makes for more efficient TWFE estimation routines is provably a variety of mundlak regression as shown by Woolridge 2021.\nWe implement the de-meaning technique and show “equivalence by python” as follows:\n\ndef demean(df, col_to_demean, group):\n    return df.assign(**{col_to_demean: (df[col_to_demean]\n                                        - df.groupby(group)[col_to_demean].transform(\"mean\")\n                                        )})\n\n\ndef apply_demeaning(df_het, by=['state', 'year'], event=True):\n    if event: \n        d = pd.get_dummies(df_het['rel_year']).drop(-1, axis=1) \n        d.columns = ['rel_year_' +str(c).replace('-', 'minus_') for c in d.columns]\n    else:\n        d = df_het[['treat']]\n    d[by[0]] = df_het[by[0]]\n    d[by[1]] = df_het[by[1]]\n    for col in d.columns: \n        if col in by:\n            pass\n        else: \n            for c in by:\n                d = demean(d, col, c)\n    d = d.drop(by, axis=1)\n    d['dep_var'] = df_het['dep_var']\n    return d\n## Demean the relative years features for event studies\nd_event = apply_demeaning(df_het, by=['state', 'year'], event=True)\n\n## Demean the treatment indicator for ATT estimation\nd = apply_demeaning(df_het, by=['unit', 'year'], event=False)\n\nd_event.head()\n\n\n\n\n\n  \n    \n      \n      rel_year_minus_20.0\n      rel_year_minus_19.0\n      rel_year_minus_18.0\n      rel_year_minus_17.0\n      rel_year_minus_16.0\n      rel_year_minus_15.0\n      rel_year_minus_14.0\n      rel_year_minus_13.0\n      rel_year_minus_12.0\n      rel_year_minus_11.0\n      ...\n      rel_year_13.0\n      rel_year_14.0\n      rel_year_15.0\n      rel_year_16.0\n      rel_year_17.0\n      rel_year_18.0\n      rel_year_19.0\n      rel_year_20.0\n      rel_year_inf\n      dep_var\n    \n  \n  \n    \n      0\n      0.651694\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      ...\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      -0.28125\n      7.022709\n    \n    \n      1\n      -0.002973\n      0.651694\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      ...\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      -0.28125\n      7.778628\n    \n    \n      2\n      -0.002973\n      -0.002973\n      0.651694\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      ...\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      -0.28125\n      8.436377\n    \n    \n      3\n      -0.002973\n      -0.002973\n      -0.002973\n      0.651694\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      ...\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      -0.28125\n      7.191207\n    \n    \n      4\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      0.651694\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      ...\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      -0.28125\n      6.918492\n    \n  \n\n5 rows × 42 columns\n\n\n\nWe now have a data set with 42 columns focused on the treatment structures, but implicitly controls for the variation due to state and time. We’ll see below that this representation of the data will correctly estimate the treatment effects.\n\n\nEvent Study and De-Meaning\nNow we’ll use the de-meaned data structure above to estimate an event study using Bambi.\n\nx_cols = ' + '.join([c for c in d_event.columns if c != 'dep_var'])\nmodel_twfe_event = bmb.Model(f\"dep_var ~ + {x_cols}\", d_event)\nidata_twfe_event = model_twfe_event.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True},)\n\n\nmodel_twfe_event\n\n       Formula: dep_var ~ + rel_year_minus_20.0 + rel_year_minus_19.0 + rel_year_minus_18.0 + rel_year_minus_17.0 + rel_year_minus_16.0 + rel_year_minus_15.0 + rel_year_minus_14.0 + rel_year_minus_13.0 + rel_year_minus_12.0 + rel_year_minus_11.0 + rel_year_minus_10.0 + rel_year_minus_9.0 + rel_year_minus_8.0 + rel_year_minus_7.0 + rel_year_minus_6.0 + rel_year_minus_5.0 + rel_year_minus_4.0 + rel_year_minus_3.0 + rel_year_minus_2.0 + rel_year_0.0 + rel_year_1.0 + rel_year_2.0 + rel_year_3.0 + rel_year_4.0 + rel_year_5.0 + rel_year_6.0 + rel_year_7.0 + rel_year_8.0 + rel_year_9.0 + rel_year_10.0 + rel_year_11.0 + rel_year_12.0 + rel_year_13.0 + rel_year_14.0 + rel_year_15.0 + rel_year_16.0 + rel_year_17.0 + rel_year_18.0 + rel_year_19.0 + rel_year_20.0 + rel_year_inf\n        Family: gaussian\n          Link: mu = identity\n  Observations: 46500\n        Priors: \n    target = mu\n        Common-level effects\n            Intercept ~ Normal(mu: 4.7062, sigma: 7.1549)\n            rel_year_minus_20.0 ~ Normal(mu: 0.0, sigma: 83.8218)\n            rel_year_minus_19.0 ~ Normal(mu: 0.0, sigma: 83.8218)\n            rel_year_minus_18.0 ~ Normal(mu: 0.0, sigma: 83.8218)\n            rel_year_minus_17.0 ~ Normal(mu: 0.0, sigma: 83.8218)\n            rel_year_minus_16.0 ~ Normal(mu: 0.0, sigma: 83.8218)\n            rel_year_minus_15.0 ~ Normal(mu: 0.0, sigma: 83.8218)\n            rel_year_minus_14.0 ~ Normal(mu: 0.0, sigma: 83.8218)\n            rel_year_minus_13.0 ~ Normal(mu: 0.0, sigma: 83.8218)\n            rel_year_minus_12.0 ~ Normal(mu: 0.0, sigma: 83.8218)\n            rel_year_minus_11.0 ~ Normal(mu: 0.0, sigma: 83.8218)\n            rel_year_minus_10.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_minus_9.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_minus_8.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_minus_7.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_minus_6.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_minus_5.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_minus_4.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_minus_3.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_minus_2.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_0.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_1.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_2.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_3.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_4.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_5.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_6.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_7.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_8.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_9.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_10.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_11.0 ~ Normal(mu: 0.0, sigma: 86.6805)\n            rel_year_12.0 ~ Normal(mu: 0.0, sigma: 86.6805)\n            rel_year_13.0 ~ Normal(mu: 0.0, sigma: 86.6805)\n            rel_year_14.0 ~ Normal(mu: 0.0, sigma: 86.6805)\n            rel_year_15.0 ~ Normal(mu: 0.0, sigma: 86.6805)\n            rel_year_16.0 ~ Normal(mu: 0.0, sigma: 86.6805)\n            rel_year_17.0 ~ Normal(mu: 0.0, sigma: 86.6805)\n            rel_year_18.0 ~ Normal(mu: 0.0, sigma: 86.6805)\n            rel_year_19.0 ~ Normal(mu: 0.0, sigma: 86.6805)\n            rel_year_20.0 ~ Normal(mu: 0.0, sigma: 86.6805)\n            rel_year_inf ~ Normal(mu: 0.0, sigma: 15.2312)\n        \n        Auxiliary parameters\n            sigma ~ HalfStudentT(nu: 4.0, sigma: 2.862)\n------\n* To see a plot of the priors call the .plot_priors() method.\n* To see a summary or plot of the posterior pass the object returned by .fit() to az.summary() or az.plot_trace()\n\n\nWe can then plot the event-study and observe a similar pattern to the one observed with pyfixest.\n\ndef plot_event_study(idata, ax, color='blue', model='demeaned'):\n    summary_df = az.summary(idata)\n    cols = [i for i in summary_df.index if 'rel' in i]\n    summary_df = summary_df[summary_df.index.isin(cols)]\n    x = range(len(summary_df))\n    ax.scatter(x, summary_df['mean'], label=model, color=color)\n    ax.plot([x, x], [summary_df['hdi_3%'],summary_df['hdi_97%']],   color=color)\n    ax.set_title(\"Event Study\")\n    return ax\n\nfig, ax = plt.subplots(figsize=(10, 5))\nplot_event_study(idata_twfe_event, ax)\nax.legend();\n\n\n\n\nSimilarly, we can de-mean the simple treatment indicator using the group means and marginalise over time periods to find a single treatment effect estimate.\n\nmodel_twfe_trt_demean = bmb.Model(f\"dep_var ~ treat\", d)\nidata_twfe_trt_demean = model_twfe_trt_demean.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True},)\n\n\naz.summary(idata_twfe_trt_demean)\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      4.706\n      0.013\n      4.681\n      4.731\n      0.000\n      0.000\n      3435.0\n      2692.0\n      1.0\n    \n    \n      treat\n      1.982\n      0.049\n      1.893\n      2.077\n      0.001\n      0.001\n      3203.0\n      2855.0\n      1.0\n    \n    \n      dep_var_sigma\n      2.811\n      0.009\n      2.793\n      2.828\n      0.000\n      0.000\n      5532.0\n      2855.0\n      1.0\n    \n  \n\n\n\n\nWhich again accords with the reported values from pyfixest. This is equivalent to using a Mundlak device as we can see below:\n\n\nTWFE by Mundlak Device\nWoolridge recounts that the TWFE is equivalent to a Mundlak regression where:\n\n[The Mundlak devices] \\(\\bar{X_{i}}\\) and \\(\\bar{X_{t}}\\) effectively act as sufficient statistics in accounting for any unit-specific heterogeneity and time-specific heterogeneity that is correlated with \\(X_{it}\\). Rather than having to include (N − 1) + (T − 1) control variables, it suffices to include 2K control variables, \\(\\bar{X_{i}}\\), \\(\\bar{X_{t}}\\) - Two-Way Fixed Effects, the Two-Way Mundlak Regression, and Difference-in-Differences Estimators\n\nWe will see an example of this equivalence here.\n\ndf_het['unit_mean'] = df_het.groupby('unit')['treat'].transform(np.mean)\ndf_het['time_mean'] = df_het.groupby('year')['treat'].transform(np.mean)\n\nmodel_twfe_trt = bmb.Model(f\"dep_var ~ treat\", df_het)\nidata_twfe_trt = model_twfe_trt.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\nmodel_twfe_trt_mundlak = bmb.Model(f\"dep_var ~ treat + unit_mean + time_mean\", df_het)\nidata_twfe_trt_mundlak = model_twfe_trt_mundlak.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\n\naz.plot_forest([idata_twfe_trt_demean, idata_twfe_trt_mundlak, idata_twfe_trt], combined=True, var_names=['treat'], model_names=['De-meaned', 'Mundlak', 'Simple']);\n\n\n\n\nThe de-meaned TWFE estimator and the Mundlak specification result in identical estimates and differ from the naive estimate that fails to control group level confounds. The Mundlak specification is far easier to implement and offers altnerative ways to parameterise a model capable of adjusting for the group level confounding.\n\n\nFunctional Form and Richly Parameterised Regressions\nThe vanilla TWFE estimator can successfully recover the treatment effects in DiD designs and facilitate event studies of the same. However, the details of the estimation matter because this functional form is not always robust. Here we’ll see other options that can recover substantially the same inferences and may prove more robust as we’ll see below. The key to each is to articulate enough structural features that allow the model to modify effects based on the suspected group level confounds. Most crucially, the model needs to express (or account for) variation due to relevant data generating process.\n\ndf_het['state_mean'] = df_het.groupby('state')['treat'].transform(np.mean)\ndf_het['time_mean'] = df_het.groupby('year')['treat'].transform(np.mean)\ndf_het['cohort_mean'] = df_het.groupby('group')['treat'].transform(np.mean)\n\nmodel_twfe_event_1 = bmb.Model(f\"dep_var ~ 1 + C(year) + state_mean + C(rel_year, Treatment(reference=-1)) \", df_het)\nidata_twfe_event_1 = model_twfe_event_1.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\n\nformula = \"\"\" dep_var ~ 1 + time_mean:state_mean + C(rel_year, Treatment(reference=-1))\"\"\"\ntwfe_model_ols = smf.ols(formula, data=df_het).fit()\ntwfe_model_ols.summary()\nparam_est = pd.DataFrame(twfe_model_ols.params, columns=['estimate']).iloc[1:-1]\nparam_est['index_number'] = list(range(len(param_est)))\ntemp = (param_est.reset_index()\n)\nparam_est = temp[(~temp['index'].str.contains(':')) & (temp['index'].str.contains('rel'))]\nparam_est.reset_index(inplace=True)\n\n\nmodel_twfe_event_2 = bmb.Model(f\"dep_var ~ (1 | year) + state_mean + C(rel_year, Treatment(reference=-1)) \", df_het)\nidata_twfe_event_2 = model_twfe_event_2.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\nHaving estimated the various alternatives model specifications we compare each against our baseline de-meaned event-study.\n\n\nCode\nfig, axs = plt.subplots(2, 2, figsize=(20, 10))\naxs = axs.flatten()\nplot_event_study(idata_twfe_event, axs[0], model='Manual DeMeaned')\nplot_event_study(idata_twfe_event, axs[1], model='Manual DeMeaned')\nplot_event_study(idata_twfe_event, axs[2], model='Manual DeMeaned')\nplot_event_study(idata_twfe_event, axs[3], model='Manual DeMeaned')\nplot_event_study(idata_twfe_event_1, axs[0], color='green', model='Fixed Effects Saturated Bayes')\nplot_event_study(idata_twfe_event_2, axs[1], color='purple', model='Hierarchical Effects Saturated Bayes')\naxs[2].scatter(param_est['index'], param_est['estimate'], color='red', label='Mundlak Interaction Features OLS')\ntidy = fit_twfe_event.tidy()\nxs = range(len(tidy))\ntidy.reset_index(inplace=True)\naxs[3].scatter(xs, tidy['Estimate'], color='orange', label='pyfixest TWFE')\naxs[3].plot([xs, xs], [tidy['2.5%'],tidy['97.5%']], color='orange')\naxs[2].set_xticks([])\naxs[0].set_title(\"dep_var ~ 1 + C(year) + state_mean + C(rel_year, Treatment(reference=-1))\")\naxs[1].set_title(\"dep_var ~ (1 | year) + state_mean + C(rel_year, Treatment(reference=-1))\")\naxs[2].set_title(\"dep_var ~ 1 + time_mean:state_mean + C(rel_year, Treatment(reference=-1))\")\naxs[3].set_title(\"dep_var ~ i(rel_year, ref=-1.0) | state + year\")\naxs[0].legend()\naxs[1].legend()\naxs[2].legend()\naxs[3].legend();\n\n\n\n\n\n`\nThis suggests that there are a variety of functional forms even just using regression specifications that seek to control from different types of group level confounding. In this example data most of the functional forms that seek to control for time and state level effects seem to converge in their answers. We will now switch to an example where the vanilla TWFE breaks down."
  },
  {
    "objectID": "posts/post-with-code/multilevel_confounding/multilevel_models.html#issues-with-twfe-and-richly-parameterised-linear-models",
    "href": "posts/post-with-code/multilevel_confounding/multilevel_models.html#issues-with-twfe-and-richly-parameterised-linear-models",
    "title": "Freedom, Hierarchies and Confounded Estimates",
    "section": "Issues with TWFE and Richly Parameterised Linear Models",
    "text": "Issues with TWFE and Richly Parameterised Linear Models\nWe draw on the example from Pedro Sant’Anna here where it is demonstrated that the vanilla TWFE estimator breaks down under various conditions. These conditions are often related to staggered roll out of a treatment. This staggered roll out induces dynamic changes in the composition of treatment group over time. Appropriate inference needs to carefully control for the interaction effects due to staggered treatment. In particular we need our model of the situation to reflect this aspect of the data generating process.\nLet’s generate some data.\n\ntrue_mu = 1\n\ndef make_data(nobs = 1000, nstates = 40):\n    ids = list(range(nobs))\n    states = np.random.choice(range(nstates), size=nobs, replace=True)\n    unit = pd.DataFrame({'unit': ids, \n                        'state': states, \n                        'unit_fe': np.random.normal(states/5, 1, size=nobs),\n                        'mu': true_mu})\n    \n    year = pd.DataFrame({'year': pd.date_range('1980-01-01', '2010-01-01', freq='y'), \n    'year_fe': np.random.normal(0, 1, 30) })\n    year['year'] = year['year'].dt.year\n\n    treat_taus = pd.DataFrame({'state': np.random.choice(range(nstates), size=nstates, replace=False),\n    'cohort_year': np.sort([1986, 1992, 1998, 2004]*10)\n    })\n\n    cross_join = pd.DataFrame([row for row in product(range(nobs), year['year'].unique())], columns =['unit', 'year'])\n    cross_join = cross_join.merge(unit, how='left', left_on='unit', \n    right_on='unit')\n    cross_join = cross_join.merge(year, how='left', left_on='year', \n    right_on='year')\n    cross_join = cross_join.merge(treat_taus, how='left', left_on='state', right_on='state')\n    cross_join = cross_join.assign(\n        error = np.random.normal(0, 1, len(cross_join)),\n        treat = lambda x: np.where(x['year'] >= x['cohort_year'], 1, 0)\n    )\n    cross_join = cross_join.assign(tau = np.where(cross_join['treat'] == 1, cross_join['mu'], 0), \n    ).assign(year_fe = lambda x: x['year_fe'] + 0.1*(x['year']-x['cohort_year']))\n\n    cross_join['tau_cum'] = cross_join.groupby('unit')['tau'].transform(np.cumsum)\n    cross_join = cross_join.assign(dep_var = lambda x: 2010-x['cohort_year'] + \n    x['unit_fe'] + x['year_fe'] + x['tau_cum'] + x['error'])\n    cross_join['rel_year'] =  cross_join['year'] - cross_join['cohort_year']\n\n    \n    return cross_join\n\nsim_df = make_data(500, 40)\nsim_df.head()\n\n\n\n\n\n  \n    \n      \n      unit\n      year\n      state\n      unit_fe\n      mu\n      year_fe\n      cohort_year\n      error\n      treat\n      tau\n      tau_cum\n      dep_var\n      rel_year\n    \n  \n  \n    \n      0\n      0\n      1980\n      12\n      2.770007\n      1\n      -1.443538\n      1998\n      -0.883541\n      0\n      0\n      0\n      12.442927\n      -18\n    \n    \n      1\n      0\n      1981\n      12\n      2.770007\n      1\n      -3.140326\n      1998\n      1.044140\n      0\n      0\n      0\n      12.673821\n      -17\n    \n    \n      2\n      0\n      1982\n      12\n      2.770007\n      1\n      -2.430673\n      1998\n      0.276021\n      0\n      0\n      0\n      12.615355\n      -16\n    \n    \n      3\n      0\n      1983\n      12\n      2.770007\n      1\n      -4.253994\n      1998\n      0.181049\n      0\n      0\n      0\n      10.697061\n      -15\n    \n    \n      4\n      0\n      1984\n      12\n      2.770007\n      1\n      -3.347429\n      1998\n      -2.104055\n      0\n      0\n      0\n      9.318523\n      -14\n    \n  \n\n\n\n\nWe can now plot the staggered nature of the imagined treatment regime.\n\n\nCode\nfig, ax = plt.subplots(figsize=(10, 6))\n\nfor unit in sim_df['unit'].unique()[0:100]:\n    temp = sim_df[sim_df['unit'] == unit]\n    ax.plot(temp['year'], temp['dep_var'], alpha=0.1, color='grey')\n\nsim_df.groupby(['cohort_year', 'year'])[['dep_var']].mean().reset_index().pivot(index='year', columns='cohort_year', values='dep_var').plot(ax=ax)\nax.axvline(1986)\nax.axvline(1992, color='orange')\nax.axvline(1998, color='green')\nax.axvline(2004, color='red')\nax.set_title(\"Simulated Cohorts Homogenous Treatment Effects \\n All Eventually Treated\", fontsize=20)\nax.legend()\n\n\n<matplotlib.legend.Legend at 0x2a273b2d0>\n\n\n\n\n\nThis data will present problems for the vanilla TWFE estimator. The issues stems from how each cohort receives a treatment and there are periods in the data when no group is in the “control” group. We can see how this plays out with de-meaning TWFE strategy.\n\nfit_twfe = pf.feols(\n    \"dep_var ~ i(rel_year, ref=-1.0) | state + year\",\n    sim_df,\n    vcov={\"CRV1\": \"state\"},\n)\n\n\nfigsize = [1200, 400]\nfit_twfe.iplot(\n    coord_flip=False,\n    title=\"TWFE-Estimator\",\n    figsize=figsize,\n    xintercept=18.5,\n    yintercept=0,\n).show()\n\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/pyfixest/estimation/feols_.py:1987: UserWarning: \n            The following variables are collinear: ['C(rel_year, contr.treatment(base=-1.0))[T.18]', 'C(rel_year, contr.treatment(base=-1.0))[T.19]', 'C(rel_year, contr.treatment(base=-1.0))[T.20]', 'C(rel_year, contr.treatment(base=-1.0))[T.21]', 'C(rel_year, contr.treatment(base=-1.0))[T.22]', 'C(rel_year, contr.treatment(base=-1.0))[T.23]'].\n            The variables are dropped from the model.\n            \n  warnings.warn(\n\n\n   \n   \n\n\nThis is not remotely close to the expected pattern. For contrast, consider an alternative estimator.\n\nfit_lpdid = lpdid(\n    data=sim_df,\n    yname=\"dep_var\",\n    gname=\"cohort_year\",\n    tname=\"year\",\n    idname=\"unit\",\n    vcov={\"CRV1\": \"state\"},\n    pre_window=-17,\n    post_window=17,\n    att=False,\n)\n\nfit_lpdid.iplot(\n    coord_flip=False,\n    title=\"Local-Projections-Estimator\",\n    figsize=figsize,\n    yintercept=0,\n    xintercept=18.5,\n).show()\n\n   \n   \n\n\nThe initial TWFE estimate is utterly skewed and entirely incorrect. Something dreadful has gone wrong under the hood. For contrast, we’ve included the Local Projections estimator from the pyfixest to show that we can recover the actual treatment effect under this event study with alternative strategies. However, there is more machinary involved in the local-projections estimator.\n\n\n\n\n\n\nWarning\n\n\n\nNote there has been a large volume of literature diagnosing precisely how and where TWFE estimator breakdown. Goodman-Bacon’s “Difference-in-differences with variation in treatment timing” in particular provides a decomposition of the causal estimand that can be recovered under varying treatment regimes. We won’t cover this rich area of research here. But we note that it’s a lively area of research.\n\n\nInstead we want show how to use mundlak devices to recover more reasonable estimates. No fancy estimators, just more regressions.\n\nWoolridge argues for this approach when he states that\n\n[T]hat there is nothing inherently wrong with TWFE, which is an estimation method. The problem with how TWFE is implemented in DiD settings is that it is applied to a restrictive model… - Two-Way Fixed Effects, the Two-Way Mundlak Regression, and Difference-in-Differences Estimators\n\nWe want to explore different ways of isolating the treatment effects under a variety of model specifications that controls for time-constant treatment intensities, covariates, and interactions between them. That is we want to better express the data generating process in our model of the situation.\n\nFitting a Variety of Models\nConsider the following model specifications.\n\nsim_df['unit_mean'] = sim_df.groupby('unit')['treat'].transform(np.mean)\n\nsim_df['state_mean'] = sim_df.groupby('state')['treat'].transform(np.mean)\n\nsim_df['cohort_mean'] = sim_df.groupby('cohort_year')['treat'].transform(np.mean)\n\nsim_df['time_mean'] = sim_df.groupby('year')['treat'].transform(np.mean)\n\n\nmodel_twfe = bmb.Model(f\"\"\"dep_var ~ 1  + time_mean:state_mean + C(rel_year, Treatment(reference=-1))\"\"\", sim_df)\n\nidata_twfe = model_twfe.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\n\nmodel_twfe1 = bmb.Model(f\"\"\"dep_var ~ 1  + time_mean* state_mean + C(cohort_year) + C(rel_year, Treatment(reference=-1))\"\"\", sim_df)\n\nidata_twfe1 = model_twfe1.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\n\nmodel_twfe2 = bmb.Model(f\"\"\"dep_var ~ 1  + cohort_mean: state_mean + C(rel_year, Treatment(reference=-1))\"\"\", sim_df)\n\nidata_twfe2 = model_twfe2.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\nmodel_twfe3 = bmb.Model(f\"\"\"dep_var ~ (1| year)  + state_mean + C(rel_year, Treatment(reference=-1))\"\"\", sim_df)\nidata_twfe3 = model_twfe3.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\nThese latter models will recover the appropriate treatment effects with slight variations due to the adapted functional form. The initial models will fail to capture the pattern correctly.\n\n\nCode\nfig, axs = plt.subplots(4, 1, figsize=(10, 15), sharey=True)\naxs = axs.flatten()\nplot_event_study(idata_twfe, ax=axs[0], model='Additive Mundlak')\nplot_event_study(idata_twfe1, ax=axs[1], color='red', model='Mundlak State & Time Interactions')\nplot_event_study(idata_twfe2, ax=axs[2], color='green', model='Mundlak Cohort & State Interactions')\nplot_event_study(idata_twfe3, ax=axs[3], color='purple', model='Hierarchical Time + Mundlak State')\naxs[0].set_title(str(model_twfe.formula))\naxs[1].set_title(str(model_twfe1.formula))\naxs[2].set_title(str(model_twfe2.formula))\naxs[3].set_title(str(model_twfe3.formula))\nxs = np.linspace(0, 45, 45)\nxs_centered = xs - 22\ntrue_effect = np.where(xs_centered <= 0, 0, (xs_centered +1))\naxs[0].plot(xs, true_effect, color='k', alpha=0.6, label='True effect')\naxs[1].plot(xs, true_effect, color='k', alpha=0.6, label='True effect')\naxs[2].plot(xs, true_effect, color='k', alpha=0.6, \nlabel='True effect')\naxs[3].plot(xs, true_effect, color='k', alpha=0.6,\nlabel='True effect')\naxs[0].legend()\naxs[1].legend()\naxs[2].legend()\naxs[3].legend();\n\n\n\n\n\nNote how the naive mundlak approach also exhibits odd behaviour as we saw in the TWFE estimation routine above. Adding additional interactions and controlling for the staggered launch dates seems to help isolate the real pattern in the data. We’ve seen approximate success in a number of these richly parameterised versions of simple event study regressions. But the idiosyncracies of any one sample will contort and distort the estimates away from the true values. We might hope for more stability in expectation over repeated draws from this kind of data-generating process. Consider the following Bootstrapping estimation routine.\n\n\nEvaluating Robustness of Functional Form\n\n\nCode\nfig, axs = plt.subplots(2, 1, figsize=(10, 15))\naxs = axs.flatten()\nevent_coefs = []\nevent_coefs1 = []\n\ndef fit_ols(formula, df_het):\n    twfe_model_ols = smf.ols(formula, data=df_het).fit()\n    twfe_model_ols.summary()\n    param_est = pd.DataFrame(twfe_model_ols.params, columns=['estimate']).iloc[1:-1]\n    param_est['index_number'] = list(range(len(param_est)))\n    temp = (param_est.reset_index())\n    param_est = temp[(~temp['index'].str.contains(':')) & (temp['index'].str.contains('rel'))]\n    param_est.reset_index(inplace=True)\n    try:\n        param_est['rel_year'] =(param_est['index'].str.split('[', expand=True)[1].str.replace('T.', '')\n        .str.replace(']', ''))\n    except Exception as e:\n        param_est['rel_year'] = param_est['level_0'] - 22\n    param_est['rel_year'] = param_est['rel_year'].astype(int)\n    param_est.set_index('rel_year', inplace=True)\n    return param_est\n\nfor i in range(100):\n    df_het = make_data(500, 40)\n    df_het['state_mean'] = df_het.groupby('state')['treat'].transform(np.mean)\n    df_het['time_mean'] = df_het.groupby('year')['treat'].transform(np.mean)\n    df_het['cohort_mean'] = df_het.groupby('cohort_year')['treat'].transform(np.mean)\n    df_het['unit_mean'] = df_het.groupby('unit')['treat'].transform(np.mean)\n\n    formula = \"\"\" dep_var ~ 1 + time_mean + state_mean + cohort_mean  + C(rel_year, Treatment(reference=-1)) \"\"\"\n    formula1 = \"\"\" dep_var ~ 1 + C(cohort_year) + time_mean:unit_mean + C(rel_year, Treatment(reference=-1))\"\"\"\n\n    param_est = fit_ols(formula, df_het)\n    axs[0].plot(param_est['estimate'], color='blue', alpha=0.3)\n    event_coefs.append(param_est['estimate'])\n    param_est1 = fit_ols(formula1, df_het)\n    axs[1].plot(param_est1['estimate'], color='blue', alpha=0.3)\n    event_coefs1.append(param_est1['estimate'])\n\nbootstrap_df = pd.DataFrame(event_coefs)\nmean_df = bootstrap_df.mean(axis=0)\nmean_df.index = param_est.index\nmean_df.plot(ax=axs[0], color='red')\naxs[0].set_title(f\"\"\"Bootstrapped Estimates of Demean Event Study DGP \\n {formula}\"\"\")\n\nbootstrap_df1 = pd.DataFrame(event_coefs1)\nmean_df = bootstrap_df1.mean(axis=0)\nmean_df.index = param_est1.index\nmean_df.plot(ax=axs[1], color='red')\naxs[1].set_title(f\"\"\"Bootstrapped Estimates of Event Study DGP \\n\n{formula1}\"\"\");\n\n\n\n\n\nHere we see the importance of robustness tests in estimation techniques. In the above plot we see something like the correct pattern emerging in expectation, but absurd estimates occuring on individual cases. Whereas the bottom plot appears much more consistent across multiple draws from the data generating process. This points to something about the justification required for a credible result. Asymptotic accuracy is generally not sufficient for inducing credibility in any particular analysis of finite data.\nWe can also compare these models on standard adequacy measures.\n\naz.compare({'fe_mundlak_naive': idata_twfe, \n            'mundlak_state_time_interactions_cohort': idata_twfe1, \n            'mundlak_cohort_state_interactions': idata_twfe2, \n            'mundlak_state_hierarchical_year': idata_twfe3})\n\n\n\n\n\n  \n    \n      \n      rank\n      elpd_loo\n      p_loo\n      elpd_diff\n      weight\n      se\n      dse\n      warning\n      scale\n    \n  \n  \n    \n      mundlak_state_hierarchical_year\n      0\n      -35912.100213\n      71.825690\n      0.000000\n      0.801341\n      72.022818\n      0.000000\n      False\n      log\n    \n    \n      mundlak_state_time_interactions_cohort\n      1\n      -36427.310049\n      53.130023\n      515.209836\n      0.198691\n      74.337703\n      41.178348\n      False\n      log\n    \n    \n      mundlak_cohort_state_interactions\n      2\n      -38254.123775\n      47.009205\n      2342.023562\n      0.000023\n      78.060258\n      49.592305\n      False\n      log\n    \n    \n      fe_mundlak_naive\n      3\n      -46324.432551\n      38.486619\n      10412.332337\n      0.000000\n      86.756424\n      97.412077\n      False\n      log\n    \n  \n\n\n\n\nAgain, we can plot the predictive performance of these models. Here we compare two via posterior predictive checks. Notable we see how the model with the better predictive performance also correctly estimates the treatement effects in the above plots.\n\nmodel_twfe3.predict(idata_twfe3, kind='pps')\nmodel_twfe.predict(idata_twfe, kind='pps')\nfig, axs = plt.subplots(2, 1, figsize=(10, 12))\naxs = axs.flatten()\naz.plot_ppc(idata_twfe3, ax=axs[0])\naxs[0].set_title(\"PPC checks for Mundlak State Hierarchical Year\")\naxs[1].set_title(\"PPC checks for FE Mundlak Naive\")\naz.plot_ppc(idata_twfe, ax=axs[1]);\n\n\n\n\nIt’s certainly possible that parameter recovery accuracy and predictive performance come apart. But in lieu of knowledge of the true data generating processes, the route to credible models is via their calibration against real data."
  },
  {
    "objectID": "posts/post-with-code/multilevel_confounding/multilevel_models.html#conclusion",
    "href": "posts/post-with-code/multilevel_confounding/multilevel_models.html#conclusion",
    "title": "Freedom, Hierarchies and Confounded Estimates",
    "section": "Conclusion",
    "text": "Conclusion\n\n“A schema may be transported almost anywhere. The choice of territory for invasion is arbitrary; but the operation within that territory is almost never completely so.” - Nelson Goodman in Languages of Art\n\nWe’ve now seen various cases of group-level confounding and strategies to address these issues. We’ve seen how they give rise to non-trivial questions about the effects of policy, and what can be learned in the face of group-level confounds. The case of TWFE’s estimation is especially instructive - here we have an estimation technique applied well in limited circumstances, which ran into trouble when applied in contexts with a diverging data generating process. We all lose our way some of the time, but this misstep led us off the map - we’d followed the trail well past the intended territory.\nStatistical models do not mirror the world, they project patterns in the data. Each rupture between model and world is a gift that highlights deficits in our understanding - an impropriety of projections. It is therefore a delightful reversal to see, in Woolridge’s proof, that the issue is not with the tooling but the thoughtless application of the tool. The unreasonable effectiveness of regression modelling is surely driven by projections of linearity on the world, but it remains compelling in approximation when used well. There are architectural constraints in causal inference - hard unyielding facts, that shape the contours of our design, and structure the steps we can make. The degree to which we can flexibly encode those structures in our models, the more compelling our models will be. It is this recognition of dual fit requirements between world and model - the focus on the conditions for identifiability - that lends causal inference any kind of credibility. The expressive capacity of generalised linear modelling enables us to articulate too many candidate pictures of the world. It’s the business of science to cull those pictures, to find only the plausible representations of reality."
  },
  {
    "objectID": "posts/post-with-code/multilevel_confounding/multilevel_models.html#estimation-strategies-and-group-effects",
    "href": "posts/post-with-code/multilevel_confounding/multilevel_models.html#estimation-strategies-and-group-effects",
    "title": "Freedom, Hierarchies and Confounded Estimates",
    "section": "Estimation Strategies and Group Effects",
    "text": "Estimation Strategies and Group Effects\nFirst consider an estimation example due to Richard McElreath’s lecture series where we examine the various parameter recovery options available in the case of group level confounding. We want to move through these parameterisations to highlight the usefulness of Mundlak regressions. The source of these models is a paper Mundlak’s 1978 paper “On the pooling of time series and cross section data” in which he shows a method for adjusting for group-levels which proves to be a viable alternative to richly specified fixed effects model.\nFirst define a data generating process determined by group level effects:\n\ndef inv_logit(p):\n    return np.exp(p) / (1 + np.exp(p))\n\nN_groups = 30\nN_id = 3000\na0 = -2\nbXY = 1\nbZY = -0.5\ng = np.random.choice(range(N_groups), size=N_id, replace=True)\nUg = np.random.normal(1.5, size=N_groups)\n\nX = np.random.normal(Ug[g], size=N_id)\nZ = np.random.normal(size=N_groups)\n\ns = a0 + bXY*X + Ug[g] + bZY*Z[g]\np = inv_logit(s)\nY = np.random.binomial(n=1, p=p)\n\n\nsim_df = pd.DataFrame({'Y': Y, 'p': p, 's': s, 'g': g, 'X': X, 'Z': Z[g]})\nsim_df.head()\n\n\n\n\n\n  \n    \n      \n      Y\n      p\n      s\n      g\n      X\n      Z\n    \n  \n  \n    \n      0\n      1\n      0.943243\n      2.810542\n      8\n      1.384072\n      -1.818302\n    \n    \n      1\n      0\n      0.593872\n      0.379996\n      24\n      1.047128\n      -0.242817\n    \n    \n      2\n      1\n      0.874945\n      1.945406\n      3\n      2.340418\n      -0.650122\n    \n    \n      3\n      0\n      0.317089\n      -0.767181\n      7\n      0.550504\n      0.795680\n    \n    \n      4\n      1\n      0.699030\n      0.842684\n      23\n      1.183011\n      -0.111692\n    \n  \n\n\n\n\nThis data generating process is characterised by the bernoulli outcome with group level confounds on the \\(X\\) variable. If we model this relationship the confounding effects will bias naive parameter estimates on the covariates \\(X\\), \\(Z\\). We can imagine this is a class room setting with group-confounding due to teacher effects. In a DAG we see a picture something like this:\n\n\nCode\ngraph = nx.DiGraph()\ngraph.add_edges_from([(\"classroom \\n G\", \"scores \\n y\"), (\"classroom \\n G\", \"student prep \\n X\"), (\"student prep \\n X\", \"scores \\n y\"), (\"class temp \\n Z\", \"scores \\n y\")])\nfig, ax = plt.subplots(figsize=(10,6))\nnx.draw_networkx(graph, arrows=True, ax=ax, \nnode_size = 6000, font_color=\"whitesmoke\",pos={'classroom \\n G': [0, 0], 'scores \\n y': [0.5, 1], 'student prep \\n X': [0, 2], \n'class temp \\n Z': [1, 1]})\nax.set_title(\"Group Confounding in a DAG\", fontsize=12);\n\n\n\n\n\nWe can plot the data here to visualise the shares of the binary outcomes.\n\ng = sns.pairplot(sim_df[['Y', 'X', 'Z']], diag_kind=\"kde\", corner=True)\ng;\n\n\n\n\nWe will see different estimation results as we explore different ways of parameterising the relationships, and the trade-offs for model specifications will become clearer.\n\n\n\n\n\n\nWarning\n\n\n\nThere is a huge degree of confusion over the meaning of the terms “Fixed Effects” and “Random Effects”. Within this blog post when we refer to fixed effects we will mean the population level parameters. \\[\\beta X\\]\nIn contrast we will refer to group-level parameters \\(\\beta_{g}\\)\n\\[\\Big(\\underbrace{\\beta}_{pop} + \\underbrace{\\beta_{g}}_{group}\\Big)X\\]\nwhich are incorporated into our model equation modifying population level parameters as random effects. We will generally use Wilkinson notation to specify these choices. Random effects for modifying a population parameter X are denoted with a conditional bar over the variable ( X | group) and fixed effects are specified by just including the variable in the equation i.e. y ~ X + (Z | group) where X has a fixed effect parameterisation and Z a random effects parameterisation. We can also create indicator variables for group membership using this syntax with y ~ C(group) + X + Z where under the hood we pivot the group category into a zero-one variables indicating group membership. This parameterisation means each indicator variable (one for each level of the grouping variable) will receive a fixed effects population parameter.\n\n\n\nNaive Model\nWe will use Bambi to specify the majority of these generalised linear models throughout this post. The formula syntax for Bambi enables us to specify generalised linear models with bernoulli outcomes.\n\nnaive_model = bmb.Model(f\"Y['>.5'] ~ X + Z \", sim_df, \nfamily=\"bernoulli\")\nnaive_idata = naive_model.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True},)\n\n\naz.summary(naive_idata, var_names=['Intercept', 'X', 'Z'])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      -0.991\n      0.076\n      -1.129\n      -0.841\n      0.001\n      0.001\n      4859.0\n      3143.0\n      1.0\n    \n    \n      X\n      1.308\n      0.053\n      1.210\n      1.410\n      0.001\n      0.001\n      3449.0\n      2770.0\n      1.0\n    \n    \n      Z\n      -0.614\n      0.048\n      -0.702\n      -0.522\n      0.001\n      0.001\n      3822.0\n      3033.0\n      1.0\n    \n  \n\n\n\n\nHere we see that all three parameter estimates are biased away from their true values. Let’s try a simple fixed effects approach that adds indicator variables for all but one of the group levels.\n\n\nFixed Effects Model\nThe additional syntax for creating the group level indicator variables is specified here. This will create a host of indicator variables for membership in each level of the group variable.\n\nfixed_effects_model = bmb.Model(f\"Y['>.5'] ~ C(g) + X + Z\", sim_df, \nfamily=\"bernoulli\")\nfixed_effects_idata = fixed_effects_model.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True},)\n\n\naz.summary(fixed_effects_idata, var_names=['Intercept', 'X', 'Z'])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      -0.386\n      1.505\n      -3.181\n      2.308\n      0.086\n      0.061\n      307.0\n      552.0\n      1.02\n    \n    \n      X\n      0.977\n      0.059\n      0.872\n      1.097\n      0.001\n      0.001\n      2635.0\n      2189.0\n      1.00\n    \n    \n      Z\n      -0.008\n      1.408\n      -2.684\n      2.501\n      0.079\n      0.056\n      314.0\n      565.0\n      1.01\n    \n  \n\n\n\n\nNow we see that the coefficient on the \\(X\\) variable seems correct, but the coefficient on \\(Z\\) is wildly wrong. Indeed the uncertainty interval on the \\(Z\\) coefficient is huge. The fixed effect model was unable to learn anything about the correct parameter. Whereas the naive model seems to learn the correct \\(Z\\) parameter but over estimates the \\(X\\) coefficient. These are the kinds of trade-offs we need to be wary of as we account for the complexities of extensive group interactions in our model’s functional form.\n\n\nCode\nfig, axs = plt.subplots(2, 1, figsize=(10, 9))\naxs = axs.flatten()\n\naz.plot_posterior(naive_idata, var_names=['X'], ax=axs[0], \npoint_estimate=None,  hdi_prob='hide', color='red', label='Naive Model')\naxs[0].axvline(1, color='k')\n\naz.plot_posterior(fixed_effects_idata , var_names=['X'], ax=axs[0], point_estimate=None, hdi_prob='hide', label='Fixed Effect Models')\naxs[0].set_title(\"Naive/Fixed Model X Coefficient\")\n\naz.plot_posterior(naive_idata, var_names=['Z'], ax=axs[1], point_estimate=None,  hdi_prob='hide', color='red', ref_val_color='black')\n\n\naz.plot_posterior(fixed_effects_idata , var_names=['Z'], ax=axs[1], point_estimate=None, hdi_prob='hide')\n\naxs[1].set_title(\"Naive/Fixed Effect Model Z Coefficient\")\naxs[1].axvline(-0.5, color='k');\n\n\n\n\n\nWe now want to try another approach to handle to the group confounding that involves a hierarchical approach - adding group level effects to the population intercept term.\n\n\nMultilevel Model\nThe syntax for the random effects model is easily expressed in bambi too. These group level effects modify the intercept additively and are collectively drawn from a shared distribution implicit in the model.\n\nmultilevel_model = bmb.Model(f\"Y['>.5'] ~ (1 | g) + X + Z\", sim_df, \nfamily=\"bernoulli\")\nmultilevel_model_idata = multilevel_model.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True},)\n\nThis method posits a view that there is a shared underlying process generating each instance of group-behaviour i.e. the realised values of the outcome within each group. In the random effects multilevel model we replace the indivdual fixed effects indicator columns with additional parameters modifying the intercept term. This is the “hierarchy” or multi-level aspect of the specification.\n\naz.summary(multilevel_model_idata, var_names=['X', 'Z', 'Intercept'])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      X\n      1.023\n      0.059\n      0.914\n      1.134\n      0.001\n      0.001\n      4641.0\n      3115.0\n      1.0\n    \n    \n      Z\n      -0.606\n      0.169\n      -0.918\n      -0.277\n      0.006\n      0.004\n      868.0\n      1672.0\n      1.0\n    \n    \n      Intercept\n      -0.573\n      0.201\n      -0.940\n      -0.180\n      0.007\n      0.005\n      781.0\n      1053.0\n      1.0\n    \n  \n\n\n\n\nNext we’ll apply the Mundlak device method which adds the group mean back to each observation as a single additional covariate.\n\n\nMundlak Model\nFor this technique we augment the model by supplying group an additional column. This column records the group-level means of the “confounded variable” i.e. the variable which is realised under group level influences. Adding the group mean(s) to the model in this sense somewhat absolves of the requirement to include an extra multiplicity of parameters. It’s more akin to feature creation than model specification, but it serves the same purpose - it accounts for group level variation and provides a mechanism for the model to learn the appropriate weights to accord each group in the final calculation.\n\nsim_df['group_mean'] = sim_df.groupby('g')['X'].transform(np.mean)\n\nsim_df['group_mean_Z'] = sim_df.groupby('g')['Z'].transform(np.mean)\n\nmundlak_model = bmb.Model(f\"Y['>.5'] ~ (1 | g) + X + Z + group_mean\", sim_df, \nfamily=\"bernoulli\")\nmundlak_idata = mundlak_model.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True},)\n\nMundlak’s insight is that this mechanism is akin to adding a whole slew of fixed-effect indicator variables for each group. It is a corrective mechanism for group confounding in our focal variable. We’ll see how it allows for more flexible model specifications and has a role in making adjustments to aid identification in causal inference.\n\naz.summary(mundlak_idata, var_names=['Intercept', 'X', 'Z'])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      -1.906\n      0.141\n      -2.174\n      -1.649\n      0.002\n      0.002\n      4017.0\n      2950.0\n      1.0\n    \n    \n      X\n      0.952\n      0.059\n      0.842\n      1.062\n      0.001\n      0.001\n      4442.0\n      3188.0\n      1.0\n    \n    \n      Z\n      -0.443\n      0.064\n      -0.569\n      -0.330\n      0.001\n      0.001\n      3206.0\n      2967.0\n      1.0\n    \n  \n\n\n\n\nWe can now plot all the parameter recovery models together and we’ll see that there are some trade-offs between the fixed effects and random effects varieties of the modelling.\n\n\nPlotting the Comparisons\nThe parameter recovery exercise shows striking differences across each of the models\n\n\nCode\nfig, axs = plt.subplots(3, 1, figsize=(10, 11))\naxs = axs.flatten()\n\naz.plot_posterior(naive_idata, var_names=['X'], ax=axs[0], \npoint_estimate=None, color='red', label='Naive', hdi_prob='hide')\naxs[0].axvline(1, color='k', linestyle='--', label='True value')\n\naz.plot_posterior(fixed_effects_idata , var_names=['X'], ax=axs[0], point_estimate=None, hdi_prob='hide', label='Fixed')\n\naz.plot_posterior(multilevel_model_idata, var_names=['X'], ax=axs[0], point_estimate=None, hdi_prob='hide', color='green', label='Hierarchical')\n\naz.plot_posterior(mundlak_idata, var_names=['X'], ax=axs[0], point_estimate=None, hdi_prob='hide', color='purple', label='Mundlak')\n\n\naxs[0].set_title(\"X Coefficient\")\n\naz.plot_posterior(naive_idata, var_names=['Z'], ax=axs[1], point_estimate=None, hdi_prob='hide',  color='red', ref_val_color='black')\n\n\naz.plot_posterior(fixed_effects_idata , var_names=['Z'], ax=axs[1], point_estimate=None, hdi_prob='hide')\n\naz.plot_posterior(multilevel_model_idata, var_names=['Z'], ax=axs[1], point_estimate=None, hdi_prob='hide', color='green')\n\naz.plot_posterior(mundlak_idata, var_names=['Z'], ax=axs[1], point_estimate=None, hdi_prob='hide', color='purple')\n\naxs[1].set_title(\"Z Coefficient\")\naxs[1].axvline(-0.5, color='k', linestyle='--');\n\naz.plot_posterior(naive_idata, var_names=['Intercept'], ax=axs[2], point_estimate=None,  color='red', ref_val_color='black', hdi_prob='hide')\n\n\naz.plot_posterior(fixed_effects_idata , var_names=['Intercept'], ax=axs[2], point_estimate=None, hdi_prob='hide')\n\naz.plot_posterior(multilevel_model_idata, var_names=['Intercept'], ax=axs[2], point_estimate=None, hdi_prob='hide', color='green')\n\naz.plot_posterior(mundlak_idata, var_names=['Intercept'], ax=axs[2], point_estimate=None, hdi_prob='hide', color='purple')\n\naxs[2].axvline(-2, color='k', linestyle='--');\n\n\n\n\n\nImportantly, we see that while the fixed effects model is focused on recovering the treatment effect on the \\(X\\) covariate, it does so somewhat at the expense of accuracy on the other systematic components of the model. This focus renders the model less predictively accurate. If we compare the models on the cross-validation score, we see how the hierarchical mundlak model is to be preferred.\n\ncompare_df = az.compare({'naive': naive_idata, 'fixed': fixed_effects_idata, 'hierarchical': multilevel_model_idata, \n'mundlak': mundlak_idata})\ncompare_df\n\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/stats.py:805: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n\n\n\n\n\n\n  \n    \n      \n      rank\n      elpd_loo\n      p_loo\n      elpd_diff\n      weight\n      se\n      dse\n      warning\n      scale\n    \n  \n  \n    \n      mundlak\n      0\n      -1209.532733\n      12.255178\n      0.000000\n      0.951092\n      30.213512\n      0.000000\n      False\n      log\n    \n    \n      hierarchical\n      1\n      -1218.446608\n      27.713536\n      8.913876\n      0.025741\n      30.404197\n      4.443081\n      False\n      log\n    \n    \n      fixed\n      2\n      -1221.467513\n      33.823574\n      11.934780\n      0.000000\n      31.743164\n      5.184940\n      True\n      log\n    \n    \n      naive\n      3\n      -1295.325039\n      2.808180\n      85.792306\n      0.023167\n      29.474596\n      12.972671\n      False\n      log\n    \n  \n\n\n\n\n\naz.plot_compare(compare_df);\n\n\n\n\nThis is not the only way to assess viability of the model’s functional form but it’s not a bad way.\n\n\nFull Luxury Bayesian Mundlak Machine\nAs good Bayesians we might be worry about the false precision of adding simple point estimates for the group mean covariates in the Mundlak model. We can remedy this by explicitly incorporating these values as an extra parameter and adding uncertainty to the draws on these parameters.\n\nid_indx, unique_ids = pd.factorize(sim_df[\"g\"])\n\ncoords = {'ids': list(range(N_groups))}\nwith pm.Model(coords=coords) as model: \n\n    x_data = pm.Data('X_data', sim_df['X'])\n    z_data = pm.Data('Z_data', sim_df['Z'])\n    y_data = pm.Data('Y_data', sim_df['Y'])\n\n    alpha0 = pm.Normal('Intercept', 0, 1)\n    alpha_j = pm.Normal('alpha_j', 0, 1, dims='ids')\n    beta_xy = pm.Normal('X', 0, 1)\n    beta_zy = pm.Normal('Z', 0, 1)\n\n    group_means = pm.Normal('group_means', sim_df.groupby('g')['X'].mean().values, .1, dims='ids')\n    group_sigma = pm.HalfNormal('group_sigma', 0.1)\n    \n    group_mu = pm.Normal('group_mu', (group_means[id_indx]), group_sigma, observed=sim_df['X'].values)\n\n    mu = pm.Deterministic('mu', (alpha0 + alpha_j[id_indx]) + beta_xy*x_data + beta_zy*z_data + group_means[id_indx])\n    p = pm.Deterministic(\"p\", pm.math.invlogit(mu))\n    # likelihood\n    pm.Binomial(\"y\", n=1, p=p, observed=y_data)\n\n    idata = pm.sample(idata_kwargs={\"log_likelihood\": True})\n\nInstead of merely adding a new feature to a regression we’ve added a second likelihood term for the predictor \\(X\\) and seek to model this outcome as a function of the group means. In other words, we incorporate the idea that \\(X\\) is confounded by some aspect of the group membership, and seek to estimate the nature of that confounding as it influences both the realisations of \\(X\\) and \\(y\\). Calibrating the group_mean parameter that is also included in our likelihood term for the \\(Y\\) outcome.\n\npm.model_to_graphviz(model)\n\n\n\n\nThis model bakes more uncertainty into the process assuming a kind of measurement-error model, but again, the focus here is that we need to estimate a corrective adjustment factor for confounding effects that impact the estimation of impact due to our focal variable \\(X\\). It’s perhaps easier to see in this parameter recovery exercise than it is in real cases, but wherever we suspect plausible group level confounding you should consider Mundlak adjustments alongside your go-to fixed effects or random effects controls.\n\naz.summary(idata, var_names=['Intercept', 'X', 'Z'])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      -1.876\n      0.204\n      -2.248\n      -1.481\n      0.007\n      0.005\n      866.0\n      1519.0\n      1.0\n    \n    \n      X\n      0.990\n      0.060\n      0.871\n      1.099\n      0.001\n      0.001\n      6210.0\n      3050.0\n      1.0\n    \n    \n      Z\n      -0.523\n      0.175\n      -0.840\n      -0.185\n      0.005\n      0.004\n      1091.0\n      1951.0\n      1.0\n    \n  \n\n\n\n\nYou can see it also recovers the correct parameter specifcation well.\n\n\nRecap\nWe’ve just seen a number of different modelling specifications that attempt to deal with the threat of group level confounding. Some varieties of model result in entirely saturated regression models with more parameters than observations. These models can quickly become unwieldy. We’ll now look at a case-study where group level confounding remains highly plausible and the choice of estimation routine is not trivial. This will highlight aspects of how what we seek learn determines our model choice."
  },
  {
    "objectID": "posts/post-with-code/multilevel_confounding/multilevel_models.html#nested-groups-hierarchies-and-fixed-effects",
    "href": "posts/post-with-code/multilevel_confounding/multilevel_models.html#nested-groups-hierarchies-and-fixed-effects",
    "title": "Freedom, Hierarchies and Confounded Estimates",
    "section": "Nested Groups, Hierarchies and Fixed Effects",
    "text": "Nested Groups, Hierarchies and Fixed Effects\nWe’ve seen how various attempts to account for the group effects can more or less recover the parameters of a complex data generating process with group confounding. Now we want to look at a case where we can have interacting group effects at multiple levels.\n\n\n\nStructure\n\n\n\nPupils within Class Rooms within Schools\nA natural three level group hierarchy occurs in the context of educational organisations and business org-charts. We can use this fact to interrogate briefly how inferential statements about treatment effects vary as a function of what and how we control for group level variation. We draw the following data set from Linear Mixed Models: A Practical Guide Using Statistical Software.\n\ndf = pd.read_csv('classroom.csv')\ndf['class_mean'] = df.groupby(['classid'])['mathprep'].transform(np.mean)\ndf['school_mean'] = df.groupby(['schoolid'])['mathprep'].transform(np.mean)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      sex\n      minority\n      mathkind\n      mathgain\n      ses\n      yearstea\n      mathknow\n      housepov\n      mathprep\n      classid\n      schoolid\n      childid\n      class_mean\n      school_mean\n    \n  \n  \n    \n      0\n      1\n      1\n      448\n      32\n      0.46\n      1.0\n      NaN\n      0.082\n      2.00\n      160\n      1\n      1\n      2.00\n      2.909091\n    \n    \n      1\n      0\n      1\n      460\n      109\n      -0.27\n      1.0\n      NaN\n      0.082\n      2.00\n      160\n      1\n      2\n      2.00\n      2.909091\n    \n    \n      2\n      1\n      1\n      511\n      56\n      -0.03\n      1.0\n      NaN\n      0.082\n      2.00\n      160\n      1\n      3\n      2.00\n      2.909091\n    \n    \n      3\n      0\n      1\n      449\n      83\n      -0.38\n      2.0\n      -0.11\n      0.082\n      3.25\n      217\n      1\n      4\n      3.25\n      2.909091\n    \n    \n      4\n      0\n      1\n      425\n      53\n      -0.03\n      2.0\n      -0.11\n      0.082\n      3.25\n      217\n      1\n      5\n      3.25\n      2.909091\n    \n  \n\n\n\n\nThe data has three distinct levels: (1) the child or pupil and their demographic attributes and outcome variable mathgain, (2) the classroom and the teacher level attributes such as their experience yearstea and a record of their mathematics courses taken mathprep, (3) school and neighbourhood level with features describing poverty measures in the vicinity housepov.\nWe’ll plot the child’s outcome mathgain against the mathprep and distinguish the patterns by school.\n\n\nCode\ndef rand_jitter(arr):\n    stdev = .01 * (max(arr) - min(arr))\n    return arr + np.random.randn(len(arr)) * stdev\n\nlegend_elements = [Line2D([0], [0], marker='x', label='Minority', markerfacecolor='k'),\n                   Line2D([0], [0], marker='x', label='Minority + Poverty', markersize=14, markerfacecolor='k'), \n                   ]\n\nschools = df['schoolid'].unique()\nschools_10 = [schools[i:i+10] for i in range(0, len(schools), 10)]\nfig, axs = plt.subplots(3,4, figsize=(15, 13), \nsharey=True, sharex=True)\naxs = axs.flatten()\nmkr_dict = {1: 'x', 0: '+'}\nfor s, ax in zip(schools_10, axs):\n    temp = df[df['schoolid'].isin(s)]\n    temp['m'] = temp['minority'].map(mkr_dict)\n    for m in temp['m'].unique():\n        temp1 = temp[temp['m'] == m]\n        ax.scatter(rand_jitter(temp1['mathprep']), \n        temp1['mathgain'], \n        c=temp1['schoolid'], cmap='tab10', \n        s=temp1['housepov']*100, \n        marker = m)\n    ax.set_title(f\"Schools \\n {s}\");\n    ax.set_xlabel(\"MathPrep\")\n    ax.set_ylabel(\"MathGain\")\n\naxs[0].legend(handles=legend_elements);\n\n\n\n\n\nWe’ve plotted here the individual student outcomes. We’ve sized the dots by the poverty in their neighbourhoods and differentiated the markers by whether the student was in minority group. There are, in short, reasons here to worry about group-level confounding. There is a small number of observed students per school so the individual school level distributions show some extreme outliers but the overall distribution nicely converges to an approximately normal symmetric shape.\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 6))\naxs = axs.flatten()\nfor school in schools:\n    temp = df[df['schoolid'] ==school]\n    axs[0].hist(temp['mathgain'], color='grey', alpha=0.3, density=True, histtype='step', cumulative=True)\n\naxs[0].hist(df['mathgain'], bins=30, ec='black', density=True, cumulative=True, histtype='step')\naxs[1].hist(df['mathgain'], bins=30, ec='black', density=True, cumulative=False)\naxs[0].set_title(\"Cumulative Distribution Function by School\")\naxs[1].set_title(\"Overall Distribution\");\n\n\n\n\nWith these kinds of structures we need to be careful in how we evaluate any treatment effects when there are reasons to believe in group-level effects that impact the outcome variable. Imagine the true treatment effect is a sprinter running too fast to cleanly measure - each group interaction effect is added to his load as a weight. Enough weights absorb enough of the variation in the treatment that he is dragged to a crawl. Whatever movement he can make while dragging this burden is the effect we attribute to the treatment. Or put another way - the effects of various group interactions will modify the treatment effectiveness in some way and unless we account for this impact in our model, then the inferences regarding the treatment will be clouded.\n\n\nInteraction Effects and Nuisance Parameters\nNot all possible interactions will be present in the data. But if we specify the model to account for various interactions we may explode the number of parameters beyond the number of data points. This can cause issues in estimation routines and requires consideration about what the group parameters are aimed at capturing.\nWhy add covariates for group-membership when no observation reflects outcomes in that group? We cannot learn anything about these cases. At least if we add the group effects as a hierarchical random effect we induce shrinkage on the parameter estimates towards the mean of the hierarchical parameter in the model. This means that when predicting on “new” data with examples of these missing cases we can predict a sensible default. The distinction rests in the role we have in mind for these tools. If we seriously commit to the idea that group variation reflects a real “common” process in the larger population and we want to learn about that over-arching process then we deploy a random effects model. But if we only see these as tools for accounting to variation in the sample, allowing us to pin down an alternative focal estimate then the group indicator covariates are just “nuisance” parameters and missing cases are irrelevant.\n\n\n\n\n\n\nPhilosophical Digression\n\n\n\nThis last point skips a little quickly over a fundamental feature of interpreting these models. If we aim to interpret these models as reflecting a common process across these groups that exists in a “population”, then we’re endorsing an inferential view that extends beyond the sample. We’re actively seeking to learn a general truth about the data generating process which we deem to be adequately expressed in our model. If we seek to “soak up” the variation due to group effects, we’re treating these group effects as noise in the sample data and making inferential commitments only about the focal parameter in the model. This approach to learning differentiates approaches to credible causal inference. On the one hand, fixing your estimand and designing estimators to specifically capture that estimate seems like a modest and compelling strategy. On the other hand if your model ignores aspects of underlying phenomena or fails to retrodict the observable data, it’s dubious as to why anyone would trust its output.\n\n\nTo see the extent of redundany we can examine the dimensions of the covariate matrices that result from including more or less interaction terms.\n\ny, X = dmatrices(\"mathgain ~ mathprep + C(schoolid)+ C(classid)\", df, return_type=\"dataframe\")\nprint(X.shape)\n\ny, X1 = dmatrices(\"mathgain ~ mathprep + C(schoolid)/C(classid)\", df, return_type=\"dataframe\")\nprint(X1.shape)\n\n\ny, X2 = dmatrices(\"mathgain ~ mathprep + C(schoolid):C(childid)\", df, return_type=\"dataframe\")\nprint(X2.shape)\n\n(1190, 419)\n\n\n(1190, 33385)\n\n\n(1190, 127331)\n\n\nWe see here how different ways in which to account for group level variation and interaction effects lead to vastly inflated feature matrices. However not all interaction terms matter, or put another way… nor all the possible interactions feature in the data. So we have likely inflated the data matrix beyond necessity.\nHere we define a helper function to parse a complex interaction formula, remove the columns entirely composed of zeros and return a new formula and dataframe which has a suitable range of features to capture the variation structures in the data.\n\ndef make_interactions_df(formula, df):\n    y, X = dmatrices(formula, df, return_type=\"dataframe\")\n    n = X.shape[1]\n    X = X[X.columns[~(np.abs(X) < 1e-12).all()]]\n    n1 = X.shape[1]\n    target_name = y.columns[0]\n    d = pd.concat([y, X], axis=1)\n    d.drop(['Intercept'], axis=1, inplace=True)\n    d.columns = [c.replace('[', '').replace(']','').replace('C(', '').replace(')', '').replace('.', '_').replace(':', '_') for c in d.columns]\n    cols = ' + '.join([col for col in d.columns if col != target_name])\n    formula = f\"{target_name} ~ {cols}\"\n    print(f\"\"\"Size of original interaction features: {n} \\nSize of reduced feature set: {n1}\"\"\")\n    return formula, d\n\nformula, interaction_df = make_interactions_df(\"mathgain ~ mathprep + C(schoolid):C(childid)\", df)\n\ninteraction_df.head()\n\nSize of original interaction features: 127331 \nSize of reduced feature set: 2370\n\n\n\n\n\n\n  \n    \n      \n      mathgain\n      childidT_2\n      childidT_3\n      childidT_4\n      childidT_5\n      childidT_6\n      childidT_7\n      childidT_8\n      childidT_9\n      childidT_10\n      ...\n      schoolidT_107_childid1182\n      schoolidT_107_childid1183\n      schoolidT_107_childid1184\n      schoolidT_107_childid1185\n      schoolidT_107_childid1186\n      schoolidT_107_childid1187\n      schoolidT_107_childid1188\n      schoolidT_107_childid1189\n      schoolidT_107_childid1190\n      mathprep\n    \n  \n  \n    \n      0\n      32.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2.00\n    \n    \n      1\n      109.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2.00\n    \n    \n      2\n      56.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2.00\n    \n    \n      3\n      83.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      3.25\n    \n    \n      4\n      53.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      3.25\n    \n  \n\n5 rows × 2370 columns\n\n\n\nWe have reduced the number of interactions by an order of magnitude! We can now fit a regression model to the revised feature matrix.\n\n\nComparing Interaction Models\nConsider the variation in the coefficient values estimated for mathprep as we add more and more interaction effects. The addition of interaction effects generates a large number of completely 0 interaction terms which we remove here.\n\nformulas = [\"\"\"mathgain ~ mathprep + C(schoolid)\"\"\",\n\"\"\" mathgain ~ mathprep + school_mean*class_mean\"\"\" , \n\"\"\" mathgain ~ mathprep + mathkind + sex + school_mean*class_mean\"\"\" , \n\"\"\"mathgain ~ mathprep + C(schoolid) + C(classid)\"\"\", \n\"\"\"mathgain ~ mathprep + C(schoolid)*C(classid)\"\"\",\n\"\"\"mathgain ~ mathprep + C(classid):C(childid)\"\"\", \n]\n\nestimates_df = []\nfor f in formulas:\n    formula, interaction_df = make_interactions_df(f, df)\n    result = smf.ols(formula, interaction_df).fit()\n    estimates = [[result.params['mathprep']], list(result.conf_int().loc['mathprep', :]), [formula]]\n    estimates = [e for est in estimates for e in est]\n    estimates_df.append(estimates)\n\nestimates_df = pd.DataFrame(estimates_df, columns=['mathprep_estimate', 'lower bound', 'upper bound', 'formula'])\n\nestimates_df\n\nSize of original interaction features: 108 \nSize of reduced feature set: 108\nSize of original interaction features: 5 \nSize of reduced feature set: 5\nSize of original interaction features: 7 \nSize of reduced feature set: 7\nSize of original interaction features: 419 \nSize of reduced feature set: 419\n\n\nSize of original interaction features: 33385 \nSize of reduced feature set: 728\n\n\nSize of original interaction features: 371281 \nSize of reduced feature set: 2376\n\n\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:1717: RuntimeWarning: divide by zero encountered in scalar divide\n  return np.dot(wresid, wresid) / self.df_resid\n\n\n\n\n\n\n  \n    \n      \n      mathprep_estimate\n      lower bound\n      upper bound\n      formula\n    \n  \n  \n    \n      0\n      1.060768\n      -1.435118\n      3.556655\n      mathgain ~ schoolidT_2 + schoolidT_3 + schooli...\n    \n    \n      1\n      1.789413\n      -1.587051\n      5.165876\n      mathgain ~ mathprep + school_mean + class_mean...\n    \n    \n      2\n      0.904018\n      -2.056505\n      3.864541\n      mathgain ~ mathprep + mathkind + sex + school_...\n    \n    \n      3\n      2.948546\n      0.601150\n      5.295942\n      mathgain ~ schoolidT_2 + schoolidT_3 + schooli...\n    \n    \n      4\n      3.545931\n      1.359793\n      5.732068\n      mathgain ~ schoolidT_2 + schoolidT_3 + schooli...\n    \n    \n      5\n      2.303187\n      NaN\n      NaN\n      mathgain ~ childidT_2 + childidT_3 + childidT_...\n    \n  \n\n\n\n\nThe point here (perhap obvious) is that the estimate of treatment effects due to some policy or programme can be differently understood when the regression model is able to account for increasing aspects of individual variation. Choice of the right way to “saturate” your regression specification are at the heart of causal inference. The right control structures determine the freedom available to vary your focal treatment effect parameter.\nWe will consider a number of specifications below that incorporate these group effects in a hierarchical model which nests the effect of class-membership within school membership. This choice allows us to control for group specific interactions without worrying about over-indexing on the observed interaction effects in the sample data requiring that we handle more fixed effects parameters than we have data points.\nIn what follows we’ll specify a nested approach to the parameter specifcation using a random effects model. The idea here is that classes are already implicitly nested in schools and so we don’t need to add parameters for classes at multiple schools. Additionally we’re positing that there is independent interest in the effectiveness school/class effects i.e. the degree to which variation in a school/class nest can account for variation in the outcome.\n\nMinimal Model\n\nmodel = bmb.Model(f\"mathgain ~ mathprep + (1 | schoolid / classid)\", df)\nidata = model.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\nThe model specification here is deliberately minimalist we want to observe how much of the variation in the outcome can be accounted for by solely adding extensive controls for interactions of group level effects and the treatment but ignoring all else.\n\nmodel.graph()\n\n\n\n\nWe can see the derived sigma parameters here which can be understood as partialling out the variance of the outcome into components due to those group level effects and the unexplained residuals.\n\naz.summary(idata, var_names=['Intercept', '1|schoolid_sigma', '1|schoolid:classid_sigma', 'mathgain_sigma', 'mathprep'])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      52.561\n      3.542\n      46.010\n      59.516\n      0.066\n      0.047\n      2839.0\n      2731.0\n      1.00\n    \n    \n      1|schoolid_sigma\n      8.319\n      2.265\n      4.099\n      12.590\n      0.124\n      0.087\n      399.0\n      285.0\n      1.01\n    \n    \n      1|schoolid:classid_sigma\n      9.877\n      2.459\n      5.283\n      14.369\n      0.161\n      0.114\n      256.0\n      313.0\n      1.02\n    \n    \n      mathgain_sigma\n      32.136\n      0.786\n      30.652\n      33.620\n      0.020\n      0.014\n      1537.0\n      2143.0\n      1.00\n    \n    \n      mathprep\n      1.889\n      1.242\n      -0.563\n      4.092\n      0.023\n      0.017\n      2933.0\n      2596.0\n      1.00\n    \n  \n\n\n\n\nNote here the relative proportion of the school specifc variances 1|schoolid_sigma to the overall variance of the residuals mathgain_sigma.\n\n\n\nCalculating the IntraClass Correlation Coefficient\nThese models faciliate the calculation of the ICC statistics which is a measure of “explained variance”. The thought is to gauge the proportion of variance ascribed to one set of random effects over and above the total estimated variance in the baseline model, including the residuals mathgain_sigma.\n\na = idata['posterior']['1|schoolid_sigma']**2\n\nb = (idata['posterior']['1|schoolid:classid_sigma']**2 + idata['posterior']['1|schoolid_sigma']**2)\n\nc = (idata['posterior']['1|schoolid:classid_sigma']**2 + idata['posterior']['1|schoolid_sigma']**2 + idata['posterior']['mathgain_sigma']**2)\n\n(a / c).mean().item() \n\n0.061101339125402415\n\n\n\n((a + b) / c).mean().item()\n\n0.20748893817579722\n\n\nWe can see here that the interaction terms do seem to account for a goodly portion of the variance in the outcome and we ought to consider retaining their inclusion in our modelling work. The structure of the problem drives us towards their inclusion. Class/school effects are going to absorb a sufficient portion of the variation. So they merit study in their own right, lest the individual class/school dynamics obscure the effectiveness of the mathprep treatment. Similarly, it’s likely valuable to consider the efficacy of the average class/school in a wider policy conversation.\n\n\nAugmenting the Models\nNext we augment our model with more pupil level control variables aiming to pin down some of the aspects of the variation in the outcome.\n\nAdding Pupil Fixed Effects\nHere we add these fixed effects population parameters. But note they are not merely devices for controlling variance in the outcome, they’re interpretation is likely of independent interest.\n\nmodel_fixed = bmb.Model(f\"mathgain ~  sex + minority + ses + mathprep + (1 | schoolid / classid)\", df)\nidata_fixed = model_fixed.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\n\naz.summary(idata_fixed, var_names=['Intercept', 'sex', 'minority', 'ses', 'mathprep',\n'1|schoolid_sigma', '1|schoolid:classid_sigma', 'mathgain_sigma'])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      53.364\n      4.052\n      45.853\n      61.036\n      0.075\n      0.053\n      2911.0\n      2702.0\n      1.00\n    \n    \n      sex\n      -2.320\n      1.980\n      -6.060\n      1.392\n      0.029\n      0.024\n      4785.0\n      2927.0\n      1.00\n    \n    \n      minority\n      0.285\n      2.666\n      -4.744\n      5.240\n      0.046\n      0.038\n      3342.0\n      2993.0\n      1.00\n    \n    \n      ses\n      0.916\n      1.442\n      -1.713\n      3.641\n      0.024\n      0.020\n      3726.0\n      2813.0\n      1.00\n    \n    \n      mathprep\n      1.933\n      1.207\n      -0.444\n      4.032\n      0.024\n      0.017\n      2571.0\n      2478.0\n      1.00\n    \n    \n      1|schoolid_sigma\n      8.575\n      2.000\n      4.787\n      12.441\n      0.082\n      0.058\n      632.0\n      623.0\n      1.01\n    \n    \n      1|schoolid:classid_sigma\n      9.857\n      2.345\n      5.219\n      14.041\n      0.120\n      0.085\n      402.0\n      537.0\n      1.01\n    \n    \n      mathgain_sigma\n      32.155\n      0.768\n      30.760\n      33.633\n      0.020\n      0.014\n      1425.0\n      2531.0\n      1.00\n    \n  \n\n\n\n\nNow we add a further class level control.\n\n\nAdding More Class and Pupil Level Fixed Effects\n\nmodel_fixed_1 = bmb.Model(f\"mathgain ~ mathkind + sex + minority + ses + yearstea + mathknow + mathprep + (1 | schoolid / classid)\", df.dropna())\nidata_fixed_1 = model_fixed_1.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\n\naz.summary(idata_fixed_1, var_names=['Intercept', \n'mathkind', 'sex', 'minority', 'ses', 'yearstea', 'mathknow', 'mathprep','1|schoolid_sigma', '1|schoolid:classid_sigma', 'mathgain_sigma'])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      281.822\n      11.832\n      259.430\n      303.680\n      0.204\n      0.144\n      3361.0\n      2999.0\n      1.00\n    \n    \n      mathkind\n      -0.475\n      0.023\n      -0.520\n      -0.434\n      0.000\n      0.000\n      3596.0\n      2843.0\n      1.00\n    \n    \n      sex\n      -1.413\n      1.677\n      -4.369\n      1.928\n      0.026\n      0.022\n      4234.0\n      2940.0\n      1.00\n    \n    \n      minority\n      -7.866\n      2.428\n      -12.438\n      -3.209\n      0.043\n      0.030\n      3230.0\n      3086.0\n      1.00\n    \n    \n      ses\n      5.414\n      1.256\n      3.165\n      7.864\n      0.020\n      0.014\n      4041.0\n      2963.0\n      1.00\n    \n    \n      yearstea\n      0.042\n      0.118\n      -0.178\n      0.253\n      0.002\n      0.002\n      2778.0\n      2847.0\n      1.00\n    \n    \n      mathknow\n      1.861\n      1.211\n      -0.236\n      4.339\n      0.028\n      0.020\n      1915.0\n      2552.0\n      1.00\n    \n    \n      mathprep\n      1.119\n      1.142\n      -1.145\n      3.189\n      0.022\n      0.016\n      2700.0\n      2959.0\n      1.00\n    \n    \n      1|schoolid_sigma\n      8.603\n      1.701\n      5.402\n      11.758\n      0.070\n      0.050\n      606.0\n      871.0\n      1.01\n    \n    \n      1|schoolid:classid_sigma\n      9.348\n      1.745\n      5.827\n      12.490\n      0.092\n      0.068\n      353.0\n      899.0\n      1.02\n    \n    \n      mathgain_sigma\n      26.766\n      0.675\n      25.578\n      28.073\n      0.015\n      0.011\n      1995.0\n      2470.0\n      1.00\n    \n  \n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 7))\naz.plot_forest([idata, idata_fixed, idata_fixed_1], combined=True, var_names=['mathprep', '1|schoolid_sigma', '1|schoolid:classid_sigma', 'mathgain_sigma'], ax=ax)\nax.axvline(1)\n\n<matplotlib.lines.Line2D at 0x29f11b890>\n\n\n\n\n\nWe now make use of bambis model interpretation module to plot the marginal effect on the outcome due to changes in the treatment intensity.\n\n\nCode\nfig, axs = plt.subplots(1, 3, figsize=(20, 6), \ndpi=120, sharey=True, sharex=True)\naxs = axs.flatten()\naxs[0].axhline(50, color='red', label='Reference Line', \nlinestyle='--')\naxs[1].axhline(50, color='red', label='Reference Line', \nlinestyle='--')\naxs[2].axhline(50, color='red', label='Reference Line', \nlinestyle='--')\nbmb.interpret.plot_predictions(model, idata, \"mathprep\", ax=axs[0]);\nbmb.interpret.plot_predictions(model_fixed, idata_fixed, \"mathprep\", ax=axs[1]);\nbmb.interpret.plot_predictions(model_fixed_1, idata_fixed_1, \"mathprep\", ax=axs[2]);\naxs[0].set_title(\"Variation in Implied Outcome by Treatment \\n mathgain ~ mathprep + (1 | schoolid / classid) \")\naxs[1].set_title(\"Variation in Implied Outcome by Treatment \\n mathgain ~ sex + minority + ses + mathprep + (1 | schoolid / classid) \")\naxs[2].set_title(\"Variation in Implied Outcome by Treatment \\n mathgain ~ mathkind + sex + minority + ses + yearstea + \\n mathknow + mathprep + (1 | schoolid / classid\")\naxs[0].set_xlabel('')\naxs[1].set_xlabel('')\naxs[0].legend();\n\n\n\n\n\nAs we can see here across all the different model specifications we see quite modest effects of treatment with very wide uncertainty. You might therefore be sceptical that teacher training has any real discernible impact on child outcomes? Maybe you believe other interventions are more important to fund? These kinds of questions determine policy. So misguided policy interventions on child-hood education can have radical consequences. It’s, therefore, vital that we have robust and justifiable approaches and tooling for the analysis of these policy questions in the face of group level confounding.\nThis last point is crucial - when model complexity balloons due extensive interaction effects, then we need efficient tools to interrogate the outcome level differences implied by the model choices. Your intuition and understanding of a process is keyed to the observable effects of that process. Your understanding of the internal mechanics of a model is likely less than your concrete expectations of observable behaviour. As such, credibility requires that we be able to assess models on the variation produced in the outcome variable rather merely on the parameters values."
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "",
    "text": "library(lavaan)\nlibrary(dplyr)\nlibrary(reticulate)\nlibrary(marginaleffects)\nlibrary(modelsummary)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(egg)\nlibrary(lme4)\nlibrary(semPlot)\nlibrary(tinytable)\nlibrary(kableExtra)\nlibrary(reshape2)\nreticulate::py_run_string(\"import pymc as pm\")\n\noptions(rstudio.python.installationPath = \"/Users/nathanielforde/mambaforge/envs\")\noptions(\"modelsummary_factory_default\" = \"tinytable\")\noptions(repr.plot.width=15, repr.plot.height=8)\n\nknitr::knit_engines$set(python = reticulate::eng_python)\noptions(scipen=999)\nset.seed(130)"
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#measurment-and-measurment-constructs",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#measurment-and-measurment-constructs",
    "title": "Measurement, Latent Factors and Theory Construction",
    "section": "Measurment and Measurment Constructs",
    "text": "Measurment and Measurment Constructs\n\ndf = read.csv('sem_data.csv')\ninflate <- df$region == 'west'\nnoise <- rnorm(sum(inflate), 0.5, 1) # generate the noise to add\ndf$ls_p3[inflate] <- df$ls_p3[inflate] + noise\ndf$ls_sum <- df$ls_p1 + df$ls_p2 + df$ls_p3\ndf$ls_mean <- rowMeans(df[ c('ls_p1', 'ls_p2', 'ls_p3')])\n\nhead(df) |> kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nregion\ngender\nage\nse_acad_p1\nse_acad_p2\nse_acad_p3\nse_social_p1\nse_social_p2\nse_social_p3\nsup_friends_p1\nsup_friends_p2\nsup_friends_p3\nsup_parents_p1\nsup_parents_p2\nsup_parents_p3\nls_p1\nls_p2\nls_p3\nls_sum\nls_mean\n\n\n\n\n1\nwest\nfemale\n13\n4.857143\n5.571429\n4.500000\n5.80\n5.500000\n5.40\n6.5\n6.5\n7.0\n7.0\n7.0\n6.0\n5.333333\n6.75\n7.647112\n19.73044\n6.576815\n\n\n2\nwest\nmale\n14\n4.571429\n4.285714\n4.666667\n5.00\n5.500000\n4.80\n4.5\n4.5\n5.5\n5.0\n6.0\n4.5\n4.333333\n5.00\n4.469623\n13.80296\n4.600985\n\n\n10\nwest\nfemale\n14\n4.142857\n6.142857\n5.333333\n5.20\n4.666667\n6.00\n4.0\n4.5\n3.5\n7.0\n7.0\n6.5\n6.333333\n5.50\n4.710020\n16.54335\n5.514451\n\n\n11\nwest\nfemale\n14\n5.000000\n5.428571\n4.833333\n6.40\n5.833333\n6.40\n7.0\n7.0\n7.0\n7.0\n7.0\n7.0\n4.333333\n6.50\n5.636198\n16.46953\n5.489844\n\n\n12\nwest\nfemale\n14\n5.166667\n5.600000\n4.800000\n5.25\n5.400000\n5.25\n7.0\n7.0\n7.0\n6.5\n6.5\n7.0\n5.666667\n6.00\n5.266592\n16.93326\n5.644419\n\n\n14\nwest\nmale\n14\n4.857143\n4.857143\n4.166667\n5.20\n5.000000\n4.20\n5.5\n6.5\n7.0\n6.5\n6.5\n6.5\n5.000000\n5.50\n5.913709\n16.41371\n5.471236\n\n\n\n\n\n\n\n\nCandidate Structure\n\n\n\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\ndatasummary_skim(df)|> \n style_tt(\n   i = 15:17,\n   j = 1:1,\n   background = \"#20AACC\",\n   color = \"white\",\n   italic = TRUE) |> \n style_tt(\n   i = 18:19,\n   j = 1:1,\n   background = \"#2888A0\",\n   color = \"white\",\n   italic = TRUE) |> \n style_tt(\n   i = 2:14,\n   j = 1:1,\n   background = \"#17C2AD\",\n   color = \"white\",\n   italic = TRUE)\n\n\n \n\n  \n    \n    \n    tinytable_ftyu5q15n5ibjk84ch05\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                Unique\n                Missing Pct.\n                Mean\n                SD\n                Min\n                Median\n                Max\n                Histogram\n              \n        \n        \n        \n                \n                  ID\n                  283\n                  0\n                  187.9\n                  106.3\n                  1.0\n                  201.0\n                  367.0\n                  \n                \n                \n                  age\n                  5\n                  0\n                  14.7\n                  0.8\n                  13.0\n                  15.0\n                  17.0\n                  \n                \n                \n                  se_acad_p1\n                  32\n                  0\n                  5.2\n                  0.8\n                  3.1\n                  5.1\n                  7.0\n                  \n                \n                \n                  se_acad_p2\n                  36\n                  0\n                  5.3\n                  0.7\n                  3.1\n                  5.4\n                  7.0\n                  \n                \n                \n                  se_acad_p3\n                  29\n                  0\n                  5.2\n                  0.8\n                  2.8\n                  5.2\n                  7.0\n                  \n                \n                \n                  se_social_p1\n                  24\n                  0\n                  5.3\n                  0.8\n                  1.8\n                  5.4\n                  7.0\n                  \n                \n                \n                  se_social_p2\n                  27\n                  0\n                  5.5\n                  0.7\n                  2.7\n                  5.5\n                  7.0\n                  \n                \n                \n                  se_social_p3\n                  31\n                  0\n                  5.4\n                  0.8\n                  3.0\n                  5.5\n                  7.0\n                  \n                \n                \n                  sup_friends_p1\n                  13\n                  0\n                  5.8\n                  1.1\n                  1.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_friends_p2\n                  10\n                  0\n                  6.0\n                  0.9\n                  2.5\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_friends_p3\n                  13\n                  0\n                  6.0\n                  1.1\n                  1.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_parents_p1\n                  11\n                  0\n                  6.0\n                  1.1\n                  1.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_parents_p2\n                  11\n                  0\n                  5.9\n                  1.1\n                  2.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_parents_p3\n                  13\n                  0\n                  5.7\n                  1.1\n                  1.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  ls_p1\n                  15\n                  0\n                  5.2\n                  0.9\n                  2.0\n                  5.3\n                  7.0\n                  \n                \n                \n                  ls_p2\n                  21\n                  0\n                  5.8\n                  0.7\n                  2.5\n                  5.8\n                  7.0\n                  \n                \n                \n                  ls_p3\n                  161\n                  0\n                  5.5\n                  1.1\n                  1.7\n                  5.6\n                  9.6\n                  \n                \n                \n                  ls_sum\n                  218\n                  0\n                  16.5\n                  2.2\n                  8.2\n                  16.7\n                  21.0\n                  \n                \n                \n                  ls_mean\n                  217\n                  0\n                  5.5\n                  0.7\n                  2.7\n                  5.6\n                  7.0\n                  \n                \n                \n                   \n                    \n                  N\n                  %\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  region\n                  east\n                  142\n                  50.2\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  \n                  west\n                  141\n                  49.8\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  gender\n                  female\n                  132\n                  46.6\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  \n                  male\n                  151\n                  53.4\n                  \n                  \n                  \n                  \n                  \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\ndrivers = c('se_acad_p1', 'se_acad_p2', 'se_acad_p3', 'se_social_p1', 'se_social_p2', 'se_social_p3', 'sup_friends_p1','sup_friends_p2', 'sup_friends_p3', 'sup_parents_p1' , 'sup_parents_p2' , 'sup_parents_p3', 'ls_p1', 'ls_p2', 'ls_p3')\n\n\n\nplot_heatmap <- function(df, title=\"Sample Covariances\", subtitle=\"Observed Measures\") {\n  heat_df = df |> as.matrix() |> melt()\n  colnames(heat_df) <- c(\"x\", \"y\", \"value\")\n  g <- heat_df |> mutate(value = round(value, 2)) |>  ggplot(aes(x = x, y = y, fill = value)) +\n    geom_tile() + geom_text(aes(label = value), color = \"black\", size = 4) +\n   scale_fill_gradient2(\n      high = 'dodgerblue4',\n      mid = 'white',\n      low = 'firebrick2'\n    ) + theme(axis.text.x = element_text(angle=45)) + ggtitle(title, subtitle)\n  \n  g\n}\n\ng1 = plot_heatmap(cov(df[,  drivers]))\n\ng2 = plot_heatmap(cor(df[,  drivers]), \"Sample Correlations\")\n\nplot <- ggarrange(g1,g2, ncol=1, nrow=2);"
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#fit-initial-regression-models",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#fit-initial-regression-models",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Fit Initial Regression Models",
    "text": "Fit Initial Regression Models\nTo model these effects we make use of the aggregated sum and mean scores to express the relationships between these indicator metrics and life-satisfaction. We fit a variety of models each one escalating in the number of indicator metrics we incorporate into our model of the life-satisfaction outcome. This side-steps the multivariate nature of hypothesised constructs and crudely amalgamates the indicator metrics. This may be more or less justified depending on how similar in theme the three outcome questions ls_p1, ls_p2, ls_p3 are in nature.\n\nformula_sum_1st = \" ls_sum ~ se_acad_p1  + se_social_p1 +  sup_friends_p1  + sup_parents_p1\"\nformula_mean_1st = \" ls_mean ~ se_acad_p1  + se_social_p1 +  sup_friends_p1  + sup_parents_p1\"\n\nformula_sum_12 = \" ls_sum ~ se_acad_p1  + se_acad_p2 +  se_social_p1 + se_social_p2 + \nsup_friends_p1 + sup_friends_p2  + sup_parents_p1 + sup_parents_p2\"\nformula_mean_12 = \" ls_mean ~ se_acad_p1  + se_acad_p2 +  se_social_p1 + se_social_p2 + \nsup_friends_p1 + sup_friends_p2  + sup_parents_p1 + sup_parents_p2\"\n\n\nformula_sum = \" ls_sum ~ se_acad_p1 + se_acad_p2 + se_acad_p3 + se_social_p1 +  se_social_p2 + se_social_p3 +  sup_friends_p1 + sup_friends_p2 + sup_friends_p3 + sup_parents_p1 + sup_parents_p2 + sup_parents_p3\"\nformula_mean = \" ls_mean ~ se_acad_p1 + se_acad_p2 + se_acad_p3 + se_social_p1 +  se_social_p2 + se_social_p3 +  sup_friends_p1 + sup_friends_p2 + sup_friends_p3 + sup_parents_p1 + sup_parents_p2 + sup_parents_p3\"\nmod_sum = lm(formula_sum, df)\nmod_sum_1st = lm(formula_sum_1st, df)\nmod_sum_12 = lm(formula_sum_12, df)\nmod_mean = lm(formula_mean, df)\nmod_mean_1st = lm(formula_mean_1st, df)\nmod_mean_12 = lm(formula_mean_12, df)\n\nmin_max_norm <- function(x) {\n    (x - min(x)) / (max(x) - min(x))\n}\n\ndf_norm <- as.data.frame(lapply(df[c(5:19)], min_max_norm))\n\ndf_norm$ls_sum <- df$ls_sum\ndf_norm$ls_mean <- df$ls_mean\n\nmod_sum_norm = lm(formula_sum, df_norm)\nmod_mean_norm = lm(formula_mean, df_norm)\n\nmodels = list(\n    \"Outcome: sum_score\" = list(\"model_sum_1st_factors\" = mod_sum_1st,\n     \"model_sum_1st_2nd_factors\" = mod_sum_12,\n     \"model_sum_score\" = mod_sum,\n     \"model_sum_score_norm\" = mod_sum_norm\n     ),\n    \"Outcome: mean_score\" = list(\n      \"model_mean_1st_factors\" = mod_mean_1st,\n     \"model_mean_1st_2nd_factors\" = mod_mean_12,\n     \"model_mean_score\"= mod_mean, \n     \"model_mean_score_norm\" = mod_mean_norm\n    )\n    )\n\nThe classical presentation of regression models reports the coefficient weights accorded to each of the input variables. We present these models to highlight that the manner in which we represent our theoretical constructs has ramifications for the interpretation of the data generating process. In particular, note how different degrees of significance are accorded to the different variables depending on which variables are included.\n\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\nmodelsummary(models, stars=TRUE, shape =\"cbind\") |> \n style_tt(\n   i = 2:25,\n   j = 1:1,\n   background = \"#17C2AD\",\n   color = \"white\",\n   italic = TRUE)\n\n\n \n\n  \n    \n    \n    tinytable_pclqteqllnuzsr3iakja\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n\n \nOutcome: sum_score\nOutcome: mean_score\n\n        \n              \n                 \n                model_sum_1st_factors\n                model_sum_1st_2nd_factors\n                model_sum_score\n                model_sum_score_norm\n                model_mean_1st_factors\n                model_mean_1st_2nd_factors\n                model_mean_score\n                model_mean_score_norm\n              \n        \n        + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n        \n                \n                  (Intercept)   \n                  5.118***\n                  2.644** \n                  2.094*  \n                  7.783***\n                  1.706***\n                  0.881** \n                  0.698*  \n                  2.594***\n                \n                \n                                \n                  (0.907) \n                  (0.985) \n                  (0.954) \n                  (0.658) \n                  (0.302) \n                  (0.328) \n                  (0.318) \n                  (0.219) \n                \n                \n                  se_acad_p1    \n                  0.242   \n                  -0.034  \n                  -0.208  \n                  -0.800  \n                  0.081   \n                  -0.011  \n                  -0.069  \n                  -0.267  \n                \n                \n                                \n                  (0.147) \n                  (0.180) \n                  (0.192) \n                  (0.742) \n                  (0.049) \n                  (0.060) \n                  (0.064) \n                  (0.247) \n                \n                \n                  se_social_p1  \n                  1.088***\n                  0.501*  \n                  0.355+  \n                  1.846+  \n                  0.363***\n                  0.167*  \n                  0.118+  \n                  0.615+  \n                \n                \n                                \n                  (0.162) \n                  (0.204) \n                  (0.200) \n                  (1.039) \n                  (0.054) \n                  (0.068) \n                  (0.067) \n                  (0.346) \n                \n                \n                  sup_friends_p1\n                  0.125   \n                  -0.224+ \n                  -0.272+ \n                  -1.630+ \n                  0.042   \n                  -0.075+ \n                  -0.091+ \n                  -0.543+ \n                \n                \n                                \n                  (0.088) \n                  (0.133) \n                  (0.150) \n                  (0.901) \n                  (0.029) \n                  (0.044) \n                  (0.050) \n                  (0.300) \n                \n                \n                  sup_parents_p1\n                  0.561***\n                  0.238+  \n                  0.072   \n                  0.432   \n                  0.187***\n                  0.079+  \n                  0.024   \n                  0.144   \n                \n                \n                                \n                  (0.100) \n                  (0.141) \n                  (0.143) \n                  (0.858) \n                  (0.033) \n                  (0.047) \n                  (0.048) \n                  (0.286) \n                \n                \n                  se_acad_p2    \n                          \n                  0.448*  \n                  0.327   \n                  1.261   \n                          \n                  0.149*  \n                  0.109   \n                  0.420   \n                \n                \n                                \n                          \n                  (0.197) \n                  (0.202) \n                  (0.779) \n                          \n                  (0.066) \n                  (0.067) \n                  (0.260) \n                \n                \n                  se_social_p2  \n                          \n                  0.756***\n                  0.509*  \n                  2.206*  \n                          \n                  0.252***\n                  0.170*  \n                  0.735*  \n                \n                \n                                \n                          \n                  (0.213) \n                  (0.219) \n                  (0.949) \n                          \n                  (0.071) \n                  (0.073) \n                  (0.316) \n                \n                \n                  sup_friends_p2\n                          \n                  0.369*  \n                  0.331*  \n                  1.490*  \n                          \n                  0.123*  \n                  0.110*  \n                  0.497*  \n                \n                \n                                \n                          \n                  (0.157) \n                  (0.160) \n                  (0.720) \n                          \n                  (0.052) \n                  (0.053) \n                  (0.240) \n                \n                \n                  sup_parents_p2\n                          \n                  0.370** \n                  0.118   \n                  0.591   \n                          \n                  0.123** \n                  0.039   \n                  0.197   \n                \n                \n                                \n                          \n                  (0.138) \n                  (0.144) \n                  (0.719) \n                          \n                  (0.046) \n                  (0.048) \n                  (0.240) \n                \n                \n                  se_acad_p3    \n                          \n                          \n                  0.153   \n                  0.637   \n                          \n                          \n                  0.051   \n                  0.212   \n                \n                \n                                \n                          \n                          \n                  (0.174) \n                  (0.726) \n                          \n                          \n                  (0.058) \n                  (0.242) \n                \n                \n                  se_social_p3  \n                          \n                          \n                  0.443** \n                  1.771** \n                          \n                          \n                  0.148** \n                  0.590** \n                \n                \n                                \n                          \n                          \n                  (0.161) \n                  (0.642) \n                          \n                          \n                  (0.054) \n                  (0.214) \n                \n                \n                  sup_friends_p3\n                          \n                          \n                  0.165   \n                  0.989   \n                          \n                          \n                  0.055   \n                  0.330   \n                \n                \n                                \n                          \n                          \n                  (0.130) \n                  (0.782) \n                          \n                          \n                  (0.043) \n                  (0.261) \n                \n                \n                  sup_parents_p3\n                          \n                          \n                  0.526***\n                  3.158***\n                          \n                          \n                  0.175***\n                  1.053***\n                \n                \n                                \n                          \n                          \n                  (0.126) \n                  (0.754) \n                          \n                          \n                  (0.042) \n                  (0.251) \n                \n                \n                  Num.Obs.      \n                  283     \n                  283     \n                  283     \n                  283     \n                  283     \n                  283     \n                  283     \n                  283     \n                \n                \n                  R2            \n                  0.399   \n                  0.467   \n                  0.517   \n                  0.517   \n                  0.399   \n                  0.467   \n                  0.517   \n                  0.517   \n                \n                \n                  R2 Adj.       \n                  0.390   \n                  0.451   \n                  0.496   \n                  0.496   \n                  0.390   \n                  0.451   \n                  0.496   \n                  0.496   \n                \n                \n                  AIC           \n                  1090.9  \n                  1064.7  \n                  1044.7  \n                  1044.7  \n                  469.1   \n                  442.9   \n                  422.9   \n                  422.9   \n                \n                \n                  BIC           \n                  1112.8  \n                  1101.2  \n                  1095.7  \n                  1095.7  \n                  491.0   \n                  479.4   \n                  473.9   \n                  473.9   \n                \n                \n                  Log.Lik.      \n                  -539.455\n                  -522.373\n                  -508.341\n                  -508.341\n                  -228.548\n                  -211.466\n                  -197.434\n                  -197.434\n                \n                \n                  RMSE          \n                  1.63    \n                  1.53    \n                  1.46    \n                  1.46    \n                  0.54    \n                  0.51    \n                  0.49    \n                  0.49    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nWe can similarly plot the coefficient values and their uncertainty highlighting how the representation or scaling of the variables impact the scale of the coefficient weights and therefore the surety of any subsequent claims.\n\nmodels = list(\n     \"model_sum_1st_factors\" = mod_sum_1st,\n     \"model_sum_1st_2nd_factors\" = mod_sum_12,\n     \"model_sum_score\" = mod_sum,\n     \"model_sum_score_norm\" = mod_sum_norm,\n     \"model_mean_1st_factors\" = mod_mean_1st,\n     \"model_mean_1st_2nd_factors\" = mod_mean_12,\n     \"model_mean_score\"= mod_mean,\n     \"model_mean_score_norm\" = mod_mean_norm\n    )\n\nmodelplot(models, coef_omit = 'Intercept') + geom_vline(xintercept = 0, linetype=\"dotted\", \n                color = \"black\") + ggtitle(\"Comparing Model Parameter Estimates\", \"Across Covariates\")\n\n\n\n\n\nSignificant Coefficients?\nAn alternative lens on these figures highlights the statistical significance of the coefficients. But again, these criteria are much abused. Significance at what level? Conditional on which representation?\n\ng1 = modelplot(mod_sum, coef_omit = 'Intercept') +  aes(color = ifelse(p.value < 0.001, \"Significant at 0.001\", \"Not significant at 0.001\")) +\n  scale_color_manual(values = c(\"grey\", \"blue\")) + ggtitle(\"Significance of Coefficient Values\", \"At Different Levels\")\n\n\ng2 = modelplot(mod_mean, coef_omit = 'Intercept') +  aes(color = ifelse(p.value < 0.05, \"Significant at 0.05\", \"Not significant at 0.05\")) +\n  scale_color_manual(values = c(\"grey\", \"black\"))\n\nplot <- ggarrange(g1,g2, ncol=2, nrow=1);\n\n\n\n\n\n\nAggregate Driver Scores\nPerhaps we play with the feature representation and increase the proportion of significant indicators. Can we now tell a more definitive story about how parental support and social self-efficact are determinants of self-reported life-satisfaction scores? Let’s focus here on the sum score representation and add interaction effects.\n\ndf$se_acad_mean <- rowMeans(df[c('se_acad_p1', 'se_acad_p2', 'se_acad_p3')])\ndf$se_social_mean <- rowMeans(df[c('se_social_p1', 'se_social_p2', 'se_social_p3')])\ndf$sup_friends_mean <- rowMeans(df[c('sup_friends_p1', 'sup_friends_p2', 'sup_friends_p3')])\ndf$sup_parents_mean <- rowMeans(df[c('sup_parents_p1', 'sup_parents_p2', 'sup_parents_p3')])\n\n\nformula_parcel_sum = \"ls_sum ~ se_acad_mean + se_social_mean +\nsup_friends_mean + sup_parents_mean \" #sup_parents_mean*se_social_mean\"\n\nformula_parcel_sum_inter = \"ls_sum ~ se_acad_mean + se_social_mean + \nsup_friends_mean + sup_parents_mean + sup_parents_mean*se_social_mean\"\n\nmod_sum_parcel = lm(formula_parcel_sum, df)\nmod_sum_inter_parcel = lm(formula_parcel_sum_inter, df)\n\nmodels_parcel = list(\"model_sum_score\" = mod_sum_parcel,\n     \"model_sum_inter_score\"= mod_sum_inter_parcel\n     )\n\nmodelsummary(models_parcel, stars=TRUE)\n\n\n \n\n  \n    \n    \n    tinytable_jxxnaq7a8a1a04s6zo8g\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                model_sum_score\n                model_sum_inter_score\n              \n        \n        + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n        \n                \n                  (Intercept)                      \n                  2.728** \n                  -6.103+ \n                \n                \n                                                   \n                  (0.931) \n                  (3.356) \n                \n                \n                  se_acad_mean                     \n                  0.307+  \n                  0.370*  \n                \n                \n                                                   \n                  (0.158) \n                  (0.158) \n                \n                \n                  se_social_mean                   \n                  1.269***\n                  2.859***\n                \n                \n                                                   \n                  (0.175) \n                  (0.606) \n                \n                \n                  sup_friends_mean                 \n                  0.124   \n                  0.183+  \n                \n                \n                                                   \n                  (0.097) \n                  (0.099) \n                \n                \n                  sup_parents_mean                 \n                  0.726***\n                  2.242***\n                \n                \n                                                   \n                  (0.099) \n                  (0.562) \n                \n                \n                  se_social_mean × sup_parents_mean\n                          \n                  -0.292**\n                \n                \n                                                   \n                          \n                  (0.107) \n                \n                \n                  Num.Obs.                         \n                  283     \n                  283     \n                \n                \n                  R2                               \n                  0.489   \n                  0.503   \n                \n                \n                  R2 Adj.                          \n                  0.482   \n                  0.494   \n                \n                \n                  AIC                              \n                  1044.6  \n                  1039.0  \n                \n                \n                  BIC                              \n                  1066.4  \n                  1064.5  \n                \n                \n                  Log.Lik.                         \n                  -516.288\n                  -512.513\n                \n                \n                  RMSE                             \n                  1.50    \n                  1.48    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nWhat does definitive mean here? Is it so simple as more significant coefficents? Marginally better performance measures?\n\ng1 = modelplot(mod_sum_parcel, coef_omit = 'Intercept') +  aes(color = ifelse(p.value < 0.05, \"Significant at 0.05\", \"Not significant at 0.05\")) +\n  scale_color_manual(values = c(\"grey\", \"blue\")) + ggtitle(\"Significance of Coefficient Values\", \"At Different Levels for Sum and Mean Scores Life Satisfaction \")\n\n\ng2 = modelplot(mod_sum_inter_parcel, coef_omit = 'Intercept') +  aes(color = ifelse(p.value < 0.05, \"Significant at 0.05\", \"Not significant at 0.05\")) +\n  scale_color_manual(values = c(\"grey\", \"blue\"))\n\nplot <- ggarrange(g1,g2, ncol=2, nrow=1);\n\n\n\n\nThis kind of brinkmanship is brittle. Any one of these kinds of choice can be justified but more often than not results from an suspect exploratory process. Steps down a “garden of forking paths” seeking some kind of story to justify an analysis or promote a conclusion. This post-hoc “seeking” is just bad science undermining the significance claims that accrue to reliable procedures. It warps the nature of testing procedure by corrupting the assumed consistency of repeatable trials. The guarantees of statistical significance attach to a conclusion just when the procedure is imagined replicable and repeated under identical conditions. By exploring the different representations and criteria of narrative adequacy we break those guarantees."
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#aggregate-driver-scores",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#aggregate-driver-scores",
    "title": "Measurement, Latent Factors and Theory Construction",
    "section": "Aggregate Driver Scores",
    "text": "Aggregate Driver Scores\n\ndf$se_acad_mean <- rowMeans(df[c('se_acad_p1', 'se_acad_p2', 'se_acad_p3')])\ndf$se_social_mean <- rowMeans(df[c('se_social_p1', 'se_social_p2', 'se_social_p3')])\ndf$sup_friends_mean <- rowMeans(df[c('sup_friends_p1', 'sup_friends_p2', 'sup_friends_p3')])\ndf$sup_parents_mean <- rowMeans(df[c('sup_parents_p1', 'sup_parents_p2', 'sup_parents_p3')])\n\n\nformula_parcel_mean = \"ls_mean ~ se_acad_mean + se_social_mean + sup_friends_mean + sup_parents_mean\"\n\nformula_parcel_sum = \"ls_sum ~ se_acad_mean + se_social_mean + sup_friends_mean + sup_parents_mean\"\n\nmod_sum_parcel = lm(formula_parcel_sum, df)\nmod_mean_parcel = lm(formula_parcel_mean, df)\n\nmodels_parcel = list(\"model_sum_score\" = mod_sum_parcel,\n     \"model_mean_score\"= mod_mean_parcel\n     )\n\nmodelsummary(models_parcel, stars=TRUE)\n\n\n \n\n  \n    \n    \n    tinytable_jxxnaq7a8a1a04s6zo8g\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                model_sum_score\n                model_mean_score\n              \n        \n        + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n        \n                \n                  (Intercept)     \n                  2.728** \n                  0.909** \n                \n                \n                                  \n                  (0.931) \n                  (0.310) \n                \n                \n                  se_acad_mean    \n                  0.307+  \n                  0.102+  \n                \n                \n                                  \n                  (0.158) \n                  (0.053) \n                \n                \n                  se_social_mean  \n                  1.269***\n                  0.423***\n                \n                \n                                  \n                  (0.175) \n                  (0.058) \n                \n                \n                  sup_friends_mean\n                  0.124   \n                  0.041   \n                \n                \n                                  \n                  (0.097) \n                  (0.032) \n                \n                \n                  sup_parents_mean\n                  0.726***\n                  0.242***\n                \n                \n                                  \n                  (0.099) \n                  (0.033) \n                \n                \n                  Num.Obs.        \n                  283     \n                  283     \n                \n                \n                  R2              \n                  0.489   \n                  0.489   \n                \n                \n                  R2 Adj.         \n                  0.482   \n                  0.482   \n                \n                \n                  AIC             \n                  1044.6  \n                  422.8   \n                \n                \n                  BIC             \n                  1066.4  \n                  444.6   \n                \n                \n                  Log.Lik.        \n                  -516.288\n                  -205.381\n                \n                \n                  RMSE            \n                  1.50    \n                  0.50    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\ng1 = modelplot(mod_sum_parcel, coef_omit = 'Intercept') +  aes(color = ifelse(p.value < 0.001, \"Significant at 0.001\", \"Not significant at 0.001\")) +\n  scale_color_manual(values = c(\"grey\", \"blue\")) + ggtitle(\"Significance of Coefficient Values\", \"At Different Levels\")\n\n\ng2 = modelplot(mod_mean_parcel, coef_omit = 'Intercept') +  aes(color = ifelse(p.value < 0.05, \"Significant at 0.05\", \"Not significant at 0.05\")) +\n  scale_color_manual(values = c(\"grey\", \"black\"))\n\nplot <- ggarrange(g1,g2, ncol=2, nrow=1);"
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#hierarchical-models",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#hierarchical-models",
    "title": "Measurement, Latent Factors and Theory Construction",
    "section": "Hierarchical Models",
    "text": "Hierarchical Models\nThe garden of forking paths presents itself within any set of covariates. How do we represent their effects? Which interactions are meaningful? How do we argue for one design over another? The questionable paths are multiplied when we begin to consider additional covariates and group effects.\nLet’s assess the question but this time we allow the model to account for differences in region.\n\nformula_hierarchy_mean = \"ls_mean ~ sup_parents_p1 + sup_parents_p2 + sup_parents_p3 + sup_friends_p1 + sup_friends_p2 + sup_friends_p3 + se_acad_p1 + se_acad_p2 + se_acad_p3 +\nse_social_p1 + se_social_p2 + se_social_p3  + (1 | region)\"\n\nformula_hierarchy_sum = \"ls_sum ~ sup_parents_p1 + sup_parents_p2 + sup_parents_p3 + sup_friends_p1 + sup_friends_p2 + sup_friends_p3 + se_acad_p1 + se_acad_p2 + se_acad_p3 +\nse_social_p1 + se_social_p2 + se_social_p3 + (1 | region)\"\n\nhierarchical_mean_fit <- lmer(formula_hierarchy_mean, data = df, REML = TRUE)\n\nboundary (singular) fit: see help('isSingular')\n\nhierarchical_sum_fit <- lmer(formula_hierarchy_sum, data = df, REML = TRUE)\n\nboundary (singular) fit: see help('isSingular')\n\ng1 = modelplot(hierarchical_mean_fit, re.form=NA) +  aes(color = ifelse(p.value < 0.001, \"Significant at 0.001\", \"Not significant at 0.001\")) +\n  scale_color_manual(values = c(\"grey\", \"blue\")) + ggtitle(\"Significance of Coefficient Values\", \"At Different Levels\")\n\ng2 = modelplot(hierarchical_sum_fit, re.form=NA) +  aes(color = ifelse(p.value < 0.05, \"Significant at 0.05\", \"Not significant 0.05\")) +\n  scale_color_manual(values = c(\"grey\", \"black\"))\n\n\nplot <- ggarrange(g1,g2, ncol=2, nrow=1);\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_segment()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\n\nmodelsummary(list(\"hierarchical_mean_fit\"= hierarchical_mean_fit,\n                  \"hierarchical_sum_fit\"= hierarchical_sum_fit), \n             stars = TRUE) |> \n style_tt(\n   i = 2:25,\n   j = 1:1,\n   background = \"#17C2AD\",\n   color = \"white\",\n   italic = TRUE)\n\n\n \n\n  \n    \n    \n    tinytable_6dqjw6ypw5guaj07q8f6\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                hierarchical_mean_fit\n                hierarchical_sum_fit\n              \n        \n        + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n        \n                \n                  (Intercept)          \n                  0.698*  \n                  2.094*  \n                \n                \n                                       \n                  (0.318) \n                  (0.954) \n                \n                \n                  sup_parents_p1       \n                  0.024   \n                  0.072   \n                \n                \n                                       \n                  (0.048) \n                  (0.143) \n                \n                \n                  sup_parents_p2       \n                  0.039   \n                  0.118   \n                \n                \n                                       \n                  (0.048) \n                  (0.144) \n                \n                \n                  sup_parents_p3       \n                  0.175***\n                  0.526***\n                \n                \n                                       \n                  (0.042) \n                  (0.126) \n                \n                \n                  sup_friends_p1       \n                  -0.091+ \n                  -0.272+ \n                \n                \n                                       \n                  (0.050) \n                  (0.150) \n                \n                \n                  sup_friends_p2       \n                  0.110*  \n                  0.331*  \n                \n                \n                                       \n                  (0.053) \n                  (0.160) \n                \n                \n                  sup_friends_p3       \n                  0.055   \n                  0.165   \n                \n                \n                                       \n                  (0.043) \n                  (0.130) \n                \n                \n                  se_acad_p1           \n                  -0.069  \n                  -0.208  \n                \n                \n                                       \n                  (0.064) \n                  (0.192) \n                \n                \n                  se_acad_p2           \n                  0.109   \n                  0.327   \n                \n                \n                                       \n                  (0.067) \n                  (0.202) \n                \n                \n                  se_acad_p3           \n                  0.051   \n                  0.153   \n                \n                \n                                       \n                  (0.058) \n                  (0.174) \n                \n                \n                  se_social_p1         \n                  0.118+  \n                  0.355+  \n                \n                \n                                       \n                  (0.067) \n                  (0.200) \n                \n                \n                  se_social_p2         \n                  0.170*  \n                  0.509*  \n                \n                \n                                       \n                  (0.073) \n                  (0.219) \n                \n                \n                  se_social_p3         \n                  0.148** \n                  0.443** \n                \n                \n                                       \n                  (0.054) \n                  (0.161) \n                \n                \n                  SD (Intercept region)\n                  0.000   \n                  0.000   \n                \n                \n                  SD (Observations)    \n                  0.498   \n                  1.493   \n                \n                \n                  Num.Obs.             \n                  283     \n                  283     \n                \n                \n                  R2 Marg.             \n                  0.506   \n                  0.506   \n                \n                \n                  AIC                  \n                  482.6   \n                  1075.9  \n                \n                \n                  BIC                  \n                  537.3   \n                  1130.6  \n                \n                \n                  RMSE                 \n                  0.49    \n                  1.46    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\nHierarchical Marginal Effects\n\ng = plot_predictions(hierarchical_mean_fit, condition = c(\"sup_parents_p3\", \"region\"), type = \"response\", re.form=NA) + ggtitle(\"Counterfactual Shift of Outcome: sup_parents_p3\", \"Holding all else Fixed\")\n\ng1 = plot_predictions(hierarchical_mean_fit, condition = c(\"sup_friends_p1\", \"region\"), type = \"response\", re.form=NA) + ggtitle(\"Counterfactual Shift of Outcome: sup_friends_p1\", \"Holding all else Fixed\")\n\ng2 = plot_predictions(hierarchical_mean_fit, condition = c(\"se_acad_p1\", \"region\"), \n                      type = \"response\", re.form=NA) + ggtitle(\"Counterfactual Shift of Outcome: se_acad_p1\", \"Holding all else Fixed\")\n\nplot <- ggarrange(g,g1,g2, ncol=1, nrow=3);"
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#confirmatory-factor-analysis",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#confirmatory-factor-analysis",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Confirmatory Factor Analysis",
    "text": "Confirmatory Factor Analysis\nWe will illustrate the details of confirmatory factor modelling using the lavaan framework. We’ll focus mostly on the mechanics of how these models are estimated using maximum likelihood techniques before illustrating the salient differences of Bayesian estimation.\nFirst recall that the idea of confirmatory factor analysis is that there are some latent constructs which determine our data generating process. In our survey we’ve already clustered our questions by themes so it makes sense to extend this idea to posit latent constructs mapped to each of these themes. In the jargon of structural equation models this is called the measurement model.\n\nmodel_measurement <- \"\n# Measurement model\nSUP_Parents =~ sup_parents_p1 + sup_parents_p2 + sup_parents_p3\nSUP_Friends =~ sup_friends_p1 + sup_friends_p2 + sup_friends_p3\nSE_Academic =~ se_acad_p1 + se_acad_p2 + se_acad_p3\nSE_Social =~ se_social_p1 + se_social_p2 + se_social_p3\nLS  =~ ls_p1 + ls_p2 + ls_p3\n\"\n\nmodel_measurement1 <- \"\n# Measurement model\nSUP_Parents =~ b1*sup_parents_p1 + b2*sup_parents_p2 + b3*sup_parents_p3\nSUP_Friends =~ a1*sup_friends_p1 + a2*sup_friends_p2 + a3*sup_friends_p3\nSE_Academic =~ c1*se_acad_p1 + c2*se_acad_p2 + c3*se_acad_p3\nSE_Social =~ se_social_p1 + se_social_p2 + se_social_p3\nLS  =~ ls_p1 + ls_p2 + ls_p3\n\na1 == a2 \na1 == a3\nb1 == b2\nb1 == b3\nc1 == c2\nc1 == c3\n\n\"\n\nfit_mod <- cfa(model_measurement, data = df)\nfit_mod_1<- cfa(model_measurement1, data = df)\n\nIn the above syntax we have specified two slightly different measurement models. In each case we allow that the questions of our survey are mapped to an appropriate latent factor e.g LS =~ ls_p1 + ls_p2 + ls_p3. The “=~” syntax denotes a “measured by” relationship in which the goal is to estimate how each of observed measurements load on the latent factor. In the first model we have allowed each of the factor loadings to be estimated freely, but in the second we have forced equal weights on the SUP_Friends construct. A benefit of this framework is that we do not have to resort to crude aggregations like sum-scores or mean-scores over the outcome variables we can allow that they vary freely and let the model estimate the multivariate relationships between the observed variables and these latent constructs.\nIf we plot the estimated parameters as before we’ll see some additional parameters reported.\n\ncfa_models = list(\"full_measurement_model\" = fit_mod, \n     \"measurement_model_reduced\" = fit_mod_1)\nmodelplot(cfa_models)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\nHere there are two distinct types of parameters: (i) the factor loadings accorded (=~) to the individual observed metrics and (ii) the covariances (~~) between the latent constructs. We can further report the extent of the model fit summaries.\n\nsummary(fit_mod, fit.measures = TRUE, standardized = TRUE) \n\nlavaan 0.6-18 ended normally after 49 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        40\n\n  Number of observations                           283\n\nModel Test User Model:\n                                                      \n  Test statistic                               223.992\n  Degrees of freedom                                80\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              2696.489\n  Degrees of freedom                               105\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.944\n  Tucker-Lewis Index (TLI)                       0.927\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4285.972\n  Loglikelihood unrestricted model (H1)      -4173.976\n                                                      \n  Akaike (AIC)                                8651.944\n  Bayesian (BIC)                              8797.761\n  Sample-size adjusted Bayesian (SABIC)       8670.921\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.080\n  90 Percent confidence interval - lower         0.067\n  90 Percent confidence interval - upper         0.092\n  P-value H_0: RMSEA <= 0.050                    0.000\n  P-value H_0: RMSEA >= 0.080                    0.500\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.056\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  SUP_Parents =~                                                        \n    sup_parents_p1    1.000                               0.935    0.873\n    sup_parents_p2    1.036    0.056   18.613    0.000    0.969    0.887\n    sup_parents_p3    0.996    0.059   16.754    0.000    0.932    0.816\n  SUP_Friends =~                                                        \n    sup_friends_p1    1.000                               1.021    0.906\n    sup_friends_p2    0.792    0.043   18.463    0.000    0.809    0.857\n    sup_friends_p3    0.891    0.050   17.741    0.000    0.910    0.831\n  SE_Academic =~                                                        \n    se_acad_p1        1.000                               0.695    0.878\n    se_acad_p2        0.809    0.050   16.290    0.000    0.562    0.820\n    se_acad_p3        0.955    0.058   16.500    0.000    0.664    0.829\n  SE_Social =~                                                          \n    se_social_p1      1.000                               0.638    0.843\n    se_social_p2      0.967    0.056   17.248    0.000    0.617    0.885\n    se_social_p3      0.928    0.067   13.880    0.000    0.592    0.741\n  LS =~                                                                 \n    ls_p1             1.000                               0.667    0.718\n    ls_p2             0.778    0.074   10.498    0.000    0.519    0.712\n    ls_p3             0.968    0.090   10.730    0.000    0.645    0.732\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  SUP_Parents ~~                                                        \n    SUP_Friends       0.132    0.064    2.073    0.038    0.138    0.138\n    SE_Academic       0.218    0.046    4.727    0.000    0.336    0.336\n    SE_Social         0.282    0.045    6.224    0.000    0.472    0.472\n    LS                0.405    0.057    7.132    0.000    0.650    0.650\n  SUP_Friends ~~                                                        \n    SE_Academic       0.071    0.047    1.493    0.136    0.100    0.100\n    SE_Social         0.196    0.046    4.281    0.000    0.301    0.301\n    LS                0.175    0.051    3.445    0.001    0.257    0.257\n  SE_Academic ~~                                                        \n    SE_Social         0.271    0.036    7.493    0.000    0.611    0.611\n    LS                0.238    0.039    6.065    0.000    0.514    0.514\n  SE_Social ~~                                                          \n    LS                0.321    0.042    7.659    0.000    0.755    0.755\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .sup_parents_p1    0.273    0.037    7.358    0.000    0.273    0.238\n   .sup_parents_p2    0.255    0.038    6.738    0.000    0.255    0.213\n   .sup_parents_p3    0.437    0.048    9.201    0.000    0.437    0.335\n   .sup_friends_p1    0.227    0.040    5.656    0.000    0.227    0.179\n   .sup_friends_p2    0.238    0.030    7.936    0.000    0.238    0.266\n   .sup_friends_p3    0.371    0.042    8.809    0.000    0.371    0.310\n   .se_acad_p1        0.144    0.022    6.593    0.000    0.144    0.229\n   .se_acad_p2        0.153    0.018    8.621    0.000    0.153    0.327\n   .se_acad_p3        0.200    0.024    8.372    0.000    0.200    0.313\n   .se_social_p1      0.166    0.020    8.134    0.000    0.166    0.290\n   .se_social_p2      0.106    0.016    6.542    0.000    0.106    0.217\n   .se_social_p3      0.288    0.028   10.132    0.000    0.288    0.451\n   .ls_p1             0.417    0.045    9.233    0.000    0.417    0.484\n   .ls_p2             0.261    0.028    9.321    0.000    0.261    0.492\n   .ls_p3             0.362    0.040    9.005    0.000    0.362    0.465\n    SUP_Parents       0.875    0.098    8.910    0.000    1.000    1.000\n    SUP_Friends       1.042    0.111    9.407    0.000    1.000    1.000\n    SE_Academic       0.483    0.054    8.880    0.000    1.000    1.000\n    SE_Social         0.407    0.048    8.403    0.000    1.000    1.000\n    LS                0.444    0.069    6.394    0.000    1.000    1.000\n\n\nNote how in addition to the individual parameter estimates the summaries highlight various measures of global model fit. These model fit statistics are important for evaluating alternative ways of parameterising our models. The number of parameters is a real concern in the maximum likelihood approaches to estimating these models. Too many parameters and we can easily over fit to the particular sample data. This stems in part from the limitations of the optimisation goal in the traditional CFA framework - we are intent to optimise model parameters to recover a compelling estimate based on the observed covariance matrix. Once we have more parameters than there are points in the covariance matrix the model is free to overfit considerably. This can then be checked as measure of local model fit and may highlight infelicities or suspicious convergence between the true data generating process and the learned model.\n\ng1 = plot_heatmap(cov(df[,  drivers]))\n\ng2 = plot_heatmap(data.frame(fitted(fit_mod)$cov)[drivers, drivers], title=\"Model Implied Covariances\", \"Fitted Values\")\n\nresids = cov(df[,  drivers]) - data.frame(fitted(fit_mod)$cov)[drivers, drivers]\n\ng3 = plot_heatmap(resids, title=\"Residuals\", \"Fitted Values versus Observe Sample Covariance\")\n\n\nplot <- ggarrange(g1,g2,g3, ncol=1, nrow=3);\n\n\n\n\n\nSummary Global Fit Measures\nWe can also compare models based on their global measures of model fit giving some indication of whether parameter specifications improve or reduce fidelity with the true data generating process.\n\nsummary_df = cbind(fitMeasures(fit_mod, c(\"chisq\", \"baseline.chisq\", \"cfi\", \"aic\", \"bic\", \"rmsea\",\"srmr\")),\n      fitMeasures(fit_mod_1, c(\"chisq\", \"baseline.chisq\", \"cfi\", \"aic\", \"bic\", \"rmsea\",\"srmr\")))\ncolnames(summary_df) = c('Full Model', 'Reduced Model')\n\nsummary_df |> kable() |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n \n  \n     \n    Full Model \n    Reduced Model \n  \n \n\n  \n    chisq \n    223.9922306 \n    256.0287010 \n  \n  \n    baseline.chisq \n    2696.4887420 \n    2696.4887420 \n  \n  \n    cfi \n    0.9444365 \n    0.9343896 \n  \n  \n    aic \n    8651.9435210 \n    8671.9799914 \n  \n  \n    bic \n    8797.7613969 \n    8795.9251859 \n  \n  \n    rmsea \n    0.0797501 \n    0.0835831 \n  \n  \n    srmr \n    0.0558656 \n    0.0625710 \n  \n\n\n\n\n\nThere a wealth of metrics associated with CFA model fit and it can be hard to see the forest for the trees.\n\n\nVisualising the Relationships\nOne of the better ways to visualise these models is to use the semPlot package. Here we can plot all the parameter estimates in one graph. Following convention the rectangular boxes represent observed measures. Oval or circular objects represent the latent constructs. The self-directed arrows on each node is the variance of that measure. The two-way arrows between nodes represents the covariance between those two nodes. The single headed arrows from the latent construct to the indicator variables denotes the factor loading of the variable on the construct.\n\nsemPlot::semPaths(fit_mod, whatLabels = 'est', intercepts = FALSE, layout = \"spring\",)\n\n\n\n\nFor instance, in this plot you can see that for each latent construct one of the indicator variables has their factor loading set to 1. This is a mathematical requirement we’ll see below that is used to ensure identifiability of the parameters akin to setting a reference category in categorical regression. Additionally you can “read-off” the covariances between our constructs e.g. the covariance between LS and SUP_P is 0.36 the largest value amongst the set of covariances.\n\n\nComparing Models\nWe can use a variety of chi-squared tests to evaluate the goodness of fit for our models. If we pass in each model individually we perform a test comparing our model to the saturated model. The Chi-Squared test compares the model-implied variance-covariance matrix (expected) to the variance-covariance matrix computed from the actual data (observed). The null hypothesis for the Chi-Squared Goodness-of-Fit test is that the model fits the data perfectly, meaning that there is no significant difference between the observed and model-implied variance-covariance matrices. The goal is to see if the differences between these matrices are large enough that we can reject the null.\n\nlavTestLRT(fit_mod)\n\nChi-Squared Test Statistic (unscaled)\n\n          Df    AIC    BIC  Chisq Chisq diff Df diff           Pr(>Chisq)    \nSaturated  0                 0.00                                            \nModel     80 8651.9 8797.8 223.99     223.99      80 0.000000000000001443 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPassing in the one model we can reject the null hypothesis that the saturated model’s (perfect fit) and the candidate variance-covariance matrix are drawn from the same distribution. Comparing between our two model fits we also reject that these two models are drawn from the same distribution.\n\nlavTestLRT(fit_mod, fit_mod_1)\n\n\nChi-Squared Difference Test\n\n          Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff Pr(>Chisq)    \nfit_mod   80 8651.9 8797.8 223.99                                          \nfit_mod_1 86 8672.0 8795.9 256.03     32.036 0.12383       6 0.00001606 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe test also reports a number of other fit indices and the degrees of freedom available to each model. These are important because the Chi-Squared test is overly sensetive in large-sample data and the model adequacy is a multi-dimensional question."
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#structural-equation-models",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#structural-equation-models",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Structural Equation Models",
    "text": "Structural Equation Models\nSo far so good. We have a confirmatory factor measurement model. We’ve structured it so that we can make inferences about the correlations and covariances between 5 latent constructs of independent interest. We’ve calibrated the model fit statistics by ensuring the model can reasonably recover the observed variance-covariance structure. But what about our dependency relations between constructs? We can evaluate these too! Adding regressions to our model allows to express these relationships and then recover summary statistics of the same.\n\nmodel <- \"\n# Measurement model\nSUP_Parents =~ sup_parents_p1 + sup_parents_p2 + sup_parents_p3\nSUP_Friends =~ sup_friends_p1 + sup_friends_p2 + sup_friends_p3\nSE_Academic =~ se_acad_p1 + se_acad_p2 + se_acad_p3\nSE_Social =~ se_social_p1 + se_social_p2 + se_social_p3\nLS  =~ ls_p1 + ls_p2 + ls_p3\n\n# Structural model \n# Regressions\nSE_Academic ~ SUP_Parents + SUP_Friends\nSE_Social ~ SUP_Parents + SUP_Friends\nLS ~ SE_Academic + SE_Social + SUP_Parents + SUP_Friends\n\n# Residual covariances\nSE_Academic ~~ SE_Social\n\"\n\nfit_mod_sem <- sem(model, data = df)\n\n\nmodelplot(fit_mod_sem)\n\n\n\n\n\nsemPlot::semPaths(fit_mod_sem, whatLabels = 'est', intercepts = FALSE)\n\n\n\n\nCompare this structure against the previously simpler measurement model and we observe a puzzling phenomena. The models report identical measures of fit.\n\nsummary_df = cbind(fitMeasures(fit_mod, c(\"chisq\", \"baseline.chisq\", \"cfi\", \"aic\", \"bic\", \"rmsea\",\"srmr\")),\n      fitMeasures(fit_mod_sem, c(\"chisq\", \"baseline.chisq\", \"cfi\", \"aic\", \"bic\", \"rmsea\",\"srmr\")),\n      fitMeasures(fit_mod_1, c(\"chisq\", \"baseline.chisq\", \"cfi\", \"aic\", \"bic\", \"rmsea\",\"srmr\")))\ncolnames(summary_df) = c('Full Model', 'SEM Model', 'Reduced Model')\n\nsummary_df |> kable() |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n \n  \n     \n    Full Model \n    SEM Model \n    Reduced Model \n  \n \n\n  \n    chisq \n    223.9922306 \n    223.9922306 \n    256.0287010 \n  \n  \n    baseline.chisq \n    2696.4887420 \n    2696.4887420 \n    2696.4887420 \n  \n  \n    cfi \n    0.9444365 \n    0.9444365 \n    0.9343896 \n  \n  \n    aic \n    8651.9435210 \n    8651.9435210 \n    8671.9799914 \n  \n  \n    bic \n    8797.7613969 \n    8797.7613969 \n    8795.9251859 \n  \n  \n    rmsea \n    0.0797501 \n    0.0797501 \n    0.0835831 \n  \n  \n    srmr \n    0.0558656 \n    0.0558655 \n    0.0625710 \n  \n\n\n\n\n\nThe models have the same degrees of freedom which suggests in some sense we have already saturated our model fit and are unable to evaluate further parameter estimates.\n\nlavTestLRT(fit_mod_sem, fit_mod)\n\nWarning: lavaan->lavTestLRT():  \n   some models have the same degrees of freedom\n\n\n\nChi-Squared Difference Test\n\n            Df    AIC    BIC  Chisq      Chisq diff RMSEA Df diff Pr(>Chisq)\nfit_mod_sem 80 8651.9 8797.8 223.99                                         \nfit_mod     80 8651.9 8797.8 223.99 0.0000000043218     0       0           \n\nlavTestLRT(fit_mod_sem, fit_mod_1)\n\n\nChi-Squared Difference Test\n\n            Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff Pr(>Chisq)    \nfit_mod_sem 80 8651.9 8797.8 223.99                                          \nfit_mod_1   86 8672.0 8795.9 256.03     32.036 0.12383       6 0.00001606 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can see this similarly in the plotted residuals which are identical across the models despite meaningful structural differences.\n\nheat_df = data.frame(resid(fit_mod, type = \"standardized\")$cov) \nheat_df = heat_df |> as.matrix() |> melt()\ncolnames(heat_df) <- c(\"x\", \"y\", \"value\")\n\ng1 = heat_df  |> mutate(value = round(value, 2)) |>  ggplot(aes(x = x, y = y, fill = value)) +\n  geom_tile() + geom_text(aes(label = value), color = \"black\", size = 4) +\n scale_fill_gradient2(\n    high = 'dodgerblue4',\n    mid = 'white',\n    low = 'firebrick2'\n  ) + theme(axis.text.x = element_text(angle=45)) + ggtitle(\"Standardised Residuals of the Sample Covariances and Model Implied Covariances\", \"A Visual Check of SEM Model fit\")\n\n\nheat_df = data.frame(resid(fit_mod_sem, type = \"standardized\")$cov) \nheat_df = heat_df |> as.matrix() |> melt()\ncolnames(heat_df) <- c(\"x\", \"y\", \"value\")\n\ng2 = heat_df  |> mutate(value = round(value, 2)) |>  ggplot(aes(x = x, y = y, fill = value)) +\n  geom_tile() + geom_text(aes(label = value), color = \"black\", size = 4) +\n scale_fill_gradient2(\n    high = 'dodgerblue4',\n    mid = 'white',\n    low = 'firebrick2'\n  ) + theme(axis.text.x = element_text(angle=45)) + ggtitle(\"Standardised Residuals of the Sample Covariances and Model Implied Covariances\", \"A Visual Check of Measurement Model fit\")\n\n\nplot <- ggarrange(g1,g2, ncol=1, nrow=2);\n\n\n\n\nThis is a genuine limitation in the expressive power of SEM models when they are fit using maximum likelihood with finite degrees of freedom optimising for fidelity to the sample varaince-covariance matrix."
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#now-soem-python",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#now-soem-python",
    "title": "Confirmatory Factor Analysis and Structural Equation Models",
    "section": "Now soem Python",
    "text": "Now soem Python\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pytensor import tensor as pt\nimport arviz as az\nimport networkx as nx\n\n\n\ndf_p = pd.read_csv('IIS.dat', sep='\\s+')\ndf_p.head()\n\n     PI    AD   IGC   FI   FC\n0  4.00  3.38  4.67  2.6  3.2\n1  2.57  3.00  3.50  2.4  2.8\n2  2.29  3.29  4.83  2.0  3.4\n3  2.43  3.63  4.33  3.6  3.8\n4  3.00  4.00  4.83  3.4  3.8\n\n\n\ncoords = {'obs': list(range(len(df_p))), \n          'indicators': ['PI', 'AD',    'IGC', 'FI', 'FC'],\n          'indicators_1': ['PI', 'AD',  'IGC'],\n          'indicators_2': ['FI', 'FC'],\n          'latent': ['Student', 'Faculty']\n          }\n\n\nobs_idx = list(range(len(df_p)))\nwith pm.Model(coords=coords) as model:\n  \n  Psi = pm.InverseGamma('Psi', 5, 10, dims='indicators')\n  lambdas_ = pm.Normal('lambdas_1', 1, 10, dims=('indicators_1'))\n  lambdas_1 = pm.Deterministic('lambdas1', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_1'))\n  lambdas_ = pm.Normal('lambdas_2', 1, 10, dims=('indicators_2'))\n  lambdas_2 = pm.Deterministic('lambdas2', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_2'))\n  tau = pm.Normal('tau', 3, 10, dims='indicators')\n  kappa = 0\n  sd_dist = pm.Exponential.dist(1.0, shape=2)\n  chol, _, _ = pm.LKJCholeskyCov('chol_cov', n=2, eta=2,\n    sd_dist=sd_dist, compute_corr=True)\n  ksi = pm.MvNormal('ksi', kappa, chol=chol, dims=('obs', 'latent'))\n\n  m1 = tau[0] + ksi[obs_idx, 0]*lambdas_1[0]\n  m2 = tau[1] + ksi[obs_idx, 0]*lambdas_1[1]\n  m3 = tau[2] + ksi[obs_idx, 0]*lambdas_1[2]\n  m4 = tau[3] + ksi[obs_idx, 1]*lambdas_2[0]\n  m5 = tau[4] + ksi[obs_idx, 1]*lambdas_2[1]\n  \n  mu = pm.Deterministic('mu', pm.math.stack([m1, m2, m3, m4, m5]).T)\n  _  = pm.Normal('likelihood', mu, Psi, observed=df_p.values)\n\n  idata = pm.sample(nuts_sampler='numpyro', target_accept=.95)\n\n\n\n\nPyMC Confirmatory Factor Model"
  },
  {
    "objectID": "scratch_work/testing_pymc_sem.html",
    "href": "scratch_work/testing_pymc_sem.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "import pymc as pm\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pytensor import tensor as pt\nimport arviz as az\n\n\n\ndf_p = pd.read_csv('IIS.dat', sep='\\s+')\n\ncoords = {'obs': list(range(len(df_p))), 'indicators': ['PI', 'AD', 'IGC', 'FI', 'FC']}\n\nN = len(df_p)\nM = 5\nwith pm.Model(coords=coords) as model:\n  \n  Psi = pm.InverseGamma('Psi', 5, 10, dims='indicators')\n  lambdas_ = pm.Normal('lambdas_', 1, 10, dims=('indicators'))\n  lambdas_ = pm.Deterministic('lambdas', pt.set_subtensor(lambdas_[0], 1))\n  tau = pm.Normal('tau', 3, 10, dims='indicators')\n  psi = pm.InverseGamma('psi_', 5, 10)\n  kappa = 0\n  ksi = pm.Normal('ksi', kappa, psi, dims='obs')\n  mus = []\n  for j in range(M):\n    m = tau[j] + ksi*lambdas_[j]\n    mus.append(m)\n\n  mu = pm.Deterministic('mu', pm.math.stack(mus).T)\n  _  = pm.Normal('likelihood', mu, Psi, observed=df_p.values)\n\n  idata = pm.sample()\n\n\n\npm.model_to_graphviz(model)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [Psi, lambdas_, tau, psi_, ksi]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:05<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 5 seconds.\n\n\n\n\n\n\naz.summary(idata, var_names=['lambdas', 'tau', 'Psi', 'psi_', 'ksi'], coords= {'obs': [0, 7]})\n\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/diagnostics.py:592: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      lambdas[0]\n      1.000\n      0.000\n      1.000\n      1.000\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      lambdas[1]\n      0.815\n      0.052\n      0.719\n      0.910\n      0.002\n      0.002\n      515.0\n      1093.0\n      1.01\n    \n    \n      lambdas[2]\n      0.474\n      0.039\n      0.399\n      0.548\n      0.001\n      0.001\n      921.0\n      1357.0\n      1.00\n    \n    \n      lambdas[3]\n      1.105\n      0.075\n      0.963\n      1.245\n      0.003\n      0.002\n      693.0\n      1326.0\n      1.01\n    \n    \n      lambdas[4]\n      1.060\n      0.068\n      0.935\n      1.189\n      0.003\n      0.002\n      561.0\n      1206.0\n      1.01\n    \n    \n      tau[PI]\n      3.333\n      0.037\n      3.265\n      3.401\n      0.001\n      0.001\n      1380.0\n      2153.0\n      1.00\n    \n    \n      tau[AD]\n      3.898\n      0.026\n      3.852\n      3.950\n      0.001\n      0.001\n      1204.0\n      1869.0\n      1.01\n    \n    \n      tau[IGC]\n      4.596\n      0.020\n      4.560\n      4.635\n      0.000\n      0.000\n      1867.0\n      2455.0\n      1.00\n    \n    \n      tau[FI]\n      3.033\n      0.039\n      2.961\n      3.110\n      0.001\n      0.001\n      1331.0\n      2103.0\n      1.00\n    \n    \n      tau[FC]\n      3.712\n      0.035\n      3.647\n      3.780\n      0.001\n      0.001\n      1136.0\n      1954.0\n      1.00\n    \n    \n      Psi[PI]\n      0.598\n      0.023\n      0.556\n      0.641\n      0.000\n      0.000\n      4434.0\n      3298.0\n      1.00\n    \n    \n      Psi[AD]\n      0.362\n      0.017\n      0.331\n      0.396\n      0.000\n      0.000\n      2419.0\n      2866.0\n      1.00\n    \n    \n      Psi[IGC]\n      0.375\n      0.014\n      0.349\n      0.401\n      0.000\n      0.000\n      5071.0\n      2872.0\n      1.00\n    \n    \n      Psi[FI]\n      0.605\n      0.025\n      0.561\n      0.654\n      0.000\n      0.000\n      3199.0\n      3160.0\n      1.00\n    \n    \n      Psi[FC]\n      0.481\n      0.022\n      0.440\n      0.522\n      0.000\n      0.000\n      2389.0\n      2385.0\n      1.00\n    \n    \n      psi_\n      0.601\n      0.035\n      0.537\n      0.667\n      0.001\n      0.001\n      597.0\n      1241.0\n      1.00\n    \n    \n      ksi[0]\n      -0.236\n      0.225\n      -0.641\n      0.189\n      0.002\n      0.003\n      8287.0\n      3069.0\n      1.00\n    \n    \n      ksi[7]\n      0.848\n      0.225\n      0.423\n      1.277\n      0.003\n      0.002\n      5136.0\n      3133.0\n      1.00\n    \n  \n\n\n\n\n\ncoords = {'obs': list(range(len(df_p))), \n          'indicators': ['PI', 'AD',    'IGC', 'FI', 'FC'],\n          'indicators_1': ['PI', 'AD',  'IGC'],\n          'indicators_2': ['FI', 'FC'],\n          'latent': ['Student', 'Faculty']\n          }\n\nN = len(df_p)\nM = 5\nF = 2\nindicator_idx = [0, 1, 2, 3, 4]\nlatent_idx = [0, 0, 0, 1, 1]\nobs_idx = list(range(len(df_p)))\nwith pm.Model(coords=coords) as model:\n  \n  Psi = pm.InverseGamma('Psi', 5, 10, dims='indicators')\n  lambdas_ = pm.Normal('lambdas_1', 1, 10, dims=('indicators_1'))\n  lambdas_1 = pm.Deterministic('lambdas1', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_1'))\n  lambdas_ = pm.Normal('lambdas_2', 1, 10, dims=('indicators_2'))\n  lambdas_2 = pm.Deterministic('lambdas2', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_2'))\n  tau = pm.Normal('tau', 3, 10, dims='indicators')\n  kappa = 0\n  sd_dist = pm.Exponential.dist(1.0, shape=2)\n  chol, _, _ = pm.LKJCholeskyCov('chol_cov', n=2, eta=2,\n    sd_dist=sd_dist, compute_corr=True)\n  ksi = pm.MvNormal('ksi', kappa, chol=chol, dims=('obs', 'latent'))\n\n  m1 = tau[0] + ksi[obs_idx, 0]*lambdas_1[0]\n  m2 = tau[1] + ksi[obs_idx, 0]*lambdas_1[1]\n  m3 = tau[2] + ksi[obs_idx, 0]*lambdas_1[2]\n  m4 = tau[3] + ksi[obs_idx, 1]*lambdas_2[0]\n  m5 = tau[4] + ksi[obs_idx, 1]*lambdas_2[1]\n  \n  mu = pm.Deterministic('mu', pm.math.stack([m1, m2, m3, m4, m5]).T)\n  _  = pm.Normal('likelihood', mu, Psi, observed=df_p.values)\n\n  idata = pm.sample(nuts_sampler='numpyro', target_accept=.95)\n\n\n\npm.model_to_graphviz(model)\n\nCompiling...\nCompilation time = 0:00:01.713705\nSampling...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling time = 0:00:06.464597\nTransforming variables...\nTransformation time = 0:00:00.285241\n\n\n\n\n\n\naz.summary(idata, var_names=['lambdas1', 'lambdas2', 'tau', 'Psi', 'ksi', 'chol_cov_corr'], \n           coords= {'obs': [0, 7]})\n\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/diagnostics.py:592: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/diagnostics.py:592: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/diagnostics.py:592: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      lambdas1[PI]\n      1.000\n      0.000\n      1.000\n      1.000\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      lambdas1[AD]\n      0.903\n      0.062\n      0.790\n      1.022\n      0.003\n      0.002\n      364.0\n      746.0\n      1.00\n    \n    \n      lambdas1[IGC]\n      0.537\n      0.046\n      0.450\n      0.620\n      0.002\n      0.001\n      485.0\n      930.0\n      1.00\n    \n    \n      lambdas2[FI]\n      1.000\n      0.000\n      1.000\n      1.000\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      lambdas2[FC]\n      0.978\n      0.055\n      0.879\n      1.087\n      0.003\n      0.002\n      475.0\n      831.0\n      1.00\n    \n    \n      tau[PI]\n      3.332\n      0.037\n      3.259\n      3.397\n      0.002\n      0.001\n      556.0\n      1306.0\n      1.01\n    \n    \n      tau[AD]\n      3.896\n      0.027\n      3.846\n      3.947\n      0.001\n      0.001\n      395.0\n      824.0\n      1.01\n    \n    \n      tau[IGC]\n      4.595\n      0.020\n      4.557\n      4.633\n      0.001\n      0.001\n      610.0\n      1577.0\n      1.01\n    \n    \n      tau[FI]\n      3.032\n      0.040\n      2.955\n      3.107\n      0.002\n      0.001\n      469.0\n      1198.0\n      1.00\n    \n    \n      tau[FC]\n      3.711\n      0.035\n      3.643\n      3.774\n      0.002\n      0.001\n      392.0\n      983.0\n      1.00\n    \n    \n      Psi[PI]\n      0.610\n      0.025\n      0.565\n      0.656\n      0.001\n      0.000\n      1440.0\n      2685.0\n      1.00\n    \n    \n      Psi[AD]\n      0.317\n      0.020\n      0.282\n      0.354\n      0.001\n      0.001\n      718.0\n      1290.0\n      1.00\n    \n    \n      Psi[IGC]\n      0.355\n      0.013\n      0.332\n      0.382\n      0.000\n      0.000\n      2489.0\n      2569.0\n      1.00\n    \n    \n      Psi[FI]\n      0.568\n      0.026\n      0.523\n      0.620\n      0.001\n      0.001\n      1125.0\n      2174.0\n      1.00\n    \n    \n      Psi[FC]\n      0.422\n      0.026\n      0.371\n      0.467\n      0.001\n      0.001\n      670.0\n      1590.0\n      1.01\n    \n    \n      ksi[0, Student]\n      -0.222\n      0.221\n      -0.638\n      0.201\n      0.004\n      0.003\n      3391.0\n      2961.0\n      1.00\n    \n    \n      ksi[0, Faculty]\n      -0.361\n      0.274\n      -0.863\n      0.163\n      0.005\n      0.004\n      3708.0\n      2762.0\n      1.00\n    \n    \n      ksi[7, Student]\n      0.891\n      0.230\n      0.465\n      1.324\n      0.004\n      0.003\n      3365.0\n      2913.0\n      1.00\n    \n    \n      ksi[7, Faculty]\n      0.881\n      0.281\n      0.394\n      1.447\n      0.004\n      0.003\n      3960.0\n      2808.0\n      1.00\n    \n    \n      chol_cov_corr[0, 0]\n      1.000\n      0.000\n      1.000\n      1.000\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      chol_cov_corr[0, 1]\n      0.851\n      0.029\n      0.797\n      0.905\n      0.002\n      0.001\n      356.0\n      674.0\n      1.01\n    \n    \n      chol_cov_corr[1, 0]\n      0.851\n      0.029\n      0.797\n      0.905\n      0.002\n      0.001\n      356.0\n      674.0\n      1.01\n    \n    \n      chol_cov_corr[1, 1]\n      1.000\n      0.000\n      1.000\n      1.000\n      0.000\n      0.000\n      3955.0\n      3960.0\n      1.00\n    \n  \n\n\n\n\n\naz.plot_trace(idata, var_names=['lambdas1', 'lambdas2', 'tau', 'Psi', 'ksi', 'chol_cov_corr'])\n\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:487: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:487: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:487: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:699: RuntimeWarning: divide by zero encountered in divide\n  f = grid_counts / bin_width / len(x)\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:699: RuntimeWarning: invalid value encountered in divide\n  f = grid_counts / bin_width / len(x)\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:702: RuntimeWarning: divide by zero encountered in scalar divide\n  bw /= bin_width\n\n\nOverflowError: cannot convert float infinity to integer"
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#confirmatory-factor-models-with-pymc",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#confirmatory-factor-models-with-pymc",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Confirmatory Factor Models with PyMC",
    "text": "Confirmatory Factor Models with PyMC\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pytensor import tensor as pt\nimport arviz as az\nimport networkx as nx\nnp.random.seed(150)\n\n\n\ndf_p = pd.read_csv('IIS.dat', sep='\\s+')\ndf_p.head() \n\n     PI    AD   IGC   FI   FC\n0  4.00  3.38  4.67  2.6  3.2\n1  2.57  3.00  3.50  2.4  2.8\n2  2.29  3.29  4.83  2.0  3.4\n3  2.43  3.63  4.33  3.6  3.8\n4  3.00  4.00  4.83  3.4  3.8\n\n\n\ncoords = {'obs': list(range(len(df_p))), \n          'indicators': ['PI', 'AD',    'IGC', 'FI', 'FC'],\n          'indicators_1': ['PI', 'AD',  'IGC'],\n          'indicators_2': ['FI', 'FC'],\n          'latent': ['Student', 'Faculty']\n          }\n\n\nobs_idx = list(range(len(df_p)))\nwith pm.Model(coords=coords) as model:\n  \n  Psi = pm.InverseGamma('Psi', 5, 10, dims='indicators')\n  lambdas_ = pm.Normal('lambdas_1', 1, 10, dims=('indicators_1'))\n  lambdas_1 = pm.Deterministic('lambdas1', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_1'))\n  lambdas_ = pm.Normal('lambdas_2', 1, 10, dims=('indicators_2'))\n  lambdas_2 = pm.Deterministic('lambdas2', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_2'))\n  tau = pm.Normal('tau', 3, 10, dims='indicators')\n  kappa = 0\n  sd_dist = pm.Exponential.dist(1.0, shape=2)\n  chol, _, _ = pm.LKJCholeskyCov('chol_cov', n=2, eta=2,\n    sd_dist=sd_dist, compute_corr=True)\n  ksi = pm.MvNormal('ksi', kappa, chol=chol, dims=('obs', 'latent'))\n\n  m1 = tau[0] + ksi[obs_idx, 0]*lambdas_1[0]\n  m2 = tau[1] + ksi[obs_idx, 0]*lambdas_1[1]\n  m3 = tau[2] + ksi[obs_idx, 0]*lambdas_1[2]\n  m4 = tau[3] + ksi[obs_idx, 1]*lambdas_2[0]\n  m5 = tau[4] + ksi[obs_idx, 1]*lambdas_2[1]\n  \n  mu = pm.Deterministic('mu', pm.math.stack([m1, m2, m3, m4, m5]).T)\n  _  = pm.Normal('likelihood', mu, Psi, observed=df_p.values)\n\n  idata = pm.sample(nuts_sampler='numpyro', target_accept=.95, \n                    idata_kwargs={\"log_likelihood\": True})\n  idata.extend(pm.sample_posterior_predictive(idata))\n  \nsummary_df = az.summary(idata, var_names=['lambdas1', 'lambdas2', 'tau', 'Psi', 'ksi', 'chol_cov_corr'], coords= {'obs': [0, 7]})\n\n\n\n\nPyMC Confirmatory Factor Model\n\n\n\npy$summary_df |> kable() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n \n  \n     \n    mean \n    sd \n    hdi_3% \n    hdi_97% \n    mcse_mean \n    mcse_sd \n    ess_bulk \n    ess_tail \n    r_hat \n  \n \n\n  \n    lambdas1[PI] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    lambdas1[AD] \n    0.907 \n    0.066 \n    0.786 \n    1.032 \n    0.004 \n    0.003 \n    277 \n    463 \n    1.01 \n  \n  \n    lambdas1[IGC] \n    0.540 \n    0.048 \n    0.444 \n    0.625 \n    0.003 \n    0.002 \n    354 \n    584 \n    1.01 \n  \n  \n    lambdas2[FI] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    lambdas2[FC] \n    0.979 \n    0.058 \n    0.875 \n    1.091 \n    0.003 \n    0.002 \n    454 \n    1045 \n    1.00 \n  \n  \n    tau[PI] \n    3.330 \n    0.037 \n    3.256 \n    3.396 \n    0.002 \n    0.001 \n    595 \n    1123 \n    1.00 \n  \n  \n    tau[AD] \n    3.895 \n    0.027 \n    3.846 \n    3.947 \n    0.001 \n    0.001 \n    346 \n    746 \n    1.01 \n  \n  \n    tau[IGC] \n    4.595 \n    0.021 \n    4.554 \n    4.633 \n    0.001 \n    0.001 \n    639 \n    1453 \n    1.00 \n  \n  \n    tau[FI] \n    3.031 \n    0.039 \n    2.963 \n    3.110 \n    0.002 \n    0.001 \n    384 \n    976 \n    1.01 \n  \n  \n    tau[FC] \n    3.710 \n    0.035 \n    3.644 \n    3.776 \n    0.002 \n    0.001 \n    351 \n    847 \n    1.01 \n  \n  \n    Psi[PI] \n    0.610 \n    0.024 \n    0.567 \n    0.657 \n    0.001 \n    0.000 \n    1279 \n    2594 \n    1.00 \n  \n  \n    Psi[AD] \n    0.316 \n    0.020 \n    0.279 \n    0.353 \n    0.001 \n    0.001 \n    703 \n    1299 \n    1.00 \n  \n  \n    Psi[IGC] \n    0.355 \n    0.013 \n    0.330 \n    0.379 \n    0.000 \n    0.000 \n    2157 \n    2868 \n    1.00 \n  \n  \n    Psi[FI] \n    0.569 \n    0.026 \n    0.517 \n    0.617 \n    0.001 \n    0.001 \n    986 \n    1117 \n    1.00 \n  \n  \n    Psi[FC] \n    0.422 \n    0.026 \n    0.372 \n    0.470 \n    0.001 \n    0.001 \n    647 \n    1434 \n    1.00 \n  \n  \n    ksi[0, Student] \n    -0.212 \n    0.223 \n    -0.631 \n    0.207 \n    0.004 \n    0.003 \n    3679 \n    2683 \n    1.00 \n  \n  \n    ksi[0, Faculty] \n    -0.348 \n    0.270 \n    -0.860 \n    0.137 \n    0.005 \n    0.003 \n    3379 \n    3140 \n    1.00 \n  \n  \n    ksi[7, Student] \n    0.879 \n    0.223 \n    0.448 \n    1.276 \n    0.004 \n    0.003 \n    2615 \n    2912 \n    1.00 \n  \n  \n    ksi[7, Faculty] \n    0.870 \n    0.272 \n    0.395 \n    1.416 \n    0.005 \n    0.003 \n    3307 \n    2670 \n    1.00 \n  \n  \n    chol_cov_corr[0, 0] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    chol_cov_corr[0, 1] \n    0.850 \n    0.028 \n    0.800 \n    0.905 \n    0.001 \n    0.001 \n    555 \n    933 \n    1.00 \n  \n  \n    chol_cov_corr[1, 0] \n    0.850 \n    0.028 \n    0.800 \n    0.905 \n    0.001 \n    0.001 \n    555 \n    933 \n    1.00 \n  \n  \n    chol_cov_corr[1, 1] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    3997 \n    3840 \n    1.00 \n  \n\n\n\n\n\n\naz.plot_trace(idata, var_names=['lambdas1', 'lambdas2', 'tau', 'Psi', 'ksi']);\n\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:487: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:487: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n\n\n\n\n\n\n\ndf = pd.read_csv('sem_data.csv')\ndrivers = ['se_acad_p1', 'se_acad_p2',\n       'se_acad_p3', 'se_social_p1', 'se_social_p2', 'se_social_p3',\n       'sup_friends_p1', 'sup_friends_p2', 'sup_friends_p3', 'sup_parents_p1',\n       'sup_parents_p2', 'sup_parents_p3', 'ls_p1', 'ls_p2', 'ls_p3']\n       \n\n\ncoords = {'obs': list(range(len(df))), \n          'indicators': drivers,\n          'indicators_1': ['se_acad_p1','se_acad_p2','se_acad_p3'],\n          'indicators_2': ['se_social_p1','se_social_p2','se_social_p3'],\n          'indicators_3': ['sup_friends_p1','sup_friends_p2','sup_friends_p3'],\n          'indicators_4': [ 'sup_parents_p1','sup_parents_p2','sup_parents_p3'],\n          'indicators_5': ['ls_p1','ls_p2', 'ls_p3'],\n          'latent': ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS'],\n          'latent1': ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']\n          }\n\nobs_idx = list(range(len(df)))\nwith pm.Model(coords=coords) as model:\n  \n  Psi = pm.InverseGamma('Psi', 5, 10, dims='indicators')\n  lambdas_ = pm.Normal('lambdas_1', 1, 10, dims=('indicators_1'))\n  lambdas_1 = pm.Deterministic('lambdas1', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_1'))\n  lambdas_ = pm.Normal('lambdas_2', 1, 10, dims=('indicators_2'))\n  lambdas_2 = pm.Deterministic('lambdas2', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_2'))\n  lambdas_ = pm.Normal('lambdas_3', 1, 10, dims=('indicators_3'))\n  lambdas_3 = pm.Deterministic('lambdas3', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_3'))\n  lambdas_ = pm.Normal('lambdas_4', 1, 10, dims=('indicators_4'))\n  lambdas_4 = pm.Deterministic('lambdas4', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_4'))\n  lambdas_ = pm.Normal('lambdas_5', 1, 10, dims=('indicators_5'))\n  lambdas_5 = pm.Deterministic('lambdas5', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_5'))\n  tau = pm.Normal('tau', 3, 10, dims='indicators')\n  kappa = 0\n  sd_dist = pm.Exponential.dist(1.0, shape=5)\n  chol, _, _ = pm.LKJCholeskyCov('chol_cov', n=5, eta=2,\n    sd_dist=sd_dist, compute_corr=True)\n  cov = pm.Deterministic(\"cov\", chol.dot(chol.T), dims=('latent', 'latent1'))\n  ksi = pm.MvNormal('ksi', kappa, chol=chol, dims=('obs', 'latent'))\n\n  m0 = tau[0] + ksi[obs_idx, 0]*lambdas_1[0]\n  m1 = tau[1] + ksi[obs_idx, 0]*lambdas_1[1]\n  m2 = tau[2] + ksi[obs_idx, 0]*lambdas_1[2]\n  m3 = tau[3] + ksi[obs_idx, 1]*lambdas_2[0]\n  m4 = tau[4] + ksi[obs_idx, 1]*lambdas_2[1]\n  m5 = tau[5] + ksi[obs_idx, 1]*lambdas_2[2]\n  m6 = tau[6] + ksi[obs_idx, 2]*lambdas_3[0]\n  m7 = tau[7] + ksi[obs_idx, 2]*lambdas_3[1]\n  m8 = tau[8] + ksi[obs_idx, 2]*lambdas_3[2]\n  m9 = tau[9] + ksi[obs_idx, 3]*lambdas_4[0]\n  m10 = tau[10] + ksi[obs_idx, 3]*lambdas_4[1]\n  m11 = tau[11] + ksi[obs_idx, 3]*lambdas_4[2]\n  m12 = tau[12] + ksi[obs_idx, 4]*lambdas_5[0]\n  m13 = tau[13] + ksi[obs_idx, 4]*lambdas_5[1]\n  m14 = tau[14] + ksi[obs_idx, 4]*lambdas_5[2]\n  \n  mu = pm.Deterministic('mu', pm.math.stack([m0, m1, m2, m3, m4, m5, m6, m7,\n                                             m8, m9, m10, m11, m12, m13, m14]).T)\n  _  = pm.Normal('likelihood', mu, Psi, observed=df[drivers].values)\n\n  idata = pm.sample(nuts_sampler='numpyro', target_accept=.95, tune=1000,\n                    idata_kwargs={\"log_likelihood\": True}, random_seed=100)\n  idata.extend(pm.sample_posterior_predictive(idata))\n  \nsummary_df1 = az.summary(idata, var_names=['lambdas1', 'lambdas2', 'lambdas3', 'lambdas4', 'lambdas5', 'tau', 'Psi'])\n\ncov_df = pd.DataFrame(az.extract(idata['posterior'])['cov'].mean(axis=2))\ncov_df.index = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']\ncov_df.columns = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']\n\ncorrelation_df = pd.DataFrame(az.extract(idata['posterior'])['chol_cov_corr'].mean(axis=2))\ncorrelation_df.index = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']\ncorrelation_df.columns = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']\n\n\n\n\nLife Satisfaction Model\n\n\n\npy$summary_df1 |> kable() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n \n  \n     \n    mean \n    sd \n    hdi_3% \n    hdi_97% \n    mcse_mean \n    mcse_sd \n    ess_bulk \n    ess_tail \n    r_hat \n  \n \n\n  \n    lambdas1[se_acad_p1] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    lambdas1[se_acad_p2] \n    0.817 \n    0.052 \n    0.720 \n    0.915 \n    0.001 \n    0.001 \n    1193 \n    2008 \n    1.00 \n  \n  \n    lambdas1[se_acad_p3] \n    0.967 \n    0.060 \n    0.854 \n    1.076 \n    0.002 \n    0.001 \n    1286 \n    2037 \n    1.00 \n  \n  \n    lambdas2[se_social_p1] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    lambdas2[se_social_p2] \n    0.965 \n    0.058 \n    0.856 \n    1.071 \n    0.002 \n    0.002 \n    757 \n    1634 \n    1.00 \n  \n  \n    lambdas2[se_social_p3] \n    0.941 \n    0.072 \n    0.805 \n    1.076 \n    0.002 \n    0.002 \n    878 \n    1580 \n    1.00 \n  \n  \n    lambdas3[sup_friends_p1] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    lambdas3[sup_friends_p2] \n    0.802 \n    0.044 \n    0.720 \n    0.887 \n    0.001 \n    0.001 \n    1045 \n    1701 \n    1.00 \n  \n  \n    lambdas3[sup_friends_p3] \n    0.905 \n    0.053 \n    0.805 \n    1.006 \n    0.002 \n    0.001 \n    1235 \n    2150 \n    1.00 \n  \n  \n    lambdas4[sup_parents_p1] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    lambdas4[sup_parents_p2] \n    1.040 \n    0.059 \n    0.931 \n    1.150 \n    0.002 \n    0.002 \n    758 \n    1383 \n    1.00 \n  \n  \n    lambdas4[sup_parents_p3] \n    1.010 \n    0.064 \n    0.898 \n    1.137 \n    0.002 \n    0.001 \n    1051 \n    1840 \n    1.00 \n  \n  \n    lambdas5[ls_p1] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    lambdas5[ls_p2] \n    0.791 \n    0.085 \n    0.627 \n    0.944 \n    0.004 \n    0.003 \n    541 \n    1074 \n    1.00 \n  \n  \n    lambdas5[ls_p3] \n    0.990 \n    0.103 \n    0.806 \n    1.187 \n    0.004 \n    0.003 \n    543 \n    878 \n    1.00 \n  \n  \n    tau[se_acad_p1] \n    5.153 \n    0.044 \n    5.069 \n    5.234 \n    0.002 \n    0.001 \n    488 \n    1151 \n    1.01 \n  \n  \n    tau[se_acad_p2] \n    5.345 \n    0.039 \n    5.271 \n    5.414 \n    0.002 \n    0.001 \n    528 \n    1033 \n    1.01 \n  \n  \n    tau[se_acad_p3] \n    5.209 \n    0.045 \n    5.127 \n    5.297 \n    0.002 \n    0.001 \n    526 \n    1290 \n    1.01 \n  \n  \n    tau[se_social_p1] \n    5.286 \n    0.042 \n    5.208 \n    5.366 \n    0.002 \n    0.002 \n    380 \n    743 \n    1.01 \n  \n  \n    tau[se_social_p2] \n    5.473 \n    0.039 \n    5.397 \n    5.544 \n    0.002 \n    0.001 \n    363 \n    742 \n    1.01 \n  \n  \n    tau[se_social_p3] \n    5.437 \n    0.045 \n    5.351 \n    5.522 \n    0.002 \n    0.001 \n    492 \n    982 \n    1.00 \n  \n  \n    tau[sup_friends_p1] \n    5.782 \n    0.068 \n    5.651 \n    5.904 \n    0.004 \n    0.003 \n    333 \n    763 \n    1.01 \n  \n  \n    tau[sup_friends_p2] \n    6.007 \n    0.057 \n    5.909 \n    6.125 \n    0.003 \n    0.002 \n    397 \n    872 \n    1.00 \n  \n  \n    tau[sup_friends_p3] \n    5.987 \n    0.066 \n    5.864 \n    6.115 \n    0.003 \n    0.002 \n    385 \n    890 \n    1.01 \n  \n  \n    tau[sup_parents_p1] \n    5.973 \n    0.061 \n    5.858 \n    6.085 \n    0.003 \n    0.002 \n    427 \n    1059 \n    1.00 \n  \n  \n    tau[sup_parents_p2] \n    5.925 \n    0.062 \n    5.807 \n    6.040 \n    0.003 \n    0.002 \n    394 \n    924 \n    1.01 \n  \n  \n    tau[sup_parents_p3] \n    5.716 \n    0.066 \n    5.596 \n    5.840 \n    0.003 \n    0.002 \n    470 \n    1294 \n    1.00 \n  \n  \n    tau[ls_p1] \n    5.188 \n    0.053 \n    5.092 \n    5.289 \n    0.002 \n    0.001 \n    654 \n    1378 \n    1.00 \n  \n  \n    tau[ls_p2] \n    5.775 \n    0.041 \n    5.693 \n    5.849 \n    0.002 \n    0.001 \n    716 \n    1596 \n    1.00 \n  \n  \n    tau[ls_p3] \n    5.219 \n    0.051 \n    5.121 \n    5.314 \n    0.002 \n    0.001 \n    666 \n    1609 \n    1.00 \n  \n  \n    Psi[se_acad_p1] \n    0.412 \n    0.028 \n    0.359 \n    0.465 \n    0.001 \n    0.001 \n    1278 \n    1740 \n    1.00 \n  \n  \n    Psi[se_acad_p2] \n    0.413 \n    0.024 \n    0.367 \n    0.456 \n    0.001 \n    0.000 \n    2170 \n    2268 \n    1.00 \n  \n  \n    Psi[se_acad_p3] \n    0.468 \n    0.027 \n    0.418 \n    0.519 \n    0.001 \n    0.000 \n    1844 \n    2408 \n    1.00 \n  \n  \n    Psi[se_social_p1] \n    0.431 \n    0.026 \n    0.381 \n    0.477 \n    0.001 \n    0.000 \n    1382 \n    2219 \n    1.00 \n  \n  \n    Psi[se_social_p2] \n    0.361 \n    0.025 \n    0.314 \n    0.405 \n    0.001 \n    0.000 \n    1486 \n    2135 \n    1.00 \n  \n  \n    Psi[se_social_p3] \n    0.553 \n    0.029 \n    0.500 \n    0.606 \n    0.001 \n    0.000 \n    2594 \n    2803 \n    1.00 \n  \n  \n    Psi[sup_friends_p1] \n    0.517 \n    0.040 \n    0.439 \n    0.587 \n    0.001 \n    0.001 \n    866 \n    1739 \n    1.00 \n  \n  \n    Psi[sup_friends_p2] \n    0.508 \n    0.031 \n    0.454 \n    0.568 \n    0.001 \n    0.001 \n    1420 \n    1985 \n    1.00 \n  \n  \n    Psi[sup_friends_p3] \n    0.625 \n    0.036 \n    0.562 \n    0.694 \n    0.001 \n    0.001 \n    2090 \n    2329 \n    1.00 \n  \n  \n    Psi[sup_parents_p1] \n    0.550 \n    0.035 \n    0.485 \n    0.615 \n    0.001 \n    0.001 \n    1530 \n    2075 \n    1.00 \n  \n  \n    Psi[sup_parents_p2] \n    0.536 \n    0.038 \n    0.465 \n    0.605 \n    0.001 \n    0.001 \n    1192 \n    2078 \n    1.01 \n  \n  \n    Psi[sup_parents_p3] \n    0.675 \n    0.038 \n    0.602 \n    0.745 \n    0.001 \n    0.001 \n    2089 \n    2371 \n    1.00 \n  \n  \n    Psi[ls_p1] \n    0.671 \n    0.038 \n    0.603 \n    0.744 \n    0.001 \n    0.001 \n    1045 \n    2387 \n    1.00 \n  \n  \n    Psi[ls_p2] \n    0.534 \n    0.030 \n    0.477 \n    0.591 \n    0.001 \n    0.001 \n    1409 \n    2472 \n    1.00 \n  \n  \n    Psi[ls_p3] \n    0.622 \n    0.035 \n    0.554 \n    0.688 \n    0.001 \n    0.001 \n    1597 \n    2393 \n    1.00 \n  \n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(15, 8))\naz.plot_forest(idata, var_names=['lambdas1', 'lambdas2', 'lambdas3', 'lambdas4', 'lambdas5'], combined=True, ax=ax);\nax.set_title(\"Factor Loadings for each of the Five Factors\");\n\n\n\n\n\npy$cov_df |> kable(caption= \"Covariances Amongst Latent Factors\",digits=2) |> kable_styling() %>% kable_classic(full_width = F, html_font = \"Cambria\") |> row_spec(5, color = \"red\")\n\n\n\nCovariances Amongst Latent Factors\n \n  \n     \n    SE_ACAD \n    SE_SOCIAL \n    SUP_F \n    SUP_P \n    LS \n  \n \n\n  \n    SE_ACAD \n    0.47 \n    0.26 \n    0.06 \n    0.20 \n    0.22 \n  \n  \n    SE_SOCIAL \n    0.26 \n    0.39 \n    0.19 \n    0.26 \n    0.30 \n  \n  \n    SUP_F \n    0.06 \n    0.19 \n    1.03 \n    0.12 \n    0.16 \n  \n  \n    SUP_P \n    0.20 \n    0.26 \n    0.12 \n    0.86 \n    0.38 \n  \n  \n    LS \n    0.22 \n    0.30 \n    0.16 \n    0.38 \n    0.42 \n  \n\n\n\n\n\n\npy$correlation_df |> kable( caption= \"Correlations Amongst Latent Factors\", digits=2) |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\") |> row_spec(5, color = \"red\")\n\n\n\nCorrelations Amongst Latent Factors\n \n  \n     \n    SE_ACAD \n    SE_SOCIAL \n    SUP_F \n    SUP_P \n    LS \n  \n \n\n  \n    SE_ACAD \n    1.00 \n    0.60 \n    0.09 \n    0.32 \n    0.50 \n  \n  \n    SE_SOCIAL \n    0.60 \n    1.00 \n    0.29 \n    0.45 \n    0.75 \n  \n  \n    SUP_F \n    0.09 \n    0.29 \n    1.00 \n    0.12 \n    0.25 \n  \n  \n    SUP_P \n    0.32 \n    0.45 \n    0.12 \n    1.00 \n    0.64 \n  \n  \n    LS \n    0.50 \n    0.75 \n    0.25 \n    0.64 \n    1.00 \n  \n\n\n\n\n\n\ndef make_ppc(idata):\n  fig, axs = plt.subplots(5, 3, figsize=(20, 20))\n  axs = axs.flatten()\n  for i in range(15):\n    for j in range(100):\n      temp = az.extract(idata['posterior_predictive'].sel({'likelihood_dim_3': i}))['likelihood'].values[:, j]\n      temp = pd.DataFrame(temp, columns=['likelihood'])\n      if j == 0:\n        axs[i].hist(df[drivers[i]], alpha=0.3, ec='black', bins=20, label='Observed Scores')\n        axs[i].hist(temp['likelihood'], color='purple', alpha=0.1, bins=20, label='Predicted Scores')\n      else: \n        axs[i].hist(df[drivers[i]], alpha=0.3, ec='black', bins=20)\n        axs[i].hist(temp['likelihood'], color='purple', alpha=0.1, bins=20)\n      axs[i].set_title(f\"Posterior Predictive Checks {drivers[i]}\")\n      axs[i].legend();\n  plt.show()\n  \nmake_ppc(idata)\n\n\n\n\n\n\ndef get_posterior_resids(idata, samples=100, metric='cov'):\n  resids = []\n  for i in range(100):\n    if metric == 'cov':\n      model_cov = pd.DataFrame(az.extract(idata['posterior_predictive'])['likelihood'][:, :, i]).cov()\n      obs_cov = df[drivers].cov()\n    else: \n      model_cov = pd.DataFrame(az.extract(idata['posterior_predictive'])['likelihood'][:, :, i]).corr()\n      obs_cov = df[drivers].corr()\n    model_cov.index = obs_cov.index\n    model_cov.columns = obs_cov.columns\n    residuals = model_cov - obs_cov\n    resids.append(residuals.values.flatten())\n  \n  residuals_posterior = pd.DataFrame(pd.DataFrame(resids).mean().values.reshape(15, 15))\n  residuals_posterior.index = obs_cov.index\n  residuals_posterior.columns = obs_cov.index\n  return residuals_posterior\n\nresiduals_posterior_cov = get_posterior_resids(idata, 500)\nresiduals_posterior_corr = get_posterior_resids(idata, 500, metric='corr')\n\n  \n\n\nplot_heatmap(py$residuals_posterior_cov, \"Residuals of the Sample Covariances and Model Implied Covariances\", \"A Visual Check of Bayesian Model fit\")\n\n\n\nplot_heatmap(py$residuals_posterior_corr, \"Residuals of the Sample Correlations and Model Implied Correlations\", \"A Visual Check of Bayesian Model fit\")"
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#measurment-and-measurement-constructs",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#measurment-and-measurement-constructs",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Measurment and Measurement Constructs",
    "text": "Measurment and Measurement Constructs\nScience is easier when there is a recipe. When there is some procedure to adopt or routine to ape, you can out-source the responsibility for methodological justification. One egregious pattern in this vein attempts to masks implausible nonsense with the much vaunted mathematical gloss of “statistical significance”. Seen from 1000 feet, this is disappointing but not surprising. Lip-service is paid to the idea of scientific method and we absolve ourselves of the requirement for principled justification and substantive argument.\nEvidence should be marshaled in service to argument, and it’s an absurd pretense to claim that data speaks for itself in this argument. Good and compelling argumentation is at the heart of any sound inference. It is a necessary obligation if you expect anyone to make any decision on the strength of evidence you provide. Procedures and routine tests offer only a facsimile of sound argument and p-values are a poor substitute for logical consequence and substantive derivation.\nData is found, gathered or maybe even carefully curated. In all cases there is need for a defensive posture, an argument that the data is fit-for-purpose. No where is this more clear than in psychometrics where the data is often derived from a strategically constructed survey aimed at a particular target phenomena. Some intuited, but not yet measured, concept that arguably plays a role in human action, motivation or sentiment. The relative “fuzziness” of the subject matter in psychometrics has had a catalyzing effect on the methodological rigour sought in the science. Survey designs are agonised over for correct tone and rhythm of sentence structure. Measurement scales are doubly checked for reliability and correctness. Analysis steps are justified and tested under a wealth of modelling routines. Model architectures are defined and refined to better express the hypothesized structures in the data-generating process.\nWe will examine a smattering of choices available in the analysis of psychometric survey data. First stepping through these analysis routines using lmer, lavaan before switching to PyMC to demonstrate how to fit Confirmatory Factor Analysis models and Structural Equation Models in a Bayesian fashion using a probabilistic programming language.Throughout we’ll draw on Levy and Mislevy’s Bayesian Psychometric Modeling.m\n\nThe Data\nThe data is borrowed from work here demonstrating SEM modelling with Lavaan. We’ll load it up and begin some exploratory work. First let’s look at the data.\n\ndf = read.csv('sem_data.csv')\ndf$ls_sum <- df$ls_p1 + df$ls_p2 + df$ls_p3\ndf$ls_mean <- rowMeans(df[ c('ls_p1', 'ls_p2', 'ls_p3')])\n\nhead(df) |> kable() |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n \n  \n    ID \n    region \n    gender \n    age \n    se_acad_p1 \n    se_acad_p2 \n    se_acad_p3 \n    se_social_p1 \n    se_social_p2 \n    se_social_p3 \n    sup_friends_p1 \n    sup_friends_p2 \n    sup_friends_p3 \n    sup_parents_p1 \n    sup_parents_p2 \n    sup_parents_p3 \n    ls_p1 \n    ls_p2 \n    ls_p3 \n    ls_sum \n    ls_mean \n  \n \n\n  \n    1 \n    west \n    female \n    13 \n    4.857143 \n    5.571429 \n    4.500000 \n    5.80 \n    5.500000 \n    5.40 \n    6.5 \n    6.5 \n    7.0 \n    7.0 \n    7.0 \n    6.0 \n    5.333333 \n    6.75 \n    5.50 \n    17.58333 \n    5.861111 \n  \n  \n    2 \n    west \n    male \n    14 \n    4.571429 \n    4.285714 \n    4.666667 \n    5.00 \n    5.500000 \n    4.80 \n    4.5 \n    4.5 \n    5.5 \n    5.0 \n    6.0 \n    4.5 \n    4.333333 \n    5.00 \n    4.50 \n    13.83333 \n    4.611111 \n  \n  \n    10 \n    west \n    female \n    14 \n    4.142857 \n    6.142857 \n    5.333333 \n    5.20 \n    4.666667 \n    6.00 \n    4.0 \n    4.5 \n    3.5 \n    7.0 \n    7.0 \n    6.5 \n    6.333333 \n    5.50 \n    4.00 \n    15.83333 \n    5.277778 \n  \n  \n    11 \n    west \n    female \n    14 \n    5.000000 \n    5.428571 \n    4.833333 \n    6.40 \n    5.833333 \n    6.40 \n    7.0 \n    7.0 \n    7.0 \n    7.0 \n    7.0 \n    7.0 \n    4.333333 \n    6.50 \n    6.25 \n    17.08333 \n    5.694444 \n  \n  \n    12 \n    west \n    female \n    14 \n    5.166667 \n    5.600000 \n    4.800000 \n    5.25 \n    5.400000 \n    5.25 \n    7.0 \n    7.0 \n    7.0 \n    6.5 \n    6.5 \n    7.0 \n    5.666667 \n    6.00 \n    5.75 \n    17.41667 \n    5.805556 \n  \n  \n    14 \n    west \n    male \n    14 \n    4.857143 \n    4.857143 \n    4.166667 \n    5.20 \n    5.000000 \n    4.20 \n    5.5 \n    6.5 \n    7.0 \n    6.5 \n    6.5 \n    6.5 \n    5.000000 \n    5.50 \n    5.50 \n    16.00000 \n    5.333333 \n  \n\n\n\n\n\nWe have survey responses from ~300 individuals who have answered questions regarding their upbringing, self-belief and reported life-satisfaction. The hypothetical dependency structure in this life-satisfaction data set posits a moderations relationship between scores related to life-satisfaction, parental and family support and self-efficacy. It is not a trivial task to be able to design a survey that can elicit answers plausibly mapped to each of these “factors” or themes, never mind finding a model of their relationship that can inform us as to the relative of impact of each on life-satisfaction outcomes.\n We will try to show some of the subtle aspects of these relationships as we go. The high level summary statistics show the variation across these measures. We can thematically cluster them because the source survey deliberately targeted each of the underlying themes in effort to pin down the relations between these hard to measure constructs.\n\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\ndatasummary_skim(df)|> \n style_tt(\n   i = 15:17,\n   j = 1:1,\n   background = \"#20AACC\",\n   color = \"white\",\n   italic = TRUE) |> \n style_tt(\n   i = 18:19,\n   j = 1:1,\n   background = \"#2888A0\",\n   color = \"white\",\n   italic = TRUE) |> \n style_tt(\n   i = 2:14,\n   j = 1:1,\n   background = \"#17C2AD\",\n   color = \"white\",\n   italic = TRUE)\n\n\n \n\n  \n    \n    \n    tinytable_ocdtt0ymi9t18mtrz4el\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                Unique\n                Missing Pct.\n                Mean\n                SD\n                Min\n                Median\n                Max\n                Histogram\n              \n        \n        \n        \n                \n                  ID\n                  283\n                  0\n                  187.9\n                  106.3\n                  1.0\n                  201.0\n                  367.0\n                  \n                \n                \n                  age\n                  5\n                  0\n                  14.7\n                  0.8\n                  13.0\n                  15.0\n                  17.0\n                  \n                \n                \n                  se_acad_p1\n                  32\n                  0\n                  5.2\n                  0.8\n                  3.1\n                  5.1\n                  7.0\n                  \n                \n                \n                  se_acad_p2\n                  36\n                  0\n                  5.3\n                  0.7\n                  3.1\n                  5.4\n                  7.0\n                  \n                \n                \n                  se_acad_p3\n                  29\n                  0\n                  5.2\n                  0.8\n                  2.8\n                  5.2\n                  7.0\n                  \n                \n                \n                  se_social_p1\n                  24\n                  0\n                  5.3\n                  0.8\n                  1.8\n                  5.4\n                  7.0\n                  \n                \n                \n                  se_social_p2\n                  27\n                  0\n                  5.5\n                  0.7\n                  2.7\n                  5.5\n                  7.0\n                  \n                \n                \n                  se_social_p3\n                  31\n                  0\n                  5.4\n                  0.8\n                  3.0\n                  5.5\n                  7.0\n                  \n                \n                \n                  sup_friends_p1\n                  13\n                  0\n                  5.8\n                  1.1\n                  1.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_friends_p2\n                  10\n                  0\n                  6.0\n                  0.9\n                  2.5\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_friends_p3\n                  13\n                  0\n                  6.0\n                  1.1\n                  1.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_parents_p1\n                  11\n                  0\n                  6.0\n                  1.1\n                  1.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_parents_p2\n                  11\n                  0\n                  5.9\n                  1.1\n                  2.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_parents_p3\n                  13\n                  0\n                  5.7\n                  1.1\n                  1.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  ls_p1\n                  15\n                  0\n                  5.2\n                  0.9\n                  2.0\n                  5.3\n                  7.0\n                  \n                \n                \n                  ls_p2\n                  21\n                  0\n                  5.8\n                  0.7\n                  2.5\n                  5.8\n                  7.0\n                  \n                \n                \n                  ls_p3\n                  22\n                  0\n                  5.2\n                  0.9\n                  2.0\n                  5.2\n                  7.0\n                  \n                \n                \n                  ls_sum\n                  98\n                  0\n                  16.2\n                  2.1\n                  8.7\n                  16.4\n                  20.8\n                  \n                \n                \n                  ls_mean\n                  97\n                  0\n                  5.4\n                  0.7\n                  2.9\n                  5.5\n                  6.9\n                  \n                \n                \n                   \n                    \n                  N\n                  %\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  region\n                  east\n                  142\n                  50.2\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  \n                  west\n                  141\n                  49.8\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  gender\n                  female\n                  132\n                  46.6\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  \n                  male\n                  151\n                  53.4\n                  \n                  \n                  \n                  \n                  \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nNote how we’ve distinguished among the metrics for the “outcome” metrics and the “driver” metrics. Such a distinction may seem trivial, but it is only possible because we bring substantive knowledge to bear on the problem in the design of our survey and the postulation of the theoretical construct. Our data is a multivariate outcome vector. There are a large range of possible interacting effects and the patterns of realisation for our life-satisfaction scores. The true “drivers” of satisfaction may be quite different than the hypothesised structure. It is this open question that we’re aiming to uncover in the analysis of our survey data.\n\n\nSample Covariances and Correlations\nLet’s plot the relations amongst our various indicator scores.\n\ndrivers = c('se_acad_p1', 'se_acad_p2', 'se_acad_p3', 'se_social_p1', 'se_social_p2', 'se_social_p3', 'sup_friends_p1','sup_friends_p2', 'sup_friends_p3', 'sup_parents_p1' , 'sup_parents_p2' , 'sup_parents_p3', 'ls_p1', 'ls_p2', 'ls_p3')\n\n\n\nplot_heatmap <- function(df, title=\"Sample Covariances\", subtitle=\"Observed Measures\") {\n  heat_df = df |> as.matrix() |> melt()\n  colnames(heat_df) <- c(\"x\", \"y\", \"value\")\n  g <- heat_df |> mutate(value = round(value, 2)) |>  ggplot(aes(x = x, y = y, fill = value)) +\n    geom_tile() + geom_text(aes(label = value), color = \"black\", size = 4) +\n   scale_fill_gradient2(\n      high = 'dodgerblue4',\n      mid = 'white',\n      low = 'firebrick2'\n    ) + theme(axis.text.x = element_text(angle=45)) + ggtitle(title, subtitle)\n  \n  g\n}\n\ng1 = plot_heatmap(cov(df[,  drivers]))\n\ng2 = plot_heatmap(cor(df[,  drivers]), \"Sample Correlations\")\n\nplot <- ggarrange(g1,g2, ncol=1, nrow=2);\n\n\n\n\nNote the relatively strong correlations between measures of parental support and the life-satisfaction outcome ls_p3. Similarly, how the social self-efficacy scores seem similarly correlated with the secondary life satisfaction indicator ls_p2. These observed correlations merit some further investigation. We can also plot the pairs of scatter plots to “dig deeper”. What kind of correlation holds between these scores? Are any driven by extreme outliers? This is what we’re looking for in a pair plot. Should we cull the outliers? Leave them in? We leave them in.\n\ndf <- df |> \n  mutate(id = row_number()) \n\n# Prepare data to be plotted on the x axis\nx_vars <- pivot_longer(data = df,\n             cols = se_acad_p1:ls_p3,\n             names_to = \"variable_x\",\n             values_to = \"x\")\n\n# Prepare data to be plotted on the y axis  \ny_vars <- pivot_longer(data = df,\n                       cols = se_acad_p1:ls_p3,\n                       names_to = \"variable_y\",\n                       values_to = \"y\") \n\n# Join data for x and y axes and make plot\nfull_join(x_vars, y_vars, \n          by = c(\"id\"),\n          relationship = \"many-to-many\") |>\n  ggplot() + \n  aes(x = x, y = y) +\n  geom_point() + geom_smooth(method='lm') +\n  facet_grid(c(\"variable_x\", \"variable_y\"))  + ggtitle(\"Pair Plot of Indicator Metrics\", \n                                                       \"Comparing Against Life Satisfaction Scores\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe scatter plots among the highly correlated variables in the heatmap do seem to exhibit some kind of linear relationship with aspects of the life-satisfaction scores. We now turn to modelling these relationships to tease out some kind of inferential summary of those relationships."
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#structural-equation-modelling-in-pymc",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#structural-equation-modelling-in-pymc",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Structural Equation Modelling in PyMC",
    "text": "Structural Equation Modelling in PyMC\n\ndef make_sem(priors): \n\n  coords = {'obs': list(range(len(df))), \n            'indicators': drivers,\n            'indicators_1': ['se_acad_p1','se_acad_p2','se_acad_p3'],\n            'indicators_2': ['se_social_p1','se_social_p2','se_social_p3'],\n            'indicators_3': ['sup_friends_p1','sup_friends_p2','sup_friends_p3'],\n            'indicators_4': [ 'sup_parents_p1','sup_parents_p2','sup_parents_p3'],\n            'indicators_5': ['ls_p1','ls_p2', 'ls_p3'],\n            'latent': ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P'], \n            'latent1': ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P']\n            }\n\n  obs_idx = list(range(len(df)))\n  with pm.Model(coords=coords) as model:\n    \n    Psi = pm.InverseGamma('Psi', 5, 10, dims='indicators')\n    lambdas_ = pm.Normal('lambdas_1',  priors['lambda'][0], priors['lambda'][1], dims=('indicators_1'))\n    lambdas_1 = pm.Deterministic('lambdas1', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_1'))\n    lambdas_ = pm.Normal('lambdas_2', priors['lambda'][0], priors['lambda'][1], dims=('indicators_2'))\n    lambdas_2 = pm.Deterministic('lambdas2', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_2'))\n    lambdas_ = pm.Normal('lambdas_3', priors['lambda'][0], priors['lambda'][1], dims=('indicators_3'))\n    lambdas_3 = pm.Deterministic('lambdas3', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_3'))\n    lambdas_ = pm.Normal('lambdas_4', priors['lambda'][0], priors['lambda'][1], dims=('indicators_4'))\n    lambdas_4 = pm.Deterministic('lambdas4', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_4'))\n    lambdas_ = pm.Normal('lambdas_5', priors['lambda'][0], priors['lambda'][1], dims=('indicators_5'))\n    lambdas_5 = pm.Deterministic('lambdas5', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_5'))\n    tau = pm.Normal('tau', 3, 10, dims='indicators')\n    kappa = 0\n    sd_dist = pm.Exponential.dist(1.0, shape=4)\n    chol, _, _ = pm.LKJCholeskyCov('chol_cov', n=4, eta=priors['eta'],\n      sd_dist=sd_dist, compute_corr=True)\n    cov = pm.Deterministic(\"cov\", chol.dot(chol.T), dims=('latent', 'latent1'))\n    ksi = pm.MvNormal('ksi', kappa, chol=chol, dims=('obs', 'latent'))\n\n    ## Additional Regression Component\n    beta_r = pm.Normal('beta_r', 0, 1, dims='latent')\n    regression = pm.Deterministic('regr', beta_r[0]*ksi[obs_idx, 0] + beta_r[1]*ksi[obs_idx, 1] +\n                                   beta_r[2]*ksi[obs_idx, 2] + beta_r[3]*ksi[obs_idx, 3])\n\n    m0 = tau[0] + ksi[obs_idx, 0]*lambdas_1[0]\n    m1 = tau[1] + ksi[obs_idx, 0]*lambdas_1[1]\n    m2 = tau[2] + ksi[obs_idx, 0]*lambdas_1[2]\n    m3 = tau[3] + ksi[obs_idx, 1]*lambdas_2[0]\n    m4 = tau[4] + ksi[obs_idx, 1]*lambdas_2[1]\n    m5 = tau[5] + ksi[obs_idx, 1]*lambdas_2[2]\n    m6 = tau[6] + ksi[obs_idx, 2]*lambdas_3[0]\n    m7 = tau[7] + ksi[obs_idx, 2]*lambdas_3[1]\n    m8 = tau[8] + ksi[obs_idx, 2]*lambdas_3[2]\n    m9 = tau[9] + ksi[obs_idx, 3]*lambdas_4[0]\n    m10 = tau[10] + ksi[obs_idx, 3]*lambdas_4[1]\n    m11 = tau[11] + ksi[obs_idx, 3]*lambdas_4[2]\n    m12 = tau[12] + regression*lambdas_5[0]\n    m13 = tau[13] + regression*lambdas_5[1]\n    m14 = tau[14] + regression*lambdas_5[2]\n    \n    mu = pm.Deterministic('mu', pm.math.stack([m0, m1, m2, m3, m4, m5, m6, m7,\n                                              m8, m9, m10, m11, m12, m13, m14]).T)\n    _  = pm.Normal('likelihood', mu, Psi, observed=df[drivers].values)\n\n    idata = pm.sample(chains=6, nuts_sampler='numpyro', target_accept=.95, tune=1000,\n                      idata_kwargs={\"log_likelihood\": True}, random_seed=100)\n    idata.extend(pm.sample_posterior_predictive(idata))\n\n    return model, idata\n\n\nmodel1, idata1 = make_sem(priors={'eta': 2, 'lambda': [1, 2]})\n\n\n\n\nStructural Equation Models\n\n\n\naz.plot_posterior(idata1, var_names=['beta_r']);\n\n\n\n\n\nmake_ppc(idata1)"
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#exploratory-and-confirmatory-modes",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#exploratory-and-confirmatory-modes",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Exploratory and Confirmatory Modes",
    "text": "Exploratory and Confirmatory Modes\nOne of the things that psychometrics has pioneered well is the distinction between an exploratory and confirmatory models. This distinction, when made explicit, partially guards against the abuse of inferential integrity we see in more common work-flows. But additionally, models are often opaque - you may (as above) have improved some measure of model fit, changed the parameter weighting accorded to an observed feature, but what does that mean? Exploration of model architectures, design choices and feature creation is just how we come to understand the meaning of a model specification. Even in the simple case of regression we’ve seen how adding an interaction term changes the interpretability of a model. How then are we to stand behind uncertainty estimates accorded to parameter weights when we barely intuit the implications of a model design?\n\nMarginal Effects\nThe answer is to not to rely on intuition, but push forward and test the tangible implications of a fitted model. A model is hypothesis which should be applied to stringent test. We should subject the logical consequences of the design to appropriate scrutiny. We understand the implications and relative plausibility of any model in terms of the predicted outcomes more easily than we understand the subtle interaction effects expressed parameter movements. As such we should adopt this view in our evaluation of a model fit too.\nConsider how we do this using Vincent Arel-Bundock’s wonderful marginaleffects package passing a grid of new values through to our fitted model.These implications are a test of model plausibility too.\n\npred <- predictions(mod_sum_parcel, newdata = datagrid(sup_parents_mean = 1:10, se_social_mean = 1:10 ))\npred1 <- predictions(mod_sum_inter_parcel, newdata = datagrid(sup_parents_mean = 1:10, se_social_mean = 1:10))\n\npred1  |> tail(10) |> kableExtra::kable() |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n \n  \n     \n    rowid \n    estimate \n    std.error \n    statistic \n    p.value \n    s.value \n    conf.low \n    conf.high \n    se_acad_mean \n    sup_friends_mean \n    sup_parents_mean \n    se_social_mean \n    ls_sum \n  \n \n\n  \n    91 \n    91 \n    19.27615 \n    2.2881204 \n    8.424449 \n    0 \n    54.61499 \n    14.79152 \n    23.76079 \n    5.237686 \n    5.929329 \n    10 \n    1 \n    17.58333 \n  \n  \n    92 \n    92 \n    19.21698 \n    1.7840261 \n    10.771691 \n    0 \n    87.46457 \n    15.72035 \n    22.71361 \n    5.237686 \n    5.929329 \n    10 \n    2 \n    17.58333 \n  \n  \n    93 \n    93 \n    19.15780 \n    1.2888397 \n    14.864380 \n    0 \n    163.60757 \n    16.63172 \n    21.68388 \n    5.237686 \n    5.929329 \n    10 \n    3 \n    17.58333 \n  \n  \n    94 \n    94 \n    19.09863 \n    0.8188846 \n    23.322730 \n    0 \n    397.24885 \n    17.49364 \n    20.70361 \n    5.237686 \n    5.929329 \n    10 \n    4 \n    17.58333 \n  \n  \n    95 \n    95 \n    19.03945 \n    0.4595015 \n    41.435010 \n    0 \n    Inf \n    18.13884 \n    19.94006 \n    5.237686 \n    5.929329 \n    10 \n    5 \n    17.58333 \n  \n  \n    96 \n    96 \n    18.98027 \n    0.5318049 \n    35.690291 \n    0 \n    924.33454 \n    17.93795 \n    20.02259 \n    5.237686 \n    5.929329 \n    10 \n    6 \n    17.58333 \n  \n  \n    97 \n    97 \n    18.92110 \n    0.9410614 \n    20.106123 \n    0 \n    296.26806 \n    17.07665 \n    20.76554 \n    5.237686 \n    5.929329 \n    10 \n    7 \n    17.58333 \n  \n  \n    98 \n    98 \n    18.86192 \n    1.4210849 \n    13.272902 \n    0 \n    131.14398 \n    16.07665 \n    21.64720 \n    5.237686 \n    5.929329 \n    10 \n    8 \n    17.58333 \n  \n  \n    99 \n    99 \n    18.80274 \n    1.9194980 \n    9.795657 \n    0 \n    72.84938 \n    15.04060 \n    22.56489 \n    5.237686 \n    5.929329 \n    10 \n    9 \n    17.58333 \n  \n  \n    100 \n    100 \n    18.74357 \n    2.4249884 \n    7.729343 \n    0 \n    46.39459 \n    13.99068 \n    23.49646 \n    5.237686 \n    5.929329 \n    10 \n    10 \n    17.58333 \n  \n\n\n\n\n\nWe will see here how the impact of changes in se_social_mean is much reduced when sup_parent_mean is held fixed at high values (9, 10) when our model allows for an interaction effect.\n\npred |> head(10) |> kableExtra::kable() |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n \n  \n    rowid \n    estimate \n    std.error \n    statistic \n    p.value \n    s.value \n    conf.low \n    conf.high \n    se_acad_mean \n    sup_friends_mean \n    sup_parents_mean \n    se_social_mean \n    ls_sum \n  \n \n\n  \n    1 \n    7.065559 \n    0.7860786 \n    8.988362 \n    0 \n    61.78928 \n    5.524873 \n    8.606245 \n    5.237686 \n    5.929329 \n    1 \n    1 \n    17.58333 \n  \n  \n    2 \n    8.334837 \n    0.6546017 \n    12.732685 \n    0 \n    120.95075 \n    7.051842 \n    9.617833 \n    5.237686 \n    5.929329 \n    1 \n    2 \n    17.58333 \n  \n  \n    3 \n    9.604115 \n    0.5480173 \n    17.525206 \n    0 \n    226.01129 \n    8.530021 \n    10.678209 \n    5.237686 \n    5.929329 \n    1 \n    3 \n    17.58333 \n  \n  \n    4 \n    10.873394 \n    0.4830922 \n    22.507905 \n    0 \n    370.25976 \n    9.926550 \n    11.820237 \n    5.237686 \n    5.929329 \n    1 \n    4 \n    17.58333 \n  \n  \n    5 \n    12.142672 \n    0.4771468 \n    25.448505 \n    0 \n    472.16118 \n    11.207481 \n    13.077862 \n    5.237686 \n    5.929329 \n    1 \n    5 \n    17.58333 \n  \n  \n    6 \n    13.411950 \n    0.5321613 \n    25.202790 \n    0 \n    463.16951 \n    12.368933 \n    14.454967 \n    5.237686 \n    5.929329 \n    1 \n    6 \n    17.58333 \n  \n  \n    7 \n    14.681228 \n    0.6324223 \n    23.214278 \n    0 \n    393.60150 \n    13.441703 \n    15.920753 \n    5.237686 \n    5.929329 \n    1 \n    7 \n    17.58333 \n  \n  \n    8 \n    15.950506 \n    0.7602342 \n    20.981043 \n    0 \n    322.26019 \n    14.460474 \n    17.440538 \n    5.237686 \n    5.929329 \n    1 \n    8 \n    17.58333 \n  \n  \n    9 \n    17.219784 \n    0.9039855 \n    19.048739 \n    0 \n    266.32548 \n    15.448005 \n    18.991563 \n    5.237686 \n    5.929329 \n    1 \n    9 \n    17.58333 \n  \n  \n    10 \n    18.489062 \n    1.0571941 \n    17.488806 \n    0 \n    225.08895 \n    16.417000 \n    20.561124 \n    5.237686 \n    5.929329 \n    1 \n    10 \n    17.58333 \n  \n\n\n\n\n\nThis modifying effect is not as evident in the simpler model.\n\nRegression Marginal Effects\nWe can see this more clearly with a plot of the marginal effects.\n\ng = plot_predictions(mod_sum_parcel, condition = c(\"se_social_mean\", \"sup_parents_mean\"), type = \"response\") + ggtitle(\"Counterfactual Shift of Outcome: se_social_mean\", \"Holding all else Fixed: Simple Model\")\n\ng1 = plot_predictions(mod_sum_inter_parcel, condition = c(\"se_social_mean\", \"sup_parents_mean\"), type = \"response\") + ggtitle(\"Counterfactual Shift of Outcome: se_social_mean\", \"Holding all else Fixed Interaction Model\")\n\nplot <- ggarrange(g,g1, ncol=1, nrow=2);\n\n\n\n\nHere we’ve pulled out some of the implications in terms of the outcome predictions we"
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#models-with-controls",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#models-with-controls",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Models with Controls",
    "text": "Models with Controls\nThe garden of forking paths presents itself within any set of covariates. How do we represent their effects? Which interactions are meaningful? How do we argue for one model design over another? The questionable paths are multiplied when we begin to consider additional covariates and group effects. But also additional covariates help structure our expectations too. Yes, you can cut and chop your way to through the garden to find some spurious correlation but more plausibly you can bring in structurally important variables which helpfully moderate the outcomes based on our understanding of the data generating process.\nLet’s assess the question but this time we allow the model to account for differences in region.\n\nformula_no_grp_sum = \"ls_sum ~ sup_parents_p1 + sup_parents_p2 + sup_parents_p3 + sup_friends_p1 + sup_friends_p2 + sup_friends_p3 + se_acad_p1 + se_acad_p2 + se_acad_p3 +\nse_social_p1 + se_social_p2 + se_social_p3\"\n\nformula_grp_sum = \"ls_sum ~ sup_parents_p1 + sup_parents_p2 + sup_parents_p3 + sup_friends_p1 + sup_friends_p2 + sup_friends_p3 + se_acad_p1 + se_acad_p2 + se_acad_p3 +\nse_social_p1 + se_social_p2 + se_social_p3 + factor(region)\"\n\nno_grp_sum_fit <- lm(formula_no_grp_sum , data = df)\ngrp_sum_fit <- lm(formula_grp_sum, data = df)\n\ng1 = modelplot(no_grp_sum_fit, re.form=NA) +  aes(color = ifelse(p.value < 0.05, \"Significant at 0.05\", \"Not significant at 0.001\")) +\n  scale_color_manual(values = c(\"grey\", \"blue\"), guide='none') + ggtitle(\"Significance of Coefficient Values\", \"No Group Effects Model\")\n\ng2 = modelplot(grp_sum_fit, re.form=NA) +  aes(color = ifelse(p.value < 0.05, \"Significant at 0.05\", \"Not significant 0.05\")) +\n  scale_color_manual(values = c(\"grey\", \"blue\")) + ggtitle(\"Significance of Coefficient Values\", \"Group Effects Model\")\n\n\n\nplot <- ggarrange(g1,g2, ncol=2, nrow=1);\n\n\n\n\nWe can see here how the additional factor variable is reported to be significant conditional on a model specification. However the intercept is no longer well identified.\n\nmodelsummary(list(\"No Group Effects Fit\"= no_grp_sum_fit,\n                  \"Group Effects Fit\"= grp_sum_fit), \n             stars = TRUE) |> \n style_tt(\n   i = 2:25,\n   j = 1:1,\n   background = \"#17C2AD\",\n   color = \"white\",\n   italic = TRUE)\n\n\n \n\n  \n    \n    \n    tinytable_6dqjw6ypw5guaj07q8f6\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                No Group Effects Fit\n                Group Effects Fit\n              \n        \n        + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n        \n                \n                  (Intercept)       \n                  2.094*  \n                  1.979+  \n                \n                \n                                    \n                  (0.954) \n                  (1.041) \n                \n                \n                  sup_parents_p1    \n                  0.072   \n                  0.066   \n                \n                \n                                    \n                  (0.143) \n                  (0.145) \n                \n                \n                  sup_parents_p2    \n                  0.118   \n                  0.122   \n                \n                \n                                    \n                  (0.144) \n                  (0.145) \n                \n                \n                  sup_parents_p3    \n                  0.526***\n                  0.529***\n                \n                \n                                    \n                  (0.126) \n                  (0.126) \n                \n                \n                  sup_friends_p1    \n                  -0.272+ \n                  -0.272+ \n                \n                \n                                    \n                  (0.150) \n                  (0.150) \n                \n                \n                  sup_friends_p2    \n                  0.331*  \n                  0.332*  \n                \n                \n                                    \n                  (0.160) \n                  (0.160) \n                \n                \n                  sup_friends_p3    \n                  0.165   \n                  0.166   \n                \n                \n                                    \n                  (0.130) \n                  (0.131) \n                \n                \n                  se_acad_p1        \n                  -0.208  \n                  -0.212  \n                \n                \n                                    \n                  (0.192) \n                  (0.193) \n                \n                \n                  se_acad_p2        \n                  0.327   \n                  0.328   \n                \n                \n                                    \n                  (0.202) \n                  (0.202) \n                \n                \n                  se_acad_p3        \n                  0.153   \n                  0.169   \n                \n                \n                                    \n                  (0.174) \n                  (0.184) \n                \n                \n                  se_social_p1      \n                  0.355+  \n                  0.345+  \n                \n                \n                                    \n                  (0.200) \n                  (0.204) \n                \n                \n                  se_social_p2      \n                  0.509*  \n                  0.517*  \n                \n                \n                                    \n                  (0.219) \n                  (0.221) \n                \n                \n                  se_social_p3      \n                  0.443** \n                  0.446** \n                \n                \n                                    \n                  (0.161) \n                  (0.161) \n                \n                \n                  factor(region)west\n                          \n                  0.056   \n                \n                \n                                    \n                          \n                  (0.202) \n                \n                \n                  Num.Obs.          \n                  283     \n                  283     \n                \n                \n                  R2                \n                  0.517   \n                  0.517   \n                \n                \n                  R2 Adj.           \n                  0.496   \n                  0.494   \n                \n                \n                  AIC               \n                  1044.7  \n                  1046.6  \n                \n                \n                  BIC               \n                  1095.7  \n                  1101.3  \n                \n                \n                  Log.Lik.          \n                  -508.341\n                  -508.300\n                \n                \n                  RMSE              \n                  1.46    \n                  1.46    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nAgain the cleanest way to interpret the implications of these specifications is derive the conditional marginal effects and assess these for plausibility.\n\nGroup Marginal Effects\nWe see here the different levels expected for different regional responses for changes in each of these input variables.\n\ng = plot_predictions(grp_sum_fit, condition = c(\"sup_parents_p3\", \"region\"), type = \"response\") + ggtitle(\"Counterfactual Shift of Outcome: sup_parents_p3\", \"Holding all else Fixed\")\n\ng1 = plot_predictions(grp_sum_fit, condition = c(\"sup_friends_p1\", \"region\"), type = \"response\") + ggtitle(\"Counterfactual Shift of Outcome: sup_friends_p1\", \"Holding all else Fixed\")\n\ng2 = plot_predictions(grp_sum_fit, condition = c(\"se_acad_p1\", \"region\"), \n                      type = \"response\") + ggtitle(\"Counterfactual Shift of Outcome: se_acad_p1\", \"Holding all else Fixed\")\n\nplot <- ggarrange(g,g1,g2, ncol=1, nrow=3);"
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#model-design-and-conditional-exchangeability",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#model-design-and-conditional-exchangeability",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Model Design and Conditional Exchangeability",
    "text": "Model Design and Conditional Exchangeability\nModelling is nearly always about contrasts and questions of meaningful differences. What we seek to do when we build a model is to find a structure that enables “fair” or “justified” inferences about these contrasts. This is maybe most palpably brought home when we consider cases of causal inference. Here we want to define a meaningful causal estimand - some contrast between treatment group and control group where we can be confident that the two groups are suitably “representative” so that the observed differences represents the effect of the treatment. We are saying that conditional on our model design we consider the potential outcomes to be exchangable. Or another way of putting it is that we have controlled for all aspects of systematic differences between the control and treatment group and this warrants the causal interpretation of the contrast between treatment and control.\nBut as we’ve seen above the cleanest way to interpret almost any regression model is to understand the model design in terms of the marginal effects on the outcome scale. These are just contrasts. All statistical models are, in some sense, focused on finding a structure that licenses an inference about some contrast of interest between the levels of some observed variable and the implied outcomes. In this way we want to include as much structure in our model that would induce the status of conditional exchangeability between the units of study across such group contrasts. This notion of conditional exchangeability is inherently an epistemic notion. We believe that our model is apt to induce the status of conditional exchangeability such that the people in our survey have no systematic difference which biases the measured differences. The implied differences in some marginal effect (while holding all else fixed) is a “fair” representations the same counterfactual adjustment in the population or sub-population defined by the covariate profile. The model implied difference is a proxy for the result of possible future interventions and as such merits our attention in policy design.\nIn particular the view here is that we ought to be deliberate in how we structure our models to license the plausibility of the exchangeability claim. By De Finetti’s theorem a distribution of exchangeable sequence of variables be expressed as mixture of conditional independent variables. So if we specify the conditional distribution correctly, we recover the conditions that warrant inference with a well designed model.The mixture distribution is just the vector of parameters \\(\\boldsymbol{\\beta}\\) upon which we condition our model. Mislevy and Levy highlight this important observation has implications for model development by quoting Judea Pearl:\n\n[C]onditional independence is not a grace of nature for which we must wait passively, but rather a psychological necessity which we satisfy by organising our knowledge in a specific way. An important tool in such an organisation is the identification of intermediate variables that induce conditional independence among observables; if such variables are not in our vocabulary, we create them. In medical diagnosis, for instance, when some symptoms directly influence one another, the medical profession invents a name for that interaction (e.g. “syndrome”, “complication”, “pathological state”) and treats it as a new auxilary variable that induces conditional independence.” - Pearl quoted in Bayesian Psychometric Modeling p61\n\nThe organisation of our data into meaningful categories or clusters is common in machine learning, but similarly in psychometrics we see the prevalence of factor models which are more explicitly causal models where we posit some unifying theme to the cluster of measures. In each tradition we are seeking conditional independence by allowing the model to account for these dependencies in how we structure the model. These choices represent a fruitful aspect of walking down through the garden of forking paths. There is then a tension between the exploratory seeking for some pre-determined story and an experimental seeking of clarity. With this in mind we now turn to types of modeling architecture that open up new expressive possibilities and offer a way to think about the relationships between observed data and latent factors that drive the observed outcomes. This greater expressive power offers different routes to inducing patterns of conditional exchangeability and different risks too."
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#sem-and-indirect-effects",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#sem-and-indirect-effects",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "SEM and Indirect Effects",
    "text": "SEM and Indirect Effects\n\ndef make_indirect_sem(priors): \n\n  coords = {'obs': list(range(len(df))), \n            'indicators': drivers,\n            'indicators_1': ['se_acad_p1','se_acad_p2','se_acad_p3'],\n            'indicators_2': ['se_social_p1','se_social_p2','se_social_p3'],\n            'indicators_3': ['sup_friends_p1','sup_friends_p2','sup_friends_p3'],\n            'indicators_4': [ 'sup_parents_p1','sup_parents_p2','sup_parents_p3'],\n            'indicators_5': ['ls_p1','ls_p2', 'ls_p3'],\n            'latent': ['SUP_F', 'SUP_P'], \n            'latent1': ['SUP_F', 'SUP_P'], \n            'latent_regression': ['SUP_F->SE_ACAD', 'SUP_P->SE_ACAD', 'SUP_F->SE_SOC', 'SUP_P->SE_SOC'],\n            'regression': ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P']\n            }\n\n  obs_idx = list(range(len(df)))\n  with pm.Model(coords=coords) as model:\n    \n    Psi = pm.InverseGamma('Psi', 5, 10, dims='indicators')\n    lambdas_ = pm.Normal('lambdas_1',  priors['lambda'][0], priors['lambda'][1], dims=('indicators_1'))\n    lambdas_1 = pm.Deterministic('lambdas1', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_1'))\n    lambdas_ = pm.Normal('lambdas_2', priors['lambda'][0], priors['lambda'][1], dims=('indicators_2'))\n    lambdas_2 = pm.Deterministic('lambdas2', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_2'))\n    lambdas_ = pm.Normal('lambdas_3', priors['lambda'][0], priors['lambda'][1], dims=('indicators_3'))\n    lambdas_3 = pm.Deterministic('lambdas3', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_3'))\n    lambdas_ = pm.Normal('lambdas_4', priors['lambda'][0], priors['lambda'][1], dims=('indicators_4'))\n    lambdas_4 = pm.Deterministic('lambdas4', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_4'))\n    lambdas_ = pm.Normal('lambdas_5', priors['lambda'][0], priors['lambda'][1], dims=('indicators_5'))\n    lambdas_5 = pm.Deterministic('lambdas5', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_5'))\n    tau = pm.Normal('tau', 3, 10, dims='indicators')\n    kappa = 0\n    sd_dist = pm.Exponential.dist(1.0, shape=2)\n    chol, _, _ = pm.LKJCholeskyCov('chol_cov', n=2, eta=priors['eta'],\n      sd_dist=sd_dist, compute_corr=True)\n    cov = pm.Deterministic(\"cov\", chol.dot(chol.T), dims=('latent', 'latent1'))\n    ksi = pm.MvNormal('ksi', kappa, chol=chol, dims=('obs', 'latent'))\n\n    beta_r = pm.Normal('beta_r', 0, 0.5, dims='latent_regression')\n    beta_r2 = pm.Normal('beta_r2', 0, 1, dims='regression')\n\n    # SE_ACAD ~ SUP_FRIENDS + SUP_PARENTS \n    regression_se_acad = pm.Deterministic('regr_se_acad', beta_r[0]*ksi[obs_idx, 0] + beta_r[1]*ksi[obs_idx, 1])\n    # SE_SOCIAL ~ SUP_FRIENDS + SUP_PARENTS \n    regression_se_social = pm.Deterministic('regr_se_social', beta_r[2]*ksi[obs_idx, 0] + beta_r[3]*ksi[obs_idx, 1])\n\n    # LS ~ SE_ACAD + SE_SOCIAL + SUP_FRIEND + SUP_PARENTS\n    regression = pm.Deterministic('regr', beta_r2[0]*regression_se_acad + beta_r2[1]*regression_se_social +\n                                   beta_r2[2]*ksi[obs_idx, 0] + beta_r2[3]*ksi[obs_idx, 1])\n\n    m0 = tau[0] + regression_se_acad*lambdas_1[0]\n    m1 = tau[1] + regression_se_acad*lambdas_1[1]\n    m2 = tau[2] + regression_se_acad*lambdas_1[2]\n    m3 = tau[3] + regression_se_social*lambdas_2[0]\n    m4 = tau[4] + regression_se_social*lambdas_2[1]\n    m5 = tau[5] + regression_se_social*lambdas_2[2]\n    m6 = tau[6] + ksi[obs_idx, 0]*lambdas_3[0]\n    m7 = tau[7] + ksi[obs_idx, 0]*lambdas_3[1]\n    m8 = tau[8] + ksi[obs_idx, 0]*lambdas_3[2]\n    m9 = tau[9] + ksi[obs_idx, 1]*lambdas_4[0]\n    m10 = tau[10] + ksi[obs_idx, 1]*lambdas_4[1]\n    m11 = tau[11] + ksi[obs_idx, 1]*lambdas_4[2]\n    m12 = tau[12] + regression*lambdas_5[0]\n    m13 = tau[13] + regression*lambdas_5[1]\n    m14 = tau[14] + regression*lambdas_5[2]\n    \n    mu = pm.Deterministic('mu', pm.math.stack([m0, m1, m2, m3, m4, m5, m6, m7,\n                                              m8, m9, m10, m11, m12, m13, m14]).T)\n    _  = pm.Normal('likelihood', mu, Psi, observed=df[drivers].values)\n\n    idata = pm.sample(10_000, chains=4, nuts_sampler='numpyro', target_accept=.99, tune=2000,\n                      idata_kwargs={\"log_likelihood\": True}, random_seed=110)\n    idata.extend(pm.sample_posterior_predictive(idata))\n\n    return model, idata\n\n\nmodel2, idata2 = make_indirect_sem(priors={'eta': 2, 'lambda': [1, 1]})\n\n\n\n\nIndirect Effects and Total Effects Model\n\n\n\nmake_ppc(idata2)\n\n\n\n\n\n\nsummary_df = az.summary(idata2, var_names=['beta_r', 'beta_r2'])\n\ndef calculate_effects(summary_df, var='SUP_P'):\n    #Indirect Paths\n    ## VAR -> SE_SOC ->LS\n    indirect_parent_soc = summary_df.loc[f'beta_r[{var}->SE_SOC]']['mean']*summary_df.loc['beta_r2[SE_SOCIAL]']['mean']\n\n    ## VAR -> SE_SOC ->LS\n    indirect_parent_acad = summary_df.loc[f'beta_r[{var}->SE_ACAD]']['mean']*summary_df.loc['beta_r2[SE_ACAD]']['mean']\n\n    ## Total Indirect Effects\n    total_indirect = indirect_parent_soc + indirect_parent_acad\n\n    ## Total Effects\n    total_effect = total_indirect + summary_df.loc[f'beta_r2[{var}]']['mean']\n\n    return pd.DataFrame([[indirect_parent_soc, indirect_parent_acad, total_indirect, total_effect]], \n                columns=[f'{var} -> SE_SOC ->LS', f'{var} -> SE_ACAD ->LS', f'Total Indirect Effects {var}', f'Total Effects {var}']\n                )\n\nindirect_p = calculate_effects(summary_df, 'SUP_P')\nindirect_f = calculate_effects(summary_df, 'SUP_F')\n\nresiduals_posterior_cov = get_posterior_resids(idata2, 500)\n\n\npy$summary_df |> kable( caption= \"Regression Coefficients Amongst Latent Factors\", digits=2) |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\nRegression Coefficients Amongst Latent Factors\n \n  \n     \n    mean \n    sd \n    hdi_3% \n    hdi_97% \n    mcse_mean \n    mcse_sd \n    ess_bulk \n    ess_tail \n    r_hat \n  \n \n\n  \n    beta_r[SUP_F->SE_ACAD] \n    -0.01 \n    0.04 \n    -0.08 \n    0.07 \n    0.00 \n    0 \n    6737 \n    14576 \n    1 \n  \n  \n    beta_r[SUP_P->SE_ACAD] \n    0.78 \n    0.10 \n    0.60 \n    0.97 \n    0.00 \n    0 \n    2845 \n    5689 \n    1 \n  \n  \n    beta_r[SUP_F->SE_SOC] \n    0.12 \n    0.04 \n    0.05 \n    0.19 \n    0.00 \n    0 \n    6051 \n    12607 \n    1 \n  \n  \n    beta_r[SUP_P->SE_SOC] \n    0.86 \n    0.10 \n    0.68 \n    1.04 \n    0.00 \n    0 \n    2356 \n    4448 \n    1 \n  \n  \n    beta_r2[SE_ACAD] \n    0.28 \n    0.85 \n    -1.32 \n    1.90 \n    0.00 \n    0 \n    31374 \n    29782 \n    1 \n  \n  \n    beta_r2[SE_SOCIAL] \n    0.32 \n    0.82 \n    -1.23 \n    1.85 \n    0.01 \n    0 \n    21163 \n    25065 \n    1 \n  \n  \n    beta_r2[SUP_F] \n    0.05 \n    0.11 \n    -0.16 \n    0.28 \n    0.00 \n    0 \n    19210 \n    21990 \n    1 \n  \n  \n    beta_r2[SUP_P] \n    0.36 \n    0.76 \n    -1.05 \n    1.79 \n    0.00 \n    0 \n    27927 \n    28806 \n    1 \n  \n\n\n\n\n\n\npy$indirect_p |> kable( caption= \"Total and Indirect Effects: Parental Support\", digits=2) |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\nTotal and Indirect Effects: Parental Support\n \n  \n    SUP_P -> SE_SOC ->LS \n    SUP_P -> SE_ACAD ->LS \n    Total Indirect Effects SUP_P \n    Total Effects SUP_P \n  \n \n\n  \n    0.28 \n    0.22 \n    0.5 \n    0.86 \n  \n\n\n\n\npy$indirect_f |> kable( caption= \"Total and Indirect Effects: Friend Support\", digits=4) |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\nTotal and Indirect Effects: Friend Support\n \n  \n    SUP_F -> SE_SOC ->LS \n    SUP_F -> SE_ACAD ->LS \n    Total Indirect Effects SUP_F \n    Total Effects SUP_F \n  \n \n\n  \n    0.0396 \n    -0.0017 \n    0.038 \n    0.091 \n  \n\n\n\n\n\n\nplot_heatmap(py$residuals_posterior_cov, \"Residuals of the Sample Covariances and Model Implied Covariances\", \"A Visual Check of Bayesian Model fit\")"
  }
]