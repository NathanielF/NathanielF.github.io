[
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "import pymc as pm\nimport bambi as bmb\nimport pandas as pd\nimport arviz as az\nfrom bambi.plots import plot_cap\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import stats\n\n:"
  },
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#modelling-improvement-as-lift-across-pooled-experiments",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#modelling-improvement-as-lift-across-pooled-experiments",
    "title": "Examined Algorithms",
    "section": "Modelling Improvement as Lift across Pooled Experiments",
    "text": "Modelling Improvement as Lift across Pooled Experiments\nIt’s useful fact that the Lift measurement of success can be more nicely modelled under a log transform. In this analysis we’ll follow the example in Demetri’s blog: https://dpananos.github.io/posts/2022-07-20-pooling-experiments/ and demonstrate how we can pool information across seperate experiments. In particular we’ll see why this type of modelling is apt for planning expected amount of cumulative gains over successive experiments. First observe how the Lift measurement can can be transformed to facilitate modelling.\n\nnp.random.seed(11)\nfig, axs = plt.subplots(2, 2, figsize=(20, 8))\naxs = axs.flatten()\ncounts_success_control = np.random.normal(10, 2, 100)\ncounts_success_treatment = np.random.normal(8, 2, 100)\naxs[0].hist(counts_success_control, alpha=.3, label='control', edgecolor='black')\naxs[0].hist(counts_success_treatment, alpha=0.2, label='treatment', edgecolor='black')\naxs[0].legend()\naxs[0].set_title(\"Successes in Treatment and Control for 100 Experiments\")\nrr = counts_success_treatment / counts_success_control\naxs[1].hist(rr, alpha=0.3, label='RR/Lift', color='y', edgecolor='black')\naxs[1].legend()\naxs[1].set_title(\"RR/Lift\")\naxs[2].hist(np.log(rr), label='logged Lift', alpha=0.4, color='green', edgecolor='black')\naxs[2].set_title(\"Logged RR/Lift\")\naxs[3].hist(np.exp(np.log(rr)), label='exponentiated Logged Lift', alpha=0.4, color='purple', edgecolor='black')\naxs[3].set_title(\" Exp Logged RR/Lift\")\n\nText(0.5, 1.0, ' Exp Logged RR/Lift')"
  },
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#example-experiments-with-count-successes",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#example-experiments-with-count-successes",
    "title": "Examined Algorithms",
    "section": "Example Experiment(s) with Count Successes",
    "text": "Example Experiment(s) with Count Successes\nThe original data was modelled in the Stan probabilistic programming language. We’ll use this opportunity to translate the code into a pymc implementation.\nIn the scenario we have 12 seperate experiments with 50,000 units on either arm of the experiment. The were desigined to detect conversion in the arm of each trial. Only four of the experiments were successful in the sense that they showed a positive lift distinguishable from statistical noise under a 5% p-value threshold. Management wishes to achieve a total Lift of 2x over the next year. We want to determine how plausible that goal is given our track record so far.\n\ndf_pooling = pd.read_csv('pooling_data.csv')\ndf_pooling\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      n_per_group\n      y_txt\n      y_control\n      estimated_relative_lift\n      pvals\n      experiment\n      estimated_sd_relative_lift\n      estimated_log_relative_lift\n    \n  \n  \n    \n      0\n      1\n      50000\n      551\n      492\n      1.119919\n      0.035510\n      1\n      0.061704\n      0.113256\n    \n    \n      1\n      2\n      50000\n      510\n      490\n      1.040816\n      0.272968\n      2\n      0.062941\n      0.040005\n    \n    \n      2\n      3\n      50000\n      548\n      509\n      1.076621\n      0.119989\n      3\n      0.061233\n      0.073827\n    \n    \n      3\n      4\n      50000\n      537\n      511\n      1.050881\n      0.218777\n      4\n      0.061475\n      0.049629\n    \n    \n      4\n      5\n      50000\n      558\n      508\n      1.098425\n      0.065669\n      5\n      0.060997\n      0.093878\n    \n    \n      5\n      6\n      50000\n      542\n      489\n      1.108384\n      0.051774\n      6\n      0.062048\n      0.102904\n    \n    \n      6\n      7\n      50000\n      533\n      495\n      1.076768\n      0.123029\n      7\n      0.062100\n      0.073964\n    \n    \n      7\n      8\n      50000\n      544\n      468\n      1.162393\n      0.008903\n      8\n      0.062729\n      0.150481\n    \n    \n      8\n      9\n      50000\n      519\n      521\n      0.996161\n      0.512433\n      9\n      0.061694\n      -0.003846\n    \n    \n      9\n      10\n      50000\n      532\n      469\n      1.134328\n      0.024447\n      10\n      0.063023\n      0.126041\n    \n    \n      10\n      11\n      50000\n      555\n      487\n      1.139630\n      0.018467\n      11\n      0.061767\n      0.130704\n    \n    \n      11\n      12\n      50000\n      525\n      497\n      1.056338\n      0.197962\n      12\n      0.062264\n      0.054808\n    \n  \n\n\n\n\n\nfig, axs = plt.subplots(2, 2, figsize=(20, 10))\naxs = axs.flatten()\naxs[0].hist(df_pooling['y_control'], alpha=.3, label='control', edgecolor='black')\naxs[0].hist(df_pooling['y_txt'], alpha=0.2, label='treatment', edgecolor='black')\naxs[0].legend()\naxs[0].set_title(\"Successes in Treatment and Control for 100 Experiments\")\nrr = df_pooling['y_txt'] / df_pooling['y_control']\naxs[1].hist(rr, alpha=0.3, label='RR/Lift', color='y', edgecolor='black')\naxs[1].legend()\naxs[1].set_title(\"RR/Lift\")\naxs[2].hist(np.log(rr), label='logged Lift', alpha=0.4, color='green', edgecolor='black')\naxs[2].set_title(\"Logged RR/Lift\")\naxs[3].hist(np.exp(np.log(rr)), label='exponentiated Logged Lift', alpha=0.4, color='purple', edgecolor='black')\naxs[3].set_title(\" Exp Logged RR/Lift\")\n\nText(0.5, 1.0, ' Exp Logged RR/Lift')"
  },
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#the-model",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#the-model",
    "title": "Examined Algorithms",
    "section": "The Model",
    "text": "The Model\nWe want to pool the information across our 12 experiments and to do so we model them hierarchically as a draws from an overarching normal distribution. The assumptions means we have to set priors on the shape of our parameters. We allow the hierarchical Normal distribution be configured with a centre of mass drawn from the StudentT distribution ensuring that we can have heavy tails in the distribution to account for outlier experiments with massive returns. The code and structure of the model are displayed below:\n\nwith pm.Model() as model:\n     pass\n\nmodel.add_coord('exp_id', list(range(12)), mutable=True)\n\nwith model:\n    exp_id = pm.MutableData(\"exp\", list(range(12)))\n    # Priors for the Hierarchical Log Lift Distribution\n    mu_metric = pm.StudentT('mu_metric', mu=0, sigma=2.5, nu=3)\n    sig_ex = pm.HalfCauchy('sig_ex', 0.01)\n\n    # Priors for the Individual effects for each experiment\n    z_ex = pm.Normal('z_ex', 0, 1, dims='exp_id')\n    \n    # Convenience wrappers for inputting fresh data\n    est_lift_sd = pm.MutableData('est_lift_sd', df_pooling['estimated_sd_relative_lift'], dims='exp_id')\n    est_log_lift = pm.MutableData('est_log_lift', df_pooling['estimated_log_relative_lift'], dims='exp_id')\n\n    ## pooling the indivdual experiemnts, ensuring shrinkage to the overall mean\n    true_log_rr = pm.Deterministic('true_log_rr', mu_metric + z_ex[exp_id]*sig_ex, dims='exp_id')\n\n    ## Likelihood model for Logged Lift using observed values\n    estimated_log_relative_lift = pm.Normal(\"estimated_log_relative_lift\", mu=true_log_rr[exp_id], sigma=est_lift_sd[exp_id], \n                                            observed=est_log_lift, dims=\"exp_id\")\n                \n    estimated_relative_lift = pm.Deterministic('estimated_relative_lift', pm.math.exp(estimated_log_relative_lift[exp_id]), dims='exp_id')\n    \n\n\npm.model_to_graphviz(model)"
  },
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#the-estimation-step",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#the-estimation-step",
    "title": "Examined Algorithms",
    "section": "The Estimation Step",
    "text": "The Estimation Step\nIn the Bayesian workflow we sample both the priors, the prior predictive and posterior_predictive distributions. These allow us to assess model fit and the degree to which our model captures the observed data.\n\nwith model:\n    idata = pm.sample()\n    idata.extend(pm.sample_prior_predictive(samples=50, random_seed=1))\n    idata.extend(pm.sample_posterior_predictive(idata, var_names=[\"estimated_log_relative_lift\", \"mu_metric\", \"sig_ex\"]))\n\nAuto-assigning NUTS sampler...\nINFO:pymc:Auto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nINFO:pymc:Initializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nINFO:pymc:Multiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu_metric, sig_ex, z_ex]\nINFO:pymc:NUTS: [mu_metric, sig_ex, z_ex]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:07<00:00 Sampling 4 chains, 21 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 26 seconds.\nINFO:pymc:Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 26 seconds.\nThere was 1 divergence after tuning. Increase `target_accept` or reparameterize.\nERROR:pymc:There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\nThere were 6 divergences after tuning. Increase `target_accept` or reparameterize.\nERROR:pymc:There were 6 divergences after tuning. Increase `target_accept` or reparameterize.\nThere were 11 divergences after tuning. Increase `target_accept` or reparameterize.\nERROR:pymc:There were 11 divergences after tuning. Increase `target_accept` or reparameterize.\nThere were 3 divergences after tuning. Increase `target_accept` or reparameterize.\nERROR:pymc:There were 3 divergences after tuning. Increase `target_accept` or reparameterize.\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:01<00:00]"
  },
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#plotting-and-convergence-checks",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#plotting-and-convergence-checks",
    "title": "Examined Algorithms",
    "section": "Plotting and Convergence checks",
    "text": "Plotting and Convergence checks\n\naz.plot_trace(idata, var_names=['mu_metric', 'z_ex', 'true_log_rr', 'sig_ex'], figsize=(20, 8));\n\n\n\n\n\naz.plot_ppc(idata, figsize=(20, 7), kind='scatter');\n\n\n\n\n\naz.summary(idata)\n\n/Users/nathanielforde/Documents/Gitlab/async_research_club/.venv/lib/python3.9/site-packages/arviz/stats/diagnostics.py:586: RuntimeWarning: invalid value encountered in double_scalars\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      mu_metric\n      0.083\n      0.018\n      0.050\n      0.118\n      0.000\n      0.000\n      4133.0\n      2445.0\n      1.0\n    \n    \n      z_ex[0]\n      0.041\n      0.987\n      -1.774\n      1.968\n      0.014\n      0.017\n      4873.0\n      2808.0\n      1.0\n    \n    \n      z_ex[1]\n      -0.088\n      0.995\n      -2.017\n      1.718\n      0.014\n      0.017\n      5093.0\n      2906.0\n      1.0\n    \n    \n      z_ex[2]\n      -0.037\n      0.971\n      -1.795\n      1.796\n      0.014\n      0.016\n      4757.0\n      3008.0\n      1.0\n    \n    \n      z_ex[3]\n      -0.079\n      1.011\n      -1.973\n      1.788\n      0.014\n      0.017\n      4925.0\n      2573.0\n      1.0\n    \n    \n      z_ex[4]\n      0.046\n      0.974\n      -1.860\n      1.815\n      0.013\n      0.016\n      5477.0\n      2826.0\n      1.0\n    \n    \n      z_ex[5]\n      0.041\n      0.994\n      -1.815\n      1.892\n      0.015\n      0.019\n      4581.0\n      2276.0\n      1.0\n    \n    \n      z_ex[6]\n      -0.003\n      0.965\n      -1.727\n      1.879\n      0.015\n      0.016\n      4196.0\n      2759.0\n      1.0\n    \n    \n      z_ex[7]\n      0.165\n      0.987\n      -1.713\n      1.959\n      0.015\n      0.017\n      4426.0\n      2569.0\n      1.0\n    \n    \n      z_ex[8]\n      -0.183\n      0.992\n      -2.046\n      1.649\n      0.014\n      0.016\n      4770.0\n      2848.0\n      1.0\n    \n    \n      z_ex[9]\n      0.077\n      0.998\n      -1.664\n      2.037\n      0.014\n      0.017\n      4990.0\n      2714.0\n      1.0\n    \n    \n      z_ex[10]\n      0.115\n      0.998\n      -1.752\n      1.988\n      0.014\n      0.018\n      5001.0\n      2612.0\n      1.0\n    \n    \n      z_ex[11]\n      -0.082\n      0.997\n      -1.909\n      1.840\n      0.015\n      0.018\n      4634.0\n      2564.0\n      1.0\n    \n    \n      sig_ex\n      0.010\n      0.010\n      0.000\n      0.027\n      0.000\n      0.000\n      2690.0\n      1933.0\n      1.0\n    \n    \n      true_log_rr[0]\n      0.084\n      0.021\n      0.046\n      0.126\n      0.000\n      0.000\n      4049.0\n      2457.0\n      1.0\n    \n    \n      true_log_rr[1]\n      0.082\n      0.021\n      0.042\n      0.121\n      0.000\n      0.000\n      3788.0\n      2655.0\n      1.0\n    \n    \n      true_log_rr[2]\n      0.083\n      0.022\n      0.042\n      0.123\n      0.000\n      0.000\n      4097.0\n      2861.0\n      1.0\n    \n    \n      true_log_rr[3]\n      0.082\n      0.022\n      0.040\n      0.121\n      0.000\n      0.000\n      3632.0\n      2416.0\n      1.0\n    \n    \n      true_log_rr[4]\n      0.084\n      0.021\n      0.044\n      0.123\n      0.000\n      0.000\n      4032.0\n      2419.0\n      1.0\n    \n    \n      true_log_rr[5]\n      0.084\n      0.022\n      0.042\n      0.124\n      0.000\n      0.000\n      4236.0\n      2668.0\n      1.0\n    \n    \n      true_log_rr[6]\n      0.083\n      0.021\n      0.045\n      0.123\n      0.000\n      0.000\n      3702.0\n      2731.0\n      1.0\n    \n    \n      true_log_rr[7]\n      0.086\n      0.022\n      0.043\n      0.126\n      0.000\n      0.000\n      4035.0\n      2712.0\n      1.0\n    \n    \n      true_log_rr[8]\n      0.080\n      0.022\n      0.040\n      0.121\n      0.000\n      0.000\n      3619.0\n      2592.0\n      1.0\n    \n    \n      true_log_rr[9]\n      0.085\n      0.022\n      0.042\n      0.123\n      0.000\n      0.000\n      3831.0\n      2801.0\n      1.0\n    \n    \n      true_log_rr[10]\n      0.085\n      0.022\n      0.045\n      0.124\n      0.000\n      0.000\n      3814.0\n      2450.0\n      1.0\n    \n    \n      true_log_rr[11]\n      0.082\n      0.022\n      0.043\n      0.124\n      0.000\n      0.000\n      3962.0\n      2829.0\n      1.0\n    \n    \n      estimated_relative_lift[0]\n      1.120\n      0.000\n      1.120\n      1.120\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[1]\n      1.041\n      0.000\n      1.041\n      1.041\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[2]\n      1.077\n      0.000\n      1.077\n      1.077\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[3]\n      1.051\n      0.000\n      1.051\n      1.051\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[4]\n      1.098\n      0.000\n      1.098\n      1.098\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[5]\n      1.108\n      0.000\n      1.108\n      1.108\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[6]\n      1.077\n      0.000\n      1.077\n      1.077\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[7]\n      1.162\n      0.000\n      1.162\n      1.162\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[8]\n      0.996\n      0.000\n      0.996\n      0.996\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[9]\n      1.134\n      0.000\n      1.134\n      1.134\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[10]\n      1.140\n      0.000\n      1.140\n      1.140\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      estimated_relative_lift[11]\n      1.056\n      0.000\n      1.056\n      1.056\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n  \n\n\n\n\n\nSimulate Draws from the Posterior and Calculate Lift\nThe Stan implementation has the functionality to enable the calculation of generated quantities on the fly within a model run. We need to replicate that functionality outside of our model making use of the estimated posterior distributions on the model parameters. We calculate the effect size engendered by an observed difference in proportions of conversion across the experiments, then calculate whether the simulated was large enough that we had the power to detect it against a baseline of 1% conversion. The quantities are used to define the amount of detected lift in our posterior distribution. This in turn can be used to project the amount of cumulative lift we will see over 12 experiments.\n\nidata.stack(sample=[\"chain\", \"draw\"], inplace=True)\n\n\ngenerated_quanties = []\nfor i in range(12):\n    generated_data = pd.DataFrame({'mu_metric': idata['posterior']['mu_metric'].values,\n        'sig_ex': idata['posterior']['sig_ex'].values\n        }\n    )\n    generated_data['log_rr_over_the_year'] = generated_data.apply(lambda x: np.random.normal(x['mu_metric'], x['sig_ex'], 1)[0], axis=1)\n    generated_data['rr_over_the_year'] = np.exp(generated_data['log_rr_over_the_year'])\n    ## Calculate effect size for proportions against base 0.01\n    generated_data['es'] = generated_data.apply(lambda x: 2*np.arcsin(np.sqrt(x['rr_over_the_year'] * 0.01)) -  2*np.arcsin(np.sqrt(0.01)), axis=1)\n    ## Calculate power based on difference from baseline with known sample size\n    generated_data['power'] = generated_data.apply(lambda x: 1 - stats.norm.cdf( 1.644854 - x['es'] * np.sqrt(50_000/2), 0, 1), axis=1)\n    ## Weight lift by our ability to detect given power in experiment\n    generated_data['detected_lift'] = generated_data['power']* np.log(generated_data['rr_over_the_year'])\n    generated_data['experiment'] = i\n    generated_data['draw'] = generated_data.index\n    generated_quanties.append(generated_data)\n\nforecast_df = pd.concat(generated_quanties)\nforecast_df\n\n\n\n\n\n  \n    \n      \n      mu_metric\n      sig_ex\n      log_rr_over_the_year\n      rr_over_the_year\n      es\n      power\n      detected_lift\n      experiment\n      draw\n    \n  \n  \n    \n      0\n      0.053614\n      0.003751\n      0.054232\n      1.055729\n      0.005526\n      0.220312\n      0.011948\n      0\n      0\n    \n    \n      1\n      0.089769\n      0.006848\n      0.078595\n      1.081766\n      0.008058\n      0.355404\n      0.027933\n      0\n      1\n    \n    \n      2\n      0.082105\n      0.008848\n      0.075538\n      1.078464\n      0.007739\n      0.336777\n      0.025440\n      0\n      2\n    \n    \n      3\n      0.076757\n      0.031000\n      0.098424\n      1.103431\n      0.010142\n      0.483548\n      0.047593\n      0\n      3\n    \n    \n      4\n      0.076757\n      0.031000\n      0.081759\n      1.085194\n      0.008389\n      0.375086\n      0.030667\n      0\n      4\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3995\n      0.102766\n      0.012876\n      0.118111\n      1.125369\n      0.012232\n      0.613782\n      0.072494\n      11\n      3995\n    \n    \n      3996\n      0.066032\n      0.000387\n      0.065700\n      1.067907\n      0.006714\n      0.279849\n      0.018386\n      11\n      3996\n    \n    \n      3997\n      0.062600\n      0.007694\n      0.067979\n      1.070343\n      0.006951\n      0.292589\n      0.019890\n      11\n      3997\n    \n    \n      3998\n      0.090900\n      0.005162\n      0.103968\n      1.109565\n      0.010729\n      0.520525\n      0.054118\n      11\n      3998\n    \n    \n      3999\n      0.064391\n      0.010588\n      0.065984\n      1.068209\n      0.006743\n      0.281419\n      0.018569\n      11\n      3999\n    \n  \n\n48000 rows × 9 columns\n\n\n\n\n\nGenerate Cumulative Lift Curves for N-Experiments\nWe can now line up the draws for each of our experiments and calculate the cumulative lift by taking the cumulative sum and then exponentiating to return us to the raw Lift scale.\n\ndraws_per_experiment = forecast_df.pivot('experiment', 'draw', 'detected_lift')\n## Probability of Independent events sum on the log scale\ndraws_per_experiment = np.exp(draws_per_experiment.cumsum()).T\ndraws_per_experiment\n\n\n\n\n\n  \n    \n      experiment\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n    \n    \n      draw\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      1.108264\n      1.184080\n      1.270116\n      1.344759\n      1.382252\n      1.493910\n      1.574159\n      1.636984\n      1.770258\n      1.896812\n      2.051490\n      2.125217\n    \n    \n      1\n      1.111839\n      1.173511\n      1.255733\n      1.316744\n      1.336851\n      1.431039\n      1.513762\n      1.624544\n      1.733171\n      1.984078\n      2.259019\n      2.370773\n    \n    \n      2\n      1.019156\n      1.058173\n      1.080910\n      1.120521\n      1.163374\n      1.190512\n      1.231332\n      1.257548\n      1.287433\n      1.320996\n      1.347873\n      1.399662\n    \n    \n      3\n      1.027886\n      1.062590\n      1.088567\n      1.124628\n      1.149337\n      1.189305\n      1.218267\n      1.258150\n      1.290688\n      1.328989\n      1.368187\n      1.412470\n    \n    \n      4\n      1.022567\n      1.037980\n      1.046979\n      1.064810\n      1.079649\n      1.086973\n      1.094045\n      1.097738\n      1.104774\n      1.108017\n      1.125943\n      1.132601\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3995\n      1.022991\n      1.064212\n      1.091823\n      1.118289\n      1.166716\n      1.182341\n      1.208105\n      1.252252\n      1.310564\n      1.319768\n      1.351317\n      1.375962\n    \n    \n      3996\n      1.039176\n      1.084013\n      1.129208\n      1.178795\n      1.230675\n      1.283502\n      1.337933\n      1.395778\n      1.451701\n      1.512018\n      1.576763\n      1.643312\n    \n    \n      3997\n      1.022346\n      1.057272\n      1.118954\n      1.183094\n      1.205393\n      1.225715\n      1.343847\n      1.407721\n      1.506250\n      1.565272\n      1.670593\n      1.755775\n    \n    \n      3998\n      1.037186\n      1.077082\n      1.122035\n      1.166914\n      1.206876\n      1.252481\n      1.300103\n      1.352597\n      1.397989\n      1.453273\n      1.510253\n      1.565866\n    \n    \n      3999\n      1.026173\n      1.057943\n      1.072373\n      1.120320\n      1.155082\n      1.198584\n      1.220144\n      1.246629\n      1.275155\n      1.311698\n      1.351460\n      1.378449\n    \n  \n\n4000 rows × 12 columns"
  },
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#plotting-predicted-trajectories",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#plotting-predicted-trajectories",
    "title": "Examined Algorithms",
    "section": "Plotting Predicted Trajectories",
    "text": "Plotting Predicted Trajectories\nNote how there are cumulative lift curves that go up and down due to the possibility of failed experiments. We plot here a a sample of 100 possible curves along with the key quantiles.\n\nfig, ax = plt.subplots(figsize=(20, 10))\nax.plot(draws_per_experiment.sample(100).T, color='grey', alpha=0.2);\nax.plot(draws_per_experiment.mean(), color='slateblue', label='Expected', linewidth=5)\nax.plot(draws_per_experiment.quantile(0.75), color='slateblue', label='p75', alpha=0.5, linewidth=3)\nax.plot(draws_per_experiment.quantile(0.25), color='slateblue', label='p25', alpha=0.5, linewidth=3)\nax.plot(draws_per_experiment.quantile(0.99), color='slateblue', label='p99', alpha=0.5, linewidth=3)\nax.set_title(\"Credible Cumulative Lift Curves\", fontsize=20)\nax.set_ylabel(\"Lift\")\nax.set_xlabel(\"Number of Experiments\")\nax.legend()\n\n<matplotlib.legend.Legend at 0x180b0c520>\n\n\n\n\n\nWe can now also ask how credible a target lift of 2x over the course of 12 experiments really is?\n\nfig, ax = plt.subplots(figsize=(20, 6))\nN, bins, patches = ax.hist(draws_per_experiment[11], color='slateblue', edgecolor='grey', alpha=0.3, bins=30);\nax.axvline(2)\nfor i in range(9, len(patches)):\n    patches[i].set_facecolor('red')\nax.set_title(\"Proportion of Credible Curves which achieve a cumulative Lift more than 2\", fontsize=20)\nax.set_xlabel(\"Lift\")\n\nText(0.5, 0, 'Lift')"
  },
  {
    "objectID": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#generate-new-views-with-new-experimental-data",
    "href": "posts/post-with-code/pooling_experiments/nathaniel_forde_pooling.html#generate-new-views-with-new-experimental-data",
    "title": "Examined Algorithms",
    "section": "Generate new views with new Experimental data",
    "text": "Generate new views with new Experimental data\nWe can make use of PYMC’s mutable data input to feed in new experimental data and try and predict implied effects using the posterior predictive distribution. We will continue to assume that we have 50,000 observations per experiment on each arm. But now let’s assume we have 20 experiments, with a similar pattern of Lift observed on each of the experiments.\n\nN = 20\ncontrol = np.random.gumbel(2.2, 0.07, N)\ntreatment = np.random.gumbel(2.0, 0.05, N)\nsd = np.random.normal(0.1, 0.01, N)\n\nfig, axs = plt.subplots(2, 2, figsize=(20, 8))\naxs = axs.flatten()\naxs[0].hist(control, alpha=.3, label='control', edgecolor='black')\naxs[0].hist(treatment, alpha=0.2, label='treatment', edgecolor='black')\naxs[0].legend()\naxs[0].set_title(\"Successes in Treatment and Control for 100 Experiments\")\nrr = treatment / control\nlog_rr = np.log(rr)\naxs[1].hist(rr, alpha=0.3, label='RR/Lift', color='y', edgecolor='black')\naxs[1].legend()\naxs[1].set_title(\"RR/Lift\")\naxs[2].hist(log_rr, label='logged Lift', alpha=0.4, color='green', edgecolor='black')\naxs[2].set_title(\"Logged RR/Lift\")\naxs[3].hist(np.exp(log_rr), label='exponentiated Logged Lift', alpha=0.4, color='purple', edgecolor='black')\naxs[3].set_title(\" Exp Logged RR/Lift\")\n\nText(0.5, 1.0, ' Exp Logged RR/Lift')\n\n\n\n\n\nHere we can pass in the new data to our old model and re-generate the posterior predictive distribution.\n\n# Do the posterior predictions\ncoords = {'exp_id': list(range(N))}\nwith model:\n    pm.set_data({\"est_log_lift\": log_rr, 'exp':list(range(N)), 'est_lift_sd': sd}, coords=coords)\n    ppc = pm.sample_posterior_predictive(idata, var_names=[\"estimated_log_relative_lift\"])\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00<00:00]\n    \n    \n\n\n\naz.plot_ppc(ppc, figsize=(20, 10), kind='scatter');\n\n\n\n\n\nppc.stack(sample=[\"chain\", \"draw\"], inplace=True)\npredicted = ppc['posterior_predictive']['estimated_log_relative_lift'].to_dataframe().reset_index()\npredicted\n\n\n\n\n\n  \n    \n      \n      exp_id\n      chain\n      draw\n      estimated_log_relative_lift\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      0.129563\n    \n    \n      1\n      0\n      0\n      1\n      0.160167\n    \n    \n      2\n      0\n      0\n      2\n      0.015636\n    \n    \n      3\n      0\n      0\n      3\n      0.131316\n    \n    \n      4\n      0\n      0\n      4\n      0.155715\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      79995\n      19\n      3\n      995\n      0.047107\n    \n    \n      79996\n      19\n      3\n      996\n      0.152401\n    \n    \n      79997\n      19\n      3\n      997\n      0.257834\n    \n    \n      79998\n      19\n      3\n      998\n      0.037314\n    \n    \n      79999\n      19\n      3\n      999\n      -0.037456\n    \n  \n\n80000 rows × 4 columns\n\n\n\n\npredicted['rr_over_the_year'] = np.exp(predicted['estimated_log_relative_lift'])\n    ## Calculate effect size for proportions against base 0.01\npredicted['es'] = predicted.apply(lambda x: 2*np.arcsin(np.sqrt(x['rr_over_the_year'] * 0.01)) -  2*np.arcsin(np.sqrt(0.01)), axis=1)\n    ## Calculate power based on difference from baseline with known sample size\npredicted['power'] = predicted.apply(lambda x: 1 - stats.norm.cdf( 1.644854 - x['es'] * np.sqrt(50_000/2), 0, 1), axis=1)\n    ## Weight lift by our ability to detect given power in experiment\npredicted['detected_lift'] = predicted['power']* np.log(predicted['rr_over_the_year'])\npredicted\n\n\n\n\n\n  \n    \n      \n      exp_id\n      chain\n      draw\n      estimated_log_relative_lift\n      rr_over_the_year\n      es\n      power\n      detected_lift\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      0.129563\n      1.138330\n      0.013457\n      0.685424\n      0.088805\n    \n    \n      1\n      0\n      0\n      1\n      0.160167\n      1.173706\n      0.016767\n      0.842840\n      0.134995\n    \n    \n      2\n      0\n      0\n      2\n      0.015636\n      1.015759\n      0.001578\n      0.081447\n      0.001273\n    \n    \n      3\n      0\n      0\n      3\n      0.131316\n      1.140329\n      0.013646\n      0.695916\n      0.091385\n    \n    \n      4\n      0\n      0\n      4\n      0.155715\n      1.168493\n      0.016282\n      0.823705\n      0.128263\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      79995\n      19\n      3\n      995\n      0.047107\n      1.048235\n      0.004791\n      0.187461\n      0.008831\n    \n    \n      79996\n      19\n      3\n      996\n      0.152401\n      1.164627\n      0.015922\n      0.808573\n      0.123227\n    \n    \n      79997\n      19\n      3\n      997\n      0.257834\n      1.294124\n      0.027678\n      0.996847\n      0.257021\n    \n    \n      79998\n      19\n      3\n      998\n      0.037314\n      1.038019\n      0.003786\n      0.147719\n      0.005512\n    \n    \n      79999\n      19\n      3\n      999\n      -0.037456\n      0.963237\n      -0.003729\n      0.012726\n      -0.000477\n    \n  \n\n80000 rows × 8 columns\n\n\n\n\npredicted_curves  = predicted.pivot(['chain', 'draw'], 'exp_id', 'detected_lift')\npredicted_curves\n\n\n\n\n\n  \n    \n      \n      exp_id\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n      18\n      19\n    \n    \n      chain\n      draw\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      0.088805\n      0.001197\n      -0.000484\n      0.171321\n      0.307768\n      -0.000114\n      -0.000358\n      -0.000050\n      0.119042\n      0.035483\n      0.008818\n      0.017530\n      0.126463\n      0.044351\n      0.021847\n      0.077193\n      0.055479\n      8.898377e-03\n      0.010240\n      0.001336\n    \n    \n      1\n      0.134995\n      -0.000085\n      0.169952\n      -0.000517\n      0.005521\n      0.006376\n      -0.000489\n      0.134169\n      0.245929\n      0.038492\n      -0.000515\n      0.002976\n      0.209037\n      0.062656\n      0.002869\n      0.139614\n      0.254202\n      4.575356e-02\n      0.286258\n      0.206261\n    \n    \n      2\n      0.001273\n      0.191886\n      0.001264\n      0.090177\n      -0.000060\n      0.330653\n      0.202727\n      -0.000368\n      0.015053\n      0.305181\n      -0.000259\n      0.049178\n      -0.000262\n      0.015266\n      0.090864\n      0.060138\n      0.008056\n      1.787840e-01\n      0.003004\n      -0.000283\n    \n    \n      3\n      0.091385\n      -0.000165\n      0.175993\n      0.210623\n      -0.000398\n      0.000092\n      0.008886\n      0.169622\n      0.016613\n      -0.000175\n      -0.000333\n      -0.000151\n      0.069115\n      0.003197\n      0.009931\n      0.008144\n      0.069742\n      5.125832e-04\n      0.223892\n      -0.000056\n    \n    \n      4\n      0.128263\n      -0.000264\n      0.029506\n      0.074044\n      -0.000028\n      0.013448\n      0.034857\n      0.181062\n      0.059433\n      0.119991\n      0.022186\n      0.181830\n      0.078825\n      0.042061\n      0.229243\n      0.173755\n      0.131305\n      -3.022104e-08\n      0.019529\n      0.023409\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3\n      995\n      0.010141\n      0.046869\n      0.261628\n      0.187072\n      0.207434\n      0.041217\n      0.029719\n      0.098765\n      -0.000002\n      0.050363\n      -0.000449\n      0.215972\n      0.084759\n      0.035756\n      0.187955\n      0.000416\n      0.005068\n      -3.511800e-04\n      0.208030\n      0.008831\n    \n    \n      996\n      0.020431\n      0.113550\n      -0.000424\n      0.009520\n      0.283799\n      0.010694\n      -0.000157\n      0.030434\n      0.104618\n      0.032070\n      0.117800\n      0.152587\n      -0.000257\n      -0.000003\n      -0.000008\n      0.029872\n      0.224894\n      2.599086e-01\n      0.092621\n      0.123227\n    \n    \n      997\n      0.082644\n      0.004597\n      0.028806\n      0.001567\n      0.009130\n      0.000728\n      0.010956\n      0.001111\n      0.212021\n      0.240963\n      0.067241\n      0.182531\n      0.301767\n      0.221201\n      0.000654\n      0.001957\n      0.137934\n      2.372076e-01\n      0.014318\n      0.257021\n    \n    \n      998\n      -0.000116\n      0.085200\n      -0.000517\n      0.005099\n      0.029492\n      0.040047\n      -0.000026\n      0.002956\n      0.104428\n      0.082584\n      0.000486\n      0.001877\n      0.032360\n      0.001402\n      0.003078\n      0.001904\n      -0.000390\n      -1.231683e-04\n      -0.000350\n      0.005512\n    \n    \n      999\n      0.002641\n      0.138605\n      0.201605\n      0.103506\n      0.180542\n      0.034928\n      0.012054\n      -0.000369\n      0.047570\n      0.086689\n      0.257165\n      0.102930\n      0.221825\n      0.178257\n      0.010981\n      0.324702\n      0.091735\n      2.999001e-02\n      0.200721\n      -0.000477\n    \n  \n\n4000 rows × 20 columns\n\n\n\n\nfig, axs = plt.subplots(1, 2, figsize=(20, 6))\nnp.random.seed(19)\naxs = axs.flatten()\nN, bins, patches = axs[0].hist(np.exp(predicted_curves.cumsum(axis=1))[11], color='slateblue', edgecolor='grey', alpha=0.3, bins=40);\naxs[0].axvline(2)\nfor i in range(1, len(patches)):\n    patches[i].set_facecolor('red')\nax.set_title(\"Proportion of Credible Curves which achieve a cumulative Lift more than 2\", fontsize=20)\nax.set_xlabel(\"Lift\")\naxs[0].set_title(\"Proportion of Curves which achieve 2x Lift after 20 experiments\")\naxs[0].legend()\naxs[1].plot(np.exp(predicted_curves.cumsum(axis=1)).sample(100).T, color='grey');\naxs[1].plot(np.exp(predicted_curves.cumsum(axis=1)).mean(), color='red', label='Expected Growth curve');\naxs[1].set_title(\"Sample Set of Possible Growth Curves\")\naxs[1].set_ylim(0, 10)\n\nWARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n(0.0, 10.0)"
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html",
    "href": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html",
    "title": "Factor Analysis and Construct Validity in Psychology",
    "section": "",
    "text": "In a prescient paper in the 1970s Paul Meehl wrote about the lack of cumulative success in the psychologocial sciences, and how this should be attributed to poor methodology rather than the sheer difficulty of the subject. He elaborates an impressive list of problems for modeling any psychological process. Common themes criss-cross the list and interact with one another in ways which could make you despair for the discipline. So it is, I think, somewhat surprising that Meehl locates the main problem not in the subject, but in the method of significance testing.\nWe’ll narrow our focus shortly, but first consider the breadth of the issues."
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#meehls-problems-plaguing-psychology",
    "href": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#meehls-problems-plaguing-psychology",
    "title": "Factor Analysis and Construct Validity in Psychology",
    "section": "Meehl’s Problems Plaguing Psychology",
    "text": "Meehl’s Problems Plaguing Psychology\n\n\n\n\n\n\n\nProblem\nDescription\n\n\n\n\nResponse-Classification Problem\nDifficulty attributing mental process to observed behaviour\n\n\nSituation-Taxonomy Problem\nDifficulty isolating the stimulus from rough description of environment\n\n\nUnit of Measurement\nChoice of scale e.g. ratio or interval, continuous or discrete\n\n\nIndividual Differences\nCommon psychological dispositions arise from idiosyncratic mixture of influences\n\n\nPolygenetic Hereditry\nCommon psychological dispositions have complex causal roots\n\n\nDivergent Causality\nVery sensitive to initial conditions, slight differences at source result in large differences in outcomes\n\n\nIdiographic Problem\nOften relates to the specific discovery of facts rather than generalisable laws\n\n\nUnknown Critical Events\nPaucity of medical history or local context\n\n\nNuisance Variables\nDifficulty deciphering wealth of related variables\n\n\nFeedback Loops\nInterventions lead to changing behaviour, disrupting the study\n\n\nAutoCatalytic Processes\nIndividual influences on their own psychological disposition during tests\n\n\nRandom Walk\nDifferences in dispositional response often due to random flux, rather than different causal influence\n\n\nSheer Number of Variables\nCumulative influence of small random-drift can have decisive impact on the psychological disposition\n\n\nImportance of Cultural Factors\nWeight of an individual variable may vary with cultural context\n\n\nContext-Dependent Stochastilogicals\nAny derived rule of behaviour is likely only probabilisitic and subject to contextual variation.\n\n\nOpen Concepts\nLatent psychological factors under study “evolve” as we include/remove indicative measures\n\n\nIntentionality, Purpose and Meaning\nPurpose drives behaviour and changes in behaviour\n\n\nRule Governance\nPeople follow rules influencing their behaviour\n\n\nUniquely Human Events & Powers\nThere are some behaviours which have no animal/ape analogies to compare. Limiting data\n\n\nEthical Constraints on Research\nConstraints on some decisive testing methodologies due to ethical abuses.\n\n\n\nWe’re going to focus on two of the problems that most clearly relate the issue of significance testing: open concepts, context-dependent stochastilogicals. These issues are tightly coupled with the ability to falsify psychological hypotheses since they both serve as reasons to doubt contrary evidence and therefore deny us a decisive rejection of our hypotheses even when they conflict with observable data. This difficulty directly undermines the paradigm of null-hypothesis testing in psychology, since they imply that we are incapable of placing the null under severe scrutiny."
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#the-recipe-open-concepts-and-context-sensitive-stocastologicals",
    "href": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#the-recipe-open-concepts-and-context-sensitive-stocastologicals",
    "title": "Factor Analysis and Construct Validity in Psychology",
    "section": "The Recipe: Open Concepts and Context Sensitive Stocastologicals",
    "text": "The Recipe: Open Concepts and Context Sensitive Stocastologicals\nThe ugly word “stochastological” is to be contrasted with the, perhaps more familar but also pompous, notion of a nomological inference - one which is valid by appeal to a wholly general law.\n\\[ \\text{(Nomological): } \\forall x(Fx \\rightarrow Gx) \\]\nMeehl argues that the idiosyncratic and individual specific patterns of causation in psychology short-circuit any appeals to over-arching rules. Even when we can consistently measure a disposition common across individuals, the nature of the observed patterns support probabilistic inference not law-like generalisations. Even then the underlying probabilities are liable to change with the context. You might be prone to defer to authority in 9 of 10 cases, but always exhibit knee jerk refusal when prompted by a political opponent.\n\\[ \\text{(Stochastic): }  \\underbrace{P(Gx | Fx) \\geq 0.90}_{context = c} \\]\nCombine this issue with contingencies of measurement and the dynamic nature of the scales, and we have a recipe for undermining any null-hypothsis significance test. Consider iteritive model building which tests for predictive aspects of some latent factor, say risk-aversion, on performance at a related task. Say the factor was initially derived from a set of observational measures:\n\\[ \\underbrace{ feature4,  \\overbrace{ \\text{ feature5, } \\overbrace{feature1 , feature2, feature3 }^{\\text{initial feature measures}}}^{\\text{final feature measures}}}_{\\text{Intuitive Concept}} \\twoheadrightarrow \\text{Construct} \\]\nNow each iteration was designed to test the imagined psychological constructand the features are observational traits associated with risk-taking. Each additional feature seemed reasonable at time. We may even have gotten better predictive accuracy. But the model build and the changing measurements raises question over what we’re constructing and whether it truly “captures” the pre-theoretical psychological concept.\nNow we have a measure of risk-aversion and we try to render an intuitive hypothesis mathematically precise with respect to our construct. This is itself an art, but for the moment assume we can state \\((H)\\). Since the hypothesis test is supposed to infer the falsity of the assumptions from evidence contrary to the main hypothesis i.e. if when assuming the hypothesis and our auxilary commitments (regarding measurement and context) we find evidence inconsistent with our expectations, then we should reject the hypothesis! In practice the hypothesis is ussually far more entrenched in the minds of the experimenters than the extensive auxillary commitments, so we normally seek the source of predictive error in mistakes made than with the key hypothesis.\n\\[ (H \\wedge A) \\rightarrow O , \\neg O \\vdash \\neg (H \\wedge A) \\text{ ....so not A}\\]\nIn this way we preserve the hypothesis and remove it from exposure to a strict test of its accuracy. This experiment is really only validating the consistency of the data with a nebulous range of auxilary commitments, and as such allows the experimenter to move the goal-posts almost at whim.\nMeehl concedes that there may be some justification for this procedure if we would grant that \\((H)\\) has in some sense a greater “verisimilitude” than the auxilary commitments. Psychology differs from other disciplines in that the objects of measurement and the manner of measurement are not tightly bound. The truth “content” of claims about measurement of length, for instance, stand or fall with claims about the measurement instrument - they are so intimately connected that the measurement apparatus is almost definitional. You might object that appeals to versimilitude in some way puts the cart before the horse. We’re designing an experiement to test \\((H)\\), how can we presume it’s truth in testing!? This is fair but only points to difficulty of making \\((H)\\) precise and the distance between mental phenomena and our measurement of it"
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#open-concepts-and-factor-analysis",
    "href": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#open-concepts-and-factor-analysis",
    "title": "Factor Analysis and Construct Validity in Psychology",
    "section": "Open Concepts and Factor Analysis",
    "text": "Open Concepts and Factor Analysis\nIf we’re lucky we have a clear idea of how to measure the psychological phenomenon of interest and you can devise a survey to capture the details. If instead you just have some data and you think there might be a latent factor driving the observations, then the the factor analysis effectively tries to group the observed variable by their correlations. The underlying statistical model for \\(m\\) factors states:\n\nAssume the model \\[\\mathbf{X} = \\mathbf{\\mu}^{p \\times 1}  + \\mathbf{L}^{p \\times m} \\mathbf{F}^{m \\times 1} +\\epsilon^{p \\times 1} \\]\nwhere \\(\\mathbf{X}\\) is our data matrix recording the observational data while \\(\\mu\\) is the mean vector with entries for each column in \\(\\mathbf{X}\\). So our observational data is deemed a function modifying the multivariate mean as a linear function of latent factors \\(\\mathbf{F}\\) with factor loadings \\(l_{i, j} \\in \\mathbf{L}\\), for each of the observed variables \\(i\\) and for each \\(j\\) of the \\(m\\) factors.\nThe model assumes \\(\\mathbf{F}\\) and \\(\\epsilon\\) are independent and \\(E(\\mathbf{F}) = \\mathbf{0}, Cov(\\mathbf{F}) = \\mathbf{I}\\) In addition note that \\(E(\\epsilon) = \\mathbf{0}\\) and \\(Cov(\\epsilon) = \\Psi\\) where \\(\\Psi\\) is a diagonal matrix.\nThen we can show that \\[Cov(\\mathbf{X}) = \\mathbf{LL}^{'} + \\Psi\\]\n\nThat’s a bit abstract, but the point is just that each factor is a linear construct of the observed data, and we can choose how many constructs to build. An important consequence of this fact is that we can express the variance of each observed feature in terms of the loadings and a random variance, so if we can explain a high portion of their variance in a low number of factors we can be reasonably sure that the dimensional reduction remains representative of the diversity in the original data set.\n\\[ Var(X_i) = \\underbrace{l_{i, j}^{2} + l_{i, 2}^{2} ... l_{i, m}^{2}}\\_{communalities} + \\psi_{i}\\]\n\nProof. \\[ (\\mathbf{X} - \\mu) = (\\mathbf{L}\\mathbf{F} + \\epsilon)\\] \\[ \\Rightarrow (\\mathbf{X} - \\mu)(\\mathbf{X} - \\mu)^{'} = (\\mathbf{L}\\mathbf{F} + \\epsilon)(\\mathbf{L}\\mathbf{F} + \\epsilon)^{'}\\] \\[ = (\\mathbf{L}\\mathbf{F} + \\epsilon)((\\mathbf{L}\\mathbf{F})^{'} + \\epsilon^{'}) \\] \\[ = \\mathbf{L}\\mathbf{F}(\\mathbf{L}\\mathbf{F})^{'} + \\epsilon(\\mathbf{L}\\mathbf{F})^{'} + \\mathbf{L}\\mathbf{F}\\epsilon^{'} + \\epsilon\\epsilon^{'}\\] \\[ \\Rightarrow E((\\mathbf{X} - \\mu)(\\mathbf{X} - \\mu)^{'}) = E( (\\mathbf{L}\\mathbf{F} + \\epsilon)(\\mathbf{L}\\mathbf{F} + \\epsilon)^{'})\\] \\[ \\Rightarrow Cov(\\mathbf{X}) = \\mathbf{L}E(\\mathbf{F}\\mathbf{F}^{'})\\mathbf{L}^{'} + E(\\epsilon\\mathbf{F}^{'})\\mathbf{L}^{'} + \\mathbf{L}E(\\mathbf{F}\\epsilon^{'}) + E(\\epsilon\\epsilon^{'}) \\] \\[ = \\mathbf{L}\\mathbf{L}^{'} + \\Psi \\]\n\n\nEstimating the latent factor values\nVarious techniques can be applied to estimate the loadings \\(\\mathbf{L}\\) based on a strategic decomposition of sample covariance matrix to derive the principal factors. The typical technique is to use the eigenvalue decomposition, but a maximum likelihood method is also feasible. It is another question altogeher for how to estimate the factor scores \\(f_{j} \\in \\mathbf{F}\\) from our derived factor loadings.\nThis is a two step, where we base an estimate on a set of prior estimates. One or more latent factors are assumed, observable features postulated to be related are grouped and from these we derive a recipe for constructing the latent factor as a composite of the observed features. From this recipe, we can then by a process of optimisation derive estimates for the values of the latent feature. We won’t dwell on the details here, but I want to stress the level of abstraction! When it comes to test the hypotheses about the underlying psychological phenomenon, this method has certain mathematical appeal but it is not above question. It is these kind of contingencies: the bespoke assumptions of the model, but correlation and covariance relationship between the observed features and richness of the data required to discover the expected values of \\(\\mathbf{L}, \\mathbf{f}_j\\) respectively, coupled with the difficulty of interpreting the factors in light of the original psychological concept, that undermine cumulative progress in psychology."
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#weighing-the-hypothesis",
    "href": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#weighing-the-hypothesis",
    "title": "Factor Analysis and Construct Validity in Psychology",
    "section": "Weighing the Hypothesis",
    "text": "Weighing the Hypothesis\nThese observations suggest that the notion construct validity is not easily resolved and as such our initial hypotheses are plagued by innumerable auxilary commitments. It is with these considerations in mind that Paul Meehl can write:\n\n“[T]he almost universal reliance on merely refutng the null hypothesis as the standard method for corroborating substantive theories in the soft areas is a terrible mistake, is basically unsound, poor scientific strategy and one of the worst things that ever happened in theory of psychology” - Theoretical Risks and Tabular Asteriks, Sir Karl, Sir Ronald and the Slow Progress of Soft Psychology\n\nThe accumulation of auxilary commitments makes the cumulative confirmation of substantive psychological theory proportionaly unlikely. At best we may get lucky in some cases, but the character of the object under study is so dynamic that simple significance tests are next to useless. The “distance” between the latent factor and our measurement of it supply an almost endless set of auxilary commitments which can come under pressure when evaluating a given hypothesis.\nBut there is a tension since difficulty of measurement does not necessarily undermine the theory. In particular, there are theory’s which have a high degree of intuitive plausibility (“verisimilitude”) but escape our ability to properly measure. Any measurement construct is at best an attempted proxy. There are some historic measures of construct validity such as Cronbach’s alpha which at least test for a directional consistency in the observed features."
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#factor-reliability-an-example-in-code.",
    "href": "posts/post-with-code/theoretical_risks_meehl/theoretical_risks_meehl.html#factor-reliability-an-example-in-code.",
    "title": "Factor Analysis and Construct Validity in Psychology",
    "section": "Factor Reliability: An Example in Code.",
    "text": "Factor Reliability: An Example in Code."
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html",
    "href": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "import pandas as pd\nfrom matplotlib import pyplot as plt\nimport nltk\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom factor_analyzer import FactorAnalyzer\nfrom sklearn.decomposition import FactorAnalysis\nimport random\nimport seaborn as sns\nrandom.seed(30)"
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#create-the-fake-customer-purchase-data",
    "href": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#create-the-fake-customer-purchase-data",
    "title": "Examined Algorithms",
    "section": "Create the Fake Customer Purchase data",
    "text": "Create the Fake Customer Purchase data\nWe create two fake data sets with a discernible structures, but we don’t want the models to work too easily so we sample again from these data pairing both sets as if we have a customer and their products.\n\nX, y = make_blobs(n_samples=10000, \n                  centers=5, \n                  n_features=6,\n                 random_state=0\n                 )\n\nprods = ['product_desc_' + str(x) for x in range(0, 6)]\ndf_products = pd.DataFrame(data=X, columns=prods)\ndf_products['Product_Class'] = y\ndf_products['Product_ID'] = df_products.index\n\nX, y = make_blobs(n_samples=1000, \n                  centers=10,\n                  n_features=8,\n                  random_state=0)\ncusts = ['customer_desc_' + str(x) for x in range(0, 8)]\ndf_customer = pd.DataFrame(data=X,\n                           columns=custs)\n\ndf_customer['Customer_Class'] = y\ndf_customer['Customer_ID'] = df_customer.index\n\n# Randomly Select some of the customers to pair with random purchases\nday1 = pd.DataFrame(zip(\n        [random.randint(0, 1000) for x in range(0, 500)],\n        [random.randint(0, 10000) for x in range(0, 500)]\n        ), \n        columns=['Customer_ID', 'Product_ID']\n                   )\n\nday2 = pd.DataFrame(zip(\n        [random.randint(0, 1000) for x in range(0, 500)],\n        [random.randint(0, 10000) for x in range(0, 500)]\n        ), \n        columns=['Customer_ID', 'Product_ID'])\n\npurchases = pd.concat([day1, day2],\n                      axis=0, \n                      ignore_index=True)\npurchases.head()\n\n\n\n\n\n  \n    \n      \n      Customer_ID\n      Product_ID\n    \n  \n  \n    \n      0\n      552\n      773\n    \n    \n      1\n      827\n      8608\n    \n    \n      2\n      296\n      518\n    \n    \n      3\n      625\n      5394\n    \n    \n      4\n      30\n      8543\n    \n  \n\n\n\n\n\ndf_purchases = None\nfor purchase in range(0, len(purchases)):\n    cust_id = purchases['Customer_ID'][purchase]\n    prod_id = purchases['Product_ID'][purchase]\n    cust = df_customer[df_customer['Customer_ID'] == \n                       cust_id]\n    cust.reset_index(inplace=True, drop=True)\n    prod = df_products[df_products['Product_ID'] == \n                       prod_id]\n    prod.reset_index(inplace=True, drop=True)\n    temp = pd.concat([prod, cust], axis=1)\n    if df_purchases is None:\n        df_purchases = pd.concat([prod, cust], axis=1)\n    else:\n        df_purchases = df_purchases.append(\n            pd.concat([prod, cust], axis=1)\n        )\ndf_purchases.reset_index(inplace=True, drop=True)\ndf_purchases\n\n\n\n\n\n  \n    \n      \n      product_desc_0\n      product_desc_1\n      product_desc_2\n      product_desc_3\n      product_desc_4\n      product_desc_5\n      Product_Class\n      Product_ID\n      customer_desc_0\n      customer_desc_1\n      customer_desc_2\n      customer_desc_3\n      customer_desc_4\n      customer_desc_5\n      customer_desc_6\n      customer_desc_7\n      Customer_Class\n      Customer_ID\n    \n  \n  \n    \n      0\n      -1.233276\n      8.747108\n      9.165746\n      -0.007541\n      4.652380\n      1.913970\n      1\n      773\n      -6.374916\n      -2.257586\n      7.688019\n      -7.992522\n      7.593467\n      -9.193683\n      10.376847\n      -0.388405\n      8.0\n      552.0\n    \n    \n      1\n      -0.170403\n      8.219852\n      9.710727\n      -1.550177\n      5.811414\n      1.492459\n      1\n      8608\n      9.294317\n      -2.231715\n      6.061894\n      -0.438840\n      1.246116\n      8.820684\n      -9.950039\n      -7.391761\n      1.0\n      827.0\n    \n    \n      2\n      -2.822567\n      8.036298\n      10.929105\n      -0.935760\n      6.077699\n      1.265250\n      1\n      518\n      -8.793580\n      4.805865\n      6.167272\n      5.770659\n      7.451190\n      4.144325\n      1.196351\n      5.414350\n      2.0\n      296.0\n    \n    \n      3\n      1.019780\n      6.766354\n      -9.152836\n      -8.611808\n      -8.749707\n      6.637419\n      2\n      5394\n      -7.047638\n      -2.131113\n      5.768863\n      -8.094387\n      6.223832\n      -8.834962\n      9.629384\n      -1.935227\n      8.0\n      625.0\n    \n    \n      4\n      4.321656\n      7.430975\n      8.847703\n      5.921750\n      -0.868445\n      5.483210\n      3\n      8543\n      -0.169121\n      1.346768\n      -10.211423\n      1.709859\n      1.655568\n      1.891809\n      7.856076\n      4.333816\n      4.0\n      30.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      2.151425\n      3.179085\n      2.337353\n      0.559543\n      -1.629433\n      2.493002\n      0\n      8383\n      -4.926378\n      -1.474424\n      2.848533\n      -10.733950\n      4.237231\n      5.048239\n      -5.263423\n      -7.005510\n      5.0\n      272.0\n    \n    \n      996\n      0.475735\n      4.378010\n      2.597665\n      1.569648\n      -1.131376\n      3.110801\n      0\n      6141\n      7.821350\n      -1.948967\n      6.039587\n      1.739432\n      2.351800\n      8.325224\n      -10.263796\n      -7.450850\n      1.0\n      616.0\n    \n    \n      997\n      4.621406\n      7.791088\n      8.592775\n      6.423716\n      0.915721\n      4.727899\n      3\n      406\n      -9.236352\n      2.760513\n      -7.290240\n      9.169618\n      -0.188279\n      -2.443252\n      -4.153840\n      6.140598\n      3.0\n      266.0\n    \n    \n      998\n      -1.720297\n      8.174031\n      9.746428\n      -1.849028\n      5.046019\n      0.951072\n      1\n      9737\n      -1.411479\n      0.830250\n      5.167232\n      -9.188249\n      3.176105\n      4.570698\n      -6.300042\n      -7.561958\n      5.0\n      263.0\n    \n    \n      999\n      2.804878\n      4.882689\n      -0.262516\n      0.311810\n      -1.867281\n      4.639076\n      0\n      3486\n      -6.696620\n      1.945182\n      -7.520248\n      8.545794\n      3.743541\n      -3.216961\n      -4.505348\n      3.400242\n      3.0\n      589.0\n    \n  \n\n1000 rows × 18 columns"
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#preliminary-plotting-do-the-factors-seperate-the-structure",
    "href": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#preliminary-plotting-do-the-factors-seperate-the-structure",
    "title": "Examined Algorithms",
    "section": "Preliminary Plotting: Do the Factors seperate the structure?",
    "text": "Preliminary Plotting: Do the Factors seperate the structure?\nA worthwhile plot to apply with any dimensional reduction technique (such as PCA or factor analysis) is to check if and how the data seperates when plotted on the reduced plane. In lieu of knowledge of the data we can always compare this representation to the output of a clustering algorithm.\n\ncust_desc = [x for x in df_purchases.columns if \n             'customer_desc' in x]\nX = df_customer[cust_desc]\nkmeans = KMeans(init='k-means++',\n                n_clusters=3, \n                n_init=30\n               )\nkmeans.fit(X)\nclusters = kmeans.predict(X)\nX['cluster'] = clusters\n\nsklearn_fa = FactorAnalysis(n_components=2, \n                            rotation='varimax'\n                           )\nY_fa = pd.DataFrame(sklearn_fa.fit_transform(X[cust_desc]))\nX = pd.concat([X, Y_fa], axis=1)\nX[[0, 1, 'cluster']].plot.scatter(x=0,\n                      y=1,\n                      c='cluster',\n                      colormap='viridis')\nplt.title(\"Factor Analysis Representation - Coloured by inferred Clusters\")\nplt.ylabel(\"Factor 1\")\nplt.xlabel(\"Factor 2\")\nplt.style.use('default')\nplt.show()\n\n/Users/nathanielforde/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  if __name__ == '__main__':\n\n\n\n\n\nWe can see here that the factors do a pretty good job of seperating the classes (0, 1), but mix up (2, 1). In addition we can see that there are 7 distinct clusters on the factor analysis representation which suggests that our choice three clustering classes is too low."
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#choosing-the-number-of-factors",
    "href": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#choosing-the-number-of-factors",
    "title": "Examined Algorithms",
    "section": "Choosing the number of Factors",
    "text": "Choosing the number of Factors\nOne suggestive way in which to determine the number of factors which we should extract is to build the skree plot of the eigenvalues and select the number of features where there is a higher relative eigenvalues.\n\nimport numpy as np\nfeature_names = ['customer_desc_' + str(x) for x in range(0, 8)]\n\nfa = FactorAnalyzer(n_factors=3)\nfa.fit(X[feature_names], 10)\nev, v = fa.get_eigenvalues()\nplt.plot(range(1,X[feature_names].shape[1]+1),ev)\nplt.show()\n\n\n\n\nOn the basis of this plot we should probably choose no more than two factors at most but we’ll continue with 3 for purposes of illustration. The factor loadings are linear functions of the observed features and so we may interpret the newly created factors by observing which of the observed features play a greater role in their composition.\n\nfa_loading_matrix = pd.DataFrame(fa.loadings_, \n                                 columns=['FA{}'.format(i) for \n                                          i in range(1, 3+1)], \n                              index=feature_names)\nfa_loading_matrix['Highest_loading'] = fa_loading_matrix.idxmax(axis=1)\nfa_loading_matrix = fa_loading_matrix.sort_values('Highest_loading')\nfa_loading_matrix\n\n\n\n\n\n  \n    \n      \n      FA1\n      FA2\n      FA3\n      Highest_loading\n    \n  \n  \n    \n      customer_desc_1\n      0.727724\n      0.024771\n      0.091443\n      FA1\n    \n    \n      customer_desc_3\n      0.826998\n      0.068252\n      0.014691\n      FA1\n    \n    \n      customer_desc_5\n      0.637687\n      -0.003234\n      0.489801\n      FA1\n    \n    \n      customer_desc_7\n      0.787048\n      0.081877\n      -0.414776\n      FA1\n    \n    \n      customer_desc_4\n      0.015911\n      1.001115\n      0.175406\n      FA2\n    \n    \n      customer_desc_6\n      -0.172587\n      0.077989\n      -0.684129\n      FA2\n    \n    \n      customer_desc_0\n      -0.229668\n      -0.628541\n      0.355323\n      FA3\n    \n    \n      customer_desc_2\n      -0.304156\n      0.217303\n      0.440891\n      FA3\n    \n  \n\n\n\n\nWe can see here that there is probably only one sensible factor to be derived from our dataset.\n\nimport seaborn as sns\n\nplt.figure(figsize=(25,5))\n\n# plot the heatmap for correlation matrix\nax = sns.heatmap(fa_loading_matrix.drop('Highest_loading', axis=1).T, \n                vmin=-1, vmax=1, center=0,\n                cmap=sns.diverging_palette(220, 20, n=200),\n                square=True, annot=True, fmt='.2f')\n\nax.set_yticklabels(\n    ax.get_yticklabels(),\n    rotation=0);\n\n\n\n\n\ncommunalities = pd.DataFrame(fa.get_communalities(), \n                             index=list(feature_names))\nfeatures_comm = list(communalities[communalities[0] > 0.33].index)\nprint('Total variables/features with communalities >0.33: {}'.format(len(features_comm)))\ncommunalities\n\nTotal variables/features with communalities >0.33: 8\n\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      customer_desc_0\n      0.574065\n    \n    \n      customer_desc_1\n      0.538558\n    \n    \n      customer_desc_2\n      0.334116\n    \n    \n      customer_desc_3\n      0.688800\n    \n    \n      customer_desc_4\n      1.033252\n    \n    \n      customer_desc_5\n      0.646560\n    \n    \n      customer_desc_6\n      0.503901\n    \n    \n      customer_desc_7\n      0.798188"
  },
  {
    "objectID": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#testing-the-validity-of-a-hypothetical-factor",
    "href": "posts/post-with-code/theoretical_risks_meehl/TheoreticalRisks_Meehl.html#testing-the-validity-of-a-hypothetical-factor",
    "title": "Examined Algorithms",
    "section": "Testing the Validity of a Hypothetical Factor",
    "text": "Testing the Validity of a Hypothetical Factor\nCronbach’s Alpha is a statistical measure of the reliability of the factor analysis derived from a test of the covariances matrix of features which make up the proposed latent factor. A cronbach alpha closer to 1 is desired.\n\\(\\alpha = \\frac{K}{K-1}\\left(1-\\frac{\\sum \\sigma^2_{x_i}}{\\sigma^2_T}\\right)\\)\nwhere\n\\(\\sigma^2_T = \\sum \\sigma^2_{x_i} + 2 \\sum_{i < j}^K {\\rm cov}(x_i,x_j)\\)\na combination of the observational measure of variance and inter-metric covariances for each observational variable. This ties this measure to the Factor analysis model since the covariances can be re-expressed in terms of the factor loadings.\n\\(\\sigma^2_T = \\sum \\sigma^2_{x_i} + 2 \\sum_{i < j}^K (l_{i, 1} + \\epsilon_{i})(l_{j, 1} + \\epsilon_{j})\\)\nwhich means that if the factors loadings are fairly high relative the the random components of the variance then we’ll get a ratio that come close to one. Conversely low loadings will ensure that the denominator drags the ratio down.\n\nimport numpy as np\n\ndef CronbachAlpha(observed_measures):\n    observed_measures = np.asarray(observed_measures)\n    sample_vars = observed_measures.var(axis=1, ddof=1)\n    total_scores = observed_measures.sum(axis=0)\n    nitems = len(observed_measures)\n\n    return nitems / (nitems-1.) * (1 - sample_vars.sum() / total_scores.var(ddof=1))\n\n\n#Collate the observed features\n\nfactor1 = [X['customer_desc_1'], X['customer_desc_3'], \n             X['customer_desc_5'], X['customer_desc_7']]\n\nfactor2 = [X['customer_desc_4'], X['customer_desc_0']]\n\nfactor3 = [X['customer_desc_5'], X['customer_desc_6'], \n             X['customer_desc_2']]\n#Get cronbach alpha\nfactor1_alpha = CronbachAlpha(factor1)\n#factor2_alpha = CronbachAlpha(factor2)\n#factor3_alpha = CronbachAlpha(factor3)\nprint(factor1_alpha, \n      factor2_alpha, \n      factor3_alpha\n     )\n\n0.7889764629492505 -3.382959230225132 -0.8158399732248119\n\n\nWhich shows as expected that only one of the proposed factors is sensible."
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html",
    "href": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "import random\nimport statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nfrom scipy.optimize import minimize_scalar, minimize\nfrom IPython.display import Latex\nfrom stargazer.stargazer import Stargazer\nfrom IPython.core.display import HTML\nThis notebook is a python port of some of the code in “Learning Microeconometrics with R” by Christopher P Adams. It corresponds to the blog post: https://nathanielf.github.io//post/mle_utility_and_choice/"
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#the-problem-modelling-discrete-choice-by-latent-utility-metrics",
    "href": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#the-problem-modelling-discrete-choice-by-latent-utility-metrics",
    "title": "Examined Algorithms",
    "section": "The Problem: Modelling Discrete Choice by Latent Utility Metrics",
    "text": "The Problem: Modelling Discrete Choice by Latent Utility Metrics\nSome assumptions about the form of the utility distribution are crucial as our modeling efforts will go wrong if we know nothing about the latent utilities. We assume that the latent utility can be expressed as by the revealed preferences i.e. as the share or proportion of choices made by the customers.\nThe utility is some function of product and consumer’s properties, perhaps mostly driven by price\n\\[ utility = \\mathbf{X'}\\beta + e\\]\nand market share is an expression of that utility \\[ demand_A = utility_{A} > 0 \\]\nIn a choice context we’re trying to determine if the implicit utility measure is sufficient to drive a purchase, and as such OLS models are inappropriate\n\nN = 1000\na = 2\nb = -3\ne = np.random.normal(0, 1, N)\nconsumer_desc = np.random.uniform(3, 1, N)\nconsumer_desc1 = np.random.uniform(2, 5, N)\nutility = 2 + 3*consumer_desc + -4*consumer_desc1 + e\n## Predicting choice over two options\ndemand_A = utility > 0\nX = pd.DataFrame({'product_desc': consumer_desc, 'product_desc1': consumer_desc1})\nX = sm.add_constant(X)\nlm1 = sm.OLS(demand_A,X)\nlm1_results = lm1.fit()\nprint(lm1_results.summary())\nprint(round(lm1_results.params, 5))\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.200\nModel:                            OLS   Adj. R-squared:                  0.198\nMethod:                 Least Squares   F-statistic:                     124.5\nDate:                Sat, 20 Feb 2021   Prob (F-statistic):           5.42e-49\nTime:                        16:01:07   Log-Likelihood:                 58.448\nNo. Observations:                1000   AIC:                            -110.9\nDf Residuals:                     997   BIC:                            -96.17\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconst             0.2095      0.040      5.291      0.000       0.132       0.287\nproduct_desc      0.1120      0.013      8.939      0.000       0.087       0.137\nproduct_desc1    -0.1045      0.008    -12.630      0.000      -0.121      -0.088\n==============================================================================\nOmnibus:                      472.919   Durbin-Watson:                   1.983\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1999.877\nSkew:                           2.298   Prob(JB):                         0.00\nKurtosis:                       8.183   Cond. No.                         23.8\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nconst            0.20953\nproduct_desc     0.11204\nproduct_desc1   -0.10449\ndtype: float64\n\n\nThis stems from the fact that we’re’trying to estimate a conditional probability over a binary choice not a continuous measure. The revealed preference assumption says that we can predict the purchase if the utility of good is positive.\n\\[Pr(demand_A = 1) = utility > 0 \\] \\[= Pr(\\mathbf{X'}\\beta + e > 0) \\] \\[ = Pr(e > - \\mathbf{X'}\\beta ) \\] \\[ = 1 - F(\\mathbf{X'}\\beta ) \\]\nwhere \\(F\\) is the distribution of the unobserved random variable \\(e\\). The challenge is using the correct distribution as this feeds the method of statistical estimation of the parameters \\(\\beta\\)"
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#maximum-likelihood-fits-over-candidate-distributions",
    "href": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#maximum-likelihood-fits-over-candidate-distributions",
    "title": "Examined Algorithms",
    "section": "Maximum Likelihood Fits over Candidate Distributions",
    "text": "Maximum Likelihood Fits over Candidate Distributions\nThere are a number of candidate distributions which might serve to replace \\(F\\) and estimate the share of purchases\n\ndef log_binomial_dist(params, *args):\n    p = params[0]\n    p_hat = args[0]\n    N = args[1]\n    return -((p_hat*N)*np.log(p) + (1-p_hat)*N*np.log(1-(p)))\n\nres = minimize(log_binomial_dist, x0 = [.1], args =(.34, 100), bounds = ((0, .99),))\nprint(res)\n    \n\n\n      fun: 64.10354778811556\n hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n      jac: array([0.])\n  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n     nfev: 16\n      nit: 6\n     njev: 8\n   status: 0\n  success: True\n        x: array([0.33999999])\n\n\n\ndef ll_ols(params, *args):\n    X, y = args[0], args[1]\n    beta = [params[0], params[1], params[2]]\n    mu, sd, = params[3], params[4]\n    z = (y - X.dot(beta)) / sd\n    log_lik = -sum(np.log(stats.norm.pdf(z)) - np.log(sd))\n    return log_lik\n\nx = np.random.normal(5, 2, 1000)\nx1 = np.random.normal(6, 1, 1000)\nx2 = np.random.uniform(2, 7, 1000)\ny = 1 + .3*x + 5*x1 + np.random.normal(0, 1, 1000)\n\nX1 = pd.DataFrame({'consumer_desc': x, 'consumer_desc1': x1})\nX1 = sm.add_constant(X1)\n\nres = minimize(ll_ols, x0 =[2, 1, 4, 2, 1], method = 'Nelder-Mead', args =(X1, y))\nprint(res)\n\n final_simplex: (array([[1.27170093, 0.28846111, 4.97078397, 2.33608166, 0.98080792],\n       [1.27163611, 0.28845751, 4.97079439, 2.33611095, 0.98081281],\n       [1.27179597, 0.28845924, 4.97076792, 2.33606271, 0.98080085],\n       [1.27178217, 0.28845315, 4.97077749, 2.33606068, 0.98081083],\n       [1.2716755 , 0.28844586, 4.97079715, 2.33609652, 0.98080788],\n       [1.27174903, 0.28845715, 4.97077654, 2.33606353, 0.98081551]]), array([1399.55005084, 1399.55005089, 1399.55005094, 1399.55005095,\n       1399.55005095, 1399.55005106]))\n           fun: 1399.5500508414766\n       message: 'Optimization terminated successfully.'\n          nfev: 384\n           nit: 238\n        status: 0\n       success: True\n             x: array([1.27170093, 0.28846111, 4.97078397, 2.33608166, 0.98080792])\n\n\n\ndef log_probit_dist(params, *args):\n    X, y = args[0], args[1]\n    beta = [params[0], params[1], params[2]]\n    mu, sd, = params[3], params[4]\n    Xb = X.dot(beta)\n    q = 2*y-1\n    log_lik = np.log(stats.norm.cdf(q*Xb))\n    return -sum(log_lik)\n\n### Optimise the probit model for determining the parameters required toe estimate the underlying utility\n### True values of the parameters 2, 3, -4\nres = minimize(log_probit_dist, x0 =[0, 0 ,0 , 0, 1], args =(X, demand_A), options={'disp': True})\nprint(res)\n\nOptimization terminated successfully.\n         Current function value: 94.044202\n         Iterations: 17\n         Function evaluations: 108\n         Gradient evaluations: 18\n      fun: 94.04420183563171\n hess_inv: array([[ 0.72799404, -0.03552851, -0.26060106,  0.        ,  0.        ],\n       [-0.03552851,  0.09513159, -0.08023828,  0.        ,  0.        ],\n       [-0.26060106, -0.08023828,  0.18756252,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  1.        ]])\n      jac: array([ 3.81469727e-06, -9.53674316e-06,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00])\n  message: 'Optimization terminated successfully.'\n     nfev: 108\n      nit: 17\n     njev: 18\n   status: 0\n  success: True\n        x: array([ 2.14197608,  2.42768647, -3.51990129,  0.        ,  1.        ])\n\n\nThese estimates are still incorrect but an awful lot closer than the fits achieved by the ols model in the first section. We can validate the above optimisation against the inbuilt model of statsmodels\n\nprobit_mod = sm.Probit(demand_A, X)\nprobit_res = probit_mod.fit()\nprobit_margeff = probit_res.get_margeff()\nprint('Parameters: ', probit_res.params)\nprint('Marginal effects: ')\nprint(probit_margeff.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.094044\n         Iterations 10\nParameters:  const            2.141972\nproduct_desc     2.427687\nproduct_desc1   -3.519900\ndtype: float64\nMarginal effects: \n       Probit Marginal Effects       \n=====================================\nDep. Variable:                      y\nMethod:                          dydx\nAt:                           overall\n=================================================================================\n                   dy/dx    std err          z      P>|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nproduct_desc      0.1285      0.011     12.137      0.000       0.108       0.149\nproduct_desc1    -0.1863      0.015    -12.674      0.000      -0.215      -0.157\n================================================================================="
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#mcfaddans-bart-discrete-choice-model",
    "href": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#mcfaddans-bart-discrete-choice-model",
    "title": "Examined Algorithms",
    "section": "McFaddan’s BART Discrete Choice Model",
    "text": "McFaddan’s BART Discrete Choice Model\nThe idea is a generalisation of the above to estimate the difference in utilities across multiple products.\n\\[ U_{i,j} = \\mathbf{X'}_{i, j}\\beta + v_{i,j} \\]\nwhere the each individual’s \\(i\\) utility for a given good \\(j\\) is expressed as a linear weighted function of the product characteristics \\(\\mathbf{X}\\). Since we need to predict demand based on utility we’re really interested in estimating the differnce in utility\n\\[U_{i,A} > U_{i, B} \\]\njust when\n\\[ \\mathbf{X'}_{i, A}\\beta + v_{i,A} > \\mathbf{X'}_{i, B}\\beta + v_{i,B}\\]\nor\n\\[  v_{i,A} -  v_{i,B} > - (\\mathbf{X'}_{i, A}  - \\mathbf{X'}_{i, B})\\beta \\]\nbut then the probability of demand is just\n\\[Pr(demand_A = 1 | \\mathbf{X'}_{i, A}, \\mathbf{X'}_{i, B}) = Pr\\Bigg( v_{i,A} -  v_{i,B} > - (\\mathbf{X'}_{i, A}  - \\mathbf{X'}_{i, B})\\beta \\Bigg) \\] \\[ = Pr\\Bigg( -v_{i,A} -  v_{i,B} < (\\mathbf{X'}_{i, A}  - \\mathbf{X'}_{i, B})\\beta \\Bigg) \\] \\[ = F\\Bigg( (\\mathbf{X'}_{i, A}  - \\mathbf{X'}_{i, B})\\beta \\Bigg) \\]"
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#an-example-in-two-products-two-models",
    "href": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#an-example-in-two-products-two-models",
    "title": "Examined Algorithms",
    "section": "An Example in Two Products & Two Models",
    "text": "An Example in Two Products & Two Models\n\nN = 100\nX_A = sm.add_constant(np.random.rand(N,2))\nX_B = sm.add_constant(np.random.rand(N, 2))\nbeta = np.array([1, -2, 3])\n\n#probit we only need one normal error term since sums of normals are normal\nv = np.random.normal(0, 1, N)\ny = (X_A.dot(beta) - X_B.dot(beta)) + v > 0\nX_diff = X_A - X_B\nX_diff[:, 0] =  1\nX_diff = pd.DataFrame(X_diff, columns=['const', 'product_desc', 'product_desc1'])\n\nAgain we can try two classification algorithms which attempt to characterise the error terms \\(v_{1}, v_{2}\\) that the McFaddan model assumes\n\nprobit_mod = sm.Probit(y, X_diff)\nprobit_res = probit_mod.fit()\nprobit_margeff = probit_res.get_margeff()\n\nOptimization terminated successfully.\n         Current function value: 0.426413\n         Iterations 6\n\n\n\nv1 = np.random.weibull(1, N)\nv2 = np.random.weibull(1, N)\ny = (X_A.dot(beta) - X_B.dot(beta)) + (v1 - v2) > 0\nlogit_mod = sm.Logit(y, X_diff)\nlogit_res = logit_mod.fit()\nlogit_margeff = logit_res.get_margeff()\n\nOptimization terminated successfully.\n         Current function value: 0.499093\n         Iterations 6\n\n\n\nstargazer = Stargazer([probit_res, logit_res])\nstargazer.custom_columns(['Probit Model', 'Logit Model'], [1, 1])\n#stargazer.add_custom_notes([str(probit_margeff.summary()), str(logit_margeff.summary())])\nHTML(stargazer.render_html())\n\n\nDependent variable:yProbit ModelLogit Model(1)(2)const-0.255-0.049(0.156)(0.246)product_desc-1.491***-2.157***(0.445)(0.690)product_desc12.964***3.833***(0.578)(0.888)Observations100100R2Adjusted R2Residual Std. Error1.000 (df=97)1.000 (df=97)F Statistic (df=2; 97) (df=2; 97)Note:\n \n  *p<0.1;\n  **p<0.05;\n  ***p<0.01"
  },
  {
    "objectID": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#generalising-to-multiple-choices-the-multinomial-distribution",
    "href": "posts/post-with-code/mle_utility_and_choice/revealed_preferences_mcfadden_BART.html#generalising-to-multiple-choices-the-multinomial-distribution",
    "title": "Examined Algorithms",
    "section": "Generalising to Multiple choices: The Multinomial Distribution",
    "text": "Generalising to Multiple choices: The Multinomial Distribution\n\nnp.random.seed(100)\nN = 1000\nmu = [0,0]\nrho = 0.1\ncov = [[1, rho], [rho, 1]]\n\n# u is N*2\nu = np.random.multivariate_normal(mu, cov, 1000)\nx1 = np.random.uniform(0, 1, size=(N,2)) #np.random.rand(N,2)\nx2 = np.random.uniform(0, 1, size=(N,2)) #np.random.rand(N,2)\n\nU = -1 + -3*x1 + 4*x2 + u\n\ny = np.zeros(shape=(N, 2))\ny[:,0] = ((U[:,0] > 0) & (U[:,0] > U[:,1]))\ny[:,1] = (U[:,1] > 0 & (U[:,1] > U[:,0]))\n\n\nW1 = pd.DataFrame({'x1':x1[:,0], 'x2':x2[:,0]})\nW2 = pd.DataFrame({'x1':x1[:,1], 'x2':x2[:,1]})\n\n\nOptimization terminated successfully.\n         Current function value: 542.609688\n         Iterations: 185\n         Function evaluations: 338\n\n\n final_simplex: (array([[-1.74855767, -5.18018459,  7.07552039],\n       [-1.74856002, -5.18016799,  7.07549515],\n       [-1.74856866, -5.18022325,  7.07555337],\n       [-1.74862589, -5.18016727,  7.07561422]]), array([542.60968847, 542.60968847, 542.60968848, 542.60968848]))\n           fun: 542.6096884689198\n       message: 'Optimization terminated successfully.'\n          nfev: 338\n           nit: 185\n        status: 0\n       success: True\n             x: array([-1.74855767, -5.18018459,  7.07552039])\n\n\n\ny_full = np.ones(shape=(N*2,1))\nclass_1 = np.where(((U[:,0] > 0) & (U[:,0] > U[:,1])), 'class_1', 'class_0')\nclass_2 = np.where((U[:,1] > 0 & (U[:,1] > U[:,0])), 'class_2', 'class_0')\ny_full = np.append(class_1, class_2)\nW_full = sm.add_constant(W1.append(W2)).reset_index(drop=True)\nmn_logit = sm.MNLogit(y_full, W_full)\nmn_logit_res = mn_logit.fit()\nmn_logit_res.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.630391\n         Iterations 7\n\n\n\n\nMNLogit Regression Results\n\n  Dep. Variable:           y          No. Observations:       2000  \n\n\n  Model:                MNLogit       Df Residuals:           1994  \n\n\n  Method:                 MLE         Df Model:                  4  \n\n\n  Date:            Thu, 25 Feb 2021   Pseudo R-squ.:        0.2924  \n\n\n  Time:                21:23:52       Log-Likelihood:       -1260.8 \n\n\n  converged:             True         LL-Null:              -1781.9 \n\n\n  Covariance Type:     nonrobust      LLR p-value:        2.587e-224\n\n\n\n\n  y=class_1    coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  const        -2.6666     0.219   -12.201  0.000    -3.095    -2.238\n\n\n  x1           -4.7724     0.315   -15.143  0.000    -5.390    -4.155\n\n\n  x2            6.2800     0.360    17.461  0.000     5.575     6.985\n\n\n  y=class_2    coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  const        -2.7293     0.212   -12.865  0.000    -3.145    -2.313\n\n\n  x1           -4.4876     0.299   -15.004  0.000    -5.074    -3.901\n\n\n  x2            6.4438     0.348    18.497  0.000     5.761     7.127\n\n\n\n\n\nfrom scipy.special import softmax\n\ndef cdf(W, beta):\n    Wb = np.dot(W, beta)\n    eXB = np.exp(Wb)\n    eXB = eXB /eXB.sum(1)[:, None]\n    return eXB\n\ndef take_log(probs):\n    epsilon = 1e-20 \n    return np.log(probs)\n\ndef calc_ll(logged, d):\n    ll = d * logged\n    return ll\n\ndef ll_mn_logistic(params, *args):\n    y, W, n_params, n_classes = args[0], args[1], args[2], args[3]\n    beta = [params[i] for i in range(0, len(params))]\n    beta = np.array(beta).reshape(n_params, -1, order='F')\n    beta[:,0] = [0 for i in range(0, n_params)]\n    \n    ## onehot_encode\n    d = pd.get_dummies(y, prefix='Flag').to_numpy()\n    \n    probs = cdf(W, beta)\n    logged = take_log(probs)\n    ll = calc_ll(logged, d)\n    \n    return -np.sum(ll)\n\nn_params = 3 \nn_classes = 3\nz = np.random.rand(3,3).flatten()\n#probs = ll_mn_logistic(list(z), *[y_full, W_full, n_params, n_classes])\n\n\nres = minimize(ll_mn_logistic, x0 =z, args =(y_full, W_full, n_params, n_classes), \n             options={'disp': True, 'maxiter':1000})\nres\n\nWarning: Desired error not necessarily achieved due to precision loss.\n         Current function value: 1260.782799\n         Iterations: 21\n         Function evaluations: 470\n         Gradient evaluations: 47\n\n\n      fun: 1260.782798764732\n hess_inv: array([[ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.15840199,  0.05230628,\n        -0.24846581,  0.08569624, -0.08026449, -0.11217585],\n       [ 0.        ,  0.        ,  0.        ,  0.05230628,  0.05960721,\n        -0.11533024,  0.03950015, -0.02890953, -0.06618335],\n       [ 0.        ,  0.        ,  0.        , -0.24846581, -0.11533024,\n         0.42805566, -0.14239161,  0.11728476,  0.20865474],\n       [ 0.        ,  0.        ,  0.        ,  0.08569624,  0.03950015,\n        -0.14239161,  0.0776523 , -0.03556134, -0.10565188],\n       [ 0.        ,  0.        ,  0.        , -0.08026449, -0.02890953,\n         0.11728476, -0.03556134,  0.07906259,  0.01257425],\n       [ 0.        ,  0.        ,  0.        , -0.11217585, -0.06618335,\n         0.20865474, -0.10565188,  0.01257425,  0.18307424]])\n      jac: array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.05175781e-05,\n       1.52587891e-05, 0.00000000e+00, 3.05175781e-05, 3.05175781e-05,\n       3.05175781e-05])\n  message: 'Desired error not necessarily achieved due to precision loss.'\n     nfev: 470\n      nit: 21\n     njev: 47\n   status: 2\n  success: False\n        x: array([ 0.09640129,  0.00798646,  0.70648002, -2.66658562, -4.77244682,\n        6.27997417, -2.72927898, -4.48757328,  6.44379724])\n\n\n\nbart_data = pd.read_csv('../data_files/mcfaddan_bart.csv')\nbart_data.head()\n\n\n\n\n\n  \n    \n      \n      HOUSEID\n      TRAVDAY\n      SAMPSTRAT\n      HOMEOWN\n      HHSIZE\n      HHVEHCNT\n      HHFAMINC\n      PC\n      SPHONE\n      TAB\n      ...\n      SMPLSRCE\n      WTHHFIN\n      HBHUR\n      HTHTNRNT\n      HTPPOPDN\n      HTRESDN\n      HTEEMPDN\n      HBHTNRNT\n      HBPPOPDN\n      HBRESDN\n    \n  \n  \n    \n      0\n      30000007\n      2\n      3\n      1\n      3\n      5\n      7\n      2\n      1\n      2\n      ...\n      2\n      187.314320\n      T\n      50\n      1500\n      750\n      750\n      20\n      750\n      300\n    \n    \n      1\n      30000008\n      5\n      2\n      1\n      2\n      4\n      8\n      1\n      1\n      2\n      ...\n      2\n      69.513032\n      R\n      5\n      300\n      300\n      150\n      5\n      300\n      300\n    \n    \n      2\n      30000012\n      5\n      3\n      1\n      1\n      2\n      10\n      1\n      1\n      3\n      ...\n      2\n      79.419586\n      C\n      80\n      17000\n      17000\n      5000\n      60\n      17000\n      7000\n    \n    \n      3\n      30000019\n      5\n      3\n      1\n      2\n      2\n      3\n      1\n      5\n      5\n      ...\n      2\n      279.143588\n      S\n      40\n      300\n      300\n      150\n      50\n      750\n      300\n    \n    \n      4\n      30000029\n      3\n      3\n      1\n      2\n      2\n      5\n      2\n      5\n      1\n      ...\n      2\n      103.240304\n      S\n      40\n      1500\n      750\n      750\n      40\n      1500\n      750\n    \n  \n\n5 rows × 58 columns\n\n\n\n\nbart_data['CHOICE'] = np.nan\nbart_data['CHOICE'] = np.where(bart_data['CAR']==1, 'car', bart_data['CHOICE'])\nbart_data['CHOICE'] = np.where(bart_data['BUS']==1, 'bus', bart_data['CHOICE'])\nbart_data['CHOICE'] = np.where(bart_data['TRAIN']==1, 'rail', bart_data['CHOICE'])\nbart_data['car1'] = bart_data['CHOICE'] == 'car'\nbart_data['train1'] = bart_data['CHOICE'] == 'rail'\nbart_data['home'] = np.where(bart_data['HOMEOWN'] == 1, 1, np.nan)\nbart_data['home'] = np.where(bart_data['HOMEOWN'] > 1, 0, bart_data['home'])\nbart_data['income'] = np.where(bart_data['HHFAMINC'] > 0, bart_data['HHFAMINC'], np.nan)\nbart_data['density'] = np.where(bart_data['HTPPOPDN']==-9, np.nan, bart_data['HTPPOPDN']/1000)\nbart_data['urban1'] = bart_data['URBAN']==1\ny = bart_data[(bart_data['WRKCOUNT'] > 0) & ((bart_data['MSACAT'] == 1) | (bart_data['MSACAT'] == 2))]\ny['rail'] = y['RAIL'] == 1\ny['row_sum'] = y[['car1','train1','home','HHSIZE','income', 'urban1','density','MSACAT', 'rail']].sum(axis=1) == 0\ny\n\n/Users/nathanielforde/.local/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  del sys.path[0]\n/Users/nathanielforde/.local/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \n\n\n\n\n\n\n  \n    \n      \n      HOUSEID\n      TRAVDAY\n      SAMPSTRAT\n      HOMEOWN\n      HHSIZE\n      HHVEHCNT\n      HHFAMINC\n      PC\n      SPHONE\n      TAB\n      ...\n      HBRESDN\n      CHOICE\n      car1\n      train1\n      home\n      income\n      density\n      urban1\n      rail\n      row_sum\n    \n  \n  \n    \n      1\n      30000008\n      5\n      2\n      1\n      2\n      4\n      8\n      1\n      1\n      2\n      ...\n      300\n      car\n      True\n      False\n      1.0\n      8.0\n      0.3\n      False\n      False\n      False\n    \n    \n      9\n      30000085\n      1\n      2\n      1\n      1\n      2\n      9\n      1\n      1\n      4\n      ...\n      17000\n      nan\n      False\n      False\n      1.0\n      9.0\n      17.0\n      True\n      False\n      False\n    \n    \n      15\n      30000130\n      1\n      1\n      1\n      2\n      1\n      5\n      -9\n      1\n      -9\n      ...\n      17000\n      rail\n      False\n      True\n      1.0\n      5.0\n      30.0\n      True\n      True\n      False\n    \n    \n      17\n      30000144\n      3\n      2\n      2\n      3\n      0\n      5\n      5\n      1\n      1\n      ...\n      3000\n      bus\n      False\n      False\n      0.0\n      5.0\n      3.0\n      True\n      False\n      False\n    \n    \n      18\n      30000145\n      5\n      2\n      2\n      2\n      2\n      7\n      1\n      1\n      2\n      ...\n      3000\n      car\n      True\n      False\n      0.0\n      7.0\n      7.0\n      True\n      False\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      129674\n      40794087\n      2\n      1\n      2\n      1\n      1\n      5\n      2\n      -9\n      -9\n      ...\n      3000\n      car\n      True\n      False\n      0.0\n      5.0\n      7.0\n      True\n      True\n      False\n    \n    \n      129688\n      40794241\n      3\n      2\n      1\n      2\n      2\n      6\n      1\n      1\n      5\n      ...\n      3000\n      car\n      True\n      False\n      1.0\n      6.0\n      7.0\n      True\n      False\n      False\n    \n    \n      129690\n      40794260\n      5\n      2\n      1\n      4\n      1\n      11\n      1\n      1\n      2\n      ...\n      1500\n      car\n      True\n      False\n      1.0\n      11.0\n      3.0\n      True\n      False\n      False\n    \n    \n      129693\n      40794294\n      5\n      2\n      1\n      2\n      2\n      10\n      1\n      1\n      5\n      ...\n      7000\n      car\n      True\n      False\n      1.0\n      10.0\n      7.0\n      True\n      False\n      False\n    \n    \n      129695\n      50515573\n      3\n      1\n      1\n      1\n      0\n      10\n      1\n      1\n      5\n      ...\n      17000\n      nan\n      False\n      False\n      1.0\n      10.0\n      30.0\n      True\n      True\n      False\n    \n  \n\n39057 rows × 67 columns\n\n\n\n\nfeatures = [\"car1\",\"train1\",\"home\",\"HHSIZE\",\"income\",\"urban1\",\"density\", 'rail', 'CHOICE']\ny_focus = y[features]\ny_focus.dropna(inplace=True)\ny_focus = y_focus[y_focus['CHOICE'] != 'nan']\ny_focus[features].groupby('rail').mean().T\n\n/Users/nathanielforde/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  This is separate from the ipykernel package so we can avoid doing imports until\n\n\n\n\n\n\n  \n    \n      rail\n      False\n      True\n    \n  \n  \n    \n      car1\n      0.973861\n      0.877925\n    \n    \n      train1\n      0.010036\n      0.096610\n    \n    \n      home\n      0.749231\n      0.730127\n    \n    \n      HHSIZE\n      2.462420\n      2.487870\n    \n    \n      income\n      7.054552\n      7.518840\n    \n    \n      urban1\n      0.869753\n      0.910272\n    \n    \n      density\n      4.661936\n      7.559076\n    \n  \n\n\n\n\n\ny_focus[features + ['CHOICE']]\n\n\n\n\n\n  \n    \n      \n      home\n      HHSIZE\n      income\n      urban1\n      density\n      CHOICE\n    \n  \n  \n    \n      1\n      1.0\n      2\n      8.0\n      False\n      0.3\n      car\n    \n    \n      15\n      1.0\n      2\n      5.0\n      True\n      30.0\n      rail\n    \n    \n      17\n      0.0\n      3\n      5.0\n      True\n      3.0\n      bus\n    \n    \n      18\n      0.0\n      2\n      7.0\n      True\n      7.0\n      car\n    \n    \n      33\n      1.0\n      3\n      11.0\n      True\n      1.5\n      car\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      129667\n      1.0\n      2\n      8.0\n      False\n      0.3\n      car\n    \n    \n      129674\n      0.0\n      1\n      5.0\n      True\n      7.0\n      car\n    \n    \n      129688\n      1.0\n      2\n      6.0\n      True\n      7.0\n      car\n    \n    \n      129690\n      1.0\n      4\n      11.0\n      True\n      3.0\n      car\n    \n    \n      129693\n      1.0\n      2\n      10.0\n      True\n      7.0\n      car\n    \n  \n\n34043 rows × 6 columns\n\n\n\n\n# Without Rail\ny_focus_nr = y_focus[y_focus['rail'] == 0]\ny_focus_r = y_focus[y_focus['rail'] == 1]\n\n\nlogit_mod_nr = sm.Logit(np.array(y_focus_nr['car1']), \n                       sm.add_constant(y_focus_nr[[\"home\",\"HHSIZE\",\"income\",\"urban1\",\"density\"]]).astype(float))\nlogit_res_nr = logit_mod_nr.fit()\nlogit_margeff_nr = logit_res_nr.get_margeff()\n\n\nlogit_mod_r = sm.Logit(np.array(y_focus_r['car1']), \n                       sm.add_constant(y_focus_r[[\"home\",\"HHSIZE\",\"income\",\"urban1\",\"density\"]]).astype(float))\nlogit_res_r = logit_mod_r.fit()\nlogit_margeff_r = logit_res.get_margeff()\n\nOptimization terminated successfully.\n         Current function value: 0.114130\n         Iterations 9\nOptimization terminated successfully.\n         Current function value: 0.300803\n         Iterations 7\n\n\n\nstargazer = Stargazer([logit_res_nr, logit_res_r])\nstargazer.custom_columns(['Probability of Car - No Rail', 'Probability of Car -Rail'], [1, 1])\n#stargazer.add_custom_notes([str(probit_margeff.summary()), str(logit_margeff.summary())])\nHTML(stargazer.render_html())\n\n\nDependent variable:yProbability of Car - No RailProbability of Car -Rail(1)(2)HHSIZE-0.0180.050*(0.033)(0.026)const3.775***3.506***(0.270)(0.211)density-0.055***-0.109***(0.008)(0.003)home0.633***0.498***(0.095)(0.071)income0.133***-0.068***(0.019)(0.013)urban1-1.160***-0.312*(0.246)(0.187)Observations22,41911,624R2Adjusted R2Residual Std. Error1.000 (df=22413)1.000 (df=11618)F Statistic (df=5; 22413) (df=5; 11618)Note:\n \n  *p<0.1;\n  **p<0.05;\n  ***p<0.01\n \n\n\n\nMN_logit_mod_r = sm.MNLogit(np.array(y_focus_r['CHOICE']), \n                       y_focus_r[[\"home\",\"HHSIZE\",\"income\",\"urban1\",\"density\"]].astype(float))\nMN_logit_res_r = MN_logit_mod_r.fit()\nMN_logit_res_r.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.387108\n         Iterations 8\n\n\n\n\nMNLogit Regression Results\n\n  Dep. Variable:           y          No. Observations:      11624  \n\n\n  Model:                MNLogit       Df Residuals:          11614  \n\n\n  Method:                 MLE         Df Model:                  8  \n\n\n  Date:            Mon, 01 Mar 2021   Pseudo R-squ.:        0.1071  \n\n\n  Time:                23:06:01       Log-Likelihood:       -4499.7 \n\n\n  converged:             True         LL-Null:              -5039.6 \n\n\n  Covariance Type:     nonrobust      LLR p-value:        9.100e-228\n\n\n\n\n   y=car     coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  home        1.0460     0.128     8.159  0.000     0.795     1.297\n\n\n  HHSIZE      0.2411     0.046     5.234  0.000     0.151     0.331\n\n\n  income      0.2235     0.021    10.829  0.000     0.183     0.264\n\n\n  urban1      1.7447     0.150    11.619  0.000     1.450     2.039\n\n\n  density    -0.0847     0.006   -13.677  0.000    -0.097    -0.073\n\n\n  y=rail     coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  home        0.3757     0.140     2.689  0.007     0.102     0.650\n\n\n  HHSIZE     -0.0495     0.051    -0.973  0.331    -0.149     0.050\n\n\n  income      0.1658     0.022     7.466  0.000     0.122     0.209\n\n\n  urban1     -0.0949     0.167    -0.569  0.569    -0.422     0.232\n\n\n  density     0.0202     0.007     3.107  0.002     0.007     0.033\n\n\n\n\n\nn_params = 5 \nn_classes = 3\nz = np.random.rand(n_params,n_classes).flatten()\n\nres = minimize(ll_mn_logistic, x0 =z, args =(y_focus_r['CHOICE'], \n                                             y_focus_r[[\"home\",\"HHSIZE\",\"income\",\"urban1\",\"density\"]].astype(float), n_params, n_classes), \n             options={'disp': True, 'maxiter':1000})\nres\n\nWarning: Desired error not necessarily achieved due to precision loss.\n         Current function value: 4499.748263\n         Iterations: 33\n         Function evaluations: 896\n         Gradient evaluations: 56\n\n\n      fun: 4499.748263266686\n hess_inv: array([[ 1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  2.49790461e-02,\n         1.22239651e-02, -1.08077892e-03, -4.52948507e-02,\n         9.24499328e-04,  2.18160612e-02,  1.53707043e-02,\n        -2.13296546e-03, -3.42635201e-02,  5.36330909e-04],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  1.22239651e-02,\n         8.71917753e-03, -1.06847160e-03, -2.23203076e-02,\n         3.46191120e-04,  1.19746027e-02,  1.05025079e-02,\n        -1.74296384e-03, -1.65180529e-02,  1.47031357e-04],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00, -1.08077892e-03,\n        -1.06847160e-03,  4.43252403e-04,  1.14187551e-03,\n        -5.55280050e-05, -8.98456837e-04, -1.17483721e-03,\n         4.60854131e-04,  6.94275713e-04, -3.43435221e-05],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00, -4.52948507e-02,\n        -2.23203076e-02,  1.14187551e-03,  8.77146917e-02,\n        -1.70194689e-03, -4.37327155e-02, -2.84783896e-02,\n         3.45189389e-03,  6.85283602e-02, -1.05179786e-03],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  9.24499328e-04,\n         3.46191120e-04, -5.55280050e-05, -1.70194689e-03,\n         6.40830138e-05,  8.65122620e-04,  4.47609224e-04,\n        -8.69531233e-05, -1.35865208e-03,  4.78235451e-05],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  2.18160612e-02,\n         1.19746027e-02, -8.98456837e-04, -4.37327155e-02,\n         8.65122620e-04,  2.42731374e-02,  1.45500604e-02,\n        -2.32279638e-03, -3.32914355e-02,  5.72986223e-04],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  1.53707043e-02,\n         1.05025079e-02, -1.17483721e-03, -2.84783896e-02,\n         4.47609224e-04,  1.45500604e-02,  1.36673618e-02,\n        -2.18820823e-03, -2.15013079e-02,  1.87660290e-04],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00, -2.13296546e-03,\n        -1.74296384e-03,  4.60854131e-04,  3.45189389e-03,\n        -8.69531233e-05, -2.32279638e-03, -2.18820823e-03,\n         7.07813187e-04,  1.90500574e-03, -5.20898364e-05],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00, -3.42635201e-02,\n        -1.65180529e-02,  6.94275713e-04,  6.85283602e-02,\n        -1.35865208e-03, -3.32914355e-02, -2.15013079e-02,\n         1.90500574e-03,  6.33830924e-02, -1.09402475e-03],\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  5.36330909e-04,\n         1.47031357e-04, -3.43435221e-05, -1.05179786e-03,\n         4.78235451e-05,  5.72986223e-04,  1.87660290e-04,\n        -5.20898364e-05, -1.09402475e-03,  5.33476436e-05]])\n      jac: array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n        0.00000000e+00,  6.10351562e-05, -6.10351562e-05, -6.10351562e-05,\n        0.00000000e+00, -1.83105469e-04,  6.10351562e-05,  2.44140625e-04,\n        3.66210938e-04,  6.10351562e-05,  4.88281250e-04])\n  message: 'Desired error not necessarily achieved due to precision loss.'\n     nfev: 896\n      nit: 33\n     njev: 56\n   status: 2\n  success: False\n        x: array([ 0.98480009,  0.00651008,  0.0863602 ,  0.13983792,  0.32455478,\n        1.04604352,  0.24109143,  0.22348356,  1.74469334, -0.08468886,\n        0.3757215 , -0.04946345,  0.16579691, -0.09489807,  0.02024353])\n\n\n\nMN_logit_mod_r = sm.MNLogit(np.array(y_focus_r['CHOICE']), \n                       y_focus_r[[\"home\",\"HHSIZE\",\"income\",\"urban1\",\"density\"]].astype(float))\nMN_logit_res_r = MN_logit_mod_r.fit()\nMN_logit_res_r.summary()\nnr = y_focus_nr[['home', 'HHSIZE', 'income', 'urban1', 'density', 'rail']].astype(float)\nr = y_focus_r[['home', 'HHSIZE', 'income', 'urban1', 'density', 'rail']].astype(float)\nfull = y_focus[['home', 'HHSIZE', 'income', 'urban1', 'density', 'rail']].astype(float)\nnr_nd = nr\nnr_nd['density'] = 0\nnr_d = nr\nnr_d['density'] = r['density'].mean()\nfull_d = full\nfull_d[full_d['rail'] == 0]['density'] = r['density'].mean()\n\n\nres = MN_logit_res_r.predict(full.drop('rail', axis=1)).mean()\n\nres = pd.DataFrame(res).T\nres.columns = ['Bus', 'Car', 'Train']\nres.round(3) * 100\n\n/Users/nathanielforde/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  if __name__ == '__main__':\n\n\n\n\n\n\n  \n    \n      \n      Bus\n      Car\n      Train\n    \n  \n  \n    \n      0\n      2.7\n      88.5\n      8.8"
  },
  {
    "objectID": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html",
    "href": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "import random\nimport statsmodels.api as sm\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html#simultaneity-bias-in-an-ols-regression",
    "href": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html#simultaneity-bias-in-an-ols-regression",
    "title": "Examined Algorithms",
    "section": "Simultaneity Bias in an OLS regression",
    "text": "Simultaneity Bias in an OLS regression\nThe coefficients in an ordinary least squares (OLS) regression are imprecisely estimated when there is a relationship between the feature variables and the error terms. This is true even when the functional form is properly specified! In our case we can see that the values for the price feature are too high due to the simultaneity bias.\n\nA Supply and Demand Example\n\nN = 1000\nnp.random.seed(0)\nmu, sigma = 0, 7\ne_1 = np.random.normal(mu, sigma, N)\nW = np.random.normal(100, 20, N)\nH = np.random.uniform(0, 10, N)\n\n# True Values\na0, a1, a2, a3 = 2, 3, 5, 10\n\nmu, sigma = 0, 6\ne_2 = np.random.normal(mu, sigma, N)\nF = np.random.uniform(6, 30, N)\nO = np.random.normal(7, 4, N)\n\n#True Values\nb0, b1, b2, b3 = 4, 6, 2, 7\n\nP = np.random.normal(5, 2, N) + .2*(e_1+e_2)\n\ns = a0 + a1*P + a2*W + a3*H + e_1\nd = b0 + b1*P +b2*F + b3*O + e_2\n\n# True coeffs             3       5       10\nX_supply = pd.DataFrame({'P': P , 'W':W, 'H':H})\nX_supply = sm.add_constant(X_supply)\nmodel = sm.OLS(s, X_supply)\nresults = model.fit()\nresults.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:            y          R-squared:             0.997 \n\n\n  Model:                   OLS         Adj. R-squared:        0.997 \n\n\n  Method:             Least Squares    F-statistic:        9.625e+04\n\n\n  Date:             Thu, 11 Feb 2021   Prob (F-statistic):    0.00  \n\n\n  Time:                 22:42:02       Log-Likelihood:      -3203.0 \n\n\n  No. Observations:        1000        AIC:                   6414. \n\n\n  Df Residuals:             996        BIC:                   6434. \n\n\n  Df Model:                   3                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n           coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  const    -3.3455     1.102    -3.037  0.002    -5.508    -1.184\n\n\n  P         4.2631     0.068    62.574  0.000     4.129     4.397\n\n\n  W         4.9892     0.010   512.078  0.000     4.970     5.008\n\n\n  H         9.9924     0.066   152.473  0.000     9.864    10.121\n\n\n\n\n  Omnibus:        0.846   Durbin-Watson:         2.049\n\n\n  Prob(Omnibus):  0.655   Jarque-Bera (JB):      0.738\n\n\n  Skew:          -0.058   Prob(JB):              0.691\n\n\n  Kurtosis:       3.066   Cond. No.               598.\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe price coefficient estimates are statistically significant and the model has a high R squared figure but fundamentally incorrect due to the simultaneity bias."
  },
  {
    "objectID": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html#detour-a-simpler-example",
    "href": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html#detour-a-simpler-example",
    "title": "Examined Algorithms",
    "section": "Detour: A simpler Example",
    "text": "Detour: A simpler Example\nWe’ll explain some the details with a simpler example first, and then proceed to show how to estimate the supply and demand system using instrumental variable regression.\n\nnp.random.seed(1235)\nz = np.random.uniform(0, 1, 1000)\ne_3 = np.random.normal(0, 3, 1000)\ne_1 = np.random.normal(0, 1, 1000)\nx = -1 + 5*z + 2*(e_1) + e_3\ny = 2 + 3*x + e_3\n\nX = pd.DataFrame({'X': x})\nX = sm.add_constant(X)\n\nZ = pd.DataFrame({'Z': z})\nZ = sm.add_constant(Z)\n\nOn the OLS linear model \\[ Y = \\beta X + \\epsilon \\] the beta coefficients can be estimated as follows: \\[ \\hat{\\beta} = (X^{'}X)^{-1}X^{'}y \\] under a number of conditions, but crucially we require that: \\[ E(X'\\epsilon) = 0 \\]\n\nbeta_hat_OLS = linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\nbeta_hat_OLS\n#True coefficient values 2,  3\n\narray([0.99580253, 3.60164346])\n\n\nThe failure of this assumption leads to skewed coefficient estimates. On the IV variable regression model we allow that the last assumption fails, so that: \\[ Y = \\beta X + \\psi \\] and we allow that \\[ E(X'\\psi) \\neq 0 \\] and choose an instrument Z as a proxy for X so specifically that \\[ E(Z'\\psi) = 0\\] then then estimator can be stated: \\[ Z'Y = Z'X\\beta + Z'\\psi \\Rightarrow \\hat\\beta =  [X'Z(Z'Z)^{-1}Z'X]^{-1}X'Z(Z'Z)^{-1}Z'y \\]\n\ndef iv_estimate(Z, X, y):\n    return linalg.inv(X.T.dot(Z).dot(linalg.inv(Z.T.dot(Z)).dot(Z.T.dot(X)))).dot(X.T.dot(Z).dot(linalg.inv(\n            Z.T.dot(Z)).dot(Z.T.dot(y))))\n\n\niv_estimate(Z, X, y)\n#True coefficient values 2,  3\n\narray([2.22432093, 2.84420838])\n\n\nwhich (a) gives better estimates of the coefficients and (b) can, thankfully, be simplified when we have as many instruments as variables that need to be “instrumented” to: \\[ (Z'X)^{-1}Z'y\\]. A good instrument needs to be carefully chosen so as to correlate with \\(X\\) with but not be influenced by the error terms in \\(y\\).\n\nlinalg.inv(Z.T.dot(X)).dot(Z.T).dot(y)\n\narray([2.22432093, 2.84420838])\n\n\n\ndef bootstrap_iv_estimator(reps, y, X_in, Z_in=None):\n    np.random.seed(100)\n    bs_mat = np.zeros(shape=(reps, X_in.shape[1]))\n    N = len(y)\n    if Z_in is None:\n        Z_in = X_in\n    for i in range(0, reps):\n        index_bs = np.random.randint(N, size=N)\n        y_bs = y[index_bs]\n        X_bs = X_in.iloc[index_bs,]\n        Z_bs = Z_in.iloc[index_bs,]\n        bs_mat[i,] = linalg.inv(Z_bs.T.dot(X_bs)).dot(Z_bs.T).dot(y_bs)\n    bs_mat = pd.DataFrame(bs_mat, columns = ['const', 'X'])\n    summary = pd.concat([bs_mat.mean(), bs_mat.std(), bs_mat.quantile(0.05), bs_mat.quantile(0.95)], axis=1)\n    summary.columns = ['coefs', 'std', '0.05', '0.95']\n    return summary\n\n\nestimates = bootstrap_iv_estimator(100, y, X) #OLS\nestimates\n\n\n\n\n\n  \n    \n      \n      coefs\n      std\n      0.05\n      0.95\n    \n  \n  \n    \n      const\n      0.996938\n      0.068499\n      0.892036\n      1.113305\n    \n    \n      X\n      3.599339\n      0.014494\n      3.576504\n      3.622485\n    \n  \n\n\n\n\n\nestimates = bootstrap_iv_estimator(100, y, X, Z) #IV\nestimates\n\n\n\n\n\n  \n    \n      \n      coefs\n      std\n      0.05\n      0.95\n    \n  \n  \n    \n      const\n      2.220228\n      0.169027\n      2.001413\n      2.465313\n    \n    \n      X\n      2.852359\n      0.080436\n      2.732629\n      2.967042"
  },
  {
    "objectID": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html#returning-to-our-model-of-supply-and-demand",
    "href": "posts/post-with-code/pricing/Simultaneity Bias and IV Regression.html#returning-to-our-model-of-supply-and-demand",
    "title": "Examined Algorithms",
    "section": "Returning to our model of Supply and Demand",
    "text": "Returning to our model of Supply and Demand\n\nN = 1000\nnp.random.seed(0)\nmu, sigma = 0, 7\ne_1 = np.random.normal(mu, sigma, N)\nW = np.random.normal(100, 20, N)\nH = np.random.uniform(0, 10, N)\n\n# True Values\na0, a1, a2, a3 = 2, 3, 5, 10\n\nmu, sigma = 0, 6\ne_2 = np.random.normal(mu, sigma, N)\nF = np.random.uniform(6, 30, N)\nO = np.random.normal(7, 4, N)\n\n#True Values\nb0, b1, b2, b3 = 4, 6, 2, 7\n\nP = np.random.normal(5, 2, N) + .2*(e_1+e_2)\n\ns = a0 + a1*P + a2*W + a3*H + e_1\nd = b0 + b1*P +b2*F + b3*O + e_2\n\nX_supply = pd.DataFrame({'P': P, 'W':W, 'H': H})\nX_supply = sm.add_constant(X_supply)\n\nZ_supply = pd.DataFrame({'F': F, 'O': O, 'W':W, 'H': H})\nZ_supply = sm.add_constant(Z_supply)\n\niv_estimate(Z_supply, X_supply, s)\n#True coefficient values 2, 3, 5, 10\n\narray([1.33465635, 3.32553682, 4.98887685, 9.97598787])"
  },
  {
    "objectID": "posts/post-with-code/bayesian_model_comparison/Hierarchical_Claims.html",
    "href": "posts/post-with-code/bayesian_model_comparison/Hierarchical_Claims.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "import numpy as np \nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport pymc3 as pm\nimport arviz as az\nimport theano.tensor as tt\nRANDOM_SEED = 13\n\n\nusetable = pd.read_csv('../data_files/insurace_res_usedata_tbl.csv')\nprint(usetable.columns)\nn_data = len(usetable)\nn_time = len(usetable['dev_year'].unique())\nn_cohort = len(usetable['acc_year'].unique())\ncohort_id, cohort = pd.factorize(usetable['acc_year'], sort=True)\ncohort_maxtime = usetable.groupby('acc_year')['dev_lag'].max().to_list()\nt_values = list(np.sort(usetable['dev_lag'].unique()).astype(int))\nt_idx, _  = pd.factorize(usetable['dev_lag'], sort=True)\npremium = usetable.groupby('acc_year')['premium'].mean().to_list()\nloss_real = usetable['cum_loss']\ncoords = {\"t_idx\": t_idx, \"cohort\": cohort, \"t_values\": t_values, 'obs': range(n_data),'cohort_id':cohort_id}\n\nIndex(['Unnamed: 0', 'grcode', 'grname', 'acc_year', 'dev_year', 'dev_lag',\n       'premium', 'cum_loss', 'loss_ratio'],\n      dtype='object')\n\n\n\nparams = {'mu_LR': [0, 0.5], 'sd_LR': [0, 0.5], 'loss_sd': [0, .7], 'omega': [0, .5], 'theta': [0, .5], \n         'tune': 2000, 'target_accept':.9}\n\ndef make_model(model_data, params,  growth_function ='logistic'):   \n    with pm.Model(coords=coords) as basic_model:\n\n        # Priors for unknown model parameters\n        mu_LR = pm.Normal('mu_LR', params['mu_LR'][0],  params['mu_LR'][1]);\n        sd_LR = pm.Lognormal('sd_LR', params['sd_LR'][0], params['sd_LR'][1]);\n\n        LR = pm.Lognormal('LR', mu_LR, sd_LR, dims='cohort')\n\n        loss_sd = pm.Lognormal('loss_sd', params['loss_sd'][0], params['loss_sd'][1]);\n\n        ## Parameters for the growth factor\n        omega = pm.Lognormal('omega', params['omega'][0], params['omega'][1]);\n        theta = pm.Lognormal('theta', params['theta'][0], params['theta'][1]);\n        \n        t = pm.Data(\"t\", t_values, dims='t_values')\n        if growth_function == 'logistic':\n            gf = pm.Deterministic('gf', (t**omega /  (t**omega + theta**omega)), dims='t_values')\n        else:\n            gf = pm.Deterministic('gf', 1-(pm.math.exp(-(t/theta)**omega)), dims='t_values')\n        ## Premium\n        prem = pm.Data(\"premium\", premium, dims='cohort')\n\n        t_indx = pm.Data(\"t_idx\", t_idx, dims='obs')\n        cohort_idx = pm.Data('c_idx', cohort_id, dims='obs')\n        obs = pm.Data('obs_idx', range(n_data), dims='obs')\n        lm = pm.Deterministic('lm', LR[cohort_idx] * prem[cohort_idx] *gf[t_indx], dims=('obs'))\n        #max_loss = pm.Deterministic('max_loss', tt.max(lm[t_idx]))\n\n        # Likelihood (sampling distribution) of observations\n        loss = pm.Normal('loss', lm, loss_sd * prem[cohort_idx], observed=loss_real, dims='obs')\n\n        prior_checks = pm.sample_prior_predictive(samples=100, random_seed=100)\n\n        trace = pm.sample(tune=params['tune'], init=\"adapt_diag\", \n                          target_accept=params['target_accept'])\n        ppc = pm.sample_posterior_predictive(trace, var_names=[\"loss\", \"LR\", \"lm\"], random_seed=100)\n\n        idata = az.from_pymc3(prior=prior_checks, posterior_predictive=ppc, trace=trace)\n        \n    return basic_model, idata, trace\n\n\nlogistic_model, logistic_idata, logistic_trace = make_model(usetable, params)\n#weibull_model, weibull_idata, weibull_trace = make_model(usetable, params, 'weibull')\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [theta, omega, loss_sd, LR, sd_LR, mu_LR]\n\n\n\n    \n        \n      \n      100.00% [6000/6000 00:11<00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\nSampling 2 chains for 2_000 tune and 1_000 draw iterations (4_000 + 2_000 draws total) took 11 seconds.\nThe number of effective samples is smaller than 25% for some parameters.\n\n\n\n    \n        \n      \n      100.00% [2000/2000 00:03<00:00]\n    \n    \n\n\n\naz.hdi(logistic_idata, var_names='lm').to_dataframe()\n\n\n\n\n\n  \n    \n      \n      \n      lm\n    \n    \n      hdi\n      obs\n      \n    \n  \n  \n    \n      lower\n      0\n      144.536642\n    \n    \n      1\n      345.880432\n    \n    \n      2\n      460.309382\n    \n    \n      3\n      522.841780\n    \n    \n      4\n      558.101366\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      higher\n      50\n      26942.142200\n    \n    \n      51\n      35365.145610\n    \n    \n      52\n      11878.581162\n    \n    \n      53\n      27013.067386\n    \n    \n      54\n      15285.784189\n    \n  \n\n110 rows × 1 columns\n\n\n\n\naxes = az.plot_forest(logistic_idata,\n                           kind='ridgeplot',\n                           var_names=['gf'],\n                           combined=True,\n                           ridgeplot_overlap=3,\n                           colors='white',\n                           figsize=(9, 7))\naxes[0].set_title('Loss Ratio Plots')\n\nText(0.5, 1.0, 'Loss Ratio Plots')\n\n\n\n\n\n\ng = pm.model_to_graphviz(logistic_model)\ng\n\n\n\n\n\nlogistic_idata\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:   (chain: 2, cohort: 10, draw: 1000, obs: 55, t_values: 10)\nCoordinates:\n  * chain     (chain) int64 0 1\n  * draw      (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * cohort    (cohort) int64 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\n  * t_values  (t_values) int64 1 2 3 4 5 6 7 8 9 10\n  * obs       (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    mu_LR     (chain, draw) float64 -0.187 -0.09882 ... -0.03967 0.007514\n    sd_LR     (chain, draw) float64 0.6209 0.3373 0.2758 ... 0.2711 0.2706\n    LR        (chain, draw, cohort) float64 0.6702 0.8334 ... 0.8889 0.7807\n    loss_sd   (chain, draw) float64 0.03186 0.02677 0.03494 ... 0.0332 0.02944\n    omega     (chain, draw) float64 2.047 2.063 2.117 ... 1.928 1.942 1.925\n    theta     (chain, draw) float64 1.73 1.75 1.699 1.712 ... 1.81 1.803 1.801\n    gf        (chain, draw, t_values) float64 0.2457 0.5738 ... 0.9568 0.9644\n    lm        (chain, draw, obs) float64 157.6 368.0 ... 2.567e+04 1.084e+04\nAttributes:\n    created_at:                 2021-09-15T19:52:34.368634\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2\n    sampling_time:              11.485157012939453\n    tuning_steps:               2000xarray.DatasetDimensions:chain: 2cohort: 10draw: 1000obs: 55t_values: 10Coordinates: (5)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])t_values(t_values)int641 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (8)mu_LR(chain, draw)float64-0.187 -0.09882 ... 0.007514array([[-0.18696659, -0.0988205 , -0.00810847, ...,  0.05311949,\n         0.20880435, -0.01687328],\n       [-0.11414088, -0.15425932, -0.22073582, ...,  0.0246603 ,\n        -0.03966546,  0.00751418]])sd_LR(chain, draw)float640.6209 0.3373 ... 0.2711 0.2706array([[0.62089146, 0.33727945, 0.27581849, ..., 0.42592517, 0.45086283,\n        0.48777715],\n       [0.32289577, 0.44645476, 0.49395078, ..., 0.32981233, 0.27107624,\n        0.27061255]])LR(chain, draw, cohort)float640.6702 0.8334 ... 0.8889 0.7807array([[[0.67022333, 0.83343362, 1.50476005, ..., 0.84746063,\n         0.75324862, 0.97680847],\n        [0.67852191, 0.85033978, 1.5128585 , ..., 0.8348323 ,\n         0.82923175, 0.78441512],\n        [0.65084057, 0.81301633, 1.50223526, ..., 0.86576549,\n         0.81385395, 1.1107245 ],\n        ...,\n        [0.69134537, 0.87360691, 1.53672948, ..., 0.88517673,\n         0.80569189, 0.80970752],\n        [0.67295471, 0.8505383 , 1.55445959, ..., 0.9003714 ,\n         0.81393564, 0.9427412 ],\n        [0.68806353, 0.85636222, 1.52987837, ..., 0.84520793,\n         0.77860489, 0.83172496]],\n\n       [[0.68118037, 0.84469149, 1.52453007, ..., 0.89498748,\n         0.80324074, 1.07317632],\n        [0.66690657, 0.85809314, 1.53552822, ..., 0.83513642,\n         0.8337994 , 0.70777644],\n        [0.69604056, 0.83549505, 1.52540707, ..., 0.85311843,\n         0.83007865, 0.58221531],\n        ...,\n        [0.66498216, 0.85115548, 1.55624145, ..., 0.84190874,\n         0.84255311, 0.92859606],\n        [0.67333019, 0.82537763, 1.546271  , ..., 0.88271328,\n         0.97106623, 0.93680019],\n        [0.66692557, 0.85914168, 1.55441355, ..., 0.88582965,\n         0.88886908, 0.78072028]]])loss_sd(chain, draw)float640.03186 0.02677 ... 0.0332 0.02944array([[0.03186241, 0.02677491, 0.03494346, ..., 0.02950565, 0.03100382,\n        0.0271407 ],\n       [0.02958521, 0.03073347, 0.03223621, ..., 0.03685306, 0.03319626,\n        0.02944148]])omega(chain, draw)float642.047 2.063 2.117 ... 1.942 1.925array([[2.04684299, 2.06348333, 2.11704502, ..., 2.02350399, 1.90782721,\n        2.02067407],\n       [2.10779663, 1.94099992, 2.01335649, ..., 1.92760786, 1.94155975,\n        1.92504071]])theta(chain, draw)float641.73 1.75 1.699 ... 1.803 1.801array([[1.72969432, 1.75027019, 1.6993544 , ..., 1.82016814, 1.74772066,\n        1.78329374],\n       [1.76581379, 1.75222828, 1.76108597, ..., 1.80953151, 1.80345161,\n        1.80096094]])gf(chain, draw, t_values)float640.2457 0.5738 ... 0.9568 0.9644array([[[0.24572263, 0.57375952, 0.75530888, ..., 0.95830295,\n         0.96694015, 0.97318136],\n        [0.23956308, 0.56837426, 0.75248162, ..., 0.9583462 ,\n         0.96703728, 0.97330626],\n        [0.24553678, 0.58537157, 0.76910278, ..., 0.9637262 ,\n         0.9715035 , 0.9770701 ],\n        ...,\n        [0.22935886, 0.54751893, 0.73323759, ..., 0.95238508,\n         0.96209965, 0.96915217],\n        [0.2563239 , 0.56395803, 0.73707071, ..., 0.94794775,\n         0.95798322, 0.96536932],\n        [0.23705689, 0.55767741, 0.74097726, ..., 0.95404246,\n         0.96342013, 0.97022592]],\n\n       [[0.23173901, 0.56524966, 0.75345663, ..., 0.96024765,\n         0.96871358, 0.9747871 ],\n        [0.25186615, 0.56382847, 0.73956758, ..., 0.95014575,\n         0.95992601, 0.96709377],\n        [0.24242684, 0.56368527, 0.74506788, ..., 0.95466309,\n         0.96388892, 0.97058815],\n        ...,\n        [0.24173286, 0.54807935, 0.72601378, ..., 0.94609631,\n         0.95656922, 0.96426648],\n        [0.24141408, 0.55004295, 0.72870832, ..., 0.94747034,\n         0.95775477, 0.96529882],\n        [0.24369253, 0.55027846, 0.72757083, ..., 0.94636716,\n         0.95677735, 0.96442982]]])lm(chain, draw, obs)float64157.6 368.0 ... 2.567e+04 1.084e+04array([[[  157.60741315,   368.01149881,   484.45793209, ...,\n          9713.72058482, 22681.4259549 , 13676.08451242],\n        [  155.55919791,   369.07124816,   488.62052768, ...,\n         10425.52435542, 24735.02909624, 10707.12910397],\n        [  152.93367252,   364.60127339,   479.03907065, ...,\n         10487.33492388, 25002.31377805, 15539.25209443],\n        ...,\n        [  151.74783701,   362.24811303,   485.12283192, ...,\n          9698.09893366, 23151.02546387, 10581.58893866],\n        [  165.07711826,   363.19893028,   474.68655257, ...,\n         10949.17187914, 24090.11955022, 13768.56860626],\n        [  156.09646279,   367.21763655,   487.91633048, ...,\n          9686.61005323, 22787.79407511, 11234.13198617]],\n\n       [[  151.0682531 ,   368.4803803 ,   491.1705532 , ...,\n          9768.92945857, 23828.02983572, 14170.24714991],\n        [  160.74842881,   359.85201422,   472.01390953, ...,\n         11021.31677114, 24672.35959206, 10157.1789823 ],\n        [  161.48314057,   375.47685914,   496.29778013, ...,\n         10560.92705732, 24556.02304463,  8042.1379405 ],\n        ...,\n        [  153.83587622,   348.79108035,   462.02640537, ...,\n         10688.95096964, 24234.98892586, 12789.97614652],\n        [  155.56168039,   354.43502325,   469.5628732 , ...,\n         12303.07167812, 28031.57876309, 12885.95974292],\n        [  155.53621549,   351.21400257,   464.37046077, ...,\n         11367.94919334, 25669.79609618, 10840.38943469]]])Attributes: (6)created_at :2021-09-15T19:52:34.368634arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2sampling_time :11.485157012939453tuning_steps :2000\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 2, cohort: 10, draw: 1000, obs: 55)\nCoordinates:\n  * chain    (chain) int64 0 1\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\n  * cohort   (cohort) int64 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\nData variables:\n    loss     (chain, draw, obs) float64 104.3 378.5 ... 2.644e+04 9.016e+03\n    LR       (chain, draw, cohort) float64 0.6702 0.8334 1.505 ... 0.8889 0.7807\n    lm       (chain, draw, obs) float64 157.6 368.0 ... 2.567e+04 1.084e+04\nAttributes:\n    created_at:                 2021-09-15T19:52:34.700411\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 2cohort: 10draw: 1000obs: 55Coordinates: (4)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])Data variables: (3)loss(chain, draw, obs)float64104.3 378.5 ... 2.644e+04 9.016e+03array([[[  104.25299878,   378.46062043,   519.61667247, ...,\n          8141.80268618, 21296.98149285, 13873.7213711 ],\n        [  168.57110391,   346.97788603,   520.63643116, ...,\n         10456.73879506, 25180.98640336,  9559.26030343],\n        [  109.58118079,   367.78282075,   464.86966193, ...,\n         11868.35118362, 22050.81611823, 19408.3032024 ],\n        ...,\n        [  106.27795502,   318.52680823,   510.43588738, ...,\n         11021.18578426, 22826.81417456, 12527.08543234],\n        [  171.64762377,   365.27633379,   457.72868241, ...,\n         12032.70208003, 22850.73831518, 12817.78476422],\n        [  138.78409136,   360.61093994,   480.48285626, ...,\n         10137.62921603, 20979.07259156, 13465.6251659 ]],\n\n       [[   97.16103159,   355.99598682,   512.81621348, ...,\n         11204.55137772, 23671.12813507, 12157.79410608],\n        [  131.26783254,   355.3390368 ,   460.1770658 , ...,\n         10682.5367999 , 23307.75834914,  8833.09949403],\n        [  194.26536002,   369.60172602,   492.15711635, ...,\n         10816.10853934, 23849.85945329,  5220.66454384],\n        ...,\n        [  103.44871872,   337.44440949,   461.41297666, ...,\n         10020.64412924, 24559.0585406 , 15059.02844165],\n        [  154.30464618,   327.38538816,   474.78895535, ...,\n         15633.51420392, 24639.37731399, 14192.66494558],\n        [  132.40295851,   395.31181651,   455.54853151, ...,\n         10602.40295703, 26440.182822  ,  9015.82177028]]])LR(chain, draw, cohort)float640.6702 0.8334 ... 0.8889 0.7807array([[[0.67022333, 0.83343362, 1.50476005, ..., 0.84746063,\n         0.75324862, 0.97680847],\n        [0.67852191, 0.85033978, 1.5128585 , ..., 0.8348323 ,\n         0.82923175, 0.78441512],\n        [0.65084057, 0.81301633, 1.50223526, ..., 0.86576549,\n         0.81385395, 1.1107245 ],\n        ...,\n        [0.69134537, 0.87360691, 1.53672948, ..., 0.88517673,\n         0.80569189, 0.80970752],\n        [0.67295471, 0.8505383 , 1.55445959, ..., 0.9003714 ,\n         0.81393564, 0.9427412 ],\n        [0.68806353, 0.85636222, 1.52987837, ..., 0.84520793,\n         0.77860489, 0.83172496]],\n\n       [[0.68118037, 0.84469149, 1.52453007, ..., 0.89498748,\n         0.80324074, 1.07317632],\n        [0.66690657, 0.85809314, 1.53552822, ..., 0.83513642,\n         0.8337994 , 0.70777644],\n        [0.69604056, 0.83549505, 1.52540707, ..., 0.85311843,\n         0.83007865, 0.58221531],\n        ...,\n        [0.66498216, 0.85115548, 1.55624145, ..., 0.84190874,\n         0.84255311, 0.92859606],\n        [0.67333019, 0.82537763, 1.546271  , ..., 0.88271328,\n         0.97106623, 0.93680019],\n        [0.66692557, 0.85914168, 1.55441355, ..., 0.88582965,\n         0.88886908, 0.78072028]]])lm(chain, draw, obs)float64157.6 368.0 ... 2.567e+04 1.084e+04array([[[  157.60741315,   368.01149881,   484.45793209, ...,\n          9713.72058482, 22681.4259549 , 13676.08451242],\n        [  155.55919791,   369.07124816,   488.62052768, ...,\n         10425.52435542, 24735.02909624, 10707.12910397],\n        [  152.93367252,   364.60127339,   479.03907065, ...,\n         10487.33492388, 25002.31377805, 15539.25209443],\n        ...,\n        [  151.74783701,   362.24811303,   485.12283192, ...,\n          9698.09893366, 23151.02546387, 10581.58893866],\n        [  165.07711826,   363.19893028,   474.68655257, ...,\n         10949.17187914, 24090.11955022, 13768.56860626],\n        [  156.09646279,   367.21763655,   487.91633048, ...,\n          9686.61005323, 22787.79407511, 11234.13198617]],\n\n       [[  151.0682531 ,   368.4803803 ,   491.1705532 , ...,\n          9768.92945857, 23828.02983572, 14170.24714991],\n        [  160.74842881,   359.85201422,   472.01390953, ...,\n         11021.31677114, 24672.35959206, 10157.1789823 ],\n        [  161.48314057,   375.47685914,   496.29778013, ...,\n         10560.92705732, 24556.02304463,  8042.1379405 ],\n        ...,\n        [  153.83587622,   348.79108035,   462.02640537, ...,\n         10688.95096964, 24234.98892586, 12789.97614652],\n        [  155.56168039,   354.43502325,   469.5628732 , ...,\n         12303.07167812, 28031.57876309, 12885.95974292],\n        [  155.53621549,   351.21400257,   464.37046077, ...,\n         11367.94919334, 25669.79609618, 10840.38943469]]])Attributes: (4)created_at :2021-09-15T19:52:34.700411arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 2, draw: 1000, obs: 55)\nCoordinates:\n  * chain    (chain) int64 0 1\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    loss     (chain, draw, obs) float64 -4.662 -4.996 -5.873 ... -9.298 -8.718\nAttributes:\n    created_at:                 2021-09-15T19:52:34.697576\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 2draw: 1000obs: 55Coordinates: (3)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (1)loss(chain, draw, obs)float64-4.662 -4.996 ... -9.298 -8.718array([[[ -4.66204085,  -4.99560314,  -5.8731991 , ...,  -9.83460231,\n          -8.44534821,  -8.71364938],\n        [ -4.55001055,  -5.15331207,  -6.69084417, ...,  -9.36860769,\n          -8.58761574,  -8.78868814],\n        [ -4.6063776 ,  -4.87522004,  -5.46053709, ...,  -9.09921933,\n          -8.79322533,  -9.84534805],\n        ...,\n        [ -4.47998193,  -4.79602114,  -6.09651578, ..., -10.0247998 ,\n          -8.28211525,  -8.86372827],\n        [ -4.89349193,  -4.82706018,  -5.39305197, ...,  -8.8306795 ,\n          -8.39185554,  -8.74504146],\n        [ -4.57138313,  -5.04378999,  -6.57694207, ..., -10.27797608,\n          -8.28719365,  -8.49661696]],\n\n       [[ -4.46588533,  -5.04744865,  -6.52047369, ...,  -9.93369734,\n          -8.29693424,  -8.96962124],\n        [ -4.74537964,  -4.71708996,  -5.27260529, ...,  -8.7861692 ,\n          -8.59379556,  -9.13006987],\n        [ -4.7742971 ,  -5.29597798,  -6.5881131 , ...,  -9.0816783 ,\n          -8.56772943, -11.11150275],\n        ...,\n        [ -4.65643627,  -4.58216115,  -4.86888165, ...,  -8.97653424,\n          -8.5695356 ,  -8.59666319],\n        [ -4.62960387,  -4.60504662,  -5.11415089, ...,  -8.396745  ,\n         -11.84581071,  -8.51334596],\n        [ -4.5772725 ,  -4.46633895,  -4.95876778, ...,  -8.58177161,\n          -9.29750017,  -8.71840896]]])Attributes: (4)created_at :2021-09-15T19:52:34.697576arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:           (chain: 2, draw: 1000)\nCoordinates:\n  * chain             (chain) int64 0 1\n  * draw              (draw) int64 0 1 2 3 4 5 6 ... 993 994 995 996 997 998 999\nData variables:\n    energy            (chain, draw) float64 418.4 411.7 410.1 ... 411.3 415.6\n    tree_size         (chain, draw) float64 7.0 7.0 7.0 7.0 ... 7.0 7.0 7.0 7.0\n    step_size         (chain, draw) float64 0.3709 0.3709 ... 0.3642 0.3642\n    mean_tree_accept  (chain, draw) float64 0.9878 1.0 0.6001 ... 0.9535 0.9023\n    step_size_bar     (chain, draw) float64 0.4186 0.4186 ... 0.4023 0.4023\n    max_energy_error  (chain, draw) float64 -0.3033 -0.9073 ... 0.1527 0.2162\n    diverging         (chain, draw) bool False False False ... False False False\n    energy_error      (chain, draw) float64 -0.08659 -0.9073 ... 0.05396 0.01052\n    depth             (chain, draw) int64 3 3 3 3 3 3 3 3 4 ... 4 3 3 4 3 3 3 3\n    lp                (chain, draw) float64 -409.4 -398.4 ... -405.5 -405.0\nAttributes:\n    created_at:                 2021-09-15T19:52:34.379572\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2\n    sampling_time:              11.485157012939453\n    tuning_steps:               2000xarray.DatasetDimensions:chain: 2draw: 1000Coordinates: (2)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (10)energy(chain, draw)float64418.4 411.7 410.1 ... 411.3 415.6array([[418.43883273, 411.65728711, 410.0767806 , ..., 410.51405687,\n        408.58036529, 407.35921943],\n       [412.80822536, 411.47900652, 413.17891412, ..., 411.18710152,\n        411.29444508, 415.58466124]])tree_size(chain, draw)float647.0 7.0 7.0 7.0 ... 7.0 7.0 7.0 7.0array([[ 7.,  7.,  7., ...,  7.,  7.,  7.],\n       [ 7., 15.,  7., ...,  7.,  7.,  7.]])step_size(chain, draw)float640.3709 0.3709 ... 0.3642 0.3642array([[0.37085185, 0.37085185, 0.37085185, ..., 0.37085185, 0.37085185,\n        0.37085185],\n       [0.36415012, 0.36415012, 0.36415012, ..., 0.36415012, 0.36415012,\n        0.36415012]])mean_tree_accept(chain, draw)float640.9878 1.0 0.6001 ... 0.9535 0.9023array([[0.98779061, 1.        , 0.60011115, ..., 0.93739839, 0.98545547,\n        0.9950132 ],\n       [0.43950928, 0.89151176, 0.84991465, ..., 0.90923646, 0.95350635,\n        0.90231548]])step_size_bar(chain, draw)float640.4186 0.4186 ... 0.4023 0.4023array([[0.41864123, 0.41864123, 0.41864123, ..., 0.41864123, 0.41864123,\n        0.41864123],\n       [0.40234341, 0.40234341, 0.40234341, ..., 0.40234341, 0.40234341,\n        0.40234341]])max_energy_error(chain, draw)float64-0.3033 -0.9073 ... 0.1527 0.2162array([[-0.30333558, -0.90728229,  0.69568241, ..., -0.31374491,\n        -0.2200231 , -0.40118408],\n       [ 1.42927712,  0.33138796,  0.36049719, ...,  0.18928538,\n         0.15266867,  0.21622557]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])energy_error(chain, draw)float64-0.08659 -0.9073 ... 0.01052array([[-0.08658624, -0.90728229,  0.52606004, ..., -0.05300835,\n         0.02450066, -0.40118408],\n       [ 0.38322751,  0.00231922,  0.16376489, ...,  0.12195152,\n         0.05395713,  0.01051789]])depth(chain, draw)int643 3 3 3 3 3 3 3 ... 4 3 3 4 3 3 3 3array([[3, 3, 3, ..., 3, 3, 3],\n       [3, 4, 3, ..., 3, 3, 3]])lp(chain, draw)float64-409.4 -398.4 ... -405.5 -405.0array([[-409.43533657, -398.44740163, -404.77855576, ..., -402.57284061,\n        -403.52737457, -400.50675223],\n       [-401.88597959, -402.44777691, -407.25696595, ..., -404.72076691,\n        -405.45093988, -404.98509764]])Attributes: (6)created_at :2021-09-15T19:52:34.379572arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2sampling_time :11.485157012939453tuning_steps :2000\n                      \n                  \n            \n            \n            \n                  \n                  prior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:         (LR_log___dim_0: 10, chain: 1, cohort: 10, draw: 100, obs: 55, t_values: 10)\nCoordinates:\n  * chain           (chain) int64 0\n  * draw            (draw) int64 0 1 2 3 4 5 6 7 8 ... 92 93 94 95 96 97 98 99\n  * LR_log___dim_0  (LR_log___dim_0) int64 0 1 2 3 4 5 6 7 8 9\n  * cohort          (cohort) int64 1988 1989 1990 1991 ... 1994 1995 1996 1997\n  * obs             (obs) int64 0 1 2 3 4 5 6 7 8 ... 46 47 48 49 50 51 52 53 54\n  * t_values        (t_values) int64 1 2 3 4 5 6 7 8 9 10\nData variables:\n    theta           (chain, draw) float64 0.5739 1.253 3.127 ... 1.24 1.031\n    LR_log__        (chain, draw, LR_log___dim_0) float64 -0.6171 ... 1.319\n    mu_LR           (chain, draw) float64 -0.8749 0.1713 ... -0.09251 -1.244\n    loss_sd_log__   (chain, draw) float64 0.1178 -0.06791 ... -0.3026 -1.538\n    theta_log__     (chain, draw) float64 -0.5553 0.2252 1.14 ... 0.2152 0.0308\n    omega_log__     (chain, draw) float64 0.4461 0.07193 ... 0.1054 -0.7762\n    LR              (chain, draw, cohort) float64 0.5395 0.2832 ... 3.739\n    loss_sd         (chain, draw) float64 1.125 0.9343 0.4603 ... 0.7389 0.2147\n    sd_LR_log__     (chain, draw) float64 -0.8523 -0.5681 -1.487 ... -0.3 0.7881\n    omega           (chain, draw) float64 1.562 1.075 1.892 ... 1.111 0.4602\n    sd_LR           (chain, draw) float64 0.4264 0.5666 0.2261 ... 0.7408 2.199\n    lm              (chain, draw, obs) float64 363.6 452.0 ... 1.058e+05\n    gf              (chain, draw, t_values) float64 0.7042 0.8755 ... 0.7399\nAttributes:\n    created_at:                 2021-09-15T19:52:34.707602\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:LR_log___dim_0: 10chain: 1cohort: 10draw: 100obs: 55t_values: 10Coordinates: (6)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 6 ... 94 95 96 97 98 99array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])LR_log___dim_0(LR_log___dim_0)int640 1 2 3 4 5 6 7 8 9array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])t_values(t_values)int641 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])Data variables: (13)theta(chain, draw)float640.5739 1.253 3.127 ... 1.24 1.031array([[0.57389012, 1.25260178, 3.12725504, 2.33099523, 1.5559494 ,\n        0.7639599 , 0.82833537, 0.80557966, 2.20412348, 0.43069581,\n        2.9093212 , 0.99399548, 0.86455246, 1.32475578, 1.44164061,\n        1.10965243, 1.07620636, 1.29017089, 2.16036625, 2.32360565,\n        0.55518025, 0.90958767, 1.01729211, 0.84623772, 0.82608081,\n        1.62644821, 1.1668848 , 1.25656072, 0.83539401, 0.65453681,\n        1.83275323, 0.46182585, 1.40712532, 1.19493509, 0.94248559,\n        0.93866154, 0.83764564, 2.25087097, 1.27622612, 0.57064121,\n        0.52945567, 0.88550147, 0.75327822, 1.37076844, 0.93745489,\n        1.51501646, 0.62072393, 1.25873035, 2.29427686, 0.68248132,\n        1.06593879, 0.66061023, 0.77861607, 1.41031485, 2.15389213,\n        0.5953411 , 0.56525208, 1.340487  , 0.69754309, 1.16981863,\n        1.33030513, 1.57734588, 0.8597188 , 0.35135396, 0.82764502,\n        0.6132911 , 0.7778497 , 1.74519489, 0.71525789, 0.83759542,\n        2.2658658 , 4.32680955, 1.6694292 , 1.51162302, 0.54751728,\n        1.73666316, 1.22802165, 0.55897645, 0.70077877, 0.67783987,\n        0.70416641, 1.29484782, 0.81285727, 0.7551851 , 1.18750981,\n        2.39590503, 1.30989887, 0.78067317, 1.26049998, 1.12601871,\n        0.74383829, 1.38761336, 1.04839036, 1.80706916, 0.89511397,\n        0.67815156, 1.02856998, 1.19869844, 1.24008379, 1.03128319]])LR_log__(chain, draw, LR_log___dim_0)float64-0.6171 -1.262 ... -3.077 1.319array([[[-6.17143188e-01, -1.26166054e+00, -6.22430937e-01,\n         -1.06125666e+00, -8.31483299e-01, -3.16974749e-01,\n         -2.83178673e-01, -1.61489186e+00, -7.51912234e-01,\n         -1.01947626e+00],\n        [ 4.70138943e-01, -4.07337019e-02,  4.41652909e-01,\n         -6.38739983e-01,  4.02340063e-01, -2.97949948e-01,\n          4.11264185e-01,  8.09602667e-02,  8.74517052e-01,\n          1.11387272e+00],\n        [ 4.26835799e-01,  1.36009991e+00,  6.65032809e-01,\n          6.50516967e-01,  5.06113871e-01,  2.54364564e-01,\n          4.35217322e-01,  5.51644954e-01,  7.92007920e-01,\n          3.54152889e-01],\n        [-1.23916176e+00, -3.71620985e-01, -7.31328680e-01,\n         -1.12553935e+00, -3.65059826e-02, -7.39102641e-01,\n          1.01824470e+00,  8.16408078e-01, -2.43349269e+00,\n         -4.80237285e-01],\n        [ 3.45828028e-01,  1.56488574e+00, -6.28437141e-02,\n          2.47892004e-01,  1.50385087e+00,  1.51489840e+00,\n          1.11068047e+00, -6.43011394e-01,  1.83483899e+00,\n          3.31755049e-01],\n...\n        [ 1.24232366e+00, -1.79165022e+00, -1.05836185e+00,\n         -3.09225403e+00,  6.56009661e-02, -7.50671829e-02,\n          5.89357991e-01,  8.63156384e-01, -2.81532734e-01,\n         -1.24847464e+00],\n        [-6.86571297e-01, -3.02133976e-01,  3.05860017e-01,\n          5.61005282e-04, -1.20440890e-01,  1.16642382e+00,\n         -2.26335350e-01, -9.18702845e-01, -8.41696486e-01,\n         -3.58155741e-01],\n        [ 1.09884268e+00,  1.13119969e+00, -6.78816255e-01,\n         -5.85593856e-01,  7.15943176e-01,  2.24503315e+00,\n         -4.51843600e-01,  3.07867675e-01,  9.31380940e-01,\n         -2.27855222e+00],\n        [-3.37456137e-01,  2.42518433e-01, -4.85528021e-01,\n         -3.01018200e-01, -5.69692766e-01, -9.29658170e-01,\n         -9.53172168e-01,  3.29390486e-02, -1.36732900e-01,\n          7.26723678e-01],\n        [-2.69919207e+00, -2.03199061e+00, -1.40009199e+00,\n         -1.02835591e+00,  2.65043778e+00,  3.31279141e-01,\n         -1.79784476e+00,  2.42504669e+00, -3.07654973e+00,\n          1.31886562e+00]]])mu_LR(chain, draw)float64-0.8749 0.1713 ... -0.09251 -1.244array([[-0.87488274,  0.1713402 ,  0.5765179 , -0.12621802,  0.49066039,\n         0.25710942,  0.11058983, -0.53502167, -0.09474792,  0.12750072,\n        -0.22901349,  0.21758174, -0.29179753,  0.40842354,  0.3363604 ,\n        -0.05220557, -0.26564019,  0.51486634, -0.21906781, -0.55915912,\n         0.80949083,  0.77080259, -0.12593957, -0.42121787,  0.09225935,\n         0.4685411 ,  0.36550017,  0.68077806, -0.16311903,  0.02783801,\n         0.1111998 , -0.7216085 , -0.37817615,  0.40822701,  0.37522238,\n        -0.22797346,  0.59481113, -0.84530841, -0.67819952, -0.61621726,\n        -0.27221958, -0.33408587,  0.00365728, -0.30646937,  0.64987404,\n        -0.86654781, -0.49165505,  0.17875388, -0.80678925,  0.73535693,\n        -0.5940088 , -0.2748731 , -0.47002308, -0.41396618,  0.05443173,\n         0.2539048 , -0.43111367,  0.62473487, -0.03980562, -0.44486574,\n        -0.44089919,  0.00931947,  0.11892231,  0.00677427, -0.8177647 ,\n        -0.52210494,  0.30651944,  0.36810261,  0.51346072, -0.71609531,\n        -0.92059415,  0.18304661, -0.16588857, -0.34460899,  1.01730378,\n        -0.27535721,  0.37522667, -0.65349617,  0.29028667, -0.55226155,\n         0.34506074,  0.34344503, -0.78334376,  0.45248706,  0.3894112 ,\n         0.21411644,  0.05443599,  0.01414182, -0.28941291, -0.5997256 ,\n        -0.852976  ,  0.18458198,  0.93828671, -0.18845168,  0.91596804,\n         0.00150872, -0.03801173,  0.0019788 , -0.09250706, -1.24357577]])loss_sd_log__(chain, draw)float640.1178 -0.06791 ... -0.3026 -1.538array([[ 0.11776897, -0.06791383, -0.77586015,  0.81418801, -1.54295884,\n        -0.08635401, -0.3073733 ,  0.44099133, -0.60868549, -0.42565676,\n         0.83953297,  1.90751457,  0.31553409, -0.58763441, -0.02813683,\n         0.61956292,  1.1102456 ,  1.41804546,  1.07121072,  1.2829134 ,\n         0.67823319,  0.11953965,  0.49559607,  0.91112716,  0.26419822,\n        -0.55153079,  0.64974865,  1.42332632,  0.40688404,  1.0925064 ,\n         0.0515981 , -0.0156497 ,  0.47990747, -0.15405117, -1.44822335,\n         0.44352918, -1.63864786, -0.29389226,  0.71212342, -0.27237528,\n        -0.51608351, -0.00461874,  0.19355207, -0.40968742,  1.59689048,\n         0.80746392,  0.90807538, -0.15603457,  0.24094942, -1.39879746,\n         0.38607813, -0.58429016, -0.56743807,  0.07267915, -0.20197823,\n        -0.65064838,  0.31415215, -0.38924771,  0.91696663, -0.48047711,\n        -0.79327309, -0.11503221,  0.74234383, -0.37075465, -0.29457163,\n        -1.81513457,  0.20137675,  0.56713415,  0.07135635, -0.19635674,\n        -0.4044409 , -0.17024778, -1.04476412,  0.16943208, -0.98912851,\n        -0.66173423,  1.065507  ,  1.40047042, -0.45946529, -0.33103924,\n         1.22237942,  1.01016131, -0.64361443,  0.76310702,  0.76479435,\n        -0.22690514,  0.57659883,  1.36853881, -0.45579136,  0.67822079,\n         0.29416748, -1.19540042, -0.00414685, -0.5183479 ,  0.94649573,\n        -0.45149198, -0.10859997, -0.04805518, -0.302624  , -1.53838435]])theta_log__(chain, draw)float64-0.5553 0.2252 ... 0.2152 0.0308array([[-0.55531734,  0.22522281,  1.14015564,  0.84629531,  0.44208591,\n        -0.26923997, -0.18833717, -0.21619318,  0.79032991, -0.84235321,\n         1.06791979, -0.00602262, -0.1455433 ,  0.28122813,  0.36578178,\n         0.10404684,  0.07344223,  0.25477469,  0.77027777,  0.84312014,\n        -0.58846245, -0.09476389,  0.0171443 , -0.16695497, -0.19106268,\n         0.48639862,  0.15433763,  0.2283784 , -0.17985179, -0.42382746,\n         0.60581933, -0.7725674 ,  0.34154884,  0.17809187, -0.05923465,\n        -0.06330032, -0.17716014,  0.81131724,  0.24390738, -0.56099462,\n        -0.63590583, -0.12160116, -0.28332064,  0.31537149, -0.06458664,\n         0.4154263 , -0.47686886,  0.23010355,  0.8304177 , -0.38202012,\n         0.06385591, -0.41459128, -0.25023721,  0.34381298,  0.7672765 ,\n        -0.51862076, -0.57048348,  0.29303298, -0.360191  ,  0.15684872,\n         0.28540834,  0.45574361, -0.15114992, -1.04596112, -0.18917094,\n        -0.48891559, -0.25122197,  0.55686623, -0.33511211, -0.17722009,\n         0.81795694,  1.46483045,  0.51248177,  0.41318392, -0.60236126,\n         0.55196555,  0.20540446, -0.58164794, -0.35556303, -0.3888442 ,\n        -0.35074057,  0.25839318, -0.20719975, -0.2807924 ,  0.17185852,\n         0.87376104,  0.26994994, -0.24759869,  0.23150845,  0.11868815,\n        -0.29593161,  0.32758526,  0.047256  ,  0.59170629, -0.11080422,\n        -0.38838448,  0.02816947,  0.18123634,  0.21517895,  0.03080384]])omega_log__(chain, draw)float640.4461 0.07193 ... 0.1054 -0.7762array([[ 0.44612894,  0.0719305 ,  0.63779866, -0.23449358,  0.23706241,\n         0.33419761, -0.60235794, -0.42556604,  0.44956226,  0.46859577,\n         0.47679333, -1.10255173, -1.1406529 , -0.30581554,  1.40485057,\n         0.08792286, -0.3153778 ,  0.92777277,  0.08014537, -0.71142477,\n        -0.18382993, -0.94314418,  0.37741105,  0.89061229,  0.32631473,\n         0.1187519 ,  0.29710071, -0.56646717,  0.24871394, -0.12211726,\n         0.52034076, -0.13353552, -0.33864184, -0.33580391,  1.31280722,\n        -0.55840032,  0.46812679, -0.28309119, -0.25267346, -0.43320864,\n         0.69851556, -0.43096099, -0.05757817,  0.00526835, -0.25980818,\n        -0.12478563,  1.12182973, -0.2414983 , -1.18449009, -0.46079041,\n         0.75950166,  0.27624921, -0.34634081, -0.53000413,  0.73357778,\n        -0.08395659,  0.37989795, -0.10431087, -0.02411082, -0.0916873 ,\n         0.64151539,  0.34238158,  0.25391359,  0.20807299,  0.07147848,\n        -0.03081378, -0.15740337,  0.33052872, -0.54743424, -0.17367713,\n         0.6791806 , -0.14902945, -0.04004145,  0.24734065,  0.68666321,\n        -0.46438288, -0.5281403 ,  0.10112753, -0.33912545, -0.01100623,\n        -0.95018485, -0.51196821,  0.10753402, -0.2633491 , -0.26221749,\n         0.2735264 ,  0.05936596, -1.15266121,  0.35999557, -0.64225228,\n         0.67848939,  0.24234314, -0.12805771,  0.67731315,  1.03797529,\n         0.3334401 , -0.50685985,  0.09829753,  0.10543659, -0.77619435]])LR(chain, draw, cohort)float640.5395 0.2832 ... 0.04612 3.739array([[[5.39483441e-01, 2.83183398e-01, 5.36638317e-01, 3.46020707e-01,\n         4.35402974e-01, 7.28349146e-01, 7.53385166e-01, 1.98912181e-01,\n         4.71464140e-01, 3.60783849e-01],\n        [1.60021652e+00, 9.60084765e-01, 1.55527582e+00, 5.27957240e-01,\n         1.49531975e+00, 7.42338495e-01, 1.50872389e+00, 1.08432781e+00,\n         2.39771704e+00, 3.04613240e+00],\n        [1.53240102e+00, 3.89658259e+00, 1.94455432e+00, 1.91653136e+00,\n         1.65883222e+00, 1.28964188e+00, 1.54529885e+00, 1.73610649e+00,\n         2.20782512e+00, 1.42497303e+00],\n        [2.89626892e-01, 6.89615568e-01, 4.81269113e-01, 3.24477413e-01,\n         9.64152326e-01, 4.77542250e-01, 2.76833126e+00, 2.26235901e+00,\n         8.77298833e-02, 6.18636581e-01],\n        [1.41315957e+00, 4.78212851e+00, 9.39090229e-01, 1.28132155e+00,\n         4.49898074e+00, 4.54895891e+00, 3.03642390e+00, 5.25706928e-01,\n         6.26412548e+00, 1.39341149e+00],\n        [9.57696979e-01, 1.86763571e+00, 4.13374817e+00, 1.56821742e+00,\n         1.25487332e+00, 2.51191010e+00, 8.10945988e+00, 6.10722752e-01,\n         1.41395916e+00, 9.82281959e+00],\n        [1.71912712e+00, 1.49080130e-01, 4.37317875e+00, 6.93848752e-01,\n         2.87455238e-01, 2.20250288e+00, 8.11983450e-01, 7.26701892e-01,\n...\n         9.69395468e-01, 6.87721577e+01, 1.36484576e-01, 1.13037865e-01,\n         4.30151712e-02, 2.71561166e+02],\n        [3.40901443e+00, 2.08439147e+00, 6.84379805e+00, 9.55929917e+00,\n         1.77361679e+00, 1.82087509e+01, 4.04976821e+00, 1.13211119e+01,\n         1.86859175e+00, 1.89121247e+00],\n        [3.46365246e+00, 1.66684876e-01, 3.47023822e-01, 4.53995073e-02,\n         1.06780054e+00, 9.27681160e-01, 1.80283061e+00, 2.37063152e+00,\n         7.54626213e-01, 2.86942154e-01],\n        [5.03298776e-01, 7.39239018e-01, 1.35779223e+00, 1.00056116e+00,\n         8.86529488e-01, 3.21049079e+00, 7.97450632e-01, 3.99036318e-01,\n         4.30978753e-01, 6.98964209e-01],\n        [3.00069125e+00, 3.09937257e+00, 5.07217053e-01, 5.56775119e-01,\n         2.04611562e+00, 9.44072847e+00, 6.36453703e-01, 1.36052095e+00,\n         2.53801160e+00, 1.02432399e-01],\n        [7.13583274e-01, 1.27445474e+00, 6.15372182e-01, 7.40064303e-01,\n         5.65699214e-01, 3.94688604e-01, 3.85516160e-01, 1.03348754e+00,\n         8.72203161e-01, 2.06829310e+00],\n        [6.72598317e-02, 1.31074344e-01, 2.46574280e-01, 3.57594394e-01,\n         1.41602364e+01, 1.39274851e+00, 1.65655532e-01, 1.13027571e+01,\n         4.61181024e-02, 3.73917734e+00]]])loss_sd(chain, draw)float641.125 0.9343 ... 0.7389 0.2147array([[1.12498418, 0.93434098, 0.46030768, 2.257342  , 0.21374772,\n        0.91726945, 0.73537603, 1.55424723, 0.54406558, 0.65334056,\n        2.31528541, 6.73632528, 1.37099135, 0.55564015, 0.97225532,\n        1.85811573, 3.03510372, 4.12904219, 2.91891135, 3.60713346,\n        1.97039335, 1.12697793, 1.64147638, 2.48712433, 1.30238633,\n        0.5760673 , 1.91505942, 4.15090475, 1.5021299 , 2.98173813,\n        1.05295248, 0.98447212, 1.61592487, 0.85722816, 0.23498741,\n        1.55819668, 0.19424251, 0.7453568 , 2.03831486, 0.76156841,\n        0.59685355, 0.99539191, 1.21355257, 0.66385773, 4.93765477,\n        2.24221434, 2.47954574, 0.85552962, 1.27245668, 0.24689368,\n        1.47119961, 0.55750146, 0.56697613, 1.07538545, 0.81711272,\n        0.5217074 , 1.36909803, 0.67756641, 2.50169031, 0.61848824,\n        0.45236175, 0.89133743, 2.10085379, 0.69021326, 0.74485059,\n        0.162816  , 1.22308549, 1.76320672, 1.07396387, 0.82171904,\n        0.66734982, 0.8434558 , 0.35177479, 1.18463189, 0.37190066,\n        0.51595577, 2.90231009, 4.05710805, 0.63162129, 0.71817699,\n        3.39525688, 2.74604394, 0.52539   , 2.14493023, 2.14855249,\n        0.79699638, 1.77997412, 3.9296046 , 0.63394609, 1.97036892,\n        1.34200864, 0.30258277, 0.99586174, 0.59550356, 2.57666449,\n        0.63667753, 0.89708921, 0.9530812 , 0.73887686, 0.21472775]])sd_LR_log__(chain, draw)float64-0.8523 -0.5681 ... -0.3 0.7881array([[-0.8523256 , -0.5681305 , -1.48665774,  0.01665864, -0.12444433,\n        -0.22508822,  0.0662139 ,  0.01110696,  0.15868399, -0.37620709,\n        -0.6481959 ,  0.04756972, -0.21185755, -0.59299178, -0.182731  ,\n        -0.63551152,  0.79308547,  0.34669533, -0.97904062, -0.06740066,\n        -0.77030801,  1.02335698, -0.69849967, -0.54858599, -0.11935643,\n        -0.71453345,  0.47450239, -0.00969879,  0.44729885,  0.37984656,\n        -0.74886019, -0.59694299,  0.64813129,  0.47613781, -0.60862707,\n        -0.07863258, -0.75379258,  0.05394207,  0.37352783,  0.21483822,\n        -0.70752146, -0.32037996,  0.38981315, -0.21906046,  1.03739658,\n        -0.17164884, -0.30831469,  0.38159182,  0.0964586 , -0.17422947,\n         1.14932697, -0.08260478,  0.23314968,  0.13499362, -0.15991552,\n        -0.5738708 ,  0.85181199, -0.36107539,  0.54684332, -0.11475888,\n        -0.00444933, -0.271599  ,  0.37653109, -0.80471945,  0.97163113,\n        -0.72371806,  0.06512423,  0.47468043, -1.00759436, -0.03977029,\n         0.15052473, -0.84244998,  0.1111954 , -0.34246087, -0.06310059,\n         0.99513682,  0.2614989 , -0.0081727 , -0.20790817, -0.67925147,\n        -0.25721495, -0.10803006,  0.21119011, -0.54702147,  0.61845394,\n        -0.11514234, -0.3522091 , -0.29568756,  0.36849758,  0.21793363,\n         0.88799679,  0.25653719,  0.58526349,  1.03885612, -0.22796101,\n         0.32458646, -0.08739078,  0.50863217, -0.29999152,  0.78808336]])omega(chain, draw)float641.562 1.075 1.892 ... 1.111 0.4602array([[1.56225289, 1.07458066, 1.89231067, 0.79097131, 1.26752023,\n        1.39681914, 0.5475191 , 0.65339983, 1.56762583, 1.59774901,\n        1.61090048, 0.33202277, 0.31961028, 0.73652247, 4.07491779,\n        1.09190388, 0.72951322, 2.52887052, 1.08344456, 0.49094421,\n        0.83207731, 0.38940156, 1.4585037 , 2.43662111, 1.38585147,\n        1.1260905 , 1.34595085, 0.56752688, 1.28237514, 0.88504458,\n        1.68260091, 0.87499639, 0.71273768, 0.71476325, 3.71659238,\n        0.57212355, 1.59699987, 0.75345108, 0.77672147, 0.64842519,\n        2.01076562, 0.64988426, 0.94404809, 1.00528225, 0.7711995 ,\n        0.8826861 , 3.07046718, 0.78545014, 0.30590212, 0.63078487,\n        2.13721089, 1.31817633, 0.7072714 , 0.58860254, 2.08251808,\n        0.91947117, 1.46213538, 0.90094518, 0.97617753, 0.91239041,\n        1.89935696, 1.40829757, 1.28906041, 1.23130304, 1.07409503,\n        0.96965612, 0.85435936, 1.39170375, 0.57843202, 0.84056825,\n        1.97226099, 0.86154374, 0.96074961, 1.28061528, 1.987074  ,\n        0.62852286, 0.58970061, 1.10641774, 0.71239307, 0.98905411,\n        0.38666954, 0.59931484, 1.11352875, 0.76847358, 0.76934368,\n        1.31459206, 1.06116352, 0.31579525, 1.43332306, 0.52610615,\n        1.97089823, 1.27423136, 0.87980261, 1.96858134, 2.82349447,\n        1.39576144, 0.60238419, 1.103291  , 1.11119564, 0.46015387]])sd_LR(chain, draw)float640.4264 0.5666 ... 0.7408 2.199array([[0.42642209, 0.56658368, 0.22612717, 1.01679817, 0.88298741,\n        0.79844578, 1.06845524, 1.01116888, 1.17196753, 0.68646016,\n        0.52298845, 1.04871932, 0.80907994, 0.55267134, 0.8329922 ,\n        0.52966448, 2.21020544, 1.41438574, 0.37567134, 0.93482058,\n        0.46287048, 2.78251998, 0.4973309 , 0.5777662 , 0.88749141,\n        0.4894204 , 1.60721423, 0.99034809, 1.56408166, 1.46206023,\n        0.47290527, 0.55049193, 1.91196459, 1.60984486, 0.54409737,\n        0.92437949, 0.47057846, 1.05542345, 1.45285099, 1.23966133,\n        0.49286427, 0.72587318, 1.47670485, 0.80327315, 2.82186097,\n        0.84227489, 0.73468409, 1.46461414, 1.10126398, 0.8401041 ,\n        3.15606807, 0.92071496, 1.26257045, 1.14452948, 0.85221578,\n        0.56334063, 2.34389012, 0.69692646, 1.72779033, 0.8915811 ,\n        0.99556055, 0.76215982, 1.45722085, 0.44721338, 2.64225081,\n        0.48494584, 1.0672916 , 1.60750041, 0.36509621, 0.96101016,\n        1.16244405, 0.43065414, 1.11761327, 0.7100209 , 0.93884903,\n        2.70509444, 1.29887552, 0.9918606 , 0.81228163, 0.50699635,\n        0.773202  , 0.89760061, 1.23514715, 0.57867084, 1.85605625,\n        0.89123928, 0.70313308, 0.74401985, 1.44556115, 1.24350453,\n        2.43025646, 1.29244683, 1.79546401, 2.82598257, 0.7961553 ,\n        1.38345842, 0.91631895, 1.66301492, 0.7408245 , 2.19917736]])lm(chain, draw, obs)float64363.6 452.0 ... 1.393e+03 1.058e+05array([[[3.63586344e+02, 4.52004030e+02, 4.80050323e+02, ...,\n         1.74248189e+04, 2.16622228e+04, 1.44767740e+04],\n        [6.73495425e+02, 9.54254126e+02, 1.10077873e+03, ...,\n         5.53406168e+04, 7.84103498e+04, 7.63308220e+04],\n        [1.51973891e+02, 4.40389457e+02, 7.04447107e+02, ...,\n         1.20074665e+04, 3.47951982e+04, 8.41392159e+03],\n        ...,\n        [1.29275537e+03, 1.83085465e+03, 2.10618368e+03, ...,\n         5.99623720e+04, 8.49212392e+04, 2.62740876e+03],\n        [3.00821669e+02, 4.30051269e+02, 4.96767448e+02, ...,\n         2.01637950e+04, 2.88259343e+04, 5.19124828e+04],\n        [3.19557383e+01, 3.70507050e+01, 3.99353603e+01, ...,\n         1.20158548e+03, 1.39316416e+03, 1.05770463e+05]]])gf(chain, draw, t_values)float640.7042 0.8755 ... 0.7304 0.7399array([[[0.70423484, 0.87549214, 0.92981535, 0.95405553, 0.9671339 ,\n         0.97507705, 0.98030598, 0.98395463, 0.98661528, 0.98862353],\n        [0.4397886 , 0.62312239, 0.71880211, 0.77689541, 0.81569534,\n         0.84335026, 0.86401169, 0.88000818, 0.89274339, 0.90311221],\n        [0.10362979, 0.30029807, 0.48035688, 0.61438231, 0.70848302,\n         0.77434966, 0.8212351 , 0.85537809, 0.88082887, 0.90022056],\n        [0.33863225, 0.46975303, 0.54972892, 0.60518664, 0.64648496,\n         0.67870678, 0.70469655, 0.7261912 , 0.74432036, 0.75985477],\n        [0.36346583, 0.57889167, 0.69681155, 0.76795562, 0.81451854,\n         0.84693205, 0.87058348, 0.88848725, 0.90244344, 0.91358541],\n        [0.59292719, 0.79319304, 0.87109128, 0.90990553, 0.93240122,\n         0.94679063, 0.95665138, 0.96376016, 0.96908782, 0.97320487],\n        [0.52575673, 0.6183686 , 0.66921337, 0.70310774, 0.7279638 ,\n         0.74727678, 0.76288122, 0.77585146, 0.78686662, 0.79638106],\n        [0.53525654, 0.64431972, 0.70247126, 0.74021092, 0.76725344,\n         0.78784695, 0.80419428, 0.8175696 , 0.82876883, 0.83831844],\n        [0.22462027, 0.46198695, 0.61852007, 0.71793657, 0.78313921,\n         0.82776462, 0.85954517, 0.88296832, 0.90074093, 0.91456169],\n        [0.79345344, 0.92080566, 0.95694122, 0.9723696 , 0.98049424,\n         0.98535136, 0.98851254, 0.99069905, 0.99228223, 0.99347016],\n...\n        [0.64181514, 0.87537833, 0.93982881, 0.96495661, 0.97714108,\n         0.98393041, 0.98809055, 0.99082103, 0.99270871, 0.99406781],\n        [0.39713443, 0.61439167, 0.72759741, 0.79397269, 0.83663171,\n         0.86595956, 0.88716659, 0.90311019, 0.9154717 , 0.92529784],\n        [0.48960751, 0.63836083, 0.71605695, 0.76460411, 0.798092  ,\n         0.82271025, 0.84163459, 0.85667234, 0.86893193, 0.87913289],\n        [0.23779221, 0.54975842, 0.73064268, 0.82695681, 0.88116276,\n         0.91391499, 0.93498184, 0.94924842, 0.95932411, 0.96668948],\n        [0.57758201, 0.90635774, 0.96816329, 0.98561457, 0.99228692,\n         0.99537608, 0.99700294, 0.99794238, 0.99852365, 0.99890312],\n        [0.63229895, 0.81899602, 0.88849829, 0.92251565, 0.94205031,\n         0.95447765, 0.96296374, 0.96906659, 0.97363229, 0.97715586],\n        [0.49575789, 0.5988251 , 0.65584172, 0.69383563, 0.72162392,\n         0.74314104, 0.76046247, 0.7748066 , 0.78694441, 0.79739179],\n        [0.45017679, 0.63755935, 0.7334373 , 0.79076283, 0.82859779,\n         0.85531065, 0.87511355, 0.89034527, 0.90240364, 0.91217334],\n        [0.4405067 , 0.62974341, 0.72743891, 0.78606014, 0.82481181,\n         0.85219095, 0.87249273, 0.88810812, 0.90046845, 0.91048055],\n        [0.49645643, 0.57561057, 0.62042586, 0.65106736, 0.67401798,\n         0.69217567, 0.70707985, 0.71964241, 0.73044606, 0.73988461]]])Attributes: (4)created_at :2021-09-15T19:52:34.707602arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  prior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 1, draw: 100, obs: 55)\nCoordinates:\n  * chain    (chain) int64 0\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 9 ... 90 91 92 93 94 95 96 97 98 99\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    loss     (chain, draw, obs) float64 -2.671e+03 1.343e+03 ... 9.672e+04\nAttributes:\n    created_at:                 2021-09-15T19:52:34.712755\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 1draw: 100obs: 55Coordinates: (3)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 6 ... 94 95 96 97 98 99array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (1)loss(chain, draw, obs)float64-2.671e+03 1.343e+03 ... 9.672e+04array([[[-2.67100326e+03,  1.34255155e+03,  1.40821615e+03, ...,\n          5.44546338e+04, -2.82037218e+04,  2.25204026e+04],\n        [ 6.91120357e+02,  1.25047752e+03,  2.34623536e+03, ...,\n          3.67907448e+04,  1.27985416e+05,  4.26089822e+04],\n        [-1.05994085e+02,  7.82732937e+02,  3.65909060e+02, ...,\n         -1.07724563e+04,  2.61310680e+04,  2.36614675e+04],\n        ...,\n        [ 8.83043472e+02,  2.15344855e+03,  3.03744893e+03, ...,\n          4.12138121e+04, -2.16126888e+04,  5.72089335e+03],\n        [ 1.13652047e+03,  1.26135401e+03,  1.58322237e+03, ...,\n         -4.63703410e+04,  6.47498746e+04,  1.04504263e+05],\n        [ 1.67803663e+02,  2.66545568e+02, -9.96214671e+01, ...,\n         -2.17424973e+04, -9.03439475e+03,  9.67249413e+04]]])Attributes: (4)created_at :2021-09-15T19:52:34.712755arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (obs: 55)\nCoordinates:\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    loss     (obs) int32 133 333 431 570 615 ... 26012 31677 12604 23446 12292\nAttributes:\n    created_at:                 2021-09-15T19:52:34.713725\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:obs: 55Coordinates: (1)obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (1)loss(obs)int32133 333 431 ... 12604 23446 12292array([  133,   333,   431,   570,   615,   615,   615,   614,   614,\n         614,   934,  1746,  2365,  2579,  2763,  2966,  2940,  2978,\n        2978,  2030,  4864,  6880,  8087,  8595,  8743,  8763,  8762,\n        4537, 11527, 15123, 16656, 17321, 18076, 18308,  7564, 16061,\n       22465, 25204, 26517, 27124,  8343, 19900, 26732, 30079, 31249,\n       12565, 26922, 33867, 38338, 13437, 26012, 31677, 12604, 23446,\n       12292], dtype=int32)Attributes: (4)created_at :2021-09-15T19:52:34.713725arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  constant_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:   (cohort: 10, obs: 55, t_values: 10)\nCoordinates:\n  * t_values  (t_values) int64 1 2 3 4 5 6 7 8 9 10\n  * cohort    (cohort) int64 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\n  * obs       (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    t         (t_values) int32 1 2 3 4 5 6 7 8 9 10\n    premium   (cohort) int32 957 3695 6138 17533 ... 46095 51512 52481 56978\n    t_idx     (obs) int32 0 1 2 3 4 5 6 7 8 9 0 1 2 ... 3 4 0 1 2 3 0 1 2 0 1 0\n    c_idx     (obs) int32 0 0 0 0 0 0 0 0 0 0 1 1 1 ... 5 5 6 6 6 6 7 7 7 8 8 9\n    obs_idx   (obs) float64 0.0 1.0 2.0 3.0 4.0 5.0 ... 50.0 51.0 52.0 53.0 54.0\nAttributes:\n    created_at:                 2021-09-15T19:52:34.715851\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:cohort: 10obs: 55t_values: 10Coordinates: (3)t_values(t_values)int641 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (5)t(t_values)int321 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=int32)premium(cohort)int32957 3695 6138 ... 51512 52481 56978array([  957,  3695,  6138, 17533, 29341, 37194, 46095, 51512, 52481,\n       56978], dtype=int32)t_idx(obs)int320 1 2 3 4 5 6 7 ... 2 3 0 1 2 0 1 0array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2,\n       3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3,\n       4, 0, 1, 2, 3, 0, 1, 2, 0, 1, 0], dtype=int32)c_idx(obs)int320 0 0 0 0 0 0 0 ... 6 6 7 7 7 8 8 9array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2,\n       2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5,\n       5, 6, 6, 6, 6, 7, 7, 7, 8, 8, 9], dtype=int32)obs_idx(obs)float640.0 1.0 2.0 3.0 ... 52.0 53.0 54.0array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,\n       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,\n       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51.,\n       52., 53., 54.])Attributes: (4)created_at :2021-09-15T19:52:34.715851arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n              \n            \n            \n\n\n\nusetable[['grcode','grname','acc_year','dev_year','dev_lag','premium','cum_loss','loss_ratio']].head(10)\n\n\n\n\n\n  \n    \n      \n      grcode\n      grname\n      acc_year\n      dev_year\n      dev_lag\n      premium\n      cum_loss\n      loss_ratio\n    \n  \n  \n    \n      0\n      43\n      IDS Property Cas Ins Co\n      1988\n      1988\n      1\n      957\n      133\n      0.138976\n    \n    \n      1\n      43\n      IDS Property Cas Ins Co\n      1988\n      1989\n      2\n      957\n      333\n      0.347962\n    \n    \n      2\n      43\n      IDS Property Cas Ins Co\n      1988\n      1990\n      3\n      957\n      431\n      0.450366\n    \n    \n      3\n      43\n      IDS Property Cas Ins Co\n      1988\n      1991\n      4\n      957\n      570\n      0.595611\n    \n    \n      4\n      43\n      IDS Property Cas Ins Co\n      1988\n      1992\n      5\n      957\n      615\n      0.642633\n    \n    \n      5\n      43\n      IDS Property Cas Ins Co\n      1988\n      1993\n      6\n      957\n      615\n      0.642633\n    \n    \n      6\n      43\n      IDS Property Cas Ins Co\n      1988\n      1994\n      7\n      957\n      615\n      0.642633\n    \n    \n      7\n      43\n      IDS Property Cas Ins Co\n      1988\n      1995\n      8\n      957\n      614\n      0.641588\n    \n    \n      8\n      43\n      IDS Property Cas Ins Co\n      1988\n      1996\n      9\n      957\n      614\n      0.641588\n    \n    \n      9\n      43\n      IDS Property Cas Ins Co\n      1988\n      1997\n      10\n      957\n      614\n      0.641588\n    \n  \n\n\n\n\n\nprediction_coords = {\"obs\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}\nwith logistic_model:\n    pm.set_data({'premium':np.array([800]*10), \n                 'obs_idx':[21, 11, 12, 13, 14, 15, 16, 17, 18, 50]\n                })\n    ppc = pm.sample_posterior_predictive(logistic_trace,  var_names=[\"lm\"])\n    \n\n\n    \n        \n      \n      100.00% [2000/2000 00:01<00:00]\n    \n    \n\n\n\npd.DataFrame(ppc['lm']).T[0:10].plot(legend=False)\n#logistic_idata.constant_data.to_dataframe()\n#usetable\n#cohort_id\n\n<AxesSubplot:>\n\n\n\n\n\n\nyears = usetable['acc_year'].unique()\nfig, axs = plt.subplots((int(len(years)/2)), 2, figsize=(20,10))\naxs = axs.flatten()\nfor ax, year in zip(axs, years):\n    usetable[usetable['acc_year'] == year]['loss_ratio'].plot(ax=ax, title=\"Loss Ratio Curve for: \" + str(year))\n\n\n\n\n\nfrom graphviz import Digraph, Graph\n\n# Packages\np = Digraph()\np.node('Echo', 'Echo')\np.node('Severen', 'Severen')\np.node('Panacea', 'Panacea')\n\n\n## Frameworks\nf = Digraph()\nf.node('ACDC', 'ACDC Framework')\nf.node('Jenkins', 'Jenkins')\n\np.edge('Echo', 'Panacea')\n\np.subgraph(f)\n\nf.edge('Jenkins', 'Echo')\np\n\n\n\n\n\ng = Digraph('G', filename='cluster_edge.gv')\ng.attr(compound='true')\ng.node('e', 'Echo')\ng.node('s', 'Severen')\ng.node('p', 'Panacea')\ng.node('j', 'Jenkins')\ng.node('slack', 'Slack')\n\n\nwith g.subgraph(name='cluster0') as c:\n    c.edges(['jb', 'ac', 'bd', 'cd'])\n\nwith g.subgraph(name='cluster1') as c:\n    c.edges(['ps', 'pe'])\n\n\ng\n\n\n\n\n\nidata\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:   (chain: 2, cohort: 10, draw: 1000, obs: 55, t_values: 10)\nCoordinates:\n  * chain     (chain) int64 0 1\n  * draw      (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * cohort    (cohort) int64 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\n  * t_values  (t_values) int64 1 2 3 4 5 6 7 8 9 10\n  * obs       (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    mu_LR     (chain, draw) float64 -0.187 -0.09882 ... -0.03967 0.007514\n    sd_LR     (chain, draw) float64 0.6209 0.3373 0.2758 ... 0.2711 0.2706\n    LR        (chain, draw, cohort) float64 0.6702 0.8334 ... 0.8889 0.7807\n    loss_sd   (chain, draw) float64 0.03186 0.02677 0.03494 ... 0.0332 0.02944\n    omega     (chain, draw) float64 2.047 2.063 2.117 ... 1.928 1.942 1.925\n    theta     (chain, draw) float64 1.73 1.75 1.699 1.712 ... 1.81 1.803 1.801\n    gf        (chain, draw, t_values) float64 0.2457 0.5738 ... 0.9568 0.9644\n    lm        (chain, draw, obs) float64 157.6 368.0 ... 2.567e+04 1.084e+04\nAttributes:\n    created_at:                 2021-07-10T22:10:08.100990\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2\n    sampling_time:              10.192897081375122\n    tuning_steps:               2000xarray.DatasetDimensions:chain: 2cohort: 10draw: 1000obs: 55t_values: 10Coordinates: (5)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])t_values(t_values)int641 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (8)mu_LR(chain, draw)float64-0.187 -0.09882 ... 0.007514array([[-0.18696659, -0.0988205 , -0.00810847, ...,  0.05311949,\n         0.20880435, -0.01687328],\n       [-0.11414088, -0.15425932, -0.22073582, ...,  0.0246603 ,\n        -0.03966546,  0.00751418]])sd_LR(chain, draw)float640.6209 0.3373 ... 0.2711 0.2706array([[0.62089146, 0.33727945, 0.27581849, ..., 0.42592517, 0.45086283,\n        0.48777715],\n       [0.32289577, 0.44645476, 0.49395078, ..., 0.32981233, 0.27107624,\n        0.27061255]])LR(chain, draw, cohort)float640.6702 0.8334 ... 0.8889 0.7807array([[[0.67022333, 0.83343362, 1.50476005, ..., 0.84746063,\n         0.75324862, 0.97680847],\n        [0.67852191, 0.85033978, 1.5128585 , ..., 0.8348323 ,\n         0.82923175, 0.78441512],\n        [0.65084057, 0.81301633, 1.50223526, ..., 0.86576549,\n         0.81385395, 1.1107245 ],\n        ...,\n        [0.69134537, 0.87360691, 1.53672948, ..., 0.88517673,\n         0.80569189, 0.80970752],\n        [0.67295471, 0.8505383 , 1.55445959, ..., 0.9003714 ,\n         0.81393564, 0.9427412 ],\n        [0.68806353, 0.85636222, 1.52987837, ..., 0.84520793,\n         0.77860489, 0.83172496]],\n\n       [[0.68118037, 0.84469149, 1.52453007, ..., 0.89498748,\n         0.80324074, 1.07317632],\n        [0.66690657, 0.85809314, 1.53552822, ..., 0.83513642,\n         0.8337994 , 0.70777644],\n        [0.69604056, 0.83549505, 1.52540707, ..., 0.85311843,\n         0.83007865, 0.58221531],\n        ...,\n        [0.66498216, 0.85115548, 1.55624145, ..., 0.84190874,\n         0.84255311, 0.92859606],\n        [0.67333019, 0.82537763, 1.546271  , ..., 0.88271328,\n         0.97106623, 0.93680019],\n        [0.66692557, 0.85914168, 1.55441355, ..., 0.88582965,\n         0.88886908, 0.78072028]]])loss_sd(chain, draw)float640.03186 0.02677 ... 0.0332 0.02944array([[0.03186241, 0.02677491, 0.03494346, ..., 0.02950565, 0.03100382,\n        0.0271407 ],\n       [0.02958521, 0.03073347, 0.03223621, ..., 0.03685306, 0.03319626,\n        0.02944148]])omega(chain, draw)float642.047 2.063 2.117 ... 1.942 1.925array([[2.04684299, 2.06348333, 2.11704502, ..., 2.02350399, 1.90782721,\n        2.02067407],\n       [2.10779663, 1.94099992, 2.01335649, ..., 1.92760786, 1.94155975,\n        1.92504071]])theta(chain, draw)float641.73 1.75 1.699 ... 1.803 1.801array([[1.72969432, 1.75027019, 1.6993544 , ..., 1.82016814, 1.74772066,\n        1.78329374],\n       [1.76581379, 1.75222828, 1.76108597, ..., 1.80953151, 1.80345161,\n        1.80096094]])gf(chain, draw, t_values)float640.2457 0.5738 ... 0.9568 0.9644array([[[0.24572263, 0.57375952, 0.75530888, ..., 0.95830295,\n         0.96694015, 0.97318136],\n        [0.23956308, 0.56837426, 0.75248162, ..., 0.9583462 ,\n         0.96703728, 0.97330626],\n        [0.24553678, 0.58537157, 0.76910278, ..., 0.9637262 ,\n         0.9715035 , 0.9770701 ],\n        ...,\n        [0.22935886, 0.54751893, 0.73323759, ..., 0.95238508,\n         0.96209965, 0.96915217],\n        [0.2563239 , 0.56395803, 0.73707071, ..., 0.94794775,\n         0.95798322, 0.96536932],\n        [0.23705689, 0.55767741, 0.74097726, ..., 0.95404246,\n         0.96342013, 0.97022592]],\n\n       [[0.23173901, 0.56524966, 0.75345663, ..., 0.96024765,\n         0.96871358, 0.9747871 ],\n        [0.25186615, 0.56382847, 0.73956758, ..., 0.95014575,\n         0.95992601, 0.96709377],\n        [0.24242684, 0.56368527, 0.74506788, ..., 0.95466309,\n         0.96388892, 0.97058815],\n        ...,\n        [0.24173286, 0.54807935, 0.72601378, ..., 0.94609631,\n         0.95656922, 0.96426648],\n        [0.24141408, 0.55004295, 0.72870832, ..., 0.94747034,\n         0.95775477, 0.96529882],\n        [0.24369253, 0.55027846, 0.72757083, ..., 0.94636716,\n         0.95677735, 0.96442982]]])lm(chain, draw, obs)float64157.6 368.0 ... 2.567e+04 1.084e+04array([[[  157.60741315,   368.01149881,   484.45793209, ...,\n          9713.72058482, 22681.4259549 , 13676.08451242],\n        [  155.55919791,   369.07124816,   488.62052768, ...,\n         10425.52435542, 24735.02909624, 10707.12910397],\n        [  152.93367252,   364.60127339,   479.03907065, ...,\n         10487.33492388, 25002.31377805, 15539.25209443],\n        ...,\n        [  151.74783701,   362.24811303,   485.12283192, ...,\n          9698.09893366, 23151.02546387, 10581.58893866],\n        [  165.07711826,   363.19893028,   474.68655257, ...,\n         10949.17187914, 24090.11955022, 13768.56860626],\n        [  156.09646279,   367.21763655,   487.91633048, ...,\n          9686.61005323, 22787.79407511, 11234.13198617]],\n\n       [[  151.0682531 ,   368.4803803 ,   491.1705532 , ...,\n          9768.92945857, 23828.02983572, 14170.24714991],\n        [  160.74842881,   359.85201422,   472.01390953, ...,\n         11021.31677114, 24672.35959206, 10157.1789823 ],\n        [  161.48314057,   375.47685914,   496.29778013, ...,\n         10560.92705732, 24556.02304463,  8042.1379405 ],\n        ...,\n        [  153.83587622,   348.79108035,   462.02640537, ...,\n         10688.95096964, 24234.98892586, 12789.97614652],\n        [  155.56168039,   354.43502325,   469.5628732 , ...,\n         12303.07167812, 28031.57876309, 12885.95974292],\n        [  155.53621549,   351.21400257,   464.37046077, ...,\n         11367.94919334, 25669.79609618, 10840.38943469]]])Attributes: (6)created_at :2021-07-10T22:10:08.100990arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2sampling_time :10.192897081375122tuning_steps :2000\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 2, cohort: 10, draw: 1000, obs: 55)\nCoordinates:\n  * chain    (chain) int64 0 1\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\n  * cohort   (cohort) int64 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\nData variables:\n    loss     (chain, draw, obs) float64 104.3 378.5 ... 2.644e+04 9.016e+03\n    LR       (chain, draw, cohort) float64 0.6702 0.8334 1.505 ... 0.8889 0.7807\n    lm       (chain, draw, obs) float64 157.6 368.0 ... 2.567e+04 1.084e+04\nAttributes:\n    created_at:                 2021-07-10T22:10:08.573587\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 2cohort: 10draw: 1000obs: 55Coordinates: (4)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])Data variables: (3)loss(chain, draw, obs)float64104.3 378.5 ... 2.644e+04 9.016e+03array([[[  104.25299878,   378.46062043,   519.61667247, ...,\n          8141.80268618, 21296.98149285, 13873.7213711 ],\n        [  168.57110391,   346.97788603,   520.63643116, ...,\n         10456.73879506, 25180.98640336,  9559.26030343],\n        [  109.58118079,   367.78282075,   464.86966193, ...,\n         11868.35118362, 22050.81611823, 19408.3032024 ],\n        ...,\n        [  106.27795502,   318.52680823,   510.43588738, ...,\n         11021.18578426, 22826.81417456, 12527.08543234],\n        [  171.64762377,   365.27633379,   457.72868241, ...,\n         12032.70208003, 22850.73831518, 12817.78476422],\n        [  138.78409136,   360.61093994,   480.48285626, ...,\n         10137.62921603, 20979.07259156, 13465.6251659 ]],\n\n       [[   97.16103159,   355.99598682,   512.81621348, ...,\n         11204.55137772, 23671.12813507, 12157.79410608],\n        [  131.26783254,   355.3390368 ,   460.1770658 , ...,\n         10682.5367999 , 23307.75834914,  8833.09949403],\n        [  194.26536002,   369.60172602,   492.15711635, ...,\n         10816.10853934, 23849.85945329,  5220.66454384],\n        ...,\n        [  103.44871872,   337.44440949,   461.41297666, ...,\n         10020.64412924, 24559.0585406 , 15059.02844165],\n        [  154.30464618,   327.38538816,   474.78895535, ...,\n         15633.51420392, 24639.37731399, 14192.66494558],\n        [  132.40295851,   395.31181651,   455.54853151, ...,\n         10602.40295703, 26440.182822  ,  9015.82177028]]])LR(chain, draw, cohort)float640.6702 0.8334 ... 0.8889 0.7807array([[[0.67022333, 0.83343362, 1.50476005, ..., 0.84746063,\n         0.75324862, 0.97680847],\n        [0.67852191, 0.85033978, 1.5128585 , ..., 0.8348323 ,\n         0.82923175, 0.78441512],\n        [0.65084057, 0.81301633, 1.50223526, ..., 0.86576549,\n         0.81385395, 1.1107245 ],\n        ...,\n        [0.69134537, 0.87360691, 1.53672948, ..., 0.88517673,\n         0.80569189, 0.80970752],\n        [0.67295471, 0.8505383 , 1.55445959, ..., 0.9003714 ,\n         0.81393564, 0.9427412 ],\n        [0.68806353, 0.85636222, 1.52987837, ..., 0.84520793,\n         0.77860489, 0.83172496]],\n\n       [[0.68118037, 0.84469149, 1.52453007, ..., 0.89498748,\n         0.80324074, 1.07317632],\n        [0.66690657, 0.85809314, 1.53552822, ..., 0.83513642,\n         0.8337994 , 0.70777644],\n        [0.69604056, 0.83549505, 1.52540707, ..., 0.85311843,\n         0.83007865, 0.58221531],\n        ...,\n        [0.66498216, 0.85115548, 1.55624145, ..., 0.84190874,\n         0.84255311, 0.92859606],\n        [0.67333019, 0.82537763, 1.546271  , ..., 0.88271328,\n         0.97106623, 0.93680019],\n        [0.66692557, 0.85914168, 1.55441355, ..., 0.88582965,\n         0.88886908, 0.78072028]]])lm(chain, draw, obs)float64157.6 368.0 ... 2.567e+04 1.084e+04array([[[  157.60741315,   368.01149881,   484.45793209, ...,\n          9713.72058482, 22681.4259549 , 13676.08451242],\n        [  155.55919791,   369.07124816,   488.62052768, ...,\n         10425.52435542, 24735.02909624, 10707.12910397],\n        [  152.93367252,   364.60127339,   479.03907065, ...,\n         10487.33492388, 25002.31377805, 15539.25209443],\n        ...,\n        [  151.74783701,   362.24811303,   485.12283192, ...,\n          9698.09893366, 23151.02546387, 10581.58893866],\n        [  165.07711826,   363.19893028,   474.68655257, ...,\n         10949.17187914, 24090.11955022, 13768.56860626],\n        [  156.09646279,   367.21763655,   487.91633048, ...,\n          9686.61005323, 22787.79407511, 11234.13198617]],\n\n       [[  151.0682531 ,   368.4803803 ,   491.1705532 , ...,\n          9768.92945857, 23828.02983572, 14170.24714991],\n        [  160.74842881,   359.85201422,   472.01390953, ...,\n         11021.31677114, 24672.35959206, 10157.1789823 ],\n        [  161.48314057,   375.47685914,   496.29778013, ...,\n         10560.92705732, 24556.02304463,  8042.1379405 ],\n        ...,\n        [  153.83587622,   348.79108035,   462.02640537, ...,\n         10688.95096964, 24234.98892586, 12789.97614652],\n        [  155.56168039,   354.43502325,   469.5628732 , ...,\n         12303.07167812, 28031.57876309, 12885.95974292],\n        [  155.53621549,   351.21400257,   464.37046077, ...,\n         11367.94919334, 25669.79609618, 10840.38943469]]])Attributes: (4)created_at :2021-07-10T22:10:08.573587arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 2, draw: 1000, obs: 55)\nCoordinates:\n  * chain    (chain) int64 0 1\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    loss     (chain, draw, obs) float64 -4.662 -4.996 -5.873 ... -9.298 -8.718\nAttributes:\n    created_at:                 2021-07-10T22:10:08.569795\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 2draw: 1000obs: 55Coordinates: (3)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (1)loss(chain, draw, obs)float64-4.662 -4.996 ... -9.298 -8.718array([[[ -4.66204085,  -4.99560314,  -5.8731991 , ...,  -9.83460231,\n          -8.44534821,  -8.71364938],\n        [ -4.55001055,  -5.15331207,  -6.69084417, ...,  -9.36860769,\n          -8.58761574,  -8.78868814],\n        [ -4.6063776 ,  -4.87522004,  -5.46053709, ...,  -9.09921933,\n          -8.79322533,  -9.84534805],\n        ...,\n        [ -4.47998193,  -4.79602114,  -6.09651578, ..., -10.0247998 ,\n          -8.28211525,  -8.86372827],\n        [ -4.89349193,  -4.82706018,  -5.39305197, ...,  -8.8306795 ,\n          -8.39185554,  -8.74504146],\n        [ -4.57138313,  -5.04378999,  -6.57694207, ..., -10.27797608,\n          -8.28719365,  -8.49661696]],\n\n       [[ -4.46588533,  -5.04744865,  -6.52047369, ...,  -9.93369734,\n          -8.29693424,  -8.96962124],\n        [ -4.74537964,  -4.71708996,  -5.27260529, ...,  -8.7861692 ,\n          -8.59379556,  -9.13006987],\n        [ -4.7742971 ,  -5.29597798,  -6.5881131 , ...,  -9.0816783 ,\n          -8.56772943, -11.11150275],\n        ...,\n        [ -4.65643627,  -4.58216115,  -4.86888165, ...,  -8.97653424,\n          -8.5695356 ,  -8.59666319],\n        [ -4.62960387,  -4.60504662,  -5.11415089, ...,  -8.396745  ,\n         -11.84581071,  -8.51334596],\n        [ -4.5772725 ,  -4.46633895,  -4.95876778, ...,  -8.58177161,\n          -9.29750017,  -8.71840896]]])Attributes: (4)created_at :2021-07-10T22:10:08.569795arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:           (chain: 2, draw: 1000)\nCoordinates:\n  * chain             (chain) int64 0 1\n  * draw              (draw) int64 0 1 2 3 4 5 6 ... 993 994 995 996 997 998 999\nData variables:\n    depth             (chain, draw) int64 3 3 3 3 3 3 3 3 4 ... 4 3 3 4 3 3 3 3\n    step_size         (chain, draw) float64 0.3709 0.3709 ... 0.3642 0.3642\n    max_energy_error  (chain, draw) float64 -0.3033 -0.9073 ... 0.1527 0.2162\n    mean_tree_accept  (chain, draw) float64 0.9878 1.0 0.6001 ... 0.9535 0.9023\n    tree_size         (chain, draw) float64 7.0 7.0 7.0 7.0 ... 7.0 7.0 7.0 7.0\n    step_size_bar     (chain, draw) float64 0.4186 0.4186 ... 0.4023 0.4023\n    diverging         (chain, draw) bool False False False ... False False False\n    energy            (chain, draw) float64 418.4 411.7 410.1 ... 411.3 415.6\n    lp                (chain, draw) float64 -409.4 -398.4 ... -405.5 -405.0\n    energy_error      (chain, draw) float64 -0.08659 -0.9073 ... 0.05396 0.01052\nAttributes:\n    created_at:                 2021-07-10T22:10:08.109313\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2\n    sampling_time:              10.192897081375122\n    tuning_steps:               2000xarray.DatasetDimensions:chain: 2draw: 1000Coordinates: (2)chain(chain)int640 1array([0, 1])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (10)depth(chain, draw)int643 3 3 3 3 3 3 3 ... 4 3 3 4 3 3 3 3array([[3, 3, 3, ..., 3, 3, 3],\n       [3, 4, 3, ..., 3, 3, 3]])step_size(chain, draw)float640.3709 0.3709 ... 0.3642 0.3642array([[0.37085185, 0.37085185, 0.37085185, ..., 0.37085185, 0.37085185,\n        0.37085185],\n       [0.36415012, 0.36415012, 0.36415012, ..., 0.36415012, 0.36415012,\n        0.36415012]])max_energy_error(chain, draw)float64-0.3033 -0.9073 ... 0.1527 0.2162array([[-0.30333558, -0.90728229,  0.69568241, ..., -0.31374491,\n        -0.2200231 , -0.40118408],\n       [ 1.42927712,  0.33138796,  0.36049719, ...,  0.18928538,\n         0.15266867,  0.21622557]])mean_tree_accept(chain, draw)float640.9878 1.0 0.6001 ... 0.9535 0.9023array([[0.98779061, 1.        , 0.60011115, ..., 0.93739839, 0.98545547,\n        0.9950132 ],\n       [0.43950928, 0.89151176, 0.84991465, ..., 0.90923646, 0.95350635,\n        0.90231548]])tree_size(chain, draw)float647.0 7.0 7.0 7.0 ... 7.0 7.0 7.0 7.0array([[ 7.,  7.,  7., ...,  7.,  7.,  7.],\n       [ 7., 15.,  7., ...,  7.,  7.,  7.]])step_size_bar(chain, draw)float640.4186 0.4186 ... 0.4023 0.4023array([[0.41864123, 0.41864123, 0.41864123, ..., 0.41864123, 0.41864123,\n        0.41864123],\n       [0.40234341, 0.40234341, 0.40234341, ..., 0.40234341, 0.40234341,\n        0.40234341]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])energy(chain, draw)float64418.4 411.7 410.1 ... 411.3 415.6array([[418.43883273, 411.65728711, 410.0767806 , ..., 410.51405687,\n        408.58036529, 407.35921943],\n       [412.80822536, 411.47900652, 413.17891412, ..., 411.18710152,\n        411.29444508, 415.58466124]])lp(chain, draw)float64-409.4 -398.4 ... -405.5 -405.0array([[-409.43533657, -398.44740163, -404.77855576, ..., -402.57284061,\n        -403.52737457, -400.50675223],\n       [-401.88597959, -402.44777691, -407.25696595, ..., -404.72076691,\n        -405.45093988, -404.98509764]])energy_error(chain, draw)float64-0.08659 -0.9073 ... 0.01052array([[-0.08658624, -0.90728229,  0.52606004, ..., -0.05300835,\n         0.02450066, -0.40118408],\n       [ 0.38322751,  0.00231922,  0.16376489, ...,  0.12195152,\n         0.05395713,  0.01051789]])Attributes: (6)created_at :2021-07-10T22:10:08.109313arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2sampling_time :10.192897081375122tuning_steps :2000\n                      \n                  \n            \n            \n            \n                  \n                  prior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:         (LR_log___dim_0: 10, chain: 1, cohort: 10, draw: 100, obs: 55, t_values: 10)\nCoordinates:\n  * chain           (chain) int64 0\n  * draw            (draw) int64 0 1 2 3 4 5 6 7 8 ... 92 93 94 95 96 97 98 99\n  * cohort          (cohort) int64 1988 1989 1990 1991 ... 1994 1995 1996 1997\n  * t_values        (t_values) int64 1 2 3 4 5 6 7 8 9 10\n  * LR_log___dim_0  (LR_log___dim_0) int64 0 1 2 3 4 5 6 7 8 9\n  * obs             (obs) int64 0 1 2 3 4 5 6 7 8 ... 46 47 48 49 50 51 52 53 54\nData variables:\n    LR              (chain, draw, cohort) float64 0.5395 0.2832 ... 3.739\n    omega_log__     (chain, draw) float64 0.4461 0.07193 ... 0.1054 -0.7762\n    sd_LR_log__     (chain, draw) float64 -0.8523 -0.5681 -1.487 ... -0.3 0.7881\n    mu_LR           (chain, draw) float64 -0.8749 0.1713 ... -0.09251 -1.244\n    loss_sd_log__   (chain, draw) float64 0.1178 -0.06791 ... -0.3026 -1.538\n    gf              (chain, draw, t_values) float64 0.7042 0.8755 ... 0.7399\n    theta           (chain, draw) float64 0.5739 1.253 3.127 ... 1.24 1.031\n    sd_LR           (chain, draw) float64 0.4264 0.5666 0.2261 ... 0.7408 2.199\n    loss_sd         (chain, draw) float64 1.125 0.9343 0.4603 ... 0.7389 0.2147\n    LR_log__        (chain, draw, LR_log___dim_0) float64 -0.6171 ... 1.319\n    omega           (chain, draw) float64 1.562 1.075 1.892 ... 1.111 0.4602\n    theta_log__     (chain, draw) float64 -0.5553 0.2252 1.14 ... 0.2152 0.0308\n    lm              (chain, draw, obs) float64 363.6 452.0 ... 1.058e+05\nAttributes:\n    created_at:                 2021-07-10T22:10:08.582306\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:LR_log___dim_0: 10chain: 1cohort: 10draw: 100obs: 55t_values: 10Coordinates: (6)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 6 ... 94 95 96 97 98 99array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])t_values(t_values)int641 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])LR_log___dim_0(LR_log___dim_0)int640 1 2 3 4 5 6 7 8 9array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (13)LR(chain, draw, cohort)float640.5395 0.2832 ... 0.04612 3.739array([[[5.39483441e-01, 2.83183398e-01, 5.36638317e-01, 3.46020707e-01,\n         4.35402974e-01, 7.28349146e-01, 7.53385166e-01, 1.98912181e-01,\n         4.71464140e-01, 3.60783849e-01],\n        [1.60021652e+00, 9.60084765e-01, 1.55527582e+00, 5.27957240e-01,\n         1.49531975e+00, 7.42338495e-01, 1.50872389e+00, 1.08432781e+00,\n         2.39771704e+00, 3.04613240e+00],\n        [1.53240102e+00, 3.89658259e+00, 1.94455432e+00, 1.91653136e+00,\n         1.65883222e+00, 1.28964188e+00, 1.54529885e+00, 1.73610649e+00,\n         2.20782512e+00, 1.42497303e+00],\n        [2.89626892e-01, 6.89615568e-01, 4.81269113e-01, 3.24477413e-01,\n         9.64152326e-01, 4.77542250e-01, 2.76833126e+00, 2.26235901e+00,\n         8.77298833e-02, 6.18636581e-01],\n        [1.41315957e+00, 4.78212851e+00, 9.39090229e-01, 1.28132155e+00,\n         4.49898074e+00, 4.54895891e+00, 3.03642390e+00, 5.25706928e-01,\n         6.26412548e+00, 1.39341149e+00],\n        [9.57696979e-01, 1.86763571e+00, 4.13374817e+00, 1.56821742e+00,\n         1.25487332e+00, 2.51191010e+00, 8.10945988e+00, 6.10722752e-01,\n         1.41395916e+00, 9.82281959e+00],\n        [1.71912712e+00, 1.49080130e-01, 4.37317875e+00, 6.93848752e-01,\n         2.87455238e-01, 2.20250288e+00, 8.11983450e-01, 7.26701892e-01,\n...\n         9.69395468e-01, 6.87721577e+01, 1.36484576e-01, 1.13037865e-01,\n         4.30151712e-02, 2.71561166e+02],\n        [3.40901443e+00, 2.08439147e+00, 6.84379805e+00, 9.55929917e+00,\n         1.77361679e+00, 1.82087509e+01, 4.04976821e+00, 1.13211119e+01,\n         1.86859175e+00, 1.89121247e+00],\n        [3.46365246e+00, 1.66684876e-01, 3.47023822e-01, 4.53995073e-02,\n         1.06780054e+00, 9.27681160e-01, 1.80283061e+00, 2.37063152e+00,\n         7.54626213e-01, 2.86942154e-01],\n        [5.03298776e-01, 7.39239018e-01, 1.35779223e+00, 1.00056116e+00,\n         8.86529488e-01, 3.21049079e+00, 7.97450632e-01, 3.99036318e-01,\n         4.30978753e-01, 6.98964209e-01],\n        [3.00069125e+00, 3.09937257e+00, 5.07217053e-01, 5.56775119e-01,\n         2.04611562e+00, 9.44072847e+00, 6.36453703e-01, 1.36052095e+00,\n         2.53801160e+00, 1.02432399e-01],\n        [7.13583274e-01, 1.27445474e+00, 6.15372182e-01, 7.40064303e-01,\n         5.65699214e-01, 3.94688604e-01, 3.85516160e-01, 1.03348754e+00,\n         8.72203161e-01, 2.06829310e+00],\n        [6.72598317e-02, 1.31074344e-01, 2.46574280e-01, 3.57594394e-01,\n         1.41602364e+01, 1.39274851e+00, 1.65655532e-01, 1.13027571e+01,\n         4.61181024e-02, 3.73917734e+00]]])omega_log__(chain, draw)float640.4461 0.07193 ... 0.1054 -0.7762array([[ 0.44612894,  0.0719305 ,  0.63779866, -0.23449358,  0.23706241,\n         0.33419761, -0.60235794, -0.42556604,  0.44956226,  0.46859577,\n         0.47679333, -1.10255173, -1.1406529 , -0.30581554,  1.40485057,\n         0.08792286, -0.3153778 ,  0.92777277,  0.08014537, -0.71142477,\n        -0.18382993, -0.94314418,  0.37741105,  0.89061229,  0.32631473,\n         0.1187519 ,  0.29710071, -0.56646717,  0.24871394, -0.12211726,\n         0.52034076, -0.13353552, -0.33864184, -0.33580391,  1.31280722,\n        -0.55840032,  0.46812679, -0.28309119, -0.25267346, -0.43320864,\n         0.69851556, -0.43096099, -0.05757817,  0.00526835, -0.25980818,\n        -0.12478563,  1.12182973, -0.2414983 , -1.18449009, -0.46079041,\n         0.75950166,  0.27624921, -0.34634081, -0.53000413,  0.73357778,\n        -0.08395659,  0.37989795, -0.10431087, -0.02411082, -0.0916873 ,\n         0.64151539,  0.34238158,  0.25391359,  0.20807299,  0.07147848,\n        -0.03081378, -0.15740337,  0.33052872, -0.54743424, -0.17367713,\n         0.6791806 , -0.14902945, -0.04004145,  0.24734065,  0.68666321,\n        -0.46438288, -0.5281403 ,  0.10112753, -0.33912545, -0.01100623,\n        -0.95018485, -0.51196821,  0.10753402, -0.2633491 , -0.26221749,\n         0.2735264 ,  0.05936596, -1.15266121,  0.35999557, -0.64225228,\n         0.67848939,  0.24234314, -0.12805771,  0.67731315,  1.03797529,\n         0.3334401 , -0.50685985,  0.09829753,  0.10543659, -0.77619435]])sd_LR_log__(chain, draw)float64-0.8523 -0.5681 ... -0.3 0.7881array([[-0.8523256 , -0.5681305 , -1.48665774,  0.01665864, -0.12444433,\n        -0.22508822,  0.0662139 ,  0.01110696,  0.15868399, -0.37620709,\n        -0.6481959 ,  0.04756972, -0.21185755, -0.59299178, -0.182731  ,\n        -0.63551152,  0.79308547,  0.34669533, -0.97904062, -0.06740066,\n        -0.77030801,  1.02335698, -0.69849967, -0.54858599, -0.11935643,\n        -0.71453345,  0.47450239, -0.00969879,  0.44729885,  0.37984656,\n        -0.74886019, -0.59694299,  0.64813129,  0.47613781, -0.60862707,\n        -0.07863258, -0.75379258,  0.05394207,  0.37352783,  0.21483822,\n        -0.70752146, -0.32037996,  0.38981315, -0.21906046,  1.03739658,\n        -0.17164884, -0.30831469,  0.38159182,  0.0964586 , -0.17422947,\n         1.14932697, -0.08260478,  0.23314968,  0.13499362, -0.15991552,\n        -0.5738708 ,  0.85181199, -0.36107539,  0.54684332, -0.11475888,\n        -0.00444933, -0.271599  ,  0.37653109, -0.80471945,  0.97163113,\n        -0.72371806,  0.06512423,  0.47468043, -1.00759436, -0.03977029,\n         0.15052473, -0.84244998,  0.1111954 , -0.34246087, -0.06310059,\n         0.99513682,  0.2614989 , -0.0081727 , -0.20790817, -0.67925147,\n        -0.25721495, -0.10803006,  0.21119011, -0.54702147,  0.61845394,\n        -0.11514234, -0.3522091 , -0.29568756,  0.36849758,  0.21793363,\n         0.88799679,  0.25653719,  0.58526349,  1.03885612, -0.22796101,\n         0.32458646, -0.08739078,  0.50863217, -0.29999152,  0.78808336]])mu_LR(chain, draw)float64-0.8749 0.1713 ... -0.09251 -1.244array([[-0.87488274,  0.1713402 ,  0.5765179 , -0.12621802,  0.49066039,\n         0.25710942,  0.11058983, -0.53502167, -0.09474792,  0.12750072,\n        -0.22901349,  0.21758174, -0.29179753,  0.40842354,  0.3363604 ,\n        -0.05220557, -0.26564019,  0.51486634, -0.21906781, -0.55915912,\n         0.80949083,  0.77080259, -0.12593957, -0.42121787,  0.09225935,\n         0.4685411 ,  0.36550017,  0.68077806, -0.16311903,  0.02783801,\n         0.1111998 , -0.7216085 , -0.37817615,  0.40822701,  0.37522238,\n        -0.22797346,  0.59481113, -0.84530841, -0.67819952, -0.61621726,\n        -0.27221958, -0.33408587,  0.00365728, -0.30646937,  0.64987404,\n        -0.86654781, -0.49165505,  0.17875388, -0.80678925,  0.73535693,\n        -0.5940088 , -0.2748731 , -0.47002308, -0.41396618,  0.05443173,\n         0.2539048 , -0.43111367,  0.62473487, -0.03980562, -0.44486574,\n        -0.44089919,  0.00931947,  0.11892231,  0.00677427, -0.8177647 ,\n        -0.52210494,  0.30651944,  0.36810261,  0.51346072, -0.71609531,\n        -0.92059415,  0.18304661, -0.16588857, -0.34460899,  1.01730378,\n        -0.27535721,  0.37522667, -0.65349617,  0.29028667, -0.55226155,\n         0.34506074,  0.34344503, -0.78334376,  0.45248706,  0.3894112 ,\n         0.21411644,  0.05443599,  0.01414182, -0.28941291, -0.5997256 ,\n        -0.852976  ,  0.18458198,  0.93828671, -0.18845168,  0.91596804,\n         0.00150872, -0.03801173,  0.0019788 , -0.09250706, -1.24357577]])loss_sd_log__(chain, draw)float640.1178 -0.06791 ... -0.3026 -1.538array([[ 0.11776897, -0.06791383, -0.77586015,  0.81418801, -1.54295884,\n        -0.08635401, -0.3073733 ,  0.44099133, -0.60868549, -0.42565676,\n         0.83953297,  1.90751457,  0.31553409, -0.58763441, -0.02813683,\n         0.61956292,  1.1102456 ,  1.41804546,  1.07121072,  1.2829134 ,\n         0.67823319,  0.11953965,  0.49559607,  0.91112716,  0.26419822,\n        -0.55153079,  0.64974865,  1.42332632,  0.40688404,  1.0925064 ,\n         0.0515981 , -0.0156497 ,  0.47990747, -0.15405117, -1.44822335,\n         0.44352918, -1.63864786, -0.29389226,  0.71212342, -0.27237528,\n        -0.51608351, -0.00461874,  0.19355207, -0.40968742,  1.59689048,\n         0.80746392,  0.90807538, -0.15603457,  0.24094942, -1.39879746,\n         0.38607813, -0.58429016, -0.56743807,  0.07267915, -0.20197823,\n        -0.65064838,  0.31415215, -0.38924771,  0.91696663, -0.48047711,\n        -0.79327309, -0.11503221,  0.74234383, -0.37075465, -0.29457163,\n        -1.81513457,  0.20137675,  0.56713415,  0.07135635, -0.19635674,\n        -0.4044409 , -0.17024778, -1.04476412,  0.16943208, -0.98912851,\n        -0.66173423,  1.065507  ,  1.40047042, -0.45946529, -0.33103924,\n         1.22237942,  1.01016131, -0.64361443,  0.76310702,  0.76479435,\n        -0.22690514,  0.57659883,  1.36853881, -0.45579136,  0.67822079,\n         0.29416748, -1.19540042, -0.00414685, -0.5183479 ,  0.94649573,\n        -0.45149198, -0.10859997, -0.04805518, -0.302624  , -1.53838435]])gf(chain, draw, t_values)float640.7042 0.8755 ... 0.7304 0.7399array([[[0.70423484, 0.87549214, 0.92981535, 0.95405553, 0.9671339 ,\n         0.97507705, 0.98030598, 0.98395463, 0.98661528, 0.98862353],\n        [0.4397886 , 0.62312239, 0.71880211, 0.77689541, 0.81569534,\n         0.84335026, 0.86401169, 0.88000818, 0.89274339, 0.90311221],\n        [0.10362979, 0.30029807, 0.48035688, 0.61438231, 0.70848302,\n         0.77434966, 0.8212351 , 0.85537809, 0.88082887, 0.90022056],\n        [0.33863225, 0.46975303, 0.54972892, 0.60518664, 0.64648496,\n         0.67870678, 0.70469655, 0.7261912 , 0.74432036, 0.75985477],\n        [0.36346583, 0.57889167, 0.69681155, 0.76795562, 0.81451854,\n         0.84693205, 0.87058348, 0.88848725, 0.90244344, 0.91358541],\n        [0.59292719, 0.79319304, 0.87109128, 0.90990553, 0.93240122,\n         0.94679063, 0.95665138, 0.96376016, 0.96908782, 0.97320487],\n        [0.52575673, 0.6183686 , 0.66921337, 0.70310774, 0.7279638 ,\n         0.74727678, 0.76288122, 0.77585146, 0.78686662, 0.79638106],\n        [0.53525654, 0.64431972, 0.70247126, 0.74021092, 0.76725344,\n         0.78784695, 0.80419428, 0.8175696 , 0.82876883, 0.83831844],\n        [0.22462027, 0.46198695, 0.61852007, 0.71793657, 0.78313921,\n         0.82776462, 0.85954517, 0.88296832, 0.90074093, 0.91456169],\n        [0.79345344, 0.92080566, 0.95694122, 0.9723696 , 0.98049424,\n         0.98535136, 0.98851254, 0.99069905, 0.99228223, 0.99347016],\n...\n        [0.64181514, 0.87537833, 0.93982881, 0.96495661, 0.97714108,\n         0.98393041, 0.98809055, 0.99082103, 0.99270871, 0.99406781],\n        [0.39713443, 0.61439167, 0.72759741, 0.79397269, 0.83663171,\n         0.86595956, 0.88716659, 0.90311019, 0.9154717 , 0.92529784],\n        [0.48960751, 0.63836083, 0.71605695, 0.76460411, 0.798092  ,\n         0.82271025, 0.84163459, 0.85667234, 0.86893193, 0.87913289],\n        [0.23779221, 0.54975842, 0.73064268, 0.82695681, 0.88116276,\n         0.91391499, 0.93498184, 0.94924842, 0.95932411, 0.96668948],\n        [0.57758201, 0.90635774, 0.96816329, 0.98561457, 0.99228692,\n         0.99537608, 0.99700294, 0.99794238, 0.99852365, 0.99890312],\n        [0.63229895, 0.81899602, 0.88849829, 0.92251565, 0.94205031,\n         0.95447765, 0.96296374, 0.96906659, 0.97363229, 0.97715586],\n        [0.49575789, 0.5988251 , 0.65584172, 0.69383563, 0.72162392,\n         0.74314104, 0.76046247, 0.7748066 , 0.78694441, 0.79739179],\n        [0.45017679, 0.63755935, 0.7334373 , 0.79076283, 0.82859779,\n         0.85531065, 0.87511355, 0.89034527, 0.90240364, 0.91217334],\n        [0.4405067 , 0.62974341, 0.72743891, 0.78606014, 0.82481181,\n         0.85219095, 0.87249273, 0.88810812, 0.90046845, 0.91048055],\n        [0.49645643, 0.57561057, 0.62042586, 0.65106736, 0.67401798,\n         0.69217567, 0.70707985, 0.71964241, 0.73044606, 0.73988461]]])theta(chain, draw)float640.5739 1.253 3.127 ... 1.24 1.031array([[0.57389012, 1.25260178, 3.12725504, 2.33099523, 1.5559494 ,\n        0.7639599 , 0.82833537, 0.80557966, 2.20412348, 0.43069581,\n        2.9093212 , 0.99399548, 0.86455246, 1.32475578, 1.44164061,\n        1.10965243, 1.07620636, 1.29017089, 2.16036625, 2.32360565,\n        0.55518025, 0.90958767, 1.01729211, 0.84623772, 0.82608081,\n        1.62644821, 1.1668848 , 1.25656072, 0.83539401, 0.65453681,\n        1.83275323, 0.46182585, 1.40712532, 1.19493509, 0.94248559,\n        0.93866154, 0.83764564, 2.25087097, 1.27622612, 0.57064121,\n        0.52945567, 0.88550147, 0.75327822, 1.37076844, 0.93745489,\n        1.51501646, 0.62072393, 1.25873035, 2.29427686, 0.68248132,\n        1.06593879, 0.66061023, 0.77861607, 1.41031485, 2.15389213,\n        0.5953411 , 0.56525208, 1.340487  , 0.69754309, 1.16981863,\n        1.33030513, 1.57734588, 0.8597188 , 0.35135396, 0.82764502,\n        0.6132911 , 0.7778497 , 1.74519489, 0.71525789, 0.83759542,\n        2.2658658 , 4.32680955, 1.6694292 , 1.51162302, 0.54751728,\n        1.73666316, 1.22802165, 0.55897645, 0.70077877, 0.67783987,\n        0.70416641, 1.29484782, 0.81285727, 0.7551851 , 1.18750981,\n        2.39590503, 1.30989887, 0.78067317, 1.26049998, 1.12601871,\n        0.74383829, 1.38761336, 1.04839036, 1.80706916, 0.89511397,\n        0.67815156, 1.02856998, 1.19869844, 1.24008379, 1.03128319]])sd_LR(chain, draw)float640.4264 0.5666 ... 0.7408 2.199array([[0.42642209, 0.56658368, 0.22612717, 1.01679817, 0.88298741,\n        0.79844578, 1.06845524, 1.01116888, 1.17196753, 0.68646016,\n        0.52298845, 1.04871932, 0.80907994, 0.55267134, 0.8329922 ,\n        0.52966448, 2.21020544, 1.41438574, 0.37567134, 0.93482058,\n        0.46287048, 2.78251998, 0.4973309 , 0.5777662 , 0.88749141,\n        0.4894204 , 1.60721423, 0.99034809, 1.56408166, 1.46206023,\n        0.47290527, 0.55049193, 1.91196459, 1.60984486, 0.54409737,\n        0.92437949, 0.47057846, 1.05542345, 1.45285099, 1.23966133,\n        0.49286427, 0.72587318, 1.47670485, 0.80327315, 2.82186097,\n        0.84227489, 0.73468409, 1.46461414, 1.10126398, 0.8401041 ,\n        3.15606807, 0.92071496, 1.26257045, 1.14452948, 0.85221578,\n        0.56334063, 2.34389012, 0.69692646, 1.72779033, 0.8915811 ,\n        0.99556055, 0.76215982, 1.45722085, 0.44721338, 2.64225081,\n        0.48494584, 1.0672916 , 1.60750041, 0.36509621, 0.96101016,\n        1.16244405, 0.43065414, 1.11761327, 0.7100209 , 0.93884903,\n        2.70509444, 1.29887552, 0.9918606 , 0.81228163, 0.50699635,\n        0.773202  , 0.89760061, 1.23514715, 0.57867084, 1.85605625,\n        0.89123928, 0.70313308, 0.74401985, 1.44556115, 1.24350453,\n        2.43025646, 1.29244683, 1.79546401, 2.82598257, 0.7961553 ,\n        1.38345842, 0.91631895, 1.66301492, 0.7408245 , 2.19917736]])loss_sd(chain, draw)float641.125 0.9343 ... 0.7389 0.2147array([[1.12498418, 0.93434098, 0.46030768, 2.257342  , 0.21374772,\n        0.91726945, 0.73537603, 1.55424723, 0.54406558, 0.65334056,\n        2.31528541, 6.73632528, 1.37099135, 0.55564015, 0.97225532,\n        1.85811573, 3.03510372, 4.12904219, 2.91891135, 3.60713346,\n        1.97039335, 1.12697793, 1.64147638, 2.48712433, 1.30238633,\n        0.5760673 , 1.91505942, 4.15090475, 1.5021299 , 2.98173813,\n        1.05295248, 0.98447212, 1.61592487, 0.85722816, 0.23498741,\n        1.55819668, 0.19424251, 0.7453568 , 2.03831486, 0.76156841,\n        0.59685355, 0.99539191, 1.21355257, 0.66385773, 4.93765477,\n        2.24221434, 2.47954574, 0.85552962, 1.27245668, 0.24689368,\n        1.47119961, 0.55750146, 0.56697613, 1.07538545, 0.81711272,\n        0.5217074 , 1.36909803, 0.67756641, 2.50169031, 0.61848824,\n        0.45236175, 0.89133743, 2.10085379, 0.69021326, 0.74485059,\n        0.162816  , 1.22308549, 1.76320672, 1.07396387, 0.82171904,\n        0.66734982, 0.8434558 , 0.35177479, 1.18463189, 0.37190066,\n        0.51595577, 2.90231009, 4.05710805, 0.63162129, 0.71817699,\n        3.39525688, 2.74604394, 0.52539   , 2.14493023, 2.14855249,\n        0.79699638, 1.77997412, 3.9296046 , 0.63394609, 1.97036892,\n        1.34200864, 0.30258277, 0.99586174, 0.59550356, 2.57666449,\n        0.63667753, 0.89708921, 0.9530812 , 0.73887686, 0.21472775]])LR_log__(chain, draw, LR_log___dim_0)float64-0.6171 -1.262 ... -3.077 1.319array([[[-6.17143188e-01, -1.26166054e+00, -6.22430937e-01,\n         -1.06125666e+00, -8.31483299e-01, -3.16974749e-01,\n         -2.83178673e-01, -1.61489186e+00, -7.51912234e-01,\n         -1.01947626e+00],\n        [ 4.70138943e-01, -4.07337019e-02,  4.41652909e-01,\n         -6.38739983e-01,  4.02340063e-01, -2.97949948e-01,\n          4.11264185e-01,  8.09602667e-02,  8.74517052e-01,\n          1.11387272e+00],\n        [ 4.26835799e-01,  1.36009991e+00,  6.65032809e-01,\n          6.50516967e-01,  5.06113871e-01,  2.54364564e-01,\n          4.35217322e-01,  5.51644954e-01,  7.92007920e-01,\n          3.54152889e-01],\n        [-1.23916176e+00, -3.71620985e-01, -7.31328680e-01,\n         -1.12553935e+00, -3.65059826e-02, -7.39102641e-01,\n          1.01824470e+00,  8.16408078e-01, -2.43349269e+00,\n         -4.80237285e-01],\n        [ 3.45828028e-01,  1.56488574e+00, -6.28437141e-02,\n          2.47892004e-01,  1.50385087e+00,  1.51489840e+00,\n          1.11068047e+00, -6.43011394e-01,  1.83483899e+00,\n          3.31755049e-01],\n...\n        [ 1.24232366e+00, -1.79165022e+00, -1.05836185e+00,\n         -3.09225403e+00,  6.56009661e-02, -7.50671829e-02,\n          5.89357991e-01,  8.63156384e-01, -2.81532734e-01,\n         -1.24847464e+00],\n        [-6.86571297e-01, -3.02133976e-01,  3.05860017e-01,\n          5.61005282e-04, -1.20440890e-01,  1.16642382e+00,\n         -2.26335350e-01, -9.18702845e-01, -8.41696486e-01,\n         -3.58155741e-01],\n        [ 1.09884268e+00,  1.13119969e+00, -6.78816255e-01,\n         -5.85593856e-01,  7.15943176e-01,  2.24503315e+00,\n         -4.51843600e-01,  3.07867675e-01,  9.31380940e-01,\n         -2.27855222e+00],\n        [-3.37456137e-01,  2.42518433e-01, -4.85528021e-01,\n         -3.01018200e-01, -5.69692766e-01, -9.29658170e-01,\n         -9.53172168e-01,  3.29390486e-02, -1.36732900e-01,\n          7.26723678e-01],\n        [-2.69919207e+00, -2.03199061e+00, -1.40009199e+00,\n         -1.02835591e+00,  2.65043778e+00,  3.31279141e-01,\n         -1.79784476e+00,  2.42504669e+00, -3.07654973e+00,\n          1.31886562e+00]]])omega(chain, draw)float641.562 1.075 1.892 ... 1.111 0.4602array([[1.56225289, 1.07458066, 1.89231067, 0.79097131, 1.26752023,\n        1.39681914, 0.5475191 , 0.65339983, 1.56762583, 1.59774901,\n        1.61090048, 0.33202277, 0.31961028, 0.73652247, 4.07491779,\n        1.09190388, 0.72951322, 2.52887052, 1.08344456, 0.49094421,\n        0.83207731, 0.38940156, 1.4585037 , 2.43662111, 1.38585147,\n        1.1260905 , 1.34595085, 0.56752688, 1.28237514, 0.88504458,\n        1.68260091, 0.87499639, 0.71273768, 0.71476325, 3.71659238,\n        0.57212355, 1.59699987, 0.75345108, 0.77672147, 0.64842519,\n        2.01076562, 0.64988426, 0.94404809, 1.00528225, 0.7711995 ,\n        0.8826861 , 3.07046718, 0.78545014, 0.30590212, 0.63078487,\n        2.13721089, 1.31817633, 0.7072714 , 0.58860254, 2.08251808,\n        0.91947117, 1.46213538, 0.90094518, 0.97617753, 0.91239041,\n        1.89935696, 1.40829757, 1.28906041, 1.23130304, 1.07409503,\n        0.96965612, 0.85435936, 1.39170375, 0.57843202, 0.84056825,\n        1.97226099, 0.86154374, 0.96074961, 1.28061528, 1.987074  ,\n        0.62852286, 0.58970061, 1.10641774, 0.71239307, 0.98905411,\n        0.38666954, 0.59931484, 1.11352875, 0.76847358, 0.76934368,\n        1.31459206, 1.06116352, 0.31579525, 1.43332306, 0.52610615,\n        1.97089823, 1.27423136, 0.87980261, 1.96858134, 2.82349447,\n        1.39576144, 0.60238419, 1.103291  , 1.11119564, 0.46015387]])theta_log__(chain, draw)float64-0.5553 0.2252 ... 0.2152 0.0308array([[-0.55531734,  0.22522281,  1.14015564,  0.84629531,  0.44208591,\n        -0.26923997, -0.18833717, -0.21619318,  0.79032991, -0.84235321,\n         1.06791979, -0.00602262, -0.1455433 ,  0.28122813,  0.36578178,\n         0.10404684,  0.07344223,  0.25477469,  0.77027777,  0.84312014,\n        -0.58846245, -0.09476389,  0.0171443 , -0.16695497, -0.19106268,\n         0.48639862,  0.15433763,  0.2283784 , -0.17985179, -0.42382746,\n         0.60581933, -0.7725674 ,  0.34154884,  0.17809187, -0.05923465,\n        -0.06330032, -0.17716014,  0.81131724,  0.24390738, -0.56099462,\n        -0.63590583, -0.12160116, -0.28332064,  0.31537149, -0.06458664,\n         0.4154263 , -0.47686886,  0.23010355,  0.8304177 , -0.38202012,\n         0.06385591, -0.41459128, -0.25023721,  0.34381298,  0.7672765 ,\n        -0.51862076, -0.57048348,  0.29303298, -0.360191  ,  0.15684872,\n         0.28540834,  0.45574361, -0.15114992, -1.04596112, -0.18917094,\n        -0.48891559, -0.25122197,  0.55686623, -0.33511211, -0.17722009,\n         0.81795694,  1.46483045,  0.51248177,  0.41318392, -0.60236126,\n         0.55196555,  0.20540446, -0.58164794, -0.35556303, -0.3888442 ,\n        -0.35074057,  0.25839318, -0.20719975, -0.2807924 ,  0.17185852,\n         0.87376104,  0.26994994, -0.24759869,  0.23150845,  0.11868815,\n        -0.29593161,  0.32758526,  0.047256  ,  0.59170629, -0.11080422,\n        -0.38838448,  0.02816947,  0.18123634,  0.21517895,  0.03080384]])lm(chain, draw, obs)float64363.6 452.0 ... 1.393e+03 1.058e+05array([[[3.63586344e+02, 4.52004030e+02, 4.80050323e+02, ...,\n         1.74248189e+04, 2.16622228e+04, 1.44767740e+04],\n        [6.73495425e+02, 9.54254126e+02, 1.10077873e+03, ...,\n         5.53406168e+04, 7.84103498e+04, 7.63308220e+04],\n        [1.51973891e+02, 4.40389457e+02, 7.04447107e+02, ...,\n         1.20074665e+04, 3.47951982e+04, 8.41392159e+03],\n        ...,\n        [1.29275537e+03, 1.83085465e+03, 2.10618368e+03, ...,\n         5.99623720e+04, 8.49212392e+04, 2.62740876e+03],\n        [3.00821669e+02, 4.30051269e+02, 4.96767448e+02, ...,\n         2.01637950e+04, 2.88259343e+04, 5.19124828e+04],\n        [3.19557383e+01, 3.70507050e+01, 3.99353603e+01, ...,\n         1.20158548e+03, 1.39316416e+03, 1.05770463e+05]]])Attributes: (4)created_at :2021-07-10T22:10:08.582306arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  prior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 1, draw: 100, obs: 55)\nCoordinates:\n  * chain    (chain) int64 0\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 9 ... 90 91 92 93 94 95 96 97 98 99\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    loss     (chain, draw, obs) float64 -2.671e+03 1.343e+03 ... 9.672e+04\nAttributes:\n    created_at:                 2021-07-10T22:10:08.587369\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:chain: 1draw: 100obs: 55Coordinates: (3)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 5 6 ... 94 95 96 97 98 99array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (1)loss(chain, draw, obs)float64-2.671e+03 1.343e+03 ... 9.672e+04array([[[-2.67100326e+03,  1.34255155e+03,  1.40821615e+03, ...,\n          5.44546338e+04, -2.82037218e+04,  2.25204026e+04],\n        [ 6.91120357e+02,  1.25047752e+03,  2.34623536e+03, ...,\n          3.67907448e+04,  1.27985416e+05,  4.26089822e+04],\n        [-1.05994085e+02,  7.82732937e+02,  3.65909060e+02, ...,\n         -1.07724563e+04,  2.61310680e+04,  2.36614675e+04],\n        ...,\n        [ 8.83043472e+02,  2.15344855e+03,  3.03744893e+03, ...,\n          4.12138121e+04, -2.16126888e+04,  5.72089335e+03],\n        [ 1.13652047e+03,  1.26135401e+03,  1.58322237e+03, ...,\n         -4.63703410e+04,  6.47498746e+04,  1.04504263e+05],\n        [ 1.67803663e+02,  2.66545568e+02, -9.96214671e+01, ...,\n         -2.17424973e+04, -9.03439475e+03,  9.67249413e+04]]])Attributes: (4)created_at :2021-07-10T22:10:08.587369arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (obs: 55)\nCoordinates:\n  * obs      (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    loss     (obs) int32 133 333 431 570 615 ... 26012 31677 12604 23446 12292\nAttributes:\n    created_at:                 2021-07-10T22:10:08.588339\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:obs: 55Coordinates: (1)obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (1)loss(obs)int32133 333 431 ... 12604 23446 12292array([  133,   333,   431,   570,   615,   615,   615,   614,   614,\n         614,   934,  1746,  2365,  2579,  2763,  2966,  2940,  2978,\n        2978,  2030,  4864,  6880,  8087,  8595,  8743,  8763,  8762,\n        4537, 11527, 15123, 16656, 17321, 18076, 18308,  7564, 16061,\n       22465, 25204, 26517, 27124,  8343, 19900, 26732, 30079, 31249,\n       12565, 26922, 33867, 38338, 13437, 26012, 31677, 12604, 23446,\n       12292], dtype=int32)Attributes: (4)created_at :2021-07-10T22:10:08.588339arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n            \n                  \n                  constant_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:   (cohort: 10, obs: 55, t_values: 10)\nCoordinates:\n  * t_values  (t_values) int64 1 2 3 4 5 6 7 8 9 10\n  * cohort    (cohort) int64 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\n  * obs       (obs) int64 0 1 2 3 4 5 6 7 8 9 ... 45 46 47 48 49 50 51 52 53 54\nData variables:\n    t         (t_values) int32 1 2 3 4 5 6 7 8 9 10\n    premium   (cohort) int32 957 3695 6138 17533 ... 46095 51512 52481 56978\n    t_idx     (obs) int32 0 1 2 3 4 5 6 7 8 9 0 1 2 ... 3 4 0 1 2 3 0 1 2 0 1 0\n    c_idx     (obs) int32 0 0 0 0 0 0 0 0 0 0 1 1 1 ... 5 5 6 6 6 6 7 7 7 8 8 9\nAttributes:\n    created_at:                 2021-07-10T22:10:08.590166\n    arviz_version:              0.9.0\n    inference_library:          pymc3\n    inference_library_version:  3.9.2xarray.DatasetDimensions:cohort: 10obs: 55t_values: 10Coordinates: (3)t_values(t_values)int641 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])cohort(cohort)int641988 1989 1990 ... 1995 1996 1997array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997])obs(obs)int640 1 2 3 4 5 6 ... 49 50 51 52 53 54array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54])Data variables: (4)t(t_values)int321 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=int32)premium(cohort)int32957 3695 6138 ... 51512 52481 56978array([  957,  3695,  6138, 17533, 29341, 37194, 46095, 51512, 52481,\n       56978], dtype=int32)t_idx(obs)int320 1 2 3 4 5 6 7 ... 2 3 0 1 2 0 1 0array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2,\n       3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3,\n       4, 0, 1, 2, 3, 0, 1, 2, 0, 1, 0], dtype=int32)c_idx(obs)int320 0 0 0 0 0 0 0 ... 6 6 7 7 7 8 8 9array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2,\n       2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5,\n       5, 6, 6, 6, 6, 7, 7, 7, 8, 8, 9], dtype=int32)Attributes: (4)created_at :2021-07-10T22:10:08.590166arviz_version :0.9.0inference_library :pymc3inference_library_version :3.9.2\n                      \n                  \n            \n            \n              \n            \n            \n\n\n\naz.plot_ppc(idata, alpha=0.3, kind=\"cumulative\", figsize=(12, 6), textsize=14)\n\narray([<AxesSubplot:xlabel='loss'>], dtype=object)\n\n\n\n\n\n\nmodel_compare = az.compare(\n    {\n        \"Logistic Growth Model\": logistic_idata,\n        \"Weibull Growth Model\": weibull_idata,\n    }\n)\naz.plot_compare(model_compare, figsize=(12, 4), insample_dev=False)\n\nplt.show()\n\n/Users/nathanielforde/anaconda/envs/examined_algorithms/lib/python3.6/site-packages/arviz/stats/stats.py:151: UserWarning: \nThe scale is now log by default. Use 'scale' argument or 'stats.ic_scale' rcParam if you rely on a specific value.\nA higher log-score (or a lower deviance) indicates a model with better predictive accuracy.\n  \"\\nThe scale is now log by default. Use 'scale' argument or \"\n/Users/nathanielforde/anaconda/envs/examined_algorithms/lib/python3.6/site-packages/arviz/stats/stats.py:683: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  \"Estimated shape parameter of Pareto distribution is greater than 0.7 for \"\n/Users/nathanielforde/anaconda/envs/examined_algorithms/lib/python3.6/site-packages/arviz/stats/stats.py:683: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  \"Estimated shape parameter of Pareto distribution is greater than 0.7 for \"\n\n\n\n\n\n\naz.plot_trace(logistic_idata, var_names=[\"gf\", \"LR\"], circ_var_names=[\"LR\"])\n\nTypeError: plot_trace() got an unexpected keyword argument 'circ_var_names'\n\n\n\nlogistic_model.logp\n\n<pymc3.model.LoosePointFunc at 0x14d286b70>\n\n\n\nlogp = logistic_model.logp\nlnp = np.array([logp(logistic_trace.point(i,chain=c)) for c in logistic_trace.chains for i in range(len(logistic_trace))])\n\n\n\n\n0       0.0\n1       0.0\n2       0.0\n3       0.0\n4       0.0\n       ... \n1995    0.0\n1996    0.0\n1997    0.0\n1998    0.0\n1999    0.0\nLength: 2000, dtype: float64\n\n\n\nlogistic_trace.report.log_marginal_likelihood\n\nAttributeError: 'SamplerReport' object has no attribute 'log_marginal_likelihood'\n\n\n\nmodel_compare\n\n\n\n\n\n  \n    \n      \n      rank\n      loo\n      p_loo\n      d_loo\n      weight\n      se\n      dse\n      warning\n      loo_scale\n    \n  \n  \n    \n      Weibull Growth Model\n      0\n      -387.915\n      10.6734\n      0\n      0.811983\n      11.1849\n      0\n      True\n      log\n    \n    \n      Logistic Growth Model\n      1\n      -391.599\n      10.2344\n      3.68346\n      0.188017\n      11.874\n      3.81681\n      True\n      log"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nBayesian Structural Causal Inference\n\n\nExploring Space, Finding Structure, Testing Robustness\n\n\n\n\n\n\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Workflow with SEMs\n\n\nPyCon Ireland 2025\n\n\n\nbayesian\n\n\ndiscrete choice\n\n\n\n\n\n\n\n\n\nNov 16, 2025\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nDiscrete Choice Models in PyMC-Marketing\n\n\nA Choose-Your-Own-Adventure in Bayesian Consumer Choice Modeling\n\n\n\n\n\n\n\n\nSep 10, 2025\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nMultilevel Regression and Post-Stratification\n\n\nStratum Specific effect modification with Bambi\n\n\n\nbayesian\n\n\nmister p\n\n\ncausal inference\n\n\nweighting\n\n\n\n\n\n\n\n\n\nMay 14, 2025\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nStructural Causal Models in PyMC\n\n\nConditionalisation Strategies and Valid Causal Inference\n\n\n\nbayesian\n\n\ncausal\n\n\nSCM\n\n\nSEM\n\n\n\nTalk for Workshops for Ukraine\n\n\n\n\n\nMay 13, 2025\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nValue Capture: The Horror\n\n\nExperimentation Reading Group\n\n\n\nphilosophy\n\n\nmeasurement\n\n\nvalue\n\n\nexperimentation\n\n\n\nA presentation of Thi Nguyen’s Value Capture paper with a focus on the horror of capture in hierarchical organisations structures\n\n\n\n\n\nMay 4, 2025\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Bayesian Statistics: SEM and CFA\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nsem\n\n\ncfa\n\n\n\nInterview on Learning Bayesian Statistics Podcast on SEM models in PyMC\n\n\n\n\n\nJan 1, 2025\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nUncertainty and Causal Inference in Python\n\n\nCausalPy and Quasi-Experimental Designs\n\n\n\nbayesian\n\n\nsensitivity analysis\n\n\ncausal inference\n\n\n\n\n\n\n\n\n\nNov 16, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nWorking as a Data Scientist\n\n\nFrom guessing to learning\n\n\n\ncounting\n\n\nfun\n\n\nhistogram\n\n\n\n Fun talk for Wyatt’s Junior infants class on working in STEM\n\n\n\n\n\nNov 10, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nMissing Data and Bayesian Imputation\n\n\nPyData Berlin 2024 Recording\n\n\n\nbayesian\n\n\nmissing data\n\n\n\n\n\n\n\n\n\nNov 10, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nDiscrete Choice and Random Utility Models\n\n\nPyCon 2023 Ireland Recording\n\n\n\nbayesian\n\n\ndiscrete choice\n\n\n\n\n\n\n\n\n\nSep 10, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nMissing Data and Bayesian Imputation with PyMC\n\n\nCausal Narratives in Survey Analysis\n\n\n\nbayesian\n\n\nmissing data\n\n\ncausal inference\n\n\n\n\n\n\n\n\n\nApr 25, 2024\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nModel Evaluation and Discrete Choice Scenarios\n\n\nBayesian Mixer London\n\n\n\nbayesian\n\n\ndiscrete choice\n\n\n\n\n\n\n\n\n\nMar 26, 2024\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nNon Parametric Causal Inference with PyMC\n\n\nPropensity Scores, Debiased ML and Causal Mediation\n\n\n\nbayesian\n\n\npropensity scores\n\n\ncausal inference\n\n\n\n\n\n\n\n\n\nMar 1, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Regression Models in PyMC\n\n\nTime to Attrition in People Analytics\n\n\n\nbayesian\n\n\nsurvival analysis\n\n\npeople analytics\n\n\n\n\n\n\n\n\n\nDec 30, 2023\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nDiscrete Choice and Random Utility Models\n\n\nPyCon Ireland\n\n\n\nbayesian\n\n\ndiscrete choice\n\n\n\n\n\n\n\n\n\nNov 11, 2023\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nTenuous Relations and Timeseries Analysis\n\n\nBerlin Timeseries\n\n\n\nbayesian\n\n\nVAR\n\n\ntimeseries analysis\n\n\n\nA talk on Bayesian Hierarchical Vector Autoregressive Timeseries models\n\n\n\n\n\nFeb 4, 2023\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/tenuous_relations/first_talk.html",
    "href": "talks/tenuous_relations/first_talk.html",
    "title": "Tenuous Relations and Timeseries Analysis",
    "section": "",
    "text": "Hierarchical Bayesian VARs"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "The Journey Is the Model: Dynamic Path Analysis in PyMC\n\n\nCausal Inference with Time-Varying Effects\n\n\n\n\n\n\n\n\n\n\nMeasurement, Latent Factors and the Garden of Forking Paths\n\n\nConfirmatory Factor Analysis and Structural Equations in PyMC\n\n\n\n\n\n\n\n\n\n\nHeuristics in Latent Space: VAEs and Bayesian Inference\n\n\n\n\n\n\n\n\nJul 25, 2025\n\n\n\n\n\n\n\nFreedom, Hierarchies and Confounded Estimates\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\nGAMs and GPs: Flexibility and Calibration\n\n\n\n\n\n\n\n\nApr 7, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "Hi, I'm Nathaniel 👋\n   Data Science Leadership · Bayesian Statistics & Causal Inference · Still Learning...\n  \n    View My Work\n    Read My Blog\n  \n\n\n\n  \n  About Me\n  \n    I'm an experienced Data-Scientist and Statistician helping organisations make better decisions through data and experimentation. Using my expertise in Bayesian methods, Causal inference and Probabilistic AI I've learned to drive impact across multiple industries. I share what i've learned here.\n  \n\n\n\n  Recent Highlights\n  \n    \n      \n        \n          Variational Auto-Encoders and Latent Structure with PyTorch and PyMC\n           Missing data is prevalent in job satisfaction surveys which present a problem for HR SaaS analytics. We compare different approaches to solving this problem.\n          Read More\n        \n      \n    \n    \n      \n        \n          Podcast Appearance on Learning Bayesian Statistics\n          Talking confirmatory factor analysis & Bayesian methods with Alex Andorra on Learning Bayesian Statistics.\n          Listen\n        \n      \n    \n    \n      \n        \n          Consumer Choice models in PyMC Marketing\n           Market competition eats into companies revenue bottom lines. Understanding the effective drivers of win-rates improves company revenues. I implement discrete choice models and demonstrate how to model product differentiation with PyMC marketing. \n          View Post"
  },
  {
    "objectID": "notes/Statistics/Introduction - Statistics Topics.html",
    "href": "notes/Statistics/Introduction - Statistics Topics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "The idea is that this section of my notes will serve to capture my zettelkasten style notes on topics in Statistics."
  },
  {
    "objectID": "notes/Uncertain Things.html",
    "href": "notes/Uncertain Things.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "This is a space for my notes. They’ll be sporadic, haphazard in their level of detail but I aim for them to be regular and consistently updated log of topics of interest or distraction."
  },
  {
    "objectID": "notes/Logic/Introduction - Logic Topics.html",
    "href": "notes/Logic/Introduction - Logic Topics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "The idea is that this section will serve to capture any and all notes on topics in logic -mathematical, philosophical or any style in between."
  },
  {
    "objectID": "notes/Philosophy/Introduction - Philosophy Topics.html",
    "href": "notes/Philosophy/Introduction - Philosophy Topics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "The idea is that this section will serve to capture the philosophical topics in my Zettlekasten notes."
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.html",
    "href": "oss/pymc/bayesian_var_model.html",
    "title": "Bayesian Vector Autoregressive Models in PyMC",
    "section": "",
    "text": "Bayesian Vector Autoregressive Models\nIn this project I demonstrated how to fit a hierarchical bayesian autoregressive model in PyMC. The work drew on a PyMC labs blogpost showing how to fit a simple VAR model in PyMC. We applied these types of model to econometric timeseries data to analyse the relationships between GDP, investment and consumption for Ireland.\nThe project culminated in a publication to the official PyMC documentation that can be found online here or downloaded as notebook here\n\n\n\nIreland’s GDP v Peers"
  },
  {
    "objectID": "oss/pymc/discrete_choice.html",
    "href": "oss/pymc/discrete_choice.html",
    "title": "Discrete Choice Models in PyMC",
    "section": "",
    "text": "In this project I demonstrated how to fit a discrete choice models using random utility components in PyMC. I applied these types of model to micro-econometric data over product choice to estimate market share and the correlation among good within an market.\nThe project culminated in a publication to the official PyMC documentation that can be found online here or downloaded as notebook here\n\n\n\nDiscrete Choice"
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.myst.html",
    "href": "oss/pymc/bayesian_var_model.myst.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "(Bayesian Vector Autoregressive Models)= # Bayesian Vector Autoregressive Models\n:::{post} November, 2022 :tags: time series, vector autoregressive model, hierarchical model :category: intermediate :author: Nathaniel Forde :::\n```{code-cell} ipython3 import os\nimport arviz as az import matplotlib.pyplot as plt import numpy as np import pandas as pd import pymc as pm import statsmodels.api as sm\nfrom pymc.sampling_jax import sample_blackjax_nuts"
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.myst.html#vectorautoregression-models",
    "href": "oss/pymc/bayesian_var_model.myst.html#vectorautoregression-models",
    "title": "Examined Algorithms",
    "section": "V(ector)A(uto)R(egression) Models",
    "text": "V(ector)A(uto)R(egression) Models\nIn this notebook we will outline an application of the Bayesian Vector Autoregressive Modelling. We will draw on the work in the PYMC Labs blogpost (see {cite:t}vieira2022BVAR). This will be a three part series. In the first we want to show how to fit Bayesian VAR models in PYMC. In the second we will show how to extract extra insight from the fitted model with Impulse Response analysis and make forecasts from the fitted VAR model. In the third and final post we will show in some more detail the benefits of using hierarchical priors with Bayesian VAR models. Specifically, we’ll outline how and why there are actually a range of carefully formulated industry standard priors which work with Bayesian VAR modelling.\nIn this post we will (i) demonstrate the basic pattern on a simple VAR model on fake data and show how the model recovers the true data generating parameters and (ii) we will show an example applied to macro-economic data and compare the results to those achieved on the same data with statsmodels MLE fits and (iii) show an example of estimating a hierarchical bayesian VAR model over a number of countries."
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.myst.html#autoregressive-models-in-general",
    "href": "oss/pymc/bayesian_var_model.myst.html#autoregressive-models-in-general",
    "title": "Examined Algorithms",
    "section": "Autoregressive Models in General",
    "text": "Autoregressive Models in General\nThe idea of a simple autoregressive model is to capture the manner in which past observations of the timeseries are predictive of the current observation. So in traditional fashion, if we model this as a linear phenomena we get simple autoregressive models where the current value is predicted by a weighted linear combination of the past values and an error term.\n\\[ y_t = \\alpha + \\beta_{y0} \\cdot y_{t-1} + \\beta_{y1} \\cdot y_{t-2} ... + \\epsilon \\]\nfor however many lags are deemed appropriate to the predict the current observation.\nA VAR model is kind of generalisation of this framework in that it retains the linear combination approach but allows us to model multiple timeseries at once. So concretely this mean that \\(\\mathbf{y}_{t}\\) as a vector where:\n\\[ \\mathbf{y}_{T} =  \\nu + A_{1}\\mathbf{y}_{T-1} + A_{2}\\mathbf{y}_{T-2} ... A_{p}\\mathbf{y}_{T-p} + \\mathbf{e}_{t}  \\]\nwhere the As are coefficient matrices to be combined with the past values of each individual timeseries. For example consider an economic example where we aim to model the relationship and mutual influence of each variable on themselves and one another.\n\\[ \\begin{bmatrix} gdp \\\\ inv \\\\ con \\end{bmatrix}_{T} = \\nu + A_{1}\\begin{bmatrix} gdp \\\\ inv \\\\ con \\end{bmatrix}_{T-1} +\n    A_{2}\\begin{bmatrix} gdp \\\\ inv \\\\ con \\end{bmatrix}_{T-2} ... A_{p}\\begin{bmatrix} gdp \\\\ inv \\\\ con \\end{bmatrix}_{T-p} + \\mathbf{e}_{t} \\]\nThis structure is compact representation using matrix notation. The thing we are trying to estimate when we fit a VAR model is the A matrices that determine the nature of the linear combination that best fits our timeseries data. Such timeseries models can have an auto-regressive or a moving average representation, and the details matter for some of the implication of a VAR model fit.\nWe’ll see in the next notebook of the series how the moving-average representation of a VAR lends itself to the interpretation of the covariance structure in our model as representing a kind of impulse-response relationship between the component timeseries.\n\nA Concrete Specification with Two lagged Terms\nThe matrix notation is convenient to suggest the broad patterns of the model, but it is useful to see the algebra is a simple case. Consider the case of Ireland’s GDP and consumption described as:\n\\[ gdp_{t} = \\beta_{gdp1} \\cdot gdp_{t-1} + \\beta_{gdp2} \\cdot gdp_{t-2} +  \\beta_{cons1} \\cdot cons_{t-1} + \\beta_{cons2} \\cdot cons_{t-2}  + \\epsilon_{gdp}\\] \\[ cons_{t} = \\beta_{cons1} \\cdot cons_{t-1} + \\beta_{cons2} \\cdot cons_{t-2} +  \\beta_{gdp1} \\cdot gdp_{t-1} + \\beta_{gdp2} \\cdot gdp_{t-2}  + \\epsilon_{cons}\\]\nIn this way we can see that if we can estimate the \\(\\beta\\) terms we have an estimate for the bi-directional effects of each variable on the other. This is a useful feature of the modelling. In what follows i should stress that i’m not an economist and I’m aiming to show only the functionality of these models not give you a decisive opinion about the economic relationships determining Irish GDP figures.\n\n\nCreating some Fake Data\n{code-cell} ipython3 def simulate_var(     intercepts, coefs_yy, coefs_xy, coefs_xx, coefs_yx, noises=(1, 1), *, warmup=100, steps=200 ):     draws_y = np.zeros(warmup + steps)     draws_x = np.zeros(warmup + steps)     draws_y[:2] = intercepts[0]     draws_x[:2] = intercepts[1]     for step in range(2, warmup + steps):         draws_y[step] = (             intercepts[0]             + coefs_yy[0] * draws_y[step - 1]             + coefs_yy[1] * draws_y[step - 2]             + coefs_xy[0] * draws_x[step - 1]             + coefs_xy[1] * draws_x[step - 2]             + rng.normal(0, noises[0])         )         draws_x[step] = (             intercepts[1]             + coefs_xx[0] * draws_x[step - 1]             + coefs_xx[1] * draws_x[step - 2]             + coefs_yx[0] * draws_y[step - 1]             + coefs_yx[1] * draws_y[step - 2]             + rng.normal(0, noises[1])         )     return draws_y[warmup:], draws_x[warmup:]\nFirst we generate some fake data with known parameters.\n```{code-cell} ipython3 var_y, var_x = simulate_var( intercepts=(18, 8), coefs_yy=(-0.8, 0), coefs_xy=(0.9, 0), coefs_xx=(1.3, -0.7), coefs_yx=(-0.1, 0.3), )\ndf = pd.DataFrame({“x”: var_x, “y”: var_y}) df.head()\n\n```{code-cell} ipython3\nfig, axs = plt.subplots(2, 1, figsize=(10, 3))\naxs[0].plot(df[\"x\"], label=\"x\")\naxs[0].set_title(\"Series X\")\naxs[1].plot(df[\"y\"], label=\"y\")\naxs[1].set_title(\"Series Y\");"
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.myst.html#handling-multiple-lags-and-different-dimensions",
    "href": "oss/pymc/bayesian_var_model.myst.html#handling-multiple-lags-and-different-dimensions",
    "title": "Examined Algorithms",
    "section": "Handling Multiple Lags and Different Dimensions",
    "text": "Handling Multiple Lags and Different Dimensions\nWhen Modelling multiple timeseries and accounting for potentially any number lags to incorporate in our model we need to abstract some of the model definition to helper functions. An example will make this a bit clearer.\n```{code-cell} ipython3 ### Define a helper function that will construct our autoregressive step for the marginal contribution of each lagged ### term in each of the respective time series equations def calc_ar_step(lag_coefs, n_eqs, n_lags, df): ars = [] for j in range(n_eqs): ar = pm.math.sum( [ pm.math.sum(lag_coefs[j, i] * df.values[n_lags - (i + 1) : -(i + 1)], axis=-1) for i in range(n_lags) ], axis=0, ) ars.append(ar) beta = pm.math.stack(ars, axis=-1)\nreturn beta\n\nMake the model in such a way that it can handle different specifications of the likelihood term\n\n\nand can be run for simple prior predictive checks. This latter functionality is important for debugging of\n\n\nshape handling issues. Building a VAR model involves quite a few moving parts and it is handy to\n### inspect the shape implied in the prior predictive checks. def make_model(n_lags, n_eqs, df, priors, mv_norm=True, prior_checks=True): coords = { “lags”: np.arange(n_lags) + 1, “equations”: df.columns.tolist(), “cross_vars”: df.columns.tolist(), “time”: [x for x in df.index[n_lags:]], }\nwith pm.Model(coords=coords) as model:\n    lag_coefs = pm.Normal(\n        \"lag_coefs\",\n        mu=priors[\"lag_coefs\"][\"mu\"],\n        sigma=priors[\"lag_coefs\"][\"sigma\"],\n        dims=[\"equations\", \"lags\", \"cross_vars\"],\n    )\n    alpha = pm.Normal(\n        \"alpha\", mu=priors[\"alpha\"][\"mu\"], sigma=priors[\"alpha\"][\"sigma\"], dims=(\"equations\",)\n    )\n    data_obs = pm.Data(\"data_obs\", df.values[n_lags:], dims=[\"time\", \"equations\"], mutable=True)\n\n    betaX = calc_ar_step(lag_coefs, n_eqs, n_lags, df)\n    betaX = pm.Deterministic(\n        \"betaX\",\n        betaX,\n        dims=[\n            \"time\",\n        ],\n    )\n    mean = alpha + betaX\n\n    if mv_norm:\n        n = df.shape[1]\n        ## Under the hood the LKJ prior will retain the correlation matrix too.\n        noise_chol, _, _ = pm.LKJCholeskyCov(\n            \"noise_chol\",\n            eta=priors[\"noise_chol\"][\"eta\"],\n            n=n,\n            sd_dist=pm.HalfNormal.dist(sigma=priors[\"noise_chol\"][\"sigma\"]),\n        )\n        obs = pm.MvNormal(\n            \"obs\", mu=mean, chol=noise_chol, observed=data_obs, dims=[\"time\", \"equations\"]\n        )\n    else:\n        ## This is an alternative likelihood that can recover sensible estimates of the coefficients\n        ## But lacks the multivariate correlation between the timeseries.\n        sigma = pm.HalfNormal(\"noise\", sigma=priors[\"noise\"][\"sigma\"], dims=[\"equations\"])\n        obs = pm.Normal(\n            \"obs\", mu=mean, sigma=sigma, observed=data_obs, dims=[\"time\", \"equations\"]\n        )\n\n    if prior_checks:\n        idata = pm.sample_prior_predictive()\n        return model, idata\n    else:\n        idata = pm.sample_prior_predictive()\n        idata.extend(pm.sample(draws=2000, random_seed=130))\n        pm.sample_posterior_predictive(idata, extend_inferencedata=True, random_seed=rng)\nreturn model, idata\n\nThe model has a deterministic component in the auto-regressive calculation which is required at each timestep, but the key point here is that we model the likelihood of the VAR as a multivariate normal distribution with a particular covariance relationship. The estimation of these covariance relationship gives the main insight in the manner in which our component timeseries relate to one another. \n\nWe will inspect the structure of a VAR with 2 lags and 2 equations\n\n```{code-cell} ipython3\nn_lags = 2\nn_eqs = 2\npriors = {\n    \"lag_coefs\": {\"mu\": 0.3, \"sigma\": 1},\n    \"alpha\": {\"mu\": 15, \"sigma\": 5},\n    \"noise_chol\": {\"eta\": 1, \"sigma\": 1},\n    \"noise\": {\"sigma\": 1},\n}\n\nmodel, idata = make_model(n_lags, n_eqs, df, priors)\npm.model_to_graphviz(model)\nAnother VAR with 3 lags and 2 equations.\n{code-cell} ipython3 n_lags = 3 n_eqs = 2 model, idata = make_model(n_lags, n_eqs, df, priors) for rv, shape in model.eval_rv_shapes().items():     print(f\"{rv:>11}: shape={shape}\") pm.model_to_graphviz(model)\nWe can inspect the correlation matrix between our timeseries which is implied by the prior specification, to see that we have allowed a flat uniform prior over their correlation.\n{code-cell} ipython3 ax = az.plot_posterior(     idata,     var_names=\"noise_chol_corr\",     hdi_prob=\"hide\",     group=\"prior\",     point_estimate=\"mean\",     grid=(2, 2),     kind=\"hist\",     ec=\"black\",     figsize=(10, 4), )\nNow we will fit the VAR with 2 lags and 2 equations\n{code-cell} ipython3 n_lags = 2 n_eqs = 2 model, idata_fake_data = make_model(n_lags, n_eqs, df, priors, prior_checks=False)\nWe’ll now plot some of the results to see that the parameters are being broadly recovered. The alpha parameters match well, but the individual lag coefficients show differences.\n{code-cell} ipython3 az.summary(idata_fake_data, var_names=[\"alpha\", \"lag_coefs\", \"noise_chol_corr\"])\n{code-cell} ipython3 az.plot_posterior(idata_fake_data, var_names=[\"alpha\"], ref_val=[18, 8]);\nNext we’ll plot the posterior predictive distribution to check that the fitted model can capture the patterns in the observed data. This is the primary test of goodness of fit.\n```{code-cell} ipython3 def shade_background(ppc, ax, idx, palette=“cividis”): palette = palette cmap = plt.get_cmap(palette) percs = np.linspace(51, 99, 100) colors = (percs - np.min(percs)) / (np.max(percs) - np.min(percs)) for i, p in enumerate(percs[::-1]): upper = np.percentile( ppc[:, idx, :], p, axis=1, ) lower = np.percentile( ppc[:, idx, :], 100 - p, axis=1, ) color_val = colors[i] ax[idx].fill_between( x=np.arange(ppc.shape[0]), y1=upper.flatten(), y2=lower.flatten(), color=cmap(color_val), alpha=0.1, )\ndef plot_ppc(idata, df, group=“posterior_predictive”): fig, axs = plt.subplots(2, 1, figsize=(25, 15)) df = pd.DataFrame(idata_fake_data[“observed_data”][“obs”].data, columns=[“x”, “y”]) axs = axs.flatten() ppc = az.extract_dataset(idata, group=group, num_samples=100)[“obs”] # Minus the lagged terms and the constant shade_background(ppc, axs, 0, “inferno”) axs[0].plot(np.arange(ppc.shape[0]), ppc[:, 0, :].mean(axis=1), color=“cyan”, label=“Mean”) axs[0].plot(df[“x”], “o”, mfc=“black”, mec=“white”, mew=1, markersize=7, label=“Observed”) axs[0].set_title(“VAR Series 1”) axs[0].legend() shade_background(ppc, axs, 1, “inferno”) axs[1].plot(df[“y”], “o”, mfc=“black”, mec=“white”, mew=1, markersize=7, label=“Observed”) axs[1].plot(np.arange(ppc.shape[0]), ppc[:, 1, :].mean(axis=1), color=“cyan”, label=“Mean”) axs[1].set_title(“VAR Series 2”) axs[1].legend()\nplot_ppc(idata_fake_data, df)\n\nAgain we can check the learned posterior distribution for the correlation parameter.\n\n```{code-cell} ipython3\nax = az.plot_posterior(\n    idata_fake_data,\n    var_names=\"noise_chol_corr\",\n    hdi_prob=\"hide\",\n    point_estimate=\"mean\",\n    grid=(2, 2),\n    kind=\"hist\",\n    ec=\"black\",\n    figsize=(10, 6),\n)"
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.myst.html#applying-the-theory-macro-economic-timeseries",
    "href": "oss/pymc/bayesian_var_model.myst.html#applying-the-theory-macro-economic-timeseries",
    "title": "Examined Algorithms",
    "section": "Applying the Theory: Macro Economic Timeseries",
    "text": "Applying the Theory: Macro Economic Timeseries\nThe data is from the World Bank’s World Development Indicators. In particular, we’re pulling annual values of GDP, consumption, and gross fixed capital formation (investment) for all countries from 1970. Timeseries models in general work best when we have a stable mean throughout the series, so for the estimation procedure we have taken the first difference and the natural log of each of these series.\n```{code-cell} ipython3 try: gdp_hierarchical = pd.read_csv( os.path.join(“..”, “data”, “gdp_data_hierarchical_clean.csv”), index_col=0 ) except FileNotFoundError: gdp_hierarchical = pd.read_csv(pm.get_data(“gdp_data_hierarchical_clean.csv”), …)\ngdp_hierarchical\n\n```{code-cell} ipython3\nfig, axs = plt.subplots(3, 1, figsize=(20, 10))\nfor country in gdp_hierarchical[\"country\"].unique():\n    temp = gdp_hierarchical[gdp_hierarchical[\"country\"] == country].reset_index()\n    axs[0].plot(temp[\"dl_gdp\"], label=f\"{country}\")\n    axs[1].plot(temp[\"dl_cons\"], label=f\"{country}\")\n    axs[2].plot(temp[\"dl_gfcf\"], label=f\"{country}\")\naxs[0].set_title(\"Differenced and Logged GDP\")\naxs[1].set_title(\"Differenced and Logged Consumption\")\naxs[2].set_title(\"Differenced and Logged Investment\")\naxs[0].legend()\naxs[1].legend()\naxs[2].legend()\nplt.suptitle(\"Macroeconomic Timeseries\");"
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.myst.html#irelands-economic-situation",
    "href": "oss/pymc/bayesian_var_model.myst.html#irelands-economic-situation",
    "title": "Examined Algorithms",
    "section": "Ireland’s Economic Situation",
    "text": "Ireland’s Economic Situation\nIreland is somewhat infamous for its GDP numbers that are largely the product of foreign direct investment and inflated beyond expectation in recent years by the investment and taxation deals offered to large multi-nationals. We’ll look here at just the relationship between GDP and consumption. We just want to show the mechanics of the VAR estimation, you shouldn’t read too much into the subsequent analysis.\n{code-cell} ipython3 ireland_df = gdp_hierarchical[gdp_hierarchical[\"country\"] == \"Ireland\"] ireland_df.reset_index(inplace=True, drop=True) ireland_df.head()\n{code-cell} ipython3 n_lags = 2 n_eqs = 2 priors = {     ## Set prior for expected positive relationship between the variables.     \"lag_coefs\": {\"mu\": 0.3, \"sigma\": 1},     \"alpha\": {\"mu\": 0, \"sigma\": 0.1},     \"noise_chol\": {\"eta\": 1, \"sigma\": 1},     \"noise\": {\"sigma\": 1}, } model, idata_ireland = make_model(     n_lags, n_eqs, ireland_df[[\"dl_gdp\", \"dl_cons\"]], priors, prior_checks=False ) idata_ireland\n{code-cell} ipython3 az.plot_trace(idata_ireland, var_names=[\"lag_coefs\", \"alpha\", \"betaX\"], kind=\"rank_vlines\");\n```{code-cell} ipython3 def plot_ppc_macro(idata, df, group=“posterior_predictive”): df = pd.DataFrame(idata[“observed_data”][“obs”].data, columns=[“dl_gdp”, “dl_cons”]) fig, axs = plt.subplots(2, 1, figsize=(20, 10)) axs = axs.flatten() ppc = az.extract_dataset(idata, group=group, num_samples=100)[“obs”]\nshade_background(ppc, axs, 0, \"inferno\")\naxs[0].plot(np.arange(ppc.shape[0]), ppc[:, 0, :].mean(axis=1), color=\"cyan\", label=\"Mean\")\naxs[0].plot(df[\"dl_gdp\"], \"o\", mfc=\"black\", mec=\"white\", mew=1, markersize=7, label=\"Observed\")\naxs[0].set_title(\"Differenced and Logged GDP\")\naxs[0].legend()\nshade_background(ppc, axs, 1, \"inferno\")\naxs[1].plot(df[\"dl_cons\"], \"o\", mfc=\"black\", mec=\"white\", mew=1, markersize=7, label=\"Observed\")\naxs[1].plot(np.arange(ppc.shape[0]), ppc[:, 1, :].mean(axis=1), color=\"cyan\", label=\"Mean\")\naxs[1].set_title(\"Differenced and Logged Consumption\")\naxs[1].legend()\nplot_ppc_macro(idata_ireland, ireland_df)\n\n```{code-cell} ipython3\nax = az.plot_posterior(\n    idata_ireland,\n    var_names=\"noise_chol_corr\",\n    hdi_prob=\"hide\",\n    point_estimate=\"mean\",\n    grid=(2, 2),\n    kind=\"hist\",\n    ec=\"black\",\n    figsize=(10, 6),\n)\n\nComparison with Statsmodels\nIt’s worthwhile comparing these model fits to the one achieved by Statsmodels just to see if we can recover a similar story.\n{code-cell} ipython3 VAR_model = sm.tsa.VAR(ireland_df[[\"dl_gdp\", \"dl_cons\"]]) results = VAR_model.fit(2, trend=\"c\")\n{code-cell} ipython3 results.params\nThe intercept parameters broadly agree with our Bayesian model with some differences in the implied relationships defined by the estimates for the lagged terms.\n{code-cell} ipython3 corr = pd.DataFrame(results.resid_corr, columns=[\"dl_gdp\", \"dl_cons\"]) corr.index = [\"dl_gdp\", \"dl_cons\"] corr\nThe residual correlation estimates reported by statsmodels agree quite closely with the multivariate gaussian correlation between the variables in our Bayesian model.\n{code-cell} ipython3 az.summary(idata_ireland, var_names=[\"alpha\", \"lag_coefs\", \"noise_chol_corr\"])\nWe plot the alpha parameter estimates against the Statsmodels estimates\n{code-cell} ipython3 az.plot_posterior(idata_ireland, var_names=[\"alpha\"], ref_val=[0.034145, 0.006996]);\n{code-cell} ipython3 az.plot_posterior(     idata_ireland,     var_names=[\"lag_coefs\"],     ref_val=[0.330003, -0.053677],     coords={\"equations\": \"dl_cons\", \"lags\": [1, 2], \"cross_vars\": \"dl_gdp\"}, );\nWe can see here again how the Bayesian VAR model recovers much of the same story. Similar magnitudes in the estimates for the alpha terms for both equations and a clear relationship between the first lagged GDP numbers and consumption along with a very similar covariance structure.\n+++"
  },
  {
    "objectID": "oss/pymc/bayesian_var_model.myst.html#adding-a-bayesian-twist-hierarchical-vars",
    "href": "oss/pymc/bayesian_var_model.myst.html#adding-a-bayesian-twist-hierarchical-vars",
    "title": "Examined Algorithms",
    "section": "Adding a Bayesian Twist: Hierarchical VARs",
    "text": "Adding a Bayesian Twist: Hierarchical VARs\nIn addition we can add some hierarchical parameters if we want to model multiple countries and the relationship between these economic metrics at the national level. This is a useful technique in the cases where we have reasonably short timeseries data because it allows us to “borrow” information across the countries to inform the estimates of the key parameters.\n```{code-cell} ipython3 def make_hierarchical_model(n_lags, n_eqs, df, group_field, prior_checks=True): cols = [col for col in df.columns if col != group_field] coords = {“lags”: np.arange(n_lags) + 1, “equations”: cols, “cross_vars”: cols}\ngroups = df[group_field].unique()\n\nwith pm.Model(coords=coords) as model:\n    ## Hierarchical Priors\n    rho = pm.Beta(\"rho\", alpha=2, beta=2)\n    alpha_hat_location = pm.Normal(\"alpha_hat_location\", 0, 0.1)\n    alpha_hat_scale = pm.InverseGamma(\"alpha_hat_scale\", 3, 0.5)\n    beta_hat_location = pm.Normal(\"beta_hat_location\", 0, 0.1)\n    beta_hat_scale = pm.InverseGamma(\"beta_hat_scale\", 3, 0.5)\n    omega_global, _, _ = pm.LKJCholeskyCov(\n        \"omega_global\", n=n_eqs, eta=1.0, sd_dist=pm.Exponential.dist(1)\n    )\n\n    for grp in groups:\n        df_grp = df[df[group_field] == grp][cols]\n        z_scale_beta = pm.InverseGamma(f\"z_scale_beta_{grp}\", 3, 0.5)\n        z_scale_alpha = pm.InverseGamma(f\"z_scale_alpha_{grp}\", 3, 0.5)\n        lag_coefs = pm.Normal(\n            f\"lag_coefs_{grp}\",\n            mu=beta_hat_location,\n            sigma=beta_hat_scale * z_scale_beta,\n            dims=[\"equations\", \"lags\", \"cross_vars\"],\n        )\n        alpha = pm.Normal(\n            f\"alpha_{grp}\",\n            mu=alpha_hat_location,\n            sigma=alpha_hat_scale * z_scale_alpha,\n            dims=(\"equations\",),\n        )\n\n        betaX = calc_ar_step(lag_coefs, n_eqs, n_lags, df_grp)\n        betaX = pm.Deterministic(f\"betaX_{grp}\", betaX)\n        mean = alpha + betaX\n\n        n = df_grp.shape[1]\n        noise_chol, _, _ = pm.LKJCholeskyCov(\n            f\"noise_chol_{grp}\", eta=10, n=n, sd_dist=pm.Exponential.dist(1)\n        )\n        omega = pm.Deterministic(f\"omega_{grp}\", rho * omega_global + (1 - rho) * noise_chol)\n        obs = pm.MvNormal(f\"obs_{grp}\", mu=mean, chol=omega, observed=df_grp.values[n_lags:])\n\n    if prior_checks:\n        idata = pm.sample_prior_predictive()\n        return model, idata\n    else:\n        idata = pm.sample_prior_predictive()\n        idata.extend(sample_blackjax_nuts(2000, random_seed=120))\n        pm.sample_posterior_predictive(idata, extend_inferencedata=True)\nreturn model, idata\n\nThe model design allows for a non-centred parameterisation of the key likeihood for each of the individual country components by allowing the us to shift the country specific estimates away from the hierarchical mean. This is done by `rho * omega_global + (1 - rho) * noise_chol` line. The parameter `rho` determines the share of impact each country's data contributes to the estimation of the covariance relationship among the economic variables. Similar country specific adjustments are made with the `z_alpha_scale` and `z_beta_scale` parameters.\n\n```{code-cell} ipython3\ndf_final = gdp_hierarchical[[\"country\", \"dl_gdp\", \"dl_cons\", \"dl_gfcf\"]]\nmodel_full_test, idata_full_test = make_hierarchical_model(\n    2,\n    3,\n    df_final,\n    \"country\",\n    prior_checks=False,\n)\n{code-cell} ipython3 idata_full_test\n{code-cell} ipython3 az.plot_trace(     idata_full_test,     var_names=[\"rho\", \"alpha_hat_location\", \"beta_hat_location\", \"omega_global\"],     kind=\"rank_vlines\", );\nNext we’ll look at some of the summary statistics and how they vary across the countries.\n```{code-cell} ipython3\n\n```{code-cell} ipython3\naz.summary(\n    idata_full_test,\n    var_names=[\n        \"rho\",\n        \"alpha_hat_location\",\n        \"alpha_hat_scale\",\n        \"beta_hat_location\",\n        \"beta_hat_scale\",\n        \"z_scale_alpha_Ireland\",\n        \"z_scale_alpha_United States\",\n        \"z_scale_beta_Ireland\",\n        \"z_scale_beta_United States\",\n        \"alpha_Ireland\",\n        \"alpha_United States\",\n        \"omega_global_corr\",\n        \"lag_coefs_Ireland\",\n        \"lag_coefs_United States\",\n    ],\n)\n```{code-cell} ipython3 ax = az.plot_forest( idata_full_test, var_names=[ “alpha_Ireland”, “alpha_United States”, “alpha_Australia”, “alpha_Chile”, “alpha_New Zealand”, “alpha_South Africa”, “alpha_Canada”, “alpha_United Kingdom”, ], kind=“ridgeplot”, combined=True, ridgeplot_truncate=False, ridgeplot_quantiles=[0.25, 0.5, 0.75], ridgeplot_overlap=0.7, figsize=(10, 10), )\nax[0].axvline(0, color=“red”) ax[0].set_title(“Intercept Parameters for each country and Economic Measure”);\n\n```{code-cell} ipython3\nax = az.plot_forest(\n    idata_full_test,\n    var_names=[\n        \"lag_coefs_Ireland\",\n        \"lag_coefs_United States\",\n        \"lag_coefs_Australia\",\n        \"lag_coefs_Chile\",\n        \"lag_coefs_New Zealand\",\n        \"lag_coefs_South Africa\",\n        \"lag_coefs_Canada\",\n        \"lag_coefs_United Kingdom\",\n    ],\n    kind=\"ridgeplot\",\n    ridgeplot_truncate=False,\n    figsize=(10, 10),\n    coords={\"equations\": \"dl_cons\", \"lags\": 1, \"cross_vars\": \"dl_gdp\"},\n)\nax[0].axvline(0, color=\"red\")\nax[0].set_title(\"Lag Coefficient for the first lag of GDP on Consumption \\n by Country\");\nNext we’ll examine the correlation between the three variables and see what we’ve learned by including the hierarchical structure.\n{code-cell} ipython3 corr = pd.DataFrame(     az.summary(idata_full_test, var_names=[\"omega_global_corr\"])[\"mean\"].values.reshape(3, 3),     columns=[\"GDP\", \"CONS\", \"GFCF\"], ) corr.index = [\"GDP\", \"CONS\", \"GFCF\"] corr\n{code-cell} ipython3 ax = az.plot_posterior(     idata_full_test,     var_names=\"omega_global_corr\",     hdi_prob=\"hide\",     point_estimate=\"mean\",     grid=(3, 3),     kind=\"hist\",     ec=\"black\",     figsize=(10, 7), ) titles = [     \"GDP/GDP\",     \"GDP/CONS\",     \"GDP/GFCF\",     \"CONS/GDP\",     \"CONS/CONS\",     \"CONS/GFCF\",     \"GFCF/GDP\",     \"GFCF/CONS\",     \"GFCF/GFCF\", ] for ax, t in zip(ax.ravel(), titles):     ax.set_xlim(0.6, 1)     ax.set_title(t, fontsize=10) plt.suptitle(\"The Posterior Correlation Estimates\", fontsize=20);\nWe can see these estimates of the correlations between the 3 economic variables differ markedly from the simple case where we examined Ireland alone. In particular we can see that the correlation between GDF and CONS is now much higher. Which suggests that we have learned something about the relationship between these variables which would not be clear examining the Irish case alone.\nNext we’ll plot the model fits for each country to ensure that the predictive distribution can recover the observed data. It is important for the question of model adequacy that we can recover both the outlier case of Ireland and the more regular countries such as Australia and United States.\n{code-cell} ipython3 az.plot_ppc(idata_full_test);\nAnd to see the development of these model fits over time:\n```{code-cell} ipython3 countries = gdp_hierarchical[“country”].unique()\nfig, axs = plt.subplots(8, 3, figsize=(20, 40)) for ax, country in zip(axs, countries): temp = pd.DataFrame( idata_full_test[“observed_data”][f”obs_{country}”].data, columns=[“dl_gdp”, “dl_cons”, “dl_gfcf”], ) ppc = az.extract_dataset(idata_full_test, group=“posterior_predictive”, num_samples=100)[ f”obs_{country}” ] if country == “Ireland”: color = “viridis” else: color = “inferno” for i in range(3): shade_background(ppc, ax, i, color) ax[0].plot(np.arange(ppc.shape[0]), ppc[:, 0, :].mean(axis=1), color=“cyan”, label=“Mean”) ax[0].plot(temp[“dl_gdp”], “o”, mfc=“black”, mec=“white”, mew=1, markersize=7, label=“Observed”) ax[0].set_title(f”Posterior Predictive GDP: {country}“) ax[0].legend(loc=”lower left”) ax[1].plot( temp[“dl_cons”], “o”, mfc=“black”, mec=“white”, mew=1, markersize=7, label=“Observed” ) ax[1].plot(np.arange(ppc.shape[0]), ppc[:, 1, :].mean(axis=1), color=“cyan”, label=“Mean”) ax[1].set_title(f”Posterior Predictive Consumption: {country}“) ax[1].legend(loc=”lower left”) ax[2].plot( temp[“dl_gfcf”], “o”, mfc=“black”, mec=“white”, mew=1, markersize=7, label=“Observed” ) ax[2].plot(np.arange(ppc.shape[0]), ppc[:, 2, :].mean(axis=1), color=“cyan”, label=“Mean”) ax[2].set_title(f”Posterior Predictive Investment: {country}“) ax[2].legend(loc=”lower left”) plt.suptitle(“Posterior Predictive Checks on Hierarchical VAR”, fontsize=20);\n\nHere we can see that the model appears to have recovered reasonable posterior predictions for the observed data and the volatility of the Irish GDP figures is clear next to the other countries. Whether this is a cautionary tale about data quality or the corruption of metrics we leave to the economists to figure out.\n\n+++\n\n## Conclusion\n\nVAR modelling is a rich an interesting area of research within economics and there are a range of challenges and pitfalls which come with the interpretation and understanding of these models. We hope this example encourages you to continue exploring the potential of this kind of VAR modelling in the Bayesian framework. Whether you're interested in the relationship between grand economic theory or simpler questions about the impact of poor app performance on customer feedback, VAR models give you a powerful tool for interrogating these relationships over time. As we've seen Hierarchical VARs further enables the precise quantification of outliers within a cohort and does not throw away the information because of odd accounting practices engendered by international capitalism. \n\nIn the next post in this series we will spend some time digging into the implied relationships between the timeseries which result from fitting our VAR models.\n\n+++\n\n## References\n\n:::{bibliography}\n:filter: docname in docnames\n:::\n\n+++\n\n## Authors\n* Adapted from the PYMC labs [Blog post](https://www.pymc-labs.io/blog-posts/bayesian-vector-autoregression/) and Jim Savage's discussion [here](https://rpubs.com/jimsavage/hierarchical_var) by [Nathaniel Forde](https://nathanielf.github.io/) in November 2022 ([pymc-examples#456](https://github.com/pymc-devs/pymc-examples/pull/456))\n\n+++\n\n## Watermark\n\n```{code-cell} ipython3\n%load_ext watermark\n%watermark -n -u -v -iv -w -p pytensor,aeppl,xarray\n:::{include} ../page_footer.md :::"
  },
  {
    "objectID": "oss/pymc/missing_info.html",
    "href": "oss/pymc/missing_info.html",
    "title": "Missing Data Imputation in PyMC",
    "section": "",
    "text": "Missing Data Imputation and Employee Survey Data\nIn this project I demonstrate the technique of imputation for missing data using both the standard frequentist approach full information maximum likelihod and a more nuanced Bayesian method of chained equations.\nThe project culminated in a publication to the official PyMC documentation that can be found online here or downloaded as notebook here\nThe notebook demonstates these techniques applied to employee satisfaction data. In particular we show how Bayesian hierarchical methods can be used to help predict missing data values across various teams within an organisation based on the observed values andt the characteristics of the team dynamics which drove the observed data.\n\n\n\nDeviations from the Grand Mean by Team"
  },
  {
    "objectID": "oss/pymc/reliability_stats.html",
    "href": "oss/pymc/reliability_stats.html",
    "title": "Reliability Statistics in PyMC",
    "section": "",
    "text": "Reliability Statistics and Calibrated Prediction\nThis project was inspired by the need to apply survival analysis techniques in software engineering to predict and quantify the failure time distribution of software products. The focus was on parameteric modelling of failure distributions and the notion of calibrated predictions of failure times.\nThe project culminated in a publication to the official PyMC documentation that can be found online here or downloaded as notebook here\n\n\n\nLinearised MLE fits to Failure time data"
  },
  {
    "objectID": "oss/pymc/ordinal_regression.html",
    "href": "oss/pymc/ordinal_regression.html",
    "title": "Ordinal Regression Models in PyMC",
    "section": "",
    "text": "Ordinal Models of Regression\nIn this project I outline the strategy of latent variable ordinal regression Bayesian models to evaluate the categorical choice on Likert like scales. The main theme of the work is to try and articulate the modelling approach to survey data, comparing the risk of model misspecification to the assumed metric based analysis of ordinal response variables.\nThe project culminated in a publication to the official PyMC documentation that can be found online here and downloaded here\nThe notebook demonstates these techniques applied to simulated manager engagement evaluations and applied to rotten tomatoes movie ratings data. In particular we show how a latent variable formulate can help characterise the manner in which different factors such as working from home, salary can influence an individual’s ordinal rating response and why it’s important to understand the sources of variation in such responses.\n\n\n\nLatent and Explicit Ratings"
  },
  {
    "objectID": "oss/pymc/longitudinal_models.html",
    "href": "oss/pymc/longitudinal_models.html",
    "title": "Longitudinal Models in PyMC",
    "section": "",
    "text": "Longitudinal Analysis of Growth Trajectories\nIn this project I outline the strategy of using multi-level or hierarchical Bayesian models to evaluate the growth trajectories of individuals, and estimate the between individual effects. The main theme of the work is to try and disambiguate some of the complexities of mixed level (hierarchical modelling) when applied to longitudinal data.\nThe project culminated in a publication to the official PyMC documentation that can be found online here and downloaded here\nThe notebook demonstates these techniques applied to data of youth alcohol consuption. and behaviourial data. In particular we show how Bayesian hierarchical methods can be used to help characterise the manner in which different factors such can influence an individual’s trajectory and why it’s important to understand the sources of variation in such growth trajectories.\n\n\n\nWithin and Between Individual Trajectories"
  },
  {
    "objectID": "oss/pymc/autoregressive_forecasting.html",
    "href": "oss/pymc/autoregressive_forecasting.html",
    "title": "Autoregressive Forecasting in PyMC",
    "section": "",
    "text": "Autoregressive Forecasting in PyMC\nThis project stemmed from a gap in the PyMC documentation around how to use a fitted auto-regressive model to make forecasts about the future state of the world. In this project I demonstrate how fit and make predictictions with bayesian structural timeseries models.\nThe project culminated in a publication to the official PyMC documentation that can be found online here or downloaded as notebook here\nThe notebook demonstates these techniques applied to a series of fake data building in complexity as we add more structure.\n\n\n\nForecasting"
  },
  {
    "objectID": "opensource.html",
    "href": "opensource.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nStructural Causal Models\n\n\n\n\n\n\ncausal inference\n\n\nSCM\n\n\nCausalPy\n\n\n\n\n\n\nNov 12, 2025\n\n\n1 min\n\n\nIntroduction to Structural Causal Models in CausalPy\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Workflow with Structural Equation Models\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nstructural equation models\n\n\ncausal inference\n\n\n\n\n\n\nOct 27, 2025\n\n\n2 min\n\n\nStructural Equation Models with PyMC\n\n\n\n\n\n\n\n\n\n\n\n\nDesign and Analysis: Paradox of Bayesian Propensity Scores\n\n\n\n\n\n\nregression\n\n\npropensity scores\n\n\ncausal inference\n\n\n\n\n\n\nJul 30, 2025\n\n\n2 min\n\n\nParadox of Bayesian Propensity Estimation\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Nested Logit in PyMC Marketing\n\n\n\n\n\n\nutility\n\n\nconsumer choice\n\n\ncausal inference\n\n\n\n\n\n\nJun 20, 2025\n\n\n2 min\n\n\nBayesian Estimation of Nested Logit Discrete Choice Model\n\n\n\n\n\n\n\n\n\n\n\n\nSEM and CFA in PyMC\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nsem\n\n\ncfa\n\n\n\n\n\n\nSep 29, 2024\n\n\n1 min\n\n\nBayesian Measurement Models and Structural Regressions\n\n\n\n\n\n\n\n\n\n\n\n\nJustifying Instruments in CausalPy\n\n\n\n\n\n\nregression\n\n\ninstrumental variables\n\n\ncausal inference\n\n\n\n\n\n\nJun 15, 2024\n\n\n1 min\n\n\nCredibility in Bayesian IV designs\n\n\n\n\n\n\n\n\n\n\n\n\nInverse Propensity Score Weighting in CausalPy\n\n\n\n\n\n\nregression\n\n\npropensity\n\n\ncausal inference\n\n\nweighting\n\n\n\n\n\n\nMay 11, 2024\n\n\n1 min\n\n\nPropensity Weighting Adjustment for Causal Inference\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Non Parametric Causal Inference in PyMC\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nnon parametric\n\n\ncausal inference\n\n\n\n\n\n\nFeb 27, 2024\n\n\n1 min\n\n\nNon Parametric Causal Models and Mediation Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nFrailty Survival Models in PyMC\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nsurvival models\n\n\nfrailty models\n\n\n\n\n\n\nNov 28, 2023\n\n\n1 min\n\n\nAccelerated Failure and Hierarchical Survival Models for Attrition Models\n\n\n\n\n\n\n\n\n\n\n\n\nMultilevel Regression and Post-Stratification\n\n\n\n\n\n\nregression\n\n\npost-stratification\n\n\nsurvey data\n\n\n\n\n\n\nSep 21, 2023\n\n\n1 min\n\n\nWeighting Adjustments as Correctives for Survey Bias\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian IV Regression in CausalPy\n\n\n\n\n\n\nregression\n\n\ninstrumental variables\n\n\ncausal inference\n\n\n\n\n\n\nAug 15, 2023\n\n\n1 min\n\n\nBayesian IV designs as Structural Multivariate Models\n\n\n\n\n\n\n\n\n\n\n\n\nDiscrete Choice Models in PyMC\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\ndiscrete choice\n\n\nsubjective utility\n\n\n\n\n\n\nJul 15, 2023\n\n\n1 min\n\n\nLatent Utility Econometric Models for Demand Estimation\n\n\n\n\n\n\n\n\n\n\n\n\nOrdinal Regression Models in PyMC\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nordinal regression\n\n\nlikert scales\n\n\n\n\n\n\nJun 1, 2023\n\n\n1 min\n\n\nOrdinal Regression for analysing Survey Response data\n\n\n\n\n\n\n\n\n\n\n\n\nLongitudinal Models in PyMC\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nlongitudinal models\n\n\n\n\n\n\nApr 10, 2023\n\n\n1 min\n\n\nHierarchical Regression based Growth Models\n\n\n\n\n\n\n\n\n\n\n\n\nMissing Data Imputation in PyMC\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nmissing_data\n\n\nimputation\n\n\n\n\n\n\nFeb 10, 2023\n\n\n1 min\n\n\nMissing data imputation and Employee Engagement Surveys\n\n\n\n\n\n\n\n\n\n\n\n\nReliability Statistics in PyMC\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\ntime-to-failure\n\n\ncalibration\n\n\n\n\n\n\nJan 10, 2023\n\n\n1 min\n\n\nReliability assessment with Survival analysis in PyMC\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Vector Autoregressive Models in PyMC\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nautoregressive\n\n\nhierarchical_models\n\n\n\n\n\n\nDec 15, 2022\n\n\n1 min\n\n\nVAR and Hierarchical VARs for GDP models in PyMC\n\n\n\n\n\n\n\n\n\n\n\n\nAutoregressive Forecasting in PyMC\n\n\n\n\n\n\nprobability\n\n\npymc\n\n\nautoregressive\n\n\n\n\n\n\nAug 15, 2022\n\n\n1 min\n\n\nImplementing Autoregressive Structural forecasting models in PyMC\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nThe Journey Is the Model: Dynamic Path Analysis in PyMC\n\n\nCausal Inference with Time-Varying Effects\n\n\n\npath-models\n\n\nsem\n\n\ncausal inference\n\n\n\n\n\n\n\n\n\n49 min\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurement, Latent Factors and the Garden of Forking Paths\n\n\nConfirmatory Factor Analysis and Structural Equations in PyMC\n\n\n\ncfa\n\n\nsem\n\n\nmeasurment\n\n\n\n\n\n\n\n\n\n58 min\n\n\n\n\n\n\n\n\n\n\n\n\nHeuristics in Latent Space: VAEs and Bayesian Inference\n\n\n\n\n\n\npytorch\n\n\nvariational auto-encoders\n\n\namortized bayesian inference\n\n\n\nMissing data imposes two costs: the expense of collecting new observations and the risk of distorting the underlying structure when we construct plausible replacements. In job satisfaction surveys, non-response patterns can conceal meaningful latent relationships. This study compares statistical and machine learning approaches for imputation, focusing on their ability to preserve these latent structures while producing realistic reconstructions.\n\n\n\n\n\nJul 25, 2025\n\n\n53 min\n\n\n\n\n\n\n\n\n\n\n\n\nFreedom, Hierarchies and Confounded Estimates\n\n\n\n\n\n\nTWFE\n\n\nmundlak\n\n\nhierarchical models\n\n\nDiD\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\n43 min\n\n\n\n\n\n\n\n\n\n\n\n\nGAMs and GPs: Flexibility and Calibration\n\n\n\n\n\n\nprobability\n\n\ngeneralised additive models\n\n\ngaussian processes\n\n\n\n\n\n\n\n\n\nApr 7, 2024\n\n\n40 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "oss/causalpy/instrumental_variables.html",
    "href": "oss/causalpy/instrumental_variables.html",
    "title": "Bayesian IV Regression in CausalPy",
    "section": "",
    "text": "Instrumental Variable Regression\nIn this project I sought to add the functionality for bayesian instrumental variable analysis to the CausalPy package. I adapted the work of Juan Orduz to contribute the base classes to the package and demonstrated how these classes can be used to esitmate instrumental regression by replicating the results of an Acemologu paper on the efficacy of political institutions. The demonstration can be seen here\n\n\n\nComparison with OLS"
  },
  {
    "objectID": "oss/bambi/mr_p.html",
    "href": "oss/bambi/mr_p.html",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "",
    "text": "Regression as Stratification\nIn this project I sought to understand the procedure of post-stratification adjustment used in election forecasting and regression modelling of the same. I published the documentation for the technique of using Multilevel Regression and post-stratification (MrP) with the bambi package here. In this work i tried to elaborate precisely how and why the careful modeller would want to make stratum specific adjustments to the predictions of regression models.\nI adapted the work of Martin, Philips and Gelmen’s “Multilevel Regression and Poststratification Case Studies” that can be found here. You can download the worked example here\n\n\n\nComparison with OLS"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#preliminaries",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#preliminaries",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Preliminaries",
    "text": "Preliminaries\n\n\n\n\n\n\nI am not an Economist\n\n\n\nI’m a data scientist at Personio\n\nBayesian statistician,\nReformed philosopher and logician.\n\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\n\nDisclaimer\n\n\nNone of Personio’s data was used in this presentation\n\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\nThe worked examples used here can be found here"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#agenda",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#agenda",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Agenda",
    "text": "Agenda\n\n\nHistory and Background\n\n\n\n\nModelling Choice Scenarios\n\n\n\n\nMarket Structure and Substitution Patterns\n\n\n\n\nModel Adequacy and Counterfactuals\n\n\n\n\nIndividual Heterogenous Utility\n\n\n\n\nConclusion\n\nThe World in the Model"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#mcfadden-and-bart",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#mcfadden-and-bart",
    "title": "Discrete Choice and Random Utility Models",
    "section": "McFadden and BART",
    "text": "McFadden and BART\n\n\n\n\n“Transport projects involve sinking money in expensive capital investments, which have a long life and wide repercussions. There is no escape from the attempt both to estimate the demand for their services over twenty or thirty years and to assess their repercussions on the economy as a whole.” - Denys Munby, Transport, 1968 ”\n\n\n\n\n\n\n\nBay Area Rapid Transit\n\n\n\n\n\nDublin Metrolink"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#revealed-preference-and-predicting-demand",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#revealed-preference-and-predicting-demand",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Revealed Preference and Predicting Demand",
    "text": "Revealed Preference and Predicting Demand\nSelf Centred Utility Maximisers?\n\n\n\n\nThe assumption of revealed preference theory is that if a person chooses A over B then their latent subjective utility for A is greater than for B.\n\n\n\n\nSurvey data estimated about 15% of users would adopt the newly introduced BART system. McFadden’s random utility model estimated 6%.\n\n\n\n\nHe was right.\n\n\n\n\n\n\nCopernican Shift: He estimated utility to predict choice, rather than infer utility from stated choice."
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#general-applicability-of-choice-problems",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#general-applicability-of-choice-problems",
    "title": "Discrete Choice and Random Utility Models",
    "section": "General Applicability of Choice Problems",
    "text": "General Applicability of Choice Problems\n\n\nThese models offer the possibility of predicting choice in diverse domains: policy, brand, school, car and partners.\n\n\n\n\nHard Question: What are the attributes that drive these choices? How well are they measurable?\n\n\n\n\nHarder Question: How do changes in these attributes influence the predicted market demand for these choices?"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#note-on-model-evaluation",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#note-on-model-evaluation",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Note on Model Evaluation",
    "text": "Note on Model Evaluation\nReplicating the Super Soldier Program"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#note-on-model-evaluation-1",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#note-on-model-evaluation-1",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Note on Model Evaluation",
    "text": "Note on Model Evaluation\nReplicating the Super Soldier Program\n\n\n\n\n\n\n\n\n\n\nBayesian Models aim to replicate the DGP holistically and answer the harder question"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-the-data",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-the-data",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Choice: The Data",
    "text": "Choice: The Data\nGas Central Heating and Electrical Central Heating described by their cost of installation and operation.\n\n\n\nchoice_id\nchosen\nic_gc\noc_gc\n…\noc_ec\n\n\n\n\n1\ngc\n866\n200\n…\n542\n\n\n2\nec\n802\n195\n…\n510\n\n\n3\ner\n759\n203\n…\n495\n\n\n4\ngr\n789\n220\n…\n502"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-a-naive-model",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-a-naive-model",
    "title": "Choice Models and Subjective Utility",
    "section": "Choice: A Naive Model",
    "text": "Choice: A Naive Model\nUnderspecified Utilities\nLet there be five goods described by their cost of installation and operation.\n\\[ \\begin{split} \\overbrace{\\begin{pmatrix}\n\\color{green}{u_{gc}}   \\\\\n\\color{green}{u_{gr}}   \\\\\n\\color{green}{u_{ec}}   \\\\\n\\color{green}{u_{er}}   \\\\\n\\color{green}{u_{hp}}   \\\\\n\\end{pmatrix}}^{utility} =  \\begin{pmatrix}\ngc_{ic} & gc_{oc}  \\\\\ngr_{ic} & gr_{oc}  \\\\\nec_{ic} & ec_{oc}  \\\\\ner_{ic} & er_{oc}  \\\\\nhp_{ic} & hp_{oc}  \\\\\n\\end{pmatrix} \\overbrace{\\begin{pmatrix}\n\\color{blue}{\\beta_{ic}}   \\\\\n\\color{blue}{\\beta_{oc}}   \\\\\n\\end{pmatrix}}^{parameters}  \\end{split}\n\\]"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-a-naive-model-1",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-a-naive-model-1",
    "title": "Choice Models and Subjective Utility",
    "section": "Choice: A Naive Model",
    "text": "Choice: A Naive Model\nThe utility calculation is fundamentally comparative. \\[ \\begin{split} \\begin{pmatrix}\n\\color{green}{u_{gc}}   \\\\\n\\color{green}{u_{gr}}   \\\\\n\\color{green}{u_{ec}}   \\\\\n\\color{green}{u_{er}}   \\\\\n\\color{red}{\\overbrace{0}^{\\text{outside good}}}   \\\\\n\\end{pmatrix} =  \\begin{pmatrix}\ngc_{ic} & gc_{oc}  \\\\\ngr_{ic} & gr_{oc}  \\\\\nec_{ic} & ec_{oc}  \\\\\ner_{ic} & er_{oc}  \\\\\n\\color{red}{0} & \\color{red}{0} \\\\\n\\end{pmatrix} \\begin{pmatrix}\n\\color{blue}{\\beta_{ic}}   \\\\\n\\color{blue}{\\beta_{oc}}   \\\\\n\\end{pmatrix}  \\end{split}\n\\]\nWe zero out one category in the data set to represent the “outside good” for comparison. Similar to dummy variables in Regression, this is required for the model to be identified."
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-a-naive-model-2",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-a-naive-model-2",
    "title": "Choice Models and Subjective Utility",
    "section": "Choice: A Naive Model",
    "text": "Choice: A Naive Model\nUtility determines choice probability of choice:\n\\[\\text{softmax}(\\color{green}{u})_{j} = \\frac{\\exp(\\color{green}{u_{j}})}{\\sum_{q=1}^{J}\\exp(\\color{green}{u_{q}})}\\]\nchoices determine market share where:\n\\[ s_{j}(\\mathbf{\\color{blue}{\\beta}}) = P(\\color{green}{u_{j}} > \\color{green}{u_{k}}; ∀k ̸= j) \\]"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-estimation",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-estimation",
    "title": "Choice Models and Subjective Utility",
    "section": "Choice: Estimation",
    "text": "Choice: Estimation\nThe model is traditionally estimated with maximum likelihood caclulations.\n\\[  L(\\color{blue}{\\beta}) = \\prod s_{j}(\\mathbf{\\color{blue}{\\beta}}) \\]\nor taking the log:\n\\[  l(\\color{blue}{\\beta}) = \\sum log(s_{j}(\\mathbf{\\color{blue}{\\beta}})) \\] \\[ \\text{ We find: } \\underset{\\color{blue}{\\beta}}{\\mathrm{argmax}} \\text{ } l(\\color{blue}{\\beta}) \\]\nResults are often brittle!"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-bayesian-estimation",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#choice-bayesian-estimation",
    "title": "Choice Models and Subjective Utility",
    "section": "Choice: Bayesian Estimation",
    "text": "Choice: Bayesian Estimation\nTo evaluate the integrals in the Bayesian model we use MCMC to estimate conditional probabilities of the joint distribution.\n\\[\\underbrace{\\color{blue}{\\beta}}_{\\text{prior draws}} \\sim Normal(0, 1) \\]\n\\[ \\underbrace{p(\\color{blue}{\\beta} | D)}_{\\text{posterior draws}} = \\frac{p(\\mathbb{\\color{blue}{\\beta}})p(D | \\color{blue}{\\beta} )}{\\int_{i}^{n} p(D | \\mathbf{\\color{blue}{\\beta_{i}}})p(\\mathbf{\\color{blue}{\\beta_{i}}}) } \\]\nPriors can be used flexibly regularise and improve reliability of estimation across structural causal models."
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-naive-model-in-code",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-naive-model-in-code",
    "title": "Choice Models and Subjective Utility",
    "section": "The Naive Model in Code",
    "text": "The Naive Model in Code\nwith pm.Model(coords=coords) as model_1:\n    ## Priors for the Beta Coefficients\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n\n    ## Construct Utility matrix and Pivot\n    u0 = beta_ic * wide_heating_df[\"ic.ec\"] + beta_oc * wide_heating_df[\"oc.ec\"]\n    u1 = beta_ic * wide_heating_df[\"ic.er\"] + beta_oc * wide_heating_df[\"oc.er\"]\n    u2 = beta_ic * wide_heating_df[\"ic.gc\"] + beta_oc * wide_heating_df[\"oc.gc\"]\n    u3 = beta_ic * wide_heating_df[\"ic.gr\"] + beta_oc * wide_heating_df[\"oc.gr\"]\n    u4 = np.zeros(N)  # Outside Good\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    ## Apply Softmax Transform\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n\n    ## Likelihood\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#interpreting-the-model-coefficients",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#interpreting-the-model-coefficients",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Interpreting the Model Coefficients",
    "text": "Interpreting the Model Coefficients\nRate of Substitution\n\n\nThe beta coefficients in the model are interpreted as weights of utility. However, the precision in these latent terms is relative to the variance of unobserved factors.\nThe utility scale is not fixed, but the ratio \\(\\frac{\\beta_{ic}}{\\beta_{oc}}\\) is invariant.\n\n\n\n\nRate of Substitution"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#model-posterior-predictive-fits",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#model-posterior-predictive-fits",
    "title": "Choice Models and Subjective Utility",
    "section": "Model Posterior Predictive Fits",
    "text": "Model Posterior Predictive Fits\nThe model fit fails to recapture the observed data points\n\nModel Fit"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#augmenting-the-model-1",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#augmenting-the-model-1",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Augmenting the Model",
    "text": "Augmenting the Model\nPosterior Predictions\n\nModel Fit"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#augmenting-the-model-2",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#augmenting-the-model-2",
    "title": "Choice Models and Subjective Utility",
    "section": "Augmenting the Model:",
    "text": "Augmenting the Model:\nwith pm.Model(coords=coords) as model_2:\n    ## Priors for the Beta Coefficients\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n    alphas = pm.Normal(\"alpha\", 0, 1, dims=\"alts_intercepts\")\n\n    ## Construct Utility matrix and Pivot using an intercept per alternative\n    u0 = alphas[0] + beta_ic * wide_heating_df[\"ic.ec\"] + beta_oc * wide_heating_df[\"oc.ec\"]\n    u1 = alphas[1] + beta_ic * wide_heating_df[\"ic.er\"] + beta_oc * wide_heating_df[\"oc.er\"]\n    u2 = alphas[2] + beta_ic * wide_heating_df[\"ic.gc\"] + beta_oc * wide_heating_df[\"oc.gc\"]\n    u3 = alphas[3] + beta_ic * wide_heating_df[\"ic.gr\"] + beta_oc * wide_heating_df[\"oc.gr\"]\n    u4 = np.zeros(N)  # Outside Good\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    ## Apply Softmax Transform\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n\n    ## Likelihood\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#augmenting-the-model-3",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#augmenting-the-model-3",
    "title": "Choice Models and Subjective Utility",
    "section": "Augmenting the Model:",
    "text": "Augmenting the Model:\n\nModel Structure"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#augmenting-the-model-4",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#augmenting-the-model-4",
    "title": "Choice Models and Subjective Utility",
    "section": "Augmenting the Model",
    "text": "Augmenting the Model\nPosterior Predictions\n\nModel Fit"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#independence-of-irrelevant-alternatives",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#independence-of-irrelevant-alternatives",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Independence of Irrelevant Alternatives",
    "text": "Independence of Irrelevant Alternatives\nNew Products Cannibalise Equally from all Alternatives\n\nSuppose a market choice between transport modes is determined by the above model.\nRed Bus or Car are you initial Options. Assume \\(s_{\\color{red}{bus}}(\\beta) = s_{car}(\\beta)\\). Market Share is 50% to each option.\nIntroduce the Blue Bus Option, then the Independent characteristics of the utility specification implies that \\(s_{\\color{red}{bus}}(\\beta) = s_{car}(\\beta) = s_{\\color{blue}{bus}}(\\beta)\\)\nThis implies an implausible substitution pattern for real markets.1\n\nWhat kind of Monster have we Created!?"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure-1",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure-1",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\nPriors on Parameters determine Market Structure\n\\[ \\begin{split} \\color{brown}{\\Gamma} =\n\\begin{pmatrix}\n  \\color{red}{1} , \\gamma , \\gamma , \\gamma \\\\\n  \\gamma , \\color{blue}{1} , \\gamma , \\gamma  \\\\\n   \\gamma , \\gamma  , \\color{orange}{1} , \\gamma \\\\\n  \\gamma , \\gamma , \\gamma , \\color{teal}{1}  \n\\end{pmatrix}\n\\end{split} \\]"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure-2",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure-2",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\nCovariance in Code\nwith pm.Model(coords=coords) as model_3:\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n\n    beta_income = pm.Normal(\"beta_income\", 0, 1 dims=\"alts_intercepts\")\n\n    chol, corr, stds = pm.LKJCholeskyCov(\n        \"chol\", n=4, eta=2.0, \n        sd_dist=pm.Exponential.dist(1.0, shape=4)\n    )\n    alphas = pm.MvNormal(\"alpha\", mu=0, chol=chol, dims=\"alts_intercepts\")\n\n    u0 = (\n        alphas[0]\n        + beta_ic * wide_heating_df[\"ic.gc\"]\n        + beta_oc * wide_heating_df[\"oc.gc\"]\n        + beta_income[0] * wide_heating_df[\"income\"]\n    )\n    u1 = (\n        alphas[1]\n        + beta_ic * wide_heating_df[\"ic.gc\"]\n        + beta_oc * wide_heating_df[\"oc.gc\"]\n        + beta_income[1] * wide_heating_df[\"income\"]\n    )\n    u2 = (\n        alphas[2]\n        + beta_ic * wide_heating_df[\"ic.gc\"]\n        + beta_oc * wide_heating_df[\"oc.gc\"]\n        + beta_income[2] * wide_heating_df[\"income\"]\n    )\n    u3 = (\n        alphas[3]\n        + beta_ic * wide_heating_df[\"ic.gr\"]\n        + beta_oc * wide_heating_df[\"oc.gr\"]\n        + beta_income[3] * wide_heating_df[\"income\"]\n    )\n    u4 = np.zeros(N)  # pivot\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure-3",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure-3",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\nStructural Dependence\n\nCorrelation Structure"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure-4",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure-4",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\n\nCorrelation Structure"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#counterfactual-reasoning",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#counterfactual-reasoning",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Counterfactual Reasoning",
    "text": "Counterfactual Reasoning\nInterventions and Conditionalisation\n\n\n\nThere is a sharp distinction between conditional probability distributions and probability under intervention\nIn PyMC you can implement the do-operator to intervene on the graph that represents your data generating process."
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#counterfactual-reasoning-1",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#counterfactual-reasoning-1",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Counterfactual Reasoning",
    "text": "Counterfactual Reasoning\nPricing Experiments\n\nCounterfactual Shares"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#counterfactual-reasoning-2",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#counterfactual-reasoning-2",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Counterfactual Reasoning",
    "text": "Counterfactual Reasoning\nInterventions and Conditionalisation\n\n\n\nThere is a sharp distinction between conditional probability distributions and probability under intervention\nIn PyMC you can implement the do-operator to intervene on the graph that represents your data generating process."
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#counterfactual-reasoning-3",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#counterfactual-reasoning-3",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Counterfactual Reasoning",
    "text": "Counterfactual Reasoning\nInterventions and Conditionalisation\n\n\n\nThere is a sharp distinction between conditional probability distributions and probability under intervention\nIn PyMC you can implement the do-operator to intervene on the graph that represents your data generating process."
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#individual-heterogenous-utility",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#individual-heterogenous-utility",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\nRepeated Choice and Hierarchical Structure\n\n\n\nperson_id\nchoice_id\nchosen\nnabisco_price\nkeebler_price\n\n\n\n\n1\n1\nnabisco\n3.40\n2.00\n\n\n1\n2\nnabisco\n3.45\n2.50\n\n\n1\n3\nkeebler\n3.60\n2.70\n\n\n2\n1\nkeebler\n3.48\n2.20\n\n\n2\n2\nkeebler\n3.30\n2.25"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#individual-heterogenous-utility-1",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#individual-heterogenous-utility-1",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\n\\[\n\\begin{split}\n\\begin{pmatrix}\n\\color{red}{u_{i, nb}} \\\\\n\\color{purple}{u_{i, kb}} \\\\\n\\color{orange}{u_{i, sun}} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  (\\color{red}{\\alpha_{nb}} + \\beta_{i}) + \\color{blue}{\\beta_{p}}p_{nb} + \\color{green}{\\beta_{disp}}d_{nb} \\\\\n  (\\color{purple}{\\alpha_{kb}} + \\beta_{i}) +  \\color{blue}{\\beta_{p}}p_{kb} + \\color{green}{\\beta_{disp}}d_{kb}  \\\\\n  (\\color{orange}{\\alpha_{sun}}  + \\beta_{i})  + \\color{blue}{\\beta_{p}}p_{sun} + \\color{green}{\\beta_{disp}}d_{sun}  \\\\\n   0 + 0 + 0\n\\end{pmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#individual-heterogenous-utility-2",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#individual-heterogenous-utility-2",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\nIn Code\n\nwith pm.Model(coords=coords) as model_4:\n    beta_feat = pm.TruncatedNormal(\"beta_feat\", 0, 1, upper=10, lower=0)\n    beta_disp = pm.TruncatedNormal(\"beta_disp\", 0, 1, upper=10, lower=0)\n    ## Stronger Prior on Price to ensure \n    ## an increase in price negatively impacts utility\n    beta_price = pm.TruncatedNormal(\"beta_price\", 0, 1, upper=0, lower=-10)\n    alphas = pm.Normal(\"alpha\", 0, 1, dims=\"alts_intercepts\")\n    beta_individual = pm.Normal(\"beta_individual\", 0, 0.05,\n     dims=(\"individuals\", \"alts_intercepts\"))\n\n    u0 = (\n        (alphas[0] + beta_individual[person_indx, 0])\n        + beta_disp * c_df[\"disp.sunshine\"]\n        + beta_feat * c_df[\"feat.sunshine\"]\n        + beta_price * c_df[\"price.sunshine\"]\n    )\n    u1 = (\n        (alphas[1] + beta_individual[person_indx, 1])\n        + beta_disp * c_df[\"disp.keebler\"]\n        + beta_feat * c_df[\"feat.keebler\"]\n        + beta_price * c_df[\"price.keebler\"]\n    )\n    u2 = (\n        (alphas[2] + beta_individual[person_indx, 2])\n        + beta_disp * c_df[\"disp.nabisco\"]\n        + beta_feat * c_df[\"feat.nabisco\"]\n        + beta_price * c_df[\"price.nabisco\"]\n    )\n    u3 = np.zeros(N)  # Outside Good\n    s = pm.math.stack([u0, u1, u2, u3]).T\n    # Reconstruct the total data\n\n    ## Apply Softmax Transform\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n\n    ## Likelihood\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#individual-heterogenous-utility-3",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#individual-heterogenous-utility-3",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\nRecovered Posterior Predictive Distribution"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#individual-heterogenous-utility-4",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#individual-heterogenous-utility-4",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\nIndividual Preference\n\n\n\n\n\nIndividual preferences can be derived from the model in this manner.\nThe relationship between preferences over the product offering can be seen too\nMarket stable under stable preferences?"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#conclusion",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#conclusion",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Conclusion",
    "text": "Conclusion\nThe World in the Model\n“Models… [are] like sonnets for the poet, [a] means to express accounts of life in exact, short form using languages that may easily abstract or analogise, and involve imaginative choices and even a certain degree of playfulness in expression” - Mary Morgan in The World in the Model\n\n\n\nModels should articulate the relevant structure of this world and other possible ones.\nThey serve as microscopes. Simulation systems are tools to interrogate reality.\nBayesian Conditionalisation calibrates the system against the observed facts.\nBayesian Discrete choice models help us interrogate aspects of market demand under uncertainty.\nPyMC enables us to easily build and experiment with those models.\nCausal inference is plausible to degree that we can defend the structural assumptions. Bayesian models enforce tranparency and justification of structural commitments and necessary complexity.\n\n\n \n\n\n\n\n\nDiscrete Choice with PyMC"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-multnomial-model",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-multnomial-model",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Multnomial Model:",
    "text": "The Multnomial Model:\nProduct Specific Intercepts\n\\[\n\\begin{split}\n\\begin{pmatrix}\n\\color{red}{u_{gc}} \\\\\n\\color{purple}{u_{gr}} \\\\\n\\color{orange}{u_{ec}} \\\\\n\\color{teal}{u_{er}} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  \\color{red}{\\alpha_{gc}} + \\color{blue}{\\beta_{ic}}gc_{ic} + \\color{blue}{\\beta_{oc}}gc_{oc} \\\\\n  \\color{purple}{\\alpha_{gr}} + \\color{blue}{\\beta_{ic}}gr_{ic} + \\color{blue}{\\beta_{oc}}gr_{oc}  \\\\\n  \\color{orange}{\\alpha_{ec}} + \\color{blue}{\\beta_{ic}}ec_{ic} + \\color{blue}{\\beta_{oc}}ec_{oc}  \\\\\n  \\color{teal}{\\alpha_{er}} + \\color{blue}{\\beta_{ic}}er_{ic} + \\color{blue}{\\beta_{oc}}er_{oc}  \\\\\n   0 + 0 + 0\n\\end{pmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-choice-model",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-choice-model",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Choice Model",
    "text": "The Choice Model\nUtility determines choice probability of choice:\n\\[\\text{softmax}(\\color{green}{u})_{j} = \\frac{\\exp(\\color{green}{u_{j}})}{\\sum_{q=1}^{J}\\exp(\\color{green}{u_{q}})}\\]\nchoices determine market share where:\n\\[ s_{j}(\\mathbf{\\color{blue}{\\beta}}) = P(\\color{green}{u_{j}} > \\color{green}{u_{k}}; ∀k ̸= j) \\]"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-estimation-strategy",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-estimation-strategy",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Estimation Strategy",
    "text": "The Estimation Strategy\nThe model is traditionally estimated with maximum likelihood caclulations.\n\\[  L(\\color{blue}{\\beta}) = \\prod s_{j}(\\mathbf{\\color{blue}{\\beta}}) \\]\nor taking the log:\n\\[  l(\\color{blue}{\\beta}) = \\sum log(s_{j}(\\mathbf{\\color{blue}{\\beta}})) \\] \\[ \\text{ We find: } \\underset{\\color{blue}{\\beta}}{\\mathrm{argmax}} \\text{ } l(\\color{blue}{\\beta}) \\]\nResults are often brittle!"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#bayesian-estimation",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#bayesian-estimation",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Bayesian Estimation",
    "text": "Bayesian Estimation\nTo evaluate the integrals in the Bayesian model we use MCMC to estimate conditional probabilities of the joint distribution.\n\\[\\underbrace{\\color{blue}{\\beta}}_{\\text{prior draws}} \\sim Normal(0, 1) \\]\n\\[ \\underbrace{p(\\color{blue}{\\beta} | D)}_{\\text{posterior draws}} = \\frac{p(\\mathbb{\\color{blue}{\\beta}})p(D | \\color{blue}{\\beta} )}{\\int_{i}^{n} p(D | \\mathbf{\\color{blue}{\\beta_{i}}})p(\\mathbf{\\color{blue}{\\beta_{i}}}) } \\]\nPriors can be used flexibly to regularise and improve reliability of estimation across structural causal models."
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-model-in-code",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-model-in-code",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Model in Code:",
    "text": "The Model in Code:\nwith pm.Model(coords=coords) as model_2:\n    ## Priors for the Beta Coefficients\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n    alphas = pm.Normal(\"alpha\", 0, 1, dims=\"alts_intercepts\")\n\n    ## Construct Utility matrix and Pivot using an intercept per alternative\n    u0 = alphas[0] + beta_ic * wide_heating_df[\"ic.ec\"] + beta_oc * wide_heating_df[\"oc.ec\"]\n    u1 = alphas[1] + beta_ic * wide_heating_df[\"ic.er\"] + beta_oc * wide_heating_df[\"oc.er\"]\n    u2 = alphas[2] + beta_ic * wide_heating_df[\"ic.gc\"] + beta_oc * wide_heating_df[\"oc.gc\"]\n    u3 = alphas[3] + beta_ic * wide_heating_df[\"ic.gr\"] + beta_oc * wide_heating_df[\"oc.gr\"]\n    u4 = np.zeros(N)  # Outside Good\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    ## Apply Softmax Transform\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n\n    ## Likelihood\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#augmenting-the-model",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#augmenting-the-model",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Augmenting the Model:",
    "text": "Augmenting the Model:\n\nModel Structure"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-multnomial-model-1",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-multnomial-model-1",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Multnomial Model:",
    "text": "The Multnomial Model:\nUtility determines choice probability of choice:\n\\[\\text{softmax}(\\color{green}{u})_{j} = \\frac{\\exp(\\color{green}{u_{j}})}{\\sum_{q=1}^{J}\\exp(\\color{green}{u_{q}})}\\]\nchoices determine market share where:\n\\[ s_{j}(\\mathbf{\\color{blue}{\\beta}}) = P(\\color{green}{u_{j}} > \\color{green}{u_{k}}; ∀k ̸= j) \\]"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-maximum-likelihood-estimation-strategy",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-maximum-likelihood-estimation-strategy",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Maximum Likelihood Estimation Strategy",
    "text": "The Maximum Likelihood Estimation Strategy\nThe model is traditionally estimated with maximum likelihood caclulations.\n\\[  L(\\color{blue}{\\beta}) = \\prod s_{j}(\\mathbf{\\color{blue}{\\beta}}) \\]\nor taking the log:\n\\[  l(\\color{blue}{\\beta}) = \\sum log(s_{j}(\\mathbf{\\color{blue}{\\beta}})) \\] \\[ \\text{ We find: } \\underset{\\color{blue}{\\beta}}{\\mathrm{argmax}} \\text{ } l(\\color{blue}{\\beta}) \\]\nResults are often brittle!"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#model-structure",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#model-structure",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Model Structure:",
    "text": "Model Structure:\n\nModel Structure"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#model-fit",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#model-fit",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Model Fit",
    "text": "Model Fit\nPosterior Predictive Distribution\n\nModel Fit"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Problem:",
    "text": "The Problem:\nNew Products Cannibalise Equally from all Alternatives\n\nSuppose a market choice between transport modes is determined by the above model.\nRed Bus or Car are you initial Options. Assume \\(s_{\\color{red}{bus}}(\\beta) = s_{car}(\\beta)\\). Market Share is 50% to each option.\nIntroduce the Blue Bus Option, then the Independent characteristics of the utility specification implies that \\(s_{\\color{red}{bus}}(\\beta) = s_{car}(\\beta) = s_{\\color{blue}{bus}}(\\beta)\\)\nThis implies an implausible substitution pattern for real markets.1\n\nWhat kind of Monster have we Created!?"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem-iia",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem-iia",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Problem: IIA",
    "text": "The Problem: IIA\nNew Products Cannibalise Equally from all Alternatives\n\nSuppose a market choice between transport modes is determined by the above model.\nRed Bus or Car are you initial Options. Assume \\(s_{\\color{red}{bus}}(\\beta) = s_{car}(\\beta)\\). Market Share is 50% to each option.\nIntroduce the Blue Bus Option, then the Independent characteristics of the utility specification implies that \\(s_{\\color{red}{bus}}(\\beta) = s_{car}(\\beta) = s_{\\color{blue}{bus}}(\\beta)\\)\nThis implies an implausible substitution pattern for real markets.1\n\nWhat kind of Monster have we Created!?"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\nDependence in Market Share\n\\[ \\alpha_{i} \\sim Normal(\\mathbf{0}, \\color{brown}{\\Gamma}) \\]\n\\[\n\\begin{split}\n\\begin{pmatrix}\n\\color{red}{u_{gc}} \\\\\n\\color{purple}{u_{gr}} \\\\\n\\color{orange}{u_{ec}} \\\\\n\\color{teal}{u_{er}} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  \\color{red}{\\alpha_{gc}} + \\color{blue}{\\beta_{ic}}gc_{ic} + \\color{blue}{\\beta_{oc}}gc_{oc} \\\\\n  \\color{purple}{\\alpha_{gr}} + \\color{blue}{\\beta_{ic}}gr_{ic} + \\color{blue}{\\beta_{oc}}gr_{oc}  \\\\\n  \\color{orange}{\\alpha_{ec}} + \\color{blue}{\\beta_{ic}}ec_{ic} + \\color{blue}{\\beta_{oc}}ec_{oc}  \\\\\n  \\color{teal}{\\alpha_{er}} + \\color{blue}{\\beta_{ic}}er_{ic} + \\color{blue}{\\beta_{oc}}er_{oc}  \\\\\n   0 + 0 + 0\n\\end{pmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure-in-code",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#adding-correlation-structure-in-code",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Adding Correlation Structure in Code",
    "text": "Adding Correlation Structure in Code\nwith pm.Model(coords=coords) as model_3:\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n\n    beta_income = pm.Normal(\"beta_income\", 0, 1 dims=\"alts_intercepts\")\n\n    chol, corr, stds = pm.LKJCholeskyCov(\n        \"chol\", n=4, eta=2.0, \n        sd_dist=pm.Exponential.dist(1.0, shape=4)\n    )\n    alphas = pm.MvNormal(\"alpha\", mu=0, chol=chol, dims=\"alts_intercepts\")\n\n    u0 = (\n        alphas[0]\n        + beta_ic * wide_heating_df[\"ic.gc\"]\n        + beta_oc * wide_heating_df[\"oc.gc\"]\n        + beta_income[0] * wide_heating_df[\"income\"]\n    )\n    u1 = (\n        alphas[1]\n        + beta_ic * wide_heating_df[\"ic.gc\"]\n        + beta_oc * wide_heating_df[\"oc.gc\"]\n        + beta_income[1] * wide_heating_df[\"income\"]\n    )\n    u2 = (\n        alphas[2]\n        + beta_ic * wide_heating_df[\"ic.gc\"]\n        + beta_oc * wide_heating_df[\"oc.gc\"]\n        + beta_income[2] * wide_heating_df[\"income\"]\n    )\n    u3 = (\n        alphas[3]\n        + beta_ic * wide_heating_df[\"ic.gr\"]\n        + beta_oc * wide_heating_df[\"oc.gr\"]\n        + beta_income[3] * wide_heating_df[\"income\"]\n    )\n    u4 = np.zeros(N)  # pivot\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-model",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-model",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Model:",
    "text": "The Model:\nUtilities in Code\nwith pm.Model(coords=coords) as model_2:\n    ## Priors for the Beta Coefficients\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n    alphas = pm.Normal(\"alpha\", 0, 1, dims=\"alts_intercepts\")\n\n    ## Construct Utility matrix and Pivot using an intercept per alternative\n    u0 = alphas[0] + beta_ic * wide_heating_df[\"ic.ec\"] + beta_oc * wide_heating_df[\"oc.ec\"]\n    u1 = alphas[1] + beta_ic * wide_heating_df[\"ic.er\"] + beta_oc * wide_heating_df[\"oc.er\"]\n    u2 = alphas[2] + beta_ic * wide_heating_df[\"ic.gc\"] + beta_oc * wide_heating_df[\"oc.gc\"]\n    u3 = alphas[3] + beta_ic * wide_heating_df[\"ic.gr\"] + beta_oc * wide_heating_df[\"oc.gr\"]\n    u4 = np.zeros(N)  # Outside Good\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    ## Apply Softmax Transform\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n\n    ## Likelihood\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#model-adequacy-and-counterfactuals",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#model-adequacy-and-counterfactuals",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Model Adequacy and Counterfactuals",
    "text": "Model Adequacy and Counterfactuals\nCeteris Paribus Laws\nWith a fitted PyMC model we can counterfactually reset the values for the input data and regenerate the posterior predictive distribution holding else equal in the data generating process.\n\nWhat would the market share be like if prices for electrical systems increased 20%?\n\n # update values of predictors with new 20% \n # price increase in operating costs for electrical options\nwith model_3:\n    pm.set_data({\"oc_ec\": wide_heating_df[\"oc.ec\"] * 1.2, \n                 \"oc_er\": wide_heating_df[\"oc.er\"] * 1.2})\n    # use the updated values and predict outcomes and probabilities:\n    idata_new_policy = pm.sample_posterior_predictive(\n        idata_m3,\n        var_names=[\"p\", \"y_cat\"],\n        return_inferencedata=True,\n        predictions=True,\n        extend_inferencedata=False,\n        random_seed=100,\n    )\n\nidata_new_policy"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#model-adequacy-and-counterfactuals-1",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#model-adequacy-and-counterfactuals-1",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Model Adequacy and Counterfactuals",
    "text": "Model Adequacy and Counterfactuals\nCeteris Paribus Laws\nWith a fitted PyMC model we can counterfactually reset the values for the input data and regenerate the posterior predictive distribution holding else equal in the data generating process.\n\nWhat would the market share be like if prices for electrical systems increased 20%?\n\n # update values of predictors with new 20% \n # price increase in operating costs for electrical options\nwith model_3:\n    pm.set_data({\"oc_ec\": wide_heating_df[\"oc.ec\"] * 1.2, \n                 \"oc_er\": wide_heating_df[\"oc.er\"] * 1.2})\n    # use the updated values and predict outcomes and probabilities:\n    idata_new_policy = pm.sample_posterior_predictive(\n        idata_m3,\n        var_names=[\"p\", \"y_cat\"],\n        return_inferencedata=True,\n        predictions=True,\n        extend_inferencedata=False,\n        random_seed=100,\n    )\n\nidata_new_policy"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#model-adequacy-and-counterfactuals-2",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#model-adequacy-and-counterfactuals-2",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Model Adequacy and Counterfactuals",
    "text": "Model Adequacy and Counterfactuals\nPricing Experiments\n\nCounterfactual Shares"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem-interpreting-the-model-coefficients",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem-interpreting-the-model-coefficients",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Problem: Interpreting the Model Coefficients",
    "text": "The Problem: Interpreting the Model Coefficients\nRate of Substitution\n\n\nThe beta coefficients in the model are interpreted as weights of utility. However, the precision in these latent terms is relative to the variance of unobserved factors.\nThe utility scale is not fixed, but the ratio \\(\\frac{\\beta_{ic}}{\\beta_{oc}}\\) is invariant.\n\n\n\n\nRate of Substitution"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem-model-structure",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem-model-structure",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Problem: Model Structure:",
    "text": "The Problem: Model Structure:\n\nThe Process of Bayesian Updating calibrates the parameter estimates against the data"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem-model-fit",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem-model-fit",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Problem: Model Fit",
    "text": "The Problem: Model Fit\nPosterior Predictive Distribution\n\nThe model successfully predicts observed market share"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem-interpreting-the-model",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#the-problem-interpreting-the-model",
    "title": "Discrete Choice and Random Utility Models",
    "section": "The Problem: Interpreting the Model",
    "text": "The Problem: Interpreting the Model\nRate of Substitution\n\n\nThe beta coefficients in the model are interpreted as weights of utility. However, the precision in these latent terms is relative to the variance of unobserved factors.\nThe utility scale is not fixed, but the ratio \\(\\frac{\\beta_{ic}}{\\beta_{oc}}\\) is invariant.\n\n\n\n\nRate of Substitution"
  },
  {
    "objectID": "talks/pycon_ireland/pycon_ireland_discrete.html#model-adequacy-and-counterfactuals-3",
    "href": "talks/pycon_ireland/pycon_ireland_discrete.html#model-adequacy-and-counterfactuals-3",
    "title": "Discrete Choice and Random Utility Models",
    "section": "Model Adequacy and Counterfactuals",
    "text": "Model Adequacy and Counterfactuals\nInterventions and Conditionalisation\n\n\n\nThere is a sharp distinction between conditional probability distributions and probability under intervention\nIn PyMC you can implement the do-operator to intervene on the graph that represents your data generating process."
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#preliminaries",
    "href": "talks/missing_data/missing_data_causal.html#preliminaries",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Preliminaries",
    "text": "Preliminaries\n\n\n\n\n\n\nBackground\n\n\n\nI’m a data scientist at Personio\nContributor at PyMC and PyMC labs\n\nBayesian statistician,\nReformed philosopher and logician.\n\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\n\nDisclaimer\n\n\nNone of Personio’s data was used in this presentation\n\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\nThe worked examples used here can be found here"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#agenda",
    "href": "talks/missing_data/missing_data_causal.html#agenda",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Agenda",
    "text": "Agenda"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#agenda-1",
    "href": "talks/missing_data/missing_data_causal.html#agenda-1",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Agenda",
    "text": "Agenda\n\n\nTypology of Missing-ness\n\n\n\n\nMultivariate Missing-ness with FIML\n\n\n\n\nBayesian Imputation by Chained Equations\n\n\n\n\nHierarchical Structures impacting Missing-ness\n\n\n\n\nImputation and Causal Narratives\n\n\n\n\nConclusion\n\nMissing Data: a Gateway to Causal Inference"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#typology-of-missing-ness",
    "href": "talks/missing_data/missing_data_causal.html#typology-of-missing-ness",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Typology of Missing-ness",
    "text": "Typology of Missing-ness\n\n\nMissing Completely at Random (MCAR) for \\(Y\\)\n\n\\(P(M = 1 | Y_{obs}, Y_{miss}, \\phi) = P(M=1 | \\phi)\\)\nwhere \\(\\phi\\) is the haphazard circumstance of the world\nmissing-ness cannot be predicted from the \\(Y\\) variable\n\n\n\n\n\nMissing at Random (MAR) for \\(Y\\)\n\n\\(P(M = 1 | Y_{obs}, Y_{miss}, \\phi) = P(M=1 | Y_{obs}, \\phi)\\)\nmissing-ness can be predicted from \\(Y\\) variable\n\n\n\n\n\nMissing Not at Random (MNAR) for \\(Y\\)\n\n\\(P(M = 1 | Y_{obs}, Y_{miss}, \\phi)\\)\nnon-ignorable missing-ness. Need to account for why!"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#identifiability-under-missing-ness",
    "href": "talks/missing_data/missing_data_causal.html#identifiability-under-missing-ness",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Identifiability under Missing-ness",
    "text": "Identifiability under Missing-ness\n\n\nUnder MCAR and MAR there exist consistent identifiable estimators for functions of \\(Y\\)\n\n\n\n\nUnder MNAR there does not exist in general consistent estimators for functions of \\(Y.\\)\n\n\n\n\nWe cannot use data to distinguish cases which are MNAR V (MCAR or MAR).\nWe are required to make assumptions about the nature of the missing-ness to try and account for it."
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#the-data",
    "href": "talks/missing_data/missing_data_causal.html#the-data",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "The Data",
    "text": "The Data\n\nEmployee Empowerment Survey"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#the-metrics",
    "href": "talks/missing_data/missing_data_causal.html#the-metrics",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "The Metrics",
    "text": "The Metrics\n\nMetrics with Gaps"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#modelling-the-multivariate-structure",
    "href": "talks/missing_data/missing_data_causal.html#modelling-the-multivariate-structure",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Modelling the Multivariate Structure",
    "text": "Modelling the Multivariate Structure\nWe can use a MLE variant called Full information maximum likelihood to estimate the multivariate distribution.\n\\[ \\mathbf{Y} = MvNormal(\\mu, \\Sigma) \\]\nFIML controls for the missing observations in the data by maximising the likelihood based across the different patterns of missing-ness in the data."
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#modelling-the-multivariate-structure-1",
    "href": "talks/missing_data/missing_data_causal.html#modelling-the-multivariate-structure-1",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Modelling the Multivariate Structure",
    "text": "Modelling the Multivariate Structure\nFIML in code\n\ndata = df_employee[[\"worksat\", \"empower\", \"lmx\"]]\n\n\ndef split_data_by_missing_pattern(data):\n    # We want to extract our the pattern of missing-ness in our dataset\n    # and save each sub-set of our data in a structure that can be used to feed into a log-likelihood function\n    grouped_patterns = []\n    patterns = data.notnull().drop_duplicates().values\n    # A pattern is whether the \n    # values in each column e.g. \n    # [True, True, True] or [True, True, False]\n    observed = data.notnull()\n    for p in range(len(patterns)):\n        temp = observed[\n            (observed[\"worksat\"] == patterns[p][0])\n            & (observed[\"empower\"] == patterns[p][1])\n            & (observed[\"lmx\"] == patterns[p][2])\n        ]\n        grouped_patterns.append([patterns[p], temp.index, data.iloc[temp.index].dropna(axis=1)])\n\n    return grouped_patterns\n\n\ndef reconstitute_params(params_vector, n_vars):\n    # Convenience numpy function to construct mirrored COV matrix\n    # From flattened params_vector\n    mus = params_vector[0:n_vars]\n    cov_flat = params_vector[n_vars:]\n    indices = np.tril_indices(n_vars)\n    cov = np.empty((n_vars, n_vars))\n    for i, j, c in zip(indices[0], indices[1], cov_flat):\n        cov[i, j] = c\n        cov[j, i] = c\n    cov = cov + 1e-25\n    return mus, cov\n\n\ndef optimise_ll(flat_params, n_vars, grouped_patterns):\n    mus, cov = reconstitute_params(flat_params, n_vars)\n    # Check if COV is positive definite\n    if (np.linalg.eigvalsh(cov) < 0).any():\n        return 1e100\n    objval = 0.0\n    for obs_pattern, _, obs_data in grouped_patterns:\n        # This is the key (tricky) step because we're selecting the variables which pattern\n        # the full information set within each pattern of \"missing-ness\"\n        # e.g. when the observed pattern is [True, True, False] we want the first two variables\n        # of the mus vector and we want only the covariance relations between the relevant variables from the cov\n        # in the iteration.\n        obs_mus = mus[obs_pattern]\n        obs_cov = cov[obs_pattern][:, obs_pattern]\n        ll = np.sum(multivariate_normal(obs_mus, obs_cov).logpdf(obs_data))\n        objval = ll + objval\n    return -objval\n\n\ndef estimate(data):\n    n_vars = data.shape[1]\n    # Initialise\n    mus0 = np.zeros(n_vars)\n    cov0 = np.eye(n_vars)\n    # Flatten params for optimiser\n    params0 = np.append(mus0, cov0[np.tril_indices(n_vars)])\n    # Process Data\n    grouped_patterns = split_data_by_missing_pattern(data)\n    # Run the Optimiser.\n    try:\n        result = scipy.optimize.minimize(\n            optimise_ll, params0, args=(n_vars, grouped_patterns), method=\"Powell\"\n        )\n    except Exception as e:\n        raise e\n    mean, cov = reconstitute_params(result.x, n_vars)\n    return mean, cov\n\n\nfiml_mus, fiml_cov = estimate(data)"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#estimate-model-and-samples",
    "href": "talks/missing_data/missing_data_causal.html#estimate-model-and-samples",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Estimate Model and Samples",
    "text": "Estimate Model and Samples\n\n\n\n\n\n\n\n\n\n\nDiscrete Choice with PyMC"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#estimated-model-and-samples",
    "href": "talks/missing_data/missing_data_causal.html#estimated-model-and-samples",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Estimated Model and Samples",
    "text": "Estimated Model and Samples\n\n\n\n\n\n\n\n\nThe Estimated model Parameters are used to sample from the implied distribution.\nBut the approach lacks control and insight into why the data was missing in the first place.\nCannot directly account for MNAR cases."
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#the-joint-distribution-decomposed",
    "href": "talks/missing_data/missing_data_causal.html#the-joint-distribution-decomposed",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "The Joint Distribution Decomposed",
    "text": "The Joint Distribution Decomposed\nModelling the Outcome piece-wise\nStart with the observation that: \\[f(emp, lmx, climate, male) \\\\ = f(emp | lmx, climate, male) \\cdot  \\\\f(lmx | climate, male) \\cdot f(climate | male) \\cdot f(male)^{*}\\]\nwhich can be phrased as a set of linear models\n\\[ empower = \\alpha_{2} + \\beta_{3}male + \\beta_{4}climate + \\beta_{5}lmx \\\\\nlmx = \\alpha_{1} + \\beta_{1}climate + \\beta_{2}male \\\\\nclimate = \\alpha_{0} + \\beta_{0}male \\]"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#the-model",
    "href": "talks/missing_data/missing_data_causal.html#the-model",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "The Model",
    "text": "The Model\nSensitivity Analysis with Priors in PyMC\ndata = df_employee[[\"lmx\", \"empower\", \"climate\", \"male\"]]\nlmx_mean = data[\"lmx\"].mean()\nlmx_min = data[\"lmx\"].min()\nlmx_max = data[\"lmx\"].max()\nlmx_sd = data[\"lmx\"].std()\n\ncli_mean = data[\"climate\"].mean()\ncli_min = data[\"climate\"].min()\ncli_max = data[\"climate\"].max()\ncli_sd = data[\"climate\"].std()\n\n\npriors = {\n    \"climate\": {\"normal\": [lmx_mean, lmx_sd, lmx_sd], \n                \"uniform\": [lmx_min, lmx_max]},\n    \"lmx\": {\"normal\": [cli_mean, cli_sd, cli_sd], \n            \"uniform\": [cli_min, cli_max]},\n}\n\n\ndef make_model(priors, normal_pred_assumption=True):\n    coords = {\n        \"alpha_dim\": [\"lmx_imputed\", \"climate_imputed\", \"empower_imputed\"],\n        \"beta_dim\": [\n            \"lmxB_male\",\n            \"lmxB_climate\",\n            \"climateB_male\",\n            \"empB_male\",\n            \"empB_climate\",\n            \"empB_lmx\",\n        ],\n    }\n    with pm.Model(coords=coords) as model:\n        # Priors\n        beta = pm.Normal(\"beta\", 0, 1, size=6, dims=\"beta_dim\")\n        alpha = pm.Normal(\"alphas\", 10, 5, size=3, dims=\"alpha_dim\")\n        sigma = pm.HalfNormal(\"sigmas\", 5, size=3, dims=\"alpha_dim\")\n\n        if normal_pred_assumption:\n            mu_climate = pm.Normal(\n                \"mu_climate\", priors[\"climate\"][\"normal\"][0], priors[\"climate\"][\"normal\"][1]\n            )\n            sigma_climate = pm.HalfNormal(\"sigma_climate\", priors[\"climate\"][\"normal\"][2])\n            climate_pred = pm.Normal(\n                \"climate_pred\", mu_climate, sigma_climate, observed=data[\"climate\"].values\n            )\n        else:\n            climate_pred = pm.Uniform(\"climate_pred\", 0, 40, observed=data[\"climate\"].values)\n\n        if normal_pred_assumption:\n            mu_lmx = pm.Normal(\"mu_lmx\", priors[\"lmx\"][\"normal\"][0], priors[\"lmx\"][\"normal\"][1])\n            sigma_lmx = pm.HalfNormal(\"sigma_lmx\", priors[\"lmx\"][\"normal\"][2])\n            lmx_pred = pm.Normal(\"lmx_pred\", mu_lmx, sigma_lmx, observed=data[\"lmx\"].values)\n        else:\n            lmx_pred = pm.Uniform(\"lmx_pred\", 0, 40, observed=data[\"lmx\"].values)\n\n        # Likelihood(s)\n        lmx_imputed = pm.Normal(\n            \"lmx_imputed\",\n            alpha[0] + beta[0] * data[\"male\"] + beta[1] * climate_pred,\n            sigma[0],\n            observed=data[\"lmx\"].values,\n        )\n        climate_imputed = pm.Normal(\n            \"climate_imputed\",\n            alpha[1] + beta[2] * data[\"male\"],\n            sigma[1],\n            observed=data[\"climate\"].values,\n        )\n        empower_imputed = pm.Normal(\n            \"emp_imputed\",\n            alpha[2] + beta[3] * data[\"male\"] + beta[4] * climate_pred + beta[5] * lmx_pred,\n            sigma[2],\n            observed=data[\"empower\"].values,\n        )\n\n        idata = pm.sample_prior_predictive()\n        idata.extend(pm.sample(random_seed=120))\n        pm.sample_posterior_predictive(idata, extend_inferencedata=True)\n        return idata, model\n\n\nidata_uniform, model_uniform = make_model(priors, normal_pred_assumption=False)\nidata_normal, model_normal = make_model(priors, normal_pred_assumption=True)"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#bayesian-updating-and-calibration",
    "href": "talks/missing_data/missing_data_causal.html#bayesian-updating-and-calibration",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Bayesian Updating and Calibration",
    "text": "Bayesian Updating and Calibration\nEstimating the Model\nTo evaluate the integrals in the Bayesian model we use MCMC to estimate conditional probabilities of the joint distribution.\n\\[\\underbrace{\\color{blue}{\\beta}}_{\\text{prior draws}} \\sim Normal(0, 1) \\]\n\\[ \\underbrace{p(\\color{blue}{\\beta} | D)}_{\\text{posterior draws}} = \\frac{p(\\mathbb{\\color{blue}{\\beta}})p(D | \\color{blue}{\\beta} )}{\\int_{i}^{n} p(D | \\mathbf{\\color{blue}{\\beta_{i}}})p(\\mathbf{\\color{blue}{\\beta_{i}}}) } \\]\nPriors can be used flexibly to regularise and improve reliability of estimation across structural causal models."
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#the-model-structure",
    "href": "talks/missing_data/missing_data_causal.html#the-model-structure",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "The Model Structure",
    "text": "The Model Structure\nThe PyMC model Graph\n\nChained Equation ModelImputing the values and feeding them forward into the ultimate likelihood terms to estimate the profile of the joint distribution."
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#sensitivty-to-prior-specification",
    "href": "talks/missing_data/missing_data_causal.html#sensitivty-to-prior-specification",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Sensitivty to Prior Specification",
    "text": "Sensitivty to Prior Specification\n\nThe Effect of choosing the right PriorsModel choice using predictive adequacy as a constraint. Bayesian model adequacy workflow applies here too"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#hierarchical-structures-of-non-response",
    "href": "talks/missing_data/missing_data_causal.html#hierarchical-structures-of-non-response",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Hierarchical Structures of Non-response",
    "text": "Hierarchical Structures of Non-response\nTeam dynamics determine probability of response\n\nTeam Empowerment ScoresWe can try to recover ignorable missing-ness i.e moving to MAR from MNAR conditional on group specific random effects."
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#hierarchies-in-code",
    "href": "talks/missing_data/missing_data_causal.html#hierarchies-in-code",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Hierarchies in Code",
    "text": "Hierarchies in Code\nSpecifying the hierarchical model in PyMC\nteam_idx, teams = pd.factorize(df_employee[\"team\"], sort=True)\nemployee_idx, _ = pd.factorize(df_employee[\"employee\"], sort=True)\ncoords = {\"team\": teams, \"employee\": np.arange(len(df_employee))}\n\n\nwith pm.Model(coords=coords) as hierarchical_model:\n    # Priors\n    company_beta_lmx = pm.Normal(\"company_beta_lmx\", 0, 1)\n    company_beta_male = pm.Normal(\"company_beta_male\", 0, 1)\n    company_alpha = pm.Normal(\"company_alpha\", 20, 2)\n    team_alpha = pm.Normal(\"team_alpha\", 0, 1, dims=\"team\")\n    team_beta_lmx = pm.Normal(\"team_beta_lmx\", 0, 1, dims=\"team\")\n    sigma = pm.HalfNormal(\"sigma\", 4, dims=\"employee\")\n\n    # Imputed Predictors\n    mu_lmx = pm.Normal(\"mu_lmx\", 10, 5)\n    sigma_lmx = pm.HalfNormal(\"sigma_lmx\", 5)\n    lmx_pred = pm.Normal(\"lmx_pred\", mu_lmx, sigma_lmx, observed=df_employee[\"lmx\"].values)\n\n    # Combining Levels\n    alpha_global = pm.Deterministic(\"alpha_global\", company_alpha + team_alpha[team_idx])\n    beta_global_lmx = pm.Deterministic(\n        \"beta_global_lmx\", company_beta_lmx + team_beta_lmx[team_idx]\n    )\n    beta_global_male = pm.Deterministic(\"beta_global_male\", company_beta_male)\n\n    # Likelihood\n    mu = pm.Deterministic(\n        \"mu\",\n        alpha_global + beta_global_lmx * lmx_pred + beta_global_male * df_employee[\"male\"].values,\n    )\n\n    empower_imputed = pm.Normal(\n        \"emp_imputed\",\n        mu,\n        sigma,\n        observed=df_employee[\"empower\"].values,\n    )\n\n    idata_hierarchical = pm.sample_prior_predictive()\n    idata_hierarchical.extend(\n        sample_blackjax_nuts(draws=20_000, random_seed=500, target_accept=0.99)\n    )\n    pm.sample_posterior_predictive(idata_hierarchical, extend_inferencedata=True)"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#model-structure",
    "href": "talks/missing_data/missing_data_causal.html#model-structure",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Model Structure",
    "text": "Model Structure\n\nHierarchical Team based Model"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#influence-of-team-membership",
    "href": "talks/missing_data/missing_data_causal.html#influence-of-team-membership",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Influence of Team Membership",
    "text": "Influence of Team Membership\n\nModification of Leader Impact based on Team imputed with Uncertainty"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#imputation-with-team-effects",
    "href": "talks/missing_data/missing_data_causal.html#imputation-with-team-effects",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Imputation with Team Effects",
    "text": "Imputation with Team Effects\n\nImputation patterns under team influence"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#feature-selection-is-not-causal-modelling",
    "href": "talks/missing_data/missing_data_causal.html#feature-selection-is-not-causal-modelling",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Feature Selection is not Causal Modelling",
    "text": "Feature Selection is not Causal Modelling\nOn the Importance of Theory Construction\n\n\n\n\n\nID\n\\(Y_{i}(0)\\)\n\\(Y_{i}(1)\\)\n\n\n\n\n1\n?\n1\n\n\n2\n1\n?\n\n\n3\n?\n0\n\n\n4\n?\n1\n\n\n5\n0\n?\n\n\n\nThe Fundamental problem of Causal Inference as Missing Data\n\n\n\nThe heart of causal inference is understanding the risk of confounding influence.\n\n\n\n\nNaively optimising for some in-sample predictive benchmark does not protect your model from confounding bias.\n\n\n\n\nCausal models with deliberate and careful construction of the dependence mechanism are your best hope for genuine insight and robust predictive performance\n\n\n\n\nThis is crucial for model explainability in the human-centric domains, where the decisions need to be justifiable.\n\n\n\n\nUsed to answer Counterfactuals\n\nHow do patterns of Employee empowerment change if we can intervene on their relationship with their manager?"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#counterfactuals-as-imputation.",
    "href": "talks/missing_data/missing_data_causal.html#counterfactuals-as-imputation.",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Counterfactuals as Imputation.",
    "text": "Counterfactuals as Imputation.\n\nBayesian Imputation can be highly customisable method of imputation in almost any domain.\nImputed treatment effect distributions need not be specified simply as a linear combination of observables.\n\nNon-parametric methods such as BART and Dirichlet Process Mixtures can be used.\n\nIn PyMC you can implement the do-operator to intervene on the graph that represents your data generating process."
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#conclusion",
    "href": "talks/missing_data/missing_data_causal.html#conclusion",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve seen the application of missing data analysis to survey data in the context of People Analytics.\n\nMultivariate approaches are effective but cannot help address confounding bias,\nThe flexibility of the Bayesian approach can be tailored to the appropriate complexity of our theory about why our data is missing.\nHierarchical structures pervade business - conduits for leadership influence/communication channels. Hierarchical modelling can isolate estimates of this impact and control for biases of naive aggregates.\n\nReveal inefficiencies and mismatches between team and management. Imputation gives “voice” to the missing.\n\n\n\n\n\nMissing Data with PyMC"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#bayesian-imputation.",
    "href": "talks/missing_data/missing_data_causal.html#bayesian-imputation.",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Bayesian Imputation.",
    "text": "Bayesian Imputation.\n\nBayesian Imputation can be highly customisable method of imputation in almost any domain.\nImputed treatment effect distributions need not be specified simply as a linear combination of observables.\n\nNon-parametric methods such as BART and Dirichlet Process Mixtures can be used.\n\nIn PyMC you can implement the do-operator to intervene on the graph that represents your data generating process."
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#bayesian-probabilistic-imputation.",
    "href": "talks/missing_data/missing_data_causal.html#bayesian-probabilistic-imputation.",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Bayesian Probabilistic Imputation.",
    "text": "Bayesian Probabilistic Imputation.\n\nBayesian Imputation can be highly customisable method of imputation in almost any domain.\nImputed treatment effect distributions need not be specified simply as a linear combination of observables.\n\nNon-parametric methods such as BART and Dirichlet Process Mixtures can be used.\n\nIn PyMC you can implement the do-operator to intervene on the graph that represents your data generating process.\nHow do patterns of Employee empowerment change if we can intervene on their relationship with their manager?"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#sota-bayesian-probabilistic-imputation.",
    "href": "talks/missing_data/missing_data_causal.html#sota-bayesian-probabilistic-imputation.",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "SOTA: Bayesian Probabilistic Imputation.",
    "text": "SOTA: Bayesian Probabilistic Imputation.\n\nBayesian Imputation can be highly customisable method of imputation in almost any domain.\nImputed treatment effect distributions need not be specified simply as a linear combination of observables.\n\nNon-parametric methods such as BART and Dirichlet Process Mixtures can also be used.\n\nIn PyMC you can implement the do-operator to intervene on the graph that represents your data generating process.\nUsed to answer Counterfactuals\n\nHow do patterns of Employee empowerment change if we can intervene on their relationship with their manager?"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#probabilistic-maps",
    "href": "talks/missing_data/missing_data_causal.html#probabilistic-maps",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Probabilistic Maps",
    "text": "Probabilistic Maps\nSampling the possible spaces over missing data\n\n\n\n\n\nBayes at work busily imputing undiscovered continents\n\n\n\nThe idea\n\nWhen gaps in survey data are not random\nWe need to understand the drivers of missing-ness\nThe “topology” around the gaps gives us clues\nBayesian inference helps to imputes the probable inclines and curves of the space in the gaps conditional on the observed data.\nIn our employee engagement data the question becomes - What are the enviromental influences on the probable responses?"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#agenda",
    "href": "talks/survival_regression/time_to_attrition.html#agenda",
    "title": "Survival Regression Models in PyMC",
    "section": "Agenda",
    "text": "Agenda\n\n\nTime to Event Distributions\n\nHyperObjects and Perspectives on Probability\n\n\n\n\n\nPeople Analytics and Survival Regression\n\nCox Proportional Hazard\nAccelerated Failure Time\n\n\n\n\n\nComparing Model Implications\n\nMarginal Predictions\nAcceleration Factors\n\n\n\n\n\nFrailty Models and Stratified Risk\n\n\n\n\nConclusion"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#layered-abstractions-and-hyperobjects",
    "href": "talks/survival_regression/time_to_attrition.html#layered-abstractions-and-hyperobjects",
    "title": "Survival Regression Models in PyMC",
    "section": "Layered Abstractions and Hyperobjects",
    "text": "Layered Abstractions and Hyperobjects\n\nConcepts that defy easy panoptic survey prohibit easy action e.g. climate change and mathematical structures such as families of probability distributions"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#perspectives-on-probability",
    "href": "talks/survival_regression/time_to_attrition.html#perspectives-on-probability",
    "title": "Survival Regression Models in PyMC",
    "section": "Perspectives on Probability",
    "text": "Perspectives on Probability\nTop-Down and Abstract\n\n\n\n\n\nParametric CDF and Survival functions obscure views of instantaneous hazard\n\n\n\\[ h(t) = \\frac{f(t)}{S(t)} \\\\\nH(t) = -ln(S(t))\\]"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#actuarial-tables-and-survival-curves",
    "href": "talks/survival_regression/time_to_attrition.html#actuarial-tables-and-survival-curves",
    "title": "Survival Regression Models in PyMC",
    "section": "Actuarial Tables and Survival Curves",
    "text": "Actuarial Tables and Survival Curves\nBottom-Up and Concrete\n\nActuarial Tables try to estimate CDF and Survival functionsCalculation of these abstract quantities proceeds from a clear and concrete notion of the risk-set in time."
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#time-to-attrition-data",
    "href": "talks/survival_regression/time_to_attrition.html#time-to-attrition-data",
    "title": "Survival Regression Models in PyMC",
    "section": "Time to Attrition Data",
    "text": "Time to Attrition Data\n\n\n Question: What is the relationship between individual characteristics and their expected survival times?\n\n\nQuestion: How do individual survival times vary as a function of the levels in their covariates."
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#regression-and-censored-data",
    "href": "talks/survival_regression/time_to_attrition.html#regression-and-censored-data",
    "title": "Survival Regression Models in PyMC",
    "section": "Regression and Censored Data",
    "text": "Regression and Censored Data\nCensored Data Biases simple summaries\n\\[\\mathbf{y_{i}} = \\beta \\mathbf{X_{i}} \\]\n\n\\[\\begin{split}\n\\begin{pmatrix}\n\\color{red}{y_{i, c}}  \\\\\n\\color{blue}{y_{i, \\neg c}}  \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n\\beta_{1} \\\\\n\\beta_{2} \\\\\n\\beta_{3}\n\\end{pmatrix} \\begin{pmatrix}\n\\color{red}{x_{1,c}^{i}} & \\color{red}{x_{2, c}^{i}} & \\color{red}{x_{3, c}^{i}}  \\\\\n\\color{blue}{x_{1,\\neg c}^{i}} & \\color{blue}{x_{2, \\neg c}^{i}} & \\color{blue}{x_{3, \\neg c}^{i}}  \\\\\n\\end{pmatrix}\n\\end{split}\n\\]\n\n\n\\[ \\Rightarrow L(\\mathbf{\\beta}, S(\\color{red}{y_{c}}), f(\\color{blue}{y_{\\neg c}}) ) \\]\n\n\n\\[ \\underbrace{p( \\beta | y)}_{\\text{posterior draws}} = \\frac{p(\\mathbb{\\beta})L(\\mathbf{\\beta}, S(\\color{red}{y_{c}}), f(\\color{blue}{y_{\\neg c}}) )}{\\int_{i}^{n} L(\\mathbf{\\beta}_{i}, S(\\color{red}{y_{c}}), f(\\color{blue}{y_{\\neg c}}) )p(\\mathbf{\\beta_{i}}) } \\]"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#regression-for-survival-times",
    "href": "talks/survival_regression/time_to_attrition.html#regression-for-survival-times",
    "title": "Survival Regression Models in PyMC",
    "section": "Regression for Survival Times",
    "text": "Regression for Survival Times\nDistributions for Modelling Attrition\n\nMonotonoc or non-monotonic hazards determined by distribution choice. Risk spikes important in early periods of employment."
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#accelerated-failure-time-models",
    "href": "talks/survival_regression/time_to_attrition.html#accelerated-failure-time-models",
    "title": "Survival Regression Models in PyMC",
    "section": "Accelerated Failure Time Models",
    "text": "Accelerated Failure Time Models\nParametric Models of Survival Distributions\n\\[S_{i}(t) = S_{0}\\Bigg(\\frac{t}{exp(\\mu + \\alpha_{1}x_{1} + \\alpha_{2}x_{2} ... \\alpha_{p}x_{p})} \\Bigg) \\]\nwith pm.Model(coords=coords, check_bounds=False) as aft_model:\n  X_data = pm.MutableData(\"X_data_obs\", X)\n  beta = pm.Normal(\"beta\", 0.0, 1, dims=\"preds\")\n  mu = pm.Normal(\"mu\", 0, 1)\n\n  s = pm.HalfNormal(\"s\", 5.0)\n  eta = pm.Deterministic(\"eta\", pm.math.dot(beta, X_data.T))\n  reg = pm.Deterministic(\"reg\", pt.exp(-(mu + eta) / s))\n  y_obs = pm.Weibull(\"y_obs\", beta=reg[~cens], alpha=s, observed=y[~cens])\n  y_cens = pm.Potential(\"y_cens\", weibull_lccdf(y[cens], alpha=s, beta=reg[cens]))\n  \n  idata = pm.sample_prior_predictive()\n  idata.extend(\n      pm.sample(target_accept=0.95, random_seed=100, idata_kwargs={\"log_likelihood\": True})\n  )\n  idata.extend(pm.sample_posterior_predictive(idata))"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#parametric-model-structure",
    "href": "talks/survival_regression/time_to_attrition.html#parametric-model-structure",
    "title": "Survival Regression Models in PyMC",
    "section": "Parametric Model Structure",
    "text": "Parametric Model Structure\n\n\n\n\nAccelerated Failure time models incorporate the regression component as a weighted sum that enters the parametric probability model\nFor instance: \\[ Weibull(\\alpha, \\beta)\n\\\\ = Weibull(\\alpha, reg)\\]"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#proportional-hazards-cox-regression",
    "href": "talks/survival_regression/time_to_attrition.html#proportional-hazards-cox-regression",
    "title": "Survival Regression Models in PyMC",
    "section": "Proportional Hazards Cox Regression",
    "text": "Proportional Hazards Cox Regression\nFlexible Discrete Intervals Hazards\n\\[ \\text{Baseline Hazard: } \\lambda_{0}(t) \\] \\[ \\lambda_{0}(t) \\cdot exp(\\beta_{1}X_{1} + \\beta_{2}X_{2}... \\beta_{k}X_{k}) \\]\n\nwith pm.Model(coords=coords) as base_model:\n  X_data = pm.MutableData(\"X_data_obs\", retention_df[preds], dims=(\"individuals\", \"preds\"))\n  lambda0 = pm.Gamma(\"lambda0\", 0.01, 0.01, dims=\"intervals\")\n\n  beta = pm.Normal(\"beta\", 0, sigma=1, dims=\"preds\")\n  lambda_ = pm.Deterministic(\n      \"lambda_\",\n      pt.outer(pt.exp(pm.math.dot(beta, X_data.T)), lambda0),\n      dims=(\"individuals\", \"intervals\"),\n  )\n  mu = pm.Deterministic(\"mu\", exposure * lambda_, \n                        dims=(\"individuals\", \"intervals\"))\n\n  obs = pm.Poisson(\"obs\", mu, observed=quit, \n                    dims=(\"individuals\", \"intervals\"))\n  idata = pm.sample(\n      target_accept=0.95, random_seed=100, idata_kwargs={\"log_likelihood\": True}\n  )"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#proportional-hazards-cox-regression-1",
    "href": "talks/survival_regression/time_to_attrition.html#proportional-hazards-cox-regression-1",
    "title": "Survival Regression Models in PyMC",
    "section": "Proportional Hazards Cox Regression",
    "text": "Proportional Hazards Cox Regression\n\n\n\n\n\nSorites Paradoxes and accumulation of a triggering resource\n\n\n\n\n“There’s the ‘mañana paradox’: the unwelcome task which needs to be done, but it’s always a matter of indifference whether it’s done today or tomorrow; the dieter’s paradox: I don’t care at all about the difference to my weight one chocolate will make.” - Dorothy Edgington\n\n\n\n\nBaseline Instantenous Hazard"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#proportional-hazards-cox-regression-2",
    "href": "talks/survival_regression/time_to_attrition.html#proportional-hazards-cox-regression-2",
    "title": "Survival Regression Models in PyMC",
    "section": "Proportional Hazards Cox Regression",
    "text": "Proportional Hazards Cox Regression\n\nCumulative Hazard across Individuals"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#proportional-hazards-cox-regression-3",
    "href": "talks/survival_regression/time_to_attrition.html#proportional-hazards-cox-regression-3",
    "title": "Survival Regression Models in PyMC",
    "section": "Proportional Hazards Cox Regression",
    "text": "Proportional Hazards Cox Regression\nUsing the Poisson Trick\n\n\n\n\n\\[ CoxPH(left, month) \\sim gender + level \\]\nis akin to\n\\[ left \\sim glm(gender + level + (1 | month)) \\\\ \\text{ where link is } Poisson  \\]\napplying an offset to the event rate for each time interval."
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#comparing-models",
    "href": "talks/survival_regression/time_to_attrition.html#comparing-models",
    "title": "Survival Regression Models in PyMC",
    "section": "Comparing Models",
    "text": "Comparing Models\nStated Intention and Sentiment\n\\[ CoxPH(left, month) \\sim gender + field + level + sentiment \\]\n\\[ CoxPH(left, month) \\sim gender + field + level + sentiment + intention \\]"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#comparing-models-1",
    "href": "talks/survival_regression/time_to_attrition.html#comparing-models-1",
    "title": "Survival Regression Models in PyMC",
    "section": "Comparing Models",
    "text": "Comparing Models\nInterpreting Model Coefficients\n\nIf \\(exp(\\beta)\\) > 1: An increase in X is associated with an increased hazard (risk) of the event occurring.\nIf \\(exp(\\beta)\\) < 1: An increase in X is associated with a decreased hazard (lower risk) of the event occurring.\nIf \\(exp(\\beta)\\) = 1: X has no effect on the hazard rate.\n\nPredicting Marginal Effects"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#comparing-models-2",
    "href": "talks/survival_regression/time_to_attrition.html#comparing-models-2",
    "title": "Survival Regression Models in PyMC",
    "section": "Comparing Models",
    "text": "Comparing Models\nPredicting Marginal Effects\n\n\n\n\n\nThe Intention Model absorbs some of the variance in the baseline hazard not accounted for in the sentiment model\n\n\n\n\n\n\n\nThe Sentiment Model"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#comparing-models-3",
    "href": "talks/survival_regression/time_to_attrition.html#comparing-models-3",
    "title": "Survival Regression Models in PyMC",
    "section": "Comparing Models",
    "text": "Comparing Models\n\nAFT models allow us to quantify the acceleration factor due to an individual or group’s risk profile."
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#comparing-models-4",
    "href": "talks/survival_regression/time_to_attrition.html#comparing-models-4",
    "title": "Survival Regression Models in PyMC",
    "section": "Comparing Models",
    "text": "Comparing Models\nMarginal Survival Functions and WAIC\n\n\n\nComparing Marginal Survival Functions"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#frailty-models-and-individual-heterogeneity",
    "href": "talks/survival_regression/time_to_attrition.html#frailty-models-and-individual-heterogeneity",
    "title": "Survival Regression Models in PyMC",
    "section": "Frailty Models and Individual Heterogeneity",
    "text": "Frailty Models and Individual Heterogeneity\nWe want to relax the assumptions of Cox Proportional Hazards model. We introduce (i) frailty terms and (ii) stratified risks\n\\[ \\lambda_{i}(t) = \\color{green}{z_{i}}exp(\\beta X)\\color{red}{\\lambda_{0}^{g}(t)} \\]\nThe multiplicative frailty terms \\(z_{i}\\) can be specified as a gamma distribution centred on unity with stratified risks"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#proportional-hazards-cox-regression-4",
    "href": "talks/survival_regression/time_to_attrition.html#proportional-hazards-cox-regression-4",
    "title": "Survival Regression Models in PyMC",
    "section": "Proportional Hazards Cox Regression",
    "text": "Proportional Hazards Cox Regression\nThe Proportional Hazards Assumption\n\nThe covariates enter once into the weighted sum that modifies the baseline hazard.\nWhile the baseline hazard can change over time the difference in hazard induced by different levels in the covariates remains constant over time.\n\n\\[ \\forall t \\in T:  \\frac{h(t | X_{gender} = 1)}{h(t | X_{gender} = 0)} = constant \\]"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#frailty-model-in-codes",
    "href": "talks/survival_regression/time_to_attrition.html#frailty-model-in-codes",
    "title": "Survival Regression Models in PyMC",
    "section": "Frailty Model in Codes",
    "text": "Frailty Model in Codes\n\nwith pm.Model(coords=coords) as frailty_model:\n        X_data_m = pm.MutableData(\"X_data_m\", df_m[preds], dims=(\"men\", \"preds\"))\n        X_data_f = pm.MutableData(\"X_data_f\", df_f[preds], dims=(\"women\", \"preds\"))\n        lambda0 = pm.Gamma(\"lambda0\", 0.01, 0.01, dims=(\"intervals\", \"gender\"))\n        sigma_frailty = pm.Normal(\"sigma_frailty\", opt_params[\"alpha\"], 1)\n        mu_frailty = pm.Normal(\"mu_frailty\", opt_params[\"beta\"], 1)\n        frailty = pm.Gamma(\"frailty\", mu_frailty, sigma_frailty, dims=\"frailty_id\")\n\n        beta = pm.Normal(\"beta\", 0, sigma=1, dims=\"preds\")\n\n        ## Stratified baseline hazards\n        lambda_m = pm.Deterministic(\n            \"lambda_m\",\n            pt.outer(pt.exp(pm.math.dot(beta, X_data_m.T)), lambda0[:, 0]),\n            dims=(\"men\", \"intervals\"),\n        )\n        lambda_f = pm.Deterministic(\n            \"lambda_f\",\n            pt.outer(pt.exp(pm.math.dot(beta, X_data_f.T)), lambda0[:, 1]),\n            dims=(\"women\", \"intervals\"),\n        )\n        lambda_ = pm.Deterministic(\n            \"lambda_\",\n            frailty[frailty_idx, None] * pt.concatenate([lambda_f, lambda_m], axis=0),\n            dims=(\"obs\", \"intervals\"),\n        )\n\n        mu = pm.Deterministic(\"mu\", exposure * lambda_, dims=(\"obs\", \"intervals\"))\n\n        obs = pm.Poisson(\"outcome\", mu, observed=quit, dims=(\"obs\", \"intervals\"))\n        frailty_idata = pm.sample_prior_predictive()\n        frailty_idata.extend(pm.sample(random_seed=101))"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#frailty-model-structure",
    "href": "talks/survival_regression/time_to_attrition.html#frailty-model-structure",
    "title": "Survival Regression Models in PyMC",
    "section": "Frailty Model Structure",
    "text": "Frailty Model Structure\nIndividual Frailties"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#frailty-model-and-stratified-baseline-risks",
    "href": "talks/survival_regression/time_to_attrition.html#frailty-model-and-stratified-baseline-risks",
    "title": "Survival Regression Models in PyMC",
    "section": "Frailty Model and Stratified Baseline Risks",
    "text": "Frailty Model and Stratified Baseline Risks\n\n\n\n\nSurvival Regression in PyMC"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#frailty-model-structure-1",
    "href": "talks/survival_regression/time_to_attrition.html#frailty-model-structure-1",
    "title": "Survival Regression Models in PyMC",
    "section": "Frailty Model Structure",
    "text": "Frailty Model Structure\nShared Frailties\n\n\n\n\n\n\n\nShared Frailties by Field of Occupation"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#frailty-models-and-stratified-baseline-risks",
    "href": "talks/survival_regression/time_to_attrition.html#frailty-models-and-stratified-baseline-risks",
    "title": "Survival Regression Models in PyMC",
    "section": "Frailty Models and Stratified Baseline Risks",
    "text": "Frailty Models and Stratified Baseline Risks\n\nWe see differences in the risks stratified by gender and additionally how the magnitude of the baseline hazard shrinks with more or less well-specified covariate in the model."
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#individual-frailties-and-marginal-statistics",
    "href": "talks/survival_regression/time_to_attrition.html#individual-frailties-and-marginal-statistics",
    "title": "Survival Regression Models in PyMC",
    "section": "Individual Frailties and Marginal Statistics",
    "text": "Individual Frailties and Marginal Statistics"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#conclusion",
    "href": "talks/survival_regression/time_to_attrition.html#conclusion",
    "title": "Survival Regression Models in PyMC",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\n\nSurvival Analysis is a tool for the expression of probabilities governing state-transitions\n\n\n\n\nImportant everywhere process efficiency and transformative outcomes matter. Corrects for censorship bias of naive summaries.\n\n\n\n\nAllows for sophisticated expression of risk over time and along many dimensions. Variety of hierarchical modelling options\n\n\n\n\nBayesian estimation of these complex model structures is natural and informative. Meaningful across a range of disciplines and domains.\n\n\n\n\nProvides an actionable lens on “actuarial” risk and “diagnostic” causal analysis in time.\n\n\n\n\n\n\nDicing with Death\n\n\n\n\n\nWhen sand becomes a heap? When a heap returns to sand?\n\n\n\n\n\n\n\nSurvival Regression in PyMC"
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html#actuarial-tables-and-survival-curves-1",
    "href": "talks/survival_regression/time_to_attrition.html#actuarial-tables-and-survival-curves-1",
    "title": "Survival Regression Models in PyMC",
    "section": "Actuarial Tables and Survival Curves",
    "text": "Actuarial Tables and Survival Curves\nBottom-Up and Concrete\ndef make_actuarial_table(actuarial_table):\n    ### Actuarial lifetables are used to describe the nature \n    ### of the risk over time derived from instantaneous hazard\n    actuarial_table[\"p_hat\"] = (actuarial_table[\"failed\"] / \n                                actuarial_table[\"risk_set\"])\n    actuarial_table[\"1-p_hat\"] = 1 - actuarial_table[\"p_hat\"]\n    ### Estimate of Survival function\n    actuarial_table[\"S_hat\"] = actuarial_table[\"1-p_hat\"].cumprod()\n    actuarial_table[\"CH_hat\"] = -np.log(actuarial_table[\"S_hat\"])\n    ### The Estimate of the CDF function\n    actuarial_table[\"F_hat\"] = 1 - actuarial_table[\"S_hat\"]\n    actuarial_table[\"V_hat\"] = greenwood_variance(actuarial_table)\n    return actuarial_table"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html",
    "href": "talks/missing_data/missing_data_causal.html",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "",
    "text": "Background\n\n\n\n\nI’m a data scientist at Personio\nContributor at PyMC and PyMC labs\n\nBayesian statistician,\nReformed philosopher and logician.\n\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\nDisclaimer\n\n\n\nNone of Personio’s data was used in this presentation\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\n\nThe worked examples used here can be found here"
  },
  {
    "objectID": "notes/certain_things/Statistics/PyMC Labs.html",
    "href": "notes/certain_things/Statistics/PyMC Labs.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "Work on discrete choice.\nI’m Nathaniel Forde a Senior data scientist from Dublin. Previously worked in e-commerce for car rentals on pricing where i first started thinking about discrete choice. I’ve been working with Bayesian models for about 5 years. I’ve been a regular contributer to PyMC docs, especially for this project working on discrete choice models which is why Thomas asked me to consult on this project."
  },
  {
    "objectID": "notes/certain_things/Statistics/Introduction - Statistics Topics.html",
    "href": "notes/certain_things/Statistics/Introduction - Statistics Topics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "This section will serve to capture my Zettelkasten style notes on topics in Statistics."
  },
  {
    "objectID": "notes/certain_things/Logic/Introduction - Logic Topics.html",
    "href": "notes/certain_things/Logic/Introduction - Logic Topics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "This section will serve to capture any and all Zettelkasten notes on topics in logic -mathematical, philosophical or any style in between."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Introduction - Philosophy Topics.html",
    "href": "notes/certain_things/Philosophy/Introduction - Philosophy Topics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "This section will serve to capture the philosophical topics in my Zettelkasten notes."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Sorites Paradox and Survival Analysis.html",
    "href": "notes/certain_things/Philosophy/Sorites Paradox and Survival Analysis.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "There seems to be a plausible relationship between (a) the idea of a latent evolving hazard in #survival-analysis and (b) the accumulation effect that drives our intuitions from observations of distinct sand grains to observations of a heap. The classic philosophical puzzle put forward by Sorites.\nI’ve worked on survival analysis in the context of statistics and data science , but I’m putting together this note to arrange my thoughts on the value the perspective has on the classic philosophical puzzle.\nThe first thing to observe is how survival analysis is in general a model of the probabilities of state-transition. Moving between alive-dead, sick-well, subscribed-churned, hired-fired. The Framework is quite abstract and therefore widely applicable to the analysis of all state transitions with both clear, distinct and permeable borders between states.\nTraditionally you might see frequentist elaborations of the mechanics of #survival-analysis , but it becomes more interesting from a philosophical stand point when you phrase the model in a #Bayesian fashion.\nIn the Bayesian setting the uncertainty expressed in the model regarding measures of risk of state-change can be seen to contribute to the semantic ambiguity relevant in the Sorites setting. I will try to bring out this connection in the following."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Sorites Paradox and Survival Analysis.html#the-sorites-paradox",
    "href": "notes/certain_things/Philosophy/Sorites Paradox and Survival Analysis.html#the-sorites-paradox",
    "title": "Examined Algorithms",
    "section": "The Sorites Paradox",
    "text": "The Sorites Paradox\nThe typical presentation of the sorites issue in philosophy has been as a series of conditional predications as follows:\n\nFa_0\nFa_0 -> Fa_1\n.\n.\n.\nFa_{i-1} -> Fa_i\nTherefore: Fa_i\n\nWhere we allow that there is an indifference relation for the predication of F over all elements of the sequence preceding the last entry. Then, it is argued, the last predication fails for some entry (i) in the sequence. The predicate F is said to be suffer from #vagueness .This is purportedly a paradox due to the requirement of logical validity over conditional reasoning caused by the vagueness of F over the course of the sequence.\nOther rephrases of the logical steps truncate the sequence of conditionals by appealing to a Principle of mathematical Induction to arrive at the same conclusion. For out purposes it’s not crucial which phrasing is applied. The point is just that the the vagueness of the predicate F is said to threaten some central tenet of logical validity.\n\nApproaches to the Paradox\nThere have been advocates for acceptance of the Paradox - allowing that there is just a breakdown of logic in the case of these vague predicates. So much the worse for logic. This is quite radical as the prevalence of vague predicates in natural language commits us implicitly to the view that we cannot make distinctions.\nThe more plausible approaches to the Paradox seek to establish a reason for rejecting the validity of the conclusion by denying the premises in some way e.g. claiming that the indifference over the nth predication of F and the n-1 case fails. There is a precise point which is a genuine cut-off between F and not F. This position is called Epistemicism - which locates the causes of vagueness in predication in our degrees of ignorance.\nThe suggestion above is supported somewhat empirically by the reality of borderline cases of predication among reasonable speakers of the same language. People evince hedging behaviour and deliberate vagueness in cases where they avoid commitment to a sharp distinction. “She is sorta cool, nice-ish!”. This is the data the philosophical theory needs to explain. Paradigmatic cases of attributed state-change coupled with paradigmatic cases of hedging.\nThe theoretical question then becomes - what constitutes borderline vagueness? I think this is where we can use survival analysis to elaborate and explain cases of borderline vagueness and empirical cases of disagreement in predication. In particular Bayesian approaches to survival analysis which allow that there is a cut-off point in the sequence, but there is genuine uncertainty where we locate the cut-off point. Seeing this as a problem of Bayesian modelling allows us to locate the sources of hedging in the components and precision of our model terms through which the propagates the uncertainty in our attribution patterns.\n\n\nPerspectives on Probability\nSurvival analysis can seem intimidating because it asks us to understand time-to-event distributions from four distinct perspectives. The first familiar density function, the next cumulative density function and its inverse survival function. Additionally we can view the the cumulative hazard function as a transformation of the survival function, and the instantaneous hazard as a discretisation of over intervals of the temporal sequence ranged over by the cumulative hazard.\nIt’s important to see and understand that these quantities, while appearing to be abstract mathematical objects, can be derived from simple tables which record the subjects in the risk set which experience the event of interest at each interval of time. In other words the set of conditional probabilities instance-by-instance over the range of the temporal sequence. This is how we derive the instantaneous hazard quantity.\nDifferent families of probability distribution allow us to encode different structures in the hazards. For instance if we want hazards to peak early in the sequence and decline later in the sequence non-monotonically we can use the loglogistic distribution. If we want to ensure monotonic hazard sequences we can use Weibull distributions.\n\n\nDistinguishing Risk and Uncertainty\nWe want to explain the semantic ambiguity of Sorites phenomena by the probabilistic nature of state transitions over additive sequences. However, we won’t trade on the uncertainty between distinct models i.e. it’s not merely that your model of the sand-to-heap transition is characterised by one probability distributions and mine by another (although it could be). We are interested in divergences of opinion and semantic uncertainty that arises due to the stochastic nature of the phenomena where we share the same model of the phenomena. This reflects a difference in the view of the risk not the model uncertainty.\n\n\nCox Proportional Hazards Model\nTo make this a bit more concrete consider the cox proportional hazard model. This is a #regression model which aims to characterise the probability of state transition using a statistical model with two components. The baseline hazard:\n\\[ \\lambda_0 (t) \\]\nwhich is combined multiplicatively with an exponentiated weighted linear sum as follows \\[\\lambda_0 (t) \\cdot e^{\\beta_0 X_0 + \\beta_1 X_1 ... \\beta_n X_n}\\] In this model the baseline hazard is a function of the time intervals and we estimate a hazard term for each interval when we fit the model. There is a “free” parameter for the instantaneous hazard at each timepoint. This sequence is the baseline hazard. This latent baseline hazard is akin to an intercept term(s) in more traditional regression models. Individual predictions of the evolving hazard are then determined by how the individuen’s covariate profile modifies the baseline hazard. Estimation procedures for this model find values for the baseline hazard and for the coefficient weights \\(\\beta_i\\) in our equation.\nWith these structures in mind you might be tempted to locate the source of disagreement between people’s judgments as stemming from differences in their covariates \\(X_i\\) , or put another way… we see the probability of transition as a function of the same variables, but disagree on the values of those inputs to the function. The benefit of this perspective is that instead of seeing the Sorites Paradox as an error of logical reasoning that needs to be fixed by one or more adjustments to classical logic, we can instead view the phenomena as reflecting disagreement among latent probabilistic models.\n\n\nComplexity of the Heap\nOne additional perspective on the problem is gained by noting how the Cox proportional model is a prediction model and comes with criteria of model adequacy. How many predictor variables are required to anticipate state transition? How much variance is explained? If we can gauge the complexity of the prediction task, can the complexity itself explain disagreement?\n\n\nHierarchical or Meta Vagueness\nWe’ve seen now a few different sources of divergences. At the highest level we can appeal to the Knightian distinction between risk and uncertainty, then secondarily to differences in the data used to calibrate risk or thirdly in differences due to estimation strategies and finally in pure prediction complexity.\nIf divergences are due to complete uncertainty of the appropriate model, then we concede allot to the sceptic and the quantification of any plausible cut point is hopeless. If differences result from the other candidate sources there remains hope for arriving at intersubjective consensus.\nThis can be seen in some sense in Bayesian model development workflow with hierarchical survival models. Instead of imagining agents reasoning if-then style through a sequence of additional sand grains. Let’s picture the reasoner working with a latent survival model, negotiating a contract between reality and their linguistic usage.\nHierarchical models in the Bayesian setting are typical and interesting in their own right as they allow for the expression of heterogeneity across individuals. Broadly they involve adding one or more parameters that modify the baseline model equation. We saw earlier that the Cox Proportional hazard model is expressed as \\[\\lambda_0 (t) \\cdot e^{\\beta_0 X_0 + \\beta_1 X_1 ... \\beta_n X_n}\\]\nThis can be modified as follows:\n\\[z_{i} \\cdot \\lambda_0 (t) \\cdot e^{\\beta_0 X_0 + \\beta_1 X_1 ... \\beta_n X_n}\\] Where we allow an individual “frailty” term \\(z_{i}\\) is added to the model as a multiplicative factor for each individual in the data set. The terms are drawn from a distribution, often centred on 1, so that the average individual modifies the baseline model not at all… but modifications are expressed as a reduction or increase to the multiplicative speed of state transition. The baseline model can therefore be considered\nRecall that the Bayesian modelling exercise quantifies the probability distribution of all the parameters in the model. A well specified baseline model will mean that less explanatory work needs to be done by the individual frailty terms. A poorly specified model will locate allot of weight in these terms. This is a mechanism which helps quantify the degree of irreducible uncertainty in our attribution patterns derived from our understanding of the paradigmatic cases (our sample).\n\n\nThe Bayesian Reasoner\nAn individual reasoner working with their set of paradigmatic data points y ~ f(X | \\(\\theta\\)) may fit their model to this data. The variance in the distribution of frailty terms \\(z_{i} \\in \\theta\\) estimated represents their view of the remaining uncertainty in cases after controlling for the impact of the covariate profiles across the cases.\nThese quantities represents disagreements regarding the speed up or slow down in the survival curves…but the survival curves quantifies the probability of transition at each point of accumulation. So a survival model allows us to say precisely at each point of accumulation what is the probability of transition. For any given point in series of accumulating instances, the diversity of individual frailty terms needed to account for the predications determine the quantifiable range of uncertainty in the survival probabilities we derive from the paradigmatic cases.\nEpistemicism about existence of a cut point for vague predicates will always assume the existence of threshold. The picture of Sorites Paradox for the Bayesian reasoner sees them go from uncertainty to uncertainty updating the latent model as they go. Maybe the model converges tightly in some cases, maybe not. Incorporating more paradigmatic instances, more covariate indicators as they develop conceptual clarity on what drives the attribution of state-hood under the evolving or growing pressure to change. Finding the threshold would not and could not be a solution to the paradox. Any threshold will be context specific and also learned (with uncertainty) relative to the tolerances of the domain. Understanding that paradox as yet another instance of learning in a multifaceted world at least lets us see the problem without requiring torturous modifications to classical logic."
  },
  {
    "objectID": "oss/pymc/frailty_survival.html",
    "href": "oss/pymc/frailty_survival.html",
    "title": "Frailty Survival Models in PyMC",
    "section": "",
    "text": "In this project I demonstrated how to fit a variety of hierarchical survival models in PyMC. We applied these techniques to a human resources data set attempting to measure time-to-attrition across individuals surveyed. The project is interesting within my work in Personio as example of modelling for process improvements and the measurement of drivers for efficiencies. Additionally, I think there is an interesting connection with the philosophical puzzle of the Sorites Paradox.\nThe project culminated in a publication to the official PyMC documentation that can be found online here or downloaded as notebook here"
  },
  {
    "objectID": "notes/certain_things/Statistics/Non-Parametric Bayesian Causal Inference.html",
    "href": "notes/certain_things/Statistics/Non-Parametric Bayesian Causal Inference.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "The clearest framework for causal inference has a tight relationship with missing data imputation. However, the range of problems addressed seem to require a slew of distinct estimators. It is not always clear in which circumstances we should apply each estimation procedure.\nLike most statistical work the practical details should be worked out in code for the clearest demonstration. However, here we’ll describe (as best we can) the conceptual underpinning of important estimators. Their usage and motivation.\n\nTaxonomies of Missing-ness\nCausal inference can be seen as a species of missing data problem where the missing data is the counterfactual situation(s) of how the world would have been were the course of the world different from the one we know. What if we used this treatment plan rather than another? What it the actors’ behaviour differed from the actions they in fact pursued?\nFrom an estimation perspective there are different species of missing-ness that matter. We won’t here go deep into the distinctions. It is enough to note that they vary in the operative source of the missing-ness: Missing-completely-at-random (MCAR), missing-at-random (MAR) and missing-not-at-random (MNAR).\nVery crudely estimation procedures work reasonably well under (MAR) and (MCAR) but require extra effort when there is assumptions if we hope to account for the (MNAR) cases. This stems largely from successful applications of the law of iterated expectations.\n\nThe various estimation procedures for counterfactual results trade on this property of expectation.\nMore convenient again, when we consider cases of missing data conditional the observed covariate profile, we can derive a propensity score as a one number summary for the conditional probability of missingness. This can be used in imputation techniques where we want to carefully attribute values to the missing data that respect the other observed properties of the individual.\nThe propensity score has a role in a number of reweighting schemes for the estimation of missing data. These rely on the property of expectation under (MAR). So accuracy of the propensity score is itself an important question. The missing-ness variable \\(R\\) is a binary random variable in \\(\\{ 0, 1 \\}\\). Maximum likelihood methods for logistic regression are often used to estimate these terms.\n\\[p_{R}(\\mathbf{x}) \\sim logit(X_{i} \\beta )\\]\nThis score is a summary in some sense of the factors driving missing-ness. In the context of causal inference it is often stated “in reverse” as a probability of being treated. We will keep focus here on the case of missing data, but the generality of the notion shouldn’t be lost. The propensity score is a one number summary of the covariate profile for each individual in the data and under the (MAR) assumption - it alone is sufficient to conditionalise"
  },
  {
    "objectID": "notes/certain_things/Statistics/Analogies between Missing Data and Causal Inference.html",
    "href": "notes/certain_things/Statistics/Analogies between Missing Data and Causal Inference.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "The clearest framework for causal inference has a tight relationship with #missing-data imputation. However, the range of problems addressed seem to require a slew of distinct estimators. It is not always clear in which circumstances we should apply each estimation procedure.\nLike most statistical work the practical details should be worked out in code for the clearest demonstration. However, here we’ll describe (as best we can) the conceptual underpinning of important estimators. Their usage and motivation."
  },
  {
    "objectID": "notes/certain_things/Statistics/Analogies between Missing Data and Causal Inference.html#causal-inference",
    "href": "notes/certain_things/Statistics/Analogies between Missing Data and Causal Inference.html#causal-inference",
    "title": "Examined Algorithms",
    "section": "Causal Inference",
    "text": "Causal Inference\nThe above perspective on missing data has deep analogies with the potential outcomes framework of causal inference. This is well brought out in the beautiful book Foundations of Agnostic Statistics by Aronow and Miller.\n\nPotential Outcomes and SUTVA\nIn the causal context we assume the potential outcomes framework and the notation of \\(Y(1), Y(0)\\) to denote the value of the outcome under the treatment regime \\(D\\).\n\\[ Y_{i} =  \n\\left\\{\\begin{array}{lr}\n        Y_{i}(0), & \\text{for } D = 0\\\\\n        Y_{i}(1), & \\text{for } D = 1\\\\\n        \\end{array}\\right\\}\n\\] where the (S)table (U)nit (T)reatment (V)alue (A)ssumption holds i.e. the observed data under treatment or non-treatment regimes is the potential outcome for that individual. Additionally the counterfactual outcome is assumed to be stable for each individual. It is crucially this assumption that allows for statistical identification of key metrics in causal inference under randomisation.\n\n\nAverage Treatment Effects\nSimilarly, here we rely on the properties of expectation over the observed data to isolate quantities of causal effect. In particular we tend to be interested in the average treatment effects, which we can get by using the following decomposition under random assignment.\n\\[ E[\\tau] = E[Y_{i}(1) - Y_{i}(0)] = E[Y_{i}(1)] - E[Y_{i}(0)] \\] This decomposition is crucial since it allows us to move between the expectations derived from the observed data under each regime towards an estimate of the population treatment effects.\n\n\n\nsubject\n\\(Y_{i}(1)\\)\n\\(Y_{i}(0)\\)\n\\(\\tau\\)\n\n\n\n\nJoe\n?\n115\n?\n\n\nBob\n120\n?\n?\n\n\nJames\n100\n?\n?\n\n\nMary\n115\n?\n?\n\n\nSally\n120\n?\n?\n\n\nLaila\n?\n105\n?\n\n\n\\(E[Y_{i}(D)]\\)\n113.75\n110\n3.75\n\n\n\nThe missing values in this table depict the fundamental problem of causal inference as a missing data issue. So #causal-inference as a strategy is broadly related to finding ways to solve this missing data problem under different regimes of missing-ness. For instance, the reason A/B testing works to isolate the treatment effects is that under randomisation of treatment regime we are implicitly assuming that the reason missing-data is effectively a case of MCAR missing-ness. As such the expectations of the individual columns in the above table are valid estimates which can then be combined using the above decomposition to derived the treatment effect \\(\\tau\\).\nIn this case the pattern of reasoning is akin to performing mean-imputation and then taking the difference of the averages. The imputation step is redundant in A/B testing, but it is highlighted by Aronow and Miller as a useful lens on more complex causal inference tasks on observed data. We are always (under the hood) trying to impute the missing values to gain a better view of the treatment effect distribution. ### Regression Estimators\nAgain we rely on the idea of regression as an approximation to the CEF of the data generating process. The flexibility of regression modelling for automating a host of statistical test should be reasonably familiar. The point here is not to rehash the theory but just to note the similarity with the procedures used above for regression-based imputation. Regression modelling of the treatment effect proceeds on the strong ignorability assumption that - conditional on the observed covariates knowing whether or not an individual received the treatment adds no new information i.e. it is the insistence that assignment might as well be random after accounting for the background characteristics. These assumptions mirror the conditions required for imputation under the MAR regime.\nSo we can derive estimates for the ATE from the data generating model\n\\[ Y \\sim \\beta_0 + \\beta_1 D + ... \\beta_{n} \\cdot X_{n} \\] such that out quantity of interest \\(\\tau\\) is cleanly identified in expectation by the quantity: \\[ E[\\tau] = \\beta_{1}\\] But this result can also be derived by predicting the outcomes under the different treatment regimes, using a fitted regression model, and taking the differences of the averaged predictions over the cases. The equivalence between these perspectives is the insight we want to record here. We drew out this connection in the discussion of poststratification estimators\n\nThis is a neat and beautiful connection between causal-inference and missing data analysis. Simultaneously a reminder of the versatility of regression analysis.\n\n\nPropensity Functions and Reweighting Estimators.\nWe will skip the detailed elaboration of propensity score matching, a technique for creating pseudo treatment and control groups, only noting that there is a rich and detailed literature on the topic for causal inference.\nWe do want to draw out how propensity-scores can be used in the class of reweighting estimators. Where under the strong ignorability assumption we can estimate the treatment effect as a simple expectation:\n\\[E[\\tau] = E [\\dfrac{YD}{p_D(X)} - \\dfrac{Y(1-D)}{(1 - p_D (X))}] \\]\nUsing this formula we can scale each observation by the relative probabilities for the individual falling into each treatment regime. Then the expectation of the scaled differences is an estimate of our ATE. The logic of this inverse probability weighting (IPW) estimator stems from the idea that low propensity individuals are likely underrepresented in the treatment group and over represented in the control. So this estimator down weights and unweights each option accordingly to “balance” the groups before comparison.\nThis balancing operation can work but is dependent on empirical properties of the sample data. Even if the data generating process ensures that strong ignorability holds, if our sample under represents the variety of possible individual in each group then reweighting the remaining individuals is no guarantee for sound inference. This is a small sample problem recurring."
  },
  {
    "objectID": "notes/certain_things/Statistics/Analogies between Missing Data and Causal Inference.html#taxonomies-of-missing-ness",
    "href": "notes/certain_things/Statistics/Analogies between Missing Data and Causal Inference.html#taxonomies-of-missing-ness",
    "title": "Examined Algorithms",
    "section": "Taxonomies of Missing-ness",
    "text": "Taxonomies of Missing-ness\nCausal inference can be seen as a species of missing data problem where the missing data is the counterfactual situation(s) of how the world would have been were the course of the world different from the one we know. What if we used this treatment plan rather than another? What it the actors’ behaviour differed from the actions they in fact pursued?\nFrom an estimation perspective there are different species of missing-ness that matter. We won’t here go deep into the distinctions. It is enough to note that they vary in the operative source of the missing-ness: Missing-completely-at-random (MCAR), missing-at-random (MAR) and missing-not-at-random (MNAR).\n\nThe Stable Outcomes Assumption\nMechanically we can’t work with null values under any of these assumptions. The stable outcomes model is a first step procedure for imputing the missing values. Whether we choose mean imputation or an arbitrary figure - we initially assume missing values at the individual level in a stable fashion by specifying a constant value for the missing cases.\nVery crudely, estimation procedures work reasonably well under (MCAR) but require extra effort when there is assumptions if we hope to account for the (MAR) and (MNAR) cases. Under MAR we are assuming that the values are missing as a function of the observable covariates and can be imputed under proper conditionalisation.\nImputation under MAR and MCAR succeeds largely from successive applications of the law of iterated expectations. In this case the stable outcome assumption encodes the missing data as \\(-99\\) and we then average over the joint distribution of the stable outcomes model and the missingness data.\n\nThe various estimation procedures for counterfactual results trade on this property of expectation that allow for point identification of the expected value for the outcome variable.\nBut we also want to consider individual variation due to the observed covariate profiles. When we consider cases of missing data conditional the observed covariate profile, we can derive a propensity score as a one number summary for the conditional probability of missing-ness. This can be used in imputation techniques where we want to carefully attribute values to the missing data that respect the other observed properties of the individual.\n\n\nPropensity Scores\nThe propensity score has a role in a number of re-weighting schemes for the estimation of missing data. These rely on the property of expectation under (MAR). So accuracy of the propensity score is itself an important question. Because the missing-ness variable \\(R\\) is a binary random variable in \\(\\{ 0, 1 \\}\\) maximum likelihood methods for logistic regression are often used to estimate these terms.\n\\[p_{R}(\\mathbf{x}) \\sim logit(X_{i} \\beta )\\]\nThis score is a summary in some sense of the factors driving missing-ness. In the context of causal inference it is often stated “in reverse” as a probability of being treated. We will keep focus here on the case of missing data, but the generality of the notion shouldn’t be lost. The propensity score is a one number summary of the covariate profile for each individual in the data. Under the (MAR) assumption it is often sufficient to conditionalise on the propensity score for each individual for imputation of their missing data values.\n\n\nRegression Estimators\nWe might want to simply estimate the missing values of our outcome using the conditional expectation function (CEF) property of simple regression. The imputation pattern will work well when the linear properties of the regression model are a good fit for the relationship between the outcome variables and the observed covariates. Hence the estimate for:\n\\[E[Y_{i}] = \\beta_{0} + \\beta_{1}\\cdot X_{1i} ... \\beta_{n} \\cdot X_{ni} \\] where we replace all values to be prediction of our regression model for each individual and then average the predictions.\n\n\nWeighting Estimators\nAnother approach to missing data imputation, which relies on the expectation properties of our outcome variable of interest under (MAR) and the stable outcome model, is the inverse probability weighting approach to imputation.\n\\[ E[Y_{i}] = E[\\dfrac{(YR + (-99) (1-R ))R }{p_{R}(\\mathbf{x})}]\\] With this property we can express estimates of missing values as a function of the individuals’ observed data. There are variations on this theme but sophisticated imputation schemes all rely on functions of the individual’s observed covariate profile. This specificity is important too in the context heterogenous treatment effects in causal inference."
  },
  {
    "objectID": "notes/certain_things/Intro - Zettelkasten.html",
    "href": "notes/certain_things/Intro - Zettelkasten.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "Zettelkasten is a note-taking method that originated from the work of sociologist Niklas Luhmann. The term “Zettelkasten” translates to “slip box” in English. The key idea behind the Zettelkasten method is to create and organise a collection of interconnected notes or “slips” that capture individual ideas, concepts, or pieces of information. Each note is meant to be concise and focused on a single idea. The notes are then linked together through a system of numbered or categorised references, allowing for easy navigation and discovery of connections between different concepts.\nIn this section i will capture my notes using Obsidian and publish them to my website where relevant."
  },
  {
    "objectID": "notes/certain_things/Statistics/Analogies between Missing Data and Causal Inference.html#conclusion",
    "href": "notes/certain_things/Statistics/Analogies between Missing Data and Causal Inference.html#conclusion",
    "title": "Examined Algorithms",
    "section": "Conclusion",
    "text": "Conclusion\nThe sequence of complexity in missing data imputation is as follows:\n\\[ MCAR \\Rightarrow MAR \\Rightarrow MNAR \\]\nwhich mirrors the complexity of cases in causal inference. Here we have:\n\\[ Ignorability \\Rightarrow \\text{Strong Ignorability} \\Rightarrow \\text{Non Ignorability} \\] As we consider circumstances moving up the hierarchy, we require an increase in assumptions or structural commitments to offset the risk of non-identifiability bringing us back down the hierarchy. The emphasis in the book stresses how properties of good experimental design can help recover sound inference by enforcing MAR conditions in MNAR circumstances. But the crucial role of modelling in defending the strong ignorability condition is underplayed.\nYes, we need to justify our estimator but also our model! Are we including the right covariates? Have we an appropriate covariance structure? What is the functional form and why is it reasonable? Are we accounting for heterogeneity of outcome? All such questions centre the importance of domain knowledge for causal inference. This is not a criticism of boon focused on Agnostic statistics. Their focus is appropriately on the design aspects that enable inference. However it should be abundantly clear that you cannot get away with agnostic approaches in the real world. There is no way to justify stepping back down the hierarchy without substantial commitments about the world-model fit. Even if your aesthetic preferences drive you toward design based methods, this only serves to obscure the commitments. Statistics in the real world require real world commitments."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Book - Thinking about Statistics.html",
    "href": "notes/certain_things/Philosophy/Book - Thinking about Statistics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "In this note we’ll capture reflections about Jun Otsuka’s Thinking about statistics."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Book - Thinking about Statistics.html#statistical-models-and-the-problem-of-induction",
    "href": "notes/certain_things/Philosophy/Book - Thinking about Statistics.html#statistical-models-and-the-problem-of-induction",
    "title": "Examined Algorithms",
    "section": "Statistical Models and the Problem of Induction",
    "text": "Statistical Models and the Problem of Induction\nThe book begins by framing the different epistemological projects of both #Bayesian and #Frequentist patterns of inference as approaches to solving the problem of induction expressed by David Hume.\nThis is a nice lens on the development of statistics and the applied work of statistical modelling. The two probabilistic frameworks are contrasted or compared to the more pragmatist position of model selection based on predictive power. We may prefer a model which does not capture the true data generating process just so long as it performs better in prediction tasks."
  },
  {
    "objectID": "talks/survival_regression/time_to_attrition.html",
    "href": "talks/survival_regression/time_to_attrition.html",
    "title": "Survival Regression Models in PyMC",
    "section": "",
    "text": "Time to Event Distributions\n\nHyperObjects and Perspectives on Probability\n\n\n\n\n\nPeople Analytics and Survival Regression\n\nCox Proportional Hazard\nAccelerated Failure Time\n\n\n\n\n\nComparing Model Implications\n\nMarginal Predictions\nAcceleration Factors\n\n\n\n\n\nFrailty Models and Stratified Risk\n\n\n\n\nConclusion"
  },
  {
    "objectID": "notes/certain_things/Philosophy/Thinking about Statistics.html",
    "href": "notes/certain_things/Philosophy/Thinking about Statistics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "In this note we’ll capture reflections about Jun Otsuka’s Thinking about statistics"
  },
  {
    "objectID": "notes/certain_things/Philosophy/Thinking about Statistics.html#statistical-models-and-the-problem-of-induction",
    "href": "notes/certain_things/Philosophy/Thinking about Statistics.html#statistical-models-and-the-problem-of-induction",
    "title": "Examined Algorithms",
    "section": "Statistical Models and the Problem of Induction",
    "text": "Statistical Models and the Problem of Induction\nThe book begins by framing the different epistemological projects of both #Bayesian and #Frequentist patterns of inference as approaches to solving the problem of induction expressed by David Hume. #book #philosophy #statistics\nThis is a nice lens on the development of statistics and the applied work of statistical modelling. The two probabilistic frameworks are initially contrasted or compared to each other. The distinction is drawn between the epistemological process involved in both approaches. Firstly the notion of Bayesian conditionalisation which incorporates new data to derive new beliefs coherent with the observed facts is spelled out. Then we see how the frequentist approach can be considered as a species of reliablist epistemology, where the focus is on the error control of well defined processes. In both cases the problem of induction is located as one of inference i.e. if we have an appropriate set of i.i.d sample data we can warrant the inference that the future will look like the past\nHe draws out the ontological commitments to probabilistic kinds that appear required to underwrite statistical inference in both paradigms. He supplies a justification for these commitments as being instances of real patterns in the sense of Daniel Dennett’s phrasing. This is a kind of indispensability argument for the deployment of probabilistic kinds in our best science."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Thinking about Statistics.html#the-uniformity-of-nature",
    "href": "notes/certain_things/Philosophy/Thinking about Statistics.html#the-uniformity-of-nature",
    "title": "Examined Algorithms",
    "section": "The Uniformity of Nature",
    "text": "The Uniformity of Nature\nIf probabilistic kinds inhabit the world they can exhibit different characteristics - varying fauna and flora of the natural world. One process might be well described by a Gaussian distribution, another by a Laplace distribution… this diversity is all well and good, but to go beyond descriptive statistics we need to posit more. We need the assumption that there is some stability to the processes we seek to characterise. A uniformity of nature that underwrites statistical inference and probabilistic prediction models.\nOtsuka suggests that this commitment is cashed out in contemporary statistics with the famous i.i.d assumption. This posit argues that for sound inference, we must assume that any sample data is drawn from a probability distribution that each draw is independent and identically distributed.\n\n“The IID condition is a mathematical specification of what Hume called the uniformity of nature. To say that nature is uniform means that whatever circumstances holds for the observed, the same circumstances will continue to hold for the unobserved. This is what Hume required for the possibility of inductive reasoning …” pg 25/26\n\nThis obviously is constraint on sound inference, but also implicitly an ontological assertion regarding distributional drift. This commitment is deeper than when we argue for a particular distributional characterisation of our process of interest. Our target process could be articulated as a mixture distribution or some more complicated beast, but in each case we require that (a) it is well described by some probability model and (b) the world is set up in such a way that more observations enable us to learn which particular statistical model (parametric or non-parametric) fits the data."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Thinking about Statistics.html#approaches-to-learning",
    "href": "notes/certain_things/Philosophy/Thinking about Statistics.html#approaches-to-learning",
    "title": "Examined Algorithms",
    "section": "Approaches to Learning",
    "text": "Approaches to Learning\nWith this background Otsuka goes on to describe the manner in which the Bayesian and frequentist schools approach the task of learning from data.\n\nBayesian Machinery\nThe focus in the Bayesian setting presents conditionalisation as a logic of inductive reasoning, which expands on logical inference. These are frameworks for organising and interrogating our system of beliefs and their relationship to the achievement of knowledge. Otsuka argues that Bayesian inference plays a crucial role in justification. Moving from prior to posterior distribution is seen as a change in the weighting of our beliefs. An internalist species of the justification-relation between beliefs in light of data.\nBut founding a story about epistemological justification on bayesian inference involves a defense of priors and likelihood specifications. Priors are defended using the usual moves: (1) appeal to wash-out theorems of iteratively updating on sufficiently large data. Convergence theorems assuring Bayesian updated belief is truth conducive in the limit regardless of apparently subjective prior specification. (2) Non-informative priors and (3) Objective priors or empirical Bayes.\nWe won’t spend much time on (2) because it’s just kind of silly to justify beliefs and their manipulation by constant appeals to ignorance. On (3) there is a more interesting discussion regarding the relationship between degrees of belief and chance. We want our priors to reflect our background knowledge and update our beliefs in a way that it tracks the actual occurence of the events in question. David Lewis enshrined this requirement as the Principal Principle. But while this is an agreeable sounding tenet it cannot serve as a foundational justification for our priors within an internalist picture of justification without risk of infinite regress. This is problematic for the philosopher that seeks to establish bayesian inference as the sole source of belief generation, but seems less serious if you can tolerate primitive or foundational epistemological commitments outside those justified with inductive inference in the Bayesian loop. Justification must end somewhere (the spade eventually turns), and in-practice arguments and evidential exchanges rarely get anywhere close to an infinite series.\nAdditionally the Bayesian needs to defend the incorporation of different likelihood choices. Their shape and implications. Fortunately this can be more pragmatic in so far likelihood specifications are in effect testable hypotheses about the data generating process. They can be justified by the success of the modelling endeavour and our ability to recover data akin to our observations. The data is a fixed quantity, we use it to update our probabilstic beliefs and commitments. This is to the good because it can be shown (via Dutch book style theorems) that in strategic decision making where your beliefs adhere to the probability calculus they will strictly dominate other strategies.\nThe Bayesian machinery is a set of tools for arranging coherence amongst our beliefs, commitments - tracing out the implications. We move dynamically between prior and posterior by means of the likelihood term. This process cannot serve as as an ultimate court of appeal for basic beliefs that kick off the learning process itself. It is an abstract, highly flexible set of tools applicable to a wide range of questions. It provides a very general model of learning where the concern is justification of our beliefs in the context of what we know.\n\n\nFrequentist Consolidation\nSo far so uncontroversial. The Bayesian perspective is a natural fit for a species of internalist epistemology. The framework is abstract and characterised concisely, so relatively straightforward to incorporate in a general philosophical picture. Otsuka’s synthesis of the “classical” inferential picture is in this way more impressive.\nThe classical frequentist picture has a history of poor pedagogy and can seem disparate and ad-hoc. Jaynes is famously dismissive of the absurdities engendered by the pick-and-mix approach to statistical inference adhered to in the “classical” approach.\nThe frequentist view ties statements of probability to measures of relative frequency within a collection of observations. This makes it impossible to articulate probability statements for specific hypotheses. The epistemological perspective is quite distinct from the Bayesian view of updating individual hypotheses. Instead the focus must lie of testing statements about stochastic processes - processes which are inherently repeatable.\nAs such these processes can be described by probabilistic kinds. The question then becomes - how can the properties of these observable processes feed into knowledge gathering routines. How can we go from a statement about an observed frequency to claims of knowledge or belief regarding the data generating process?\nThe route is to go via the framework of statistical testing which has some relationship to Karl Popper’s falsification. This involves positing a statistical hypothesis with direct implications. These implications can be parsed as an explicit prediction that can be compared to future observations. In this way, the hypothesis is tested against the data. This gives us a means of arguing reductio ad unlikely against the initial hypothesis and turns statistical inference into a “process of systematically plowing out falsehood”. Through iteratively testing and refining more targeted hypotheses. We define and reject the null hypotheses as we go.\nThe constraints on the test design are built to ensure reliability over the course of repeated testing under a known null hypothesis. Sample size considerations, alpha-spending and statistical power are properties of the test. These properties need to be chosen in such a way to ensure reliability of a particular test, but there are also constraints for running repeated tests of multiple hypotheses. The entire testing enterprise is set up to minimise and control errors. The epistemological picture is one of reliablism. Whether a belief is justified is determined by the nature and defensiblity of the process that generated the claim. This approach allows us to reject the Gettier style counter examples to justified true belief analyses of knowledge. If the procedures of knowledge acquisition are not themselves well founded than the coincidence between claim and fact in the Gettier cases cannot be counted as justified. The procedures of knowledge acquisition are fixed, uncertainty stems from how we learn the shape of the data probabilistically.\nFor this method to work the reliabilism the methodology seeks should be clarified. Otsuka suggests that the reliability implicit in statistical testing needs to underwrite truth-tracking counterfactuals. In particular the claims:\n\nif P were not true, S would not believe P\nIf P were true then S would believe P.\n\nWhich is not to say that any particular statistical test will lead to the endorsement/rejection of the hypothesis in question. Statistical tests cannot directly accept the null - just fail to reject. But cumulatively over many tests a reliabilist epistemology should ultimately ascertain the falsity of the null when the null is false. This points to a tension in the focus on individual tests and specific p-values. Statistical testing as an epistemological enterprise is a wholesale endeavour and the overall success or failure of conditions which underwrite the reliability of the process are not easily discerned by mere success in our world. The procedures must be valid and truth tracking in “nearby” counterfactual worlds where the null hypothesis is not how it is in our world. The conditions under which a process is ultimately truth-tracking may depend on contextual factors which cannot be easily turned to an algorithm. This perspective nicely unites all the ad-hoc approaches to defining tests with asymptotic error control properties. The concern is inherently procedural, and procedures can (and perhaps should) vary in the context of the learning. But their adoption is founded on the commitment that they would be reliably truth-tracking in all worlds similar enough to our own."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Thinking about Statistics.html#when-philosophies-conflict",
    "href": "notes/certain_things/Philosophy/Thinking about Statistics.html#when-philosophies-conflict",
    "title": "Examined Algorithms",
    "section": "When Philosophies Conflict",
    "text": "When Philosophies Conflict\nOtsuka’s survey of the two approaches is detailed and comprehensive. It sketches the motivations of each position well, and you might hope to reconcile the two. Apply each in their own domain where appropriate…, but unfortunately the motivating instincts can clash irreconcilably.\nThe Bayesian perspective necessitates the adoption of the Likelihood Principle which can be violated by the classical procedure of sequential testing. Recall how for the Bayesian the data is a fixed quantity, fed forward into the likelihood term to update our beliefs. This works the same whether we update our beliefs at time t1, t2 or tN. All the information is in the likelihood irrespective of how much data has accrued over time.\nWhile under the frequentist model if the data is analysed under different experimental designs (e.g. one with a stopping rule and one without) the results of test for a particular null hypothesis can differ I.e. with the same data and the same null hypothesis, one experiment can reject the null and the other will fail to reject it. This is because the result incorporates information about the design of the experiment that cannot be captured in the simple likelihood. Otsuka makes the point that this is a broad problem for all reliabilist philosophies where the fit between process/test and reality is contestable. There is no perfectly general procedure that returns true results in all circumstances. As such the frequentist methodologist must argue anew in each circumstances for the appropriateness of their methods.\nThis is a keen source of divisiveness between the two schools. Scientists historically cannot be trusted to review their methods with respect to their goals. Instead frequentist statistical methodology has been abused in practice. Adherents use the routine nature of procedure as a rubber stamp without due consideration for the reliability of the methods in the context of the question at hand. This pattern of abuse is rightly seen as a key contributor to the replication crisis in science. Ironically it is this aspect of frequentist methodology that introduces obscure subjective bias into the experimental exercise. The Bayesian (often accused of criminal subjectivity) wears their priors clearly and defends their appropriateness for each analysis.\nNeither school of thought is a self-contained epistemology. Neither method is self-sustaining. Both priors and procedures must be justified with an appeal to their apt fit for the problem at hand. Statistical methodology slots into a broader epistemological endeavour and there is no substitute for the careful and knowing application of inferential machinery if we hope to justify the achievements of science."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Thinking about Statistics.html#bias-and-regularisation",
    "href": "notes/certain_things/Philosophy/Thinking about Statistics.html#bias-and-regularisation",
    "title": "Examined Algorithms",
    "section": "Bias and Regularisation",
    "text": "Bias and Regularisation\nIn later chapters we move towards the more pragmatist position of model selection based on predictive power. This mirrors a move away from understanding of uncertainty due to natural variation towards an model-uncertainty in the analysis.\nWe may prefer a model which does not capture the true data generating process just so long as it performs better in prediction tasks. This effectively invokes the more typical approaches to machine learning model evaluation as driving us toward pragmatic biases that work well to optimise for some measure of out-of-sample prediction. Your inferential framework can be independent of your model selection criteria, but model ranking is still conducted under uncertainty. Each measure of performance is still an estimate.\nOtsuka contrasts the ranking of models based on information criteria with the out of sample predictive performance of deep learning systems. The AIC style methodologies penalises models with too many parameters to optimise for predictive performance. Deep learning methods have explicit methods to induce regularisation like effects with: drop-out, modified loss-functions. What is striking is that while both methods are checks against overfitting of models to the data, both are pragmatic compromises that dispense with the idea of epistemological truth-tracking. They concede that the it is often better to overcome the problem of model uncertainty with suitably practical abstraction. Don’t worry about the world-model fit so long as the outcomes of the model are workable.\nBut this comes with a burden of explainability often demanded of opaque models. Predictive success in one domain does not always translate to success in another. Regulators and stakeholders need to understand when and why the predictive reliability of deep learning systems can be expected to transfer well across tasks. Appeals to the epistemological surety of beliefs derived from predictions in these black-box systems stem from a loosely held belief in the virtues of the deep learning mechanisms. These virtues are less well established than error-control rates of a statistical test and they are somewhat transient across task."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Thinking about Statistics.html#causal-inference-and-statistics",
    "href": "notes/certain_things/Philosophy/Thinking about Statistics.html#causal-inference-and-statistics",
    "title": "Examined Algorithms",
    "section": "Causal Inference and Statistics",
    "text": "Causal Inference and Statistics\nFinally Otsuka pauses to consider the role of causal inference in contemporary statistics and how here optimising for predictive power will often fail when the task requires subtlety or insight into the data generating process. The manner in which models and measurements can be confounded by aspects of the data generating process cannot be detected automatically without knowledge of the relationships in the system. The focus here is on how tools for identification are required when we want to be sure that the causal quantities of interest are soundly derivable from the data we have to hand. Whether we use the potential outcomes framework, the do-calculus or structural estimation. The work of statistical inference using any and all of the above frameworks can only get off the ground when we have enough of a world-picture - a set of commitments or assumptions that allow our inferences to have warrant. Our priors our informed, our inferential procedures suitably well defined to sustain truth-tracking counterfactuals."
  },
  {
    "objectID": "notes/certain_things/Philosophy/Thinking about Statistics.html#conclusion",
    "href": "notes/certain_things/Philosophy/Thinking about Statistics.html#conclusion",
    "title": "Examined Algorithms",
    "section": "Conclusion",
    "text": "Conclusion\nThe broad picture painted in Otsuka’s survey of statistics is the central role of inferential procedures in our broader epistemological landscape. The diversity of roles it can play and different standards of rigour at play in different contexts. The links between the foundational questions and puzzles of epistemology have illuminated the structure of the debates in statistics."
  },
  {
    "objectID": "talks/mister_p/mister_p.html",
    "href": "talks/mister_p/mister_p.html",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "",
    "text": "Intro\n\n\n\n\nI’m a data scientist at Personio\n\nBayesian statistician,\nReformed philosopher and logician.\n\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\nDisclaimer\n\n\n\nNone of Personio’s data was used in this presentation\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\n\nThe worked examples used here can be found here"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#agenda",
    "href": "talks/mister_p/mister_p.html#agenda",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Agenda",
    "text": "Agenda\n\n\nRegression as Strata specific Summarisation\n\n\n\n\nHierarchical Regression and Probabilistic Programming\n\n\n\n\nSampling and Probability Sampling\n\n\n\n\nStratum Specific Modelling\n\n\n\n\nStratum Specific Adjustment\n\n\n\n\nConclusion\n\nWhen to Adjust and Why?"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#agenda-1",
    "href": "talks/mister_p/mister_p.html#agenda-1",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Agenda",
    "text": "Agenda\n\n\nRegression as Strata specific Summarisation\n\n\n\n\nHierarchical Regression and Probabilistic Programming\n\n\n\n\nSampling and Probability Sampling\n\n\n\n\nStratum Specific Modelling\n\n\n\n\nStratum Specific Adjustment\n\n\n\n\nConclusion\n\nWhen to Adjust and Why?"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#regression-what-are-we-even-doing",
    "href": "talks/mister_p/mister_p.html#regression-what-are-we-even-doing",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Regression: What are we even doing?",
    "text": "Regression: What are we even doing?\n\n\n\\[\\hat{y_{i}} = \\alpha + \\beta_{1}X_{1} ... \\beta_{n}X_{n}\\]\n\n\n\nOLS\n\n\nAssume \\(y = \\hat{y_{i}} + \\epsilon\\) where \\(E(\\epsilon) = 0\\)\n\n\\[ E[y | X = x] = \\alpha + \\beta_{1}X_{1} ... \\beta_{n}X_{n}\\]\n\\[ y  \\sim Normal(\\hat{y_{i}}, \\sigma) \\]\n\n\n\nMultiple Regression Optimisation"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#regression-what-are-we-even-doing-1",
    "href": "talks/mister_p/mister_p.html#regression-what-are-we-even-doing-1",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Regression: What are we even doing?",
    "text": "Regression: What are we even doing?\nm0 = smf.ols('np.log(hwage) ~ job +  educ', data=df).fit()\nm1 = smf.ols('np.log(hwage) ~ job + educ + male ', data=df).fit()\npred = m0.predict(['software_engineer', 'college'])\npred1 = m1.predict(['software_engineer', 'college', 1])\ndiff = predc - pred1\n\n\n\n\n\nChoose Your Own Path\n\n\nAs we add more covariates we add more combinatorial branches which define the available strata across our population of interest.\nA fitted regression model allows us to explore the conditional branching probabilities.\n\n\n\n\nChoose Your Own Adventure"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#regression-as-effect-modification",
    "href": "talks/mister_p/mister_p.html#regression-as-effect-modification",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Regression as Effect Modification",
    "text": "Regression as Effect Modification\nreg = bmb.Model(\"Outcome ~ 1 + Treatment\", df)\nresults = reg.fit()\n\nreg_strata = bmb.Model(\"\"\"Outcome ~ 1 + Treatment + Risk_Strata \n+ Treatment_x_Risk_Strata\"\"\", df)\nresults_strata = reg_strata.fit()\nbmb.interpret.plot_predictions(reg, results, conditional=[\"Treatment\"])\nbmb.interpret.plot_predictions(reg_strata, results_strata, conditional=[\"Treatment\"])\n\n\n\n\n\nTreatment Effect\n\n\n\n\n\n\nBambi’s Marginal Effects Interpretation module automates the implications of each model fit"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#regression-as-weighting-adjustment",
    "href": "talks/mister_p/mister_p.html#regression-as-weighting-adjustment",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Regression as Weighting Adjustment",
    "text": "Regression as Weighting Adjustment\nRegression automates the more manual re-weighting\n\nSimple Unweighted Average"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#the-need-for-re-weighting",
    "href": "talks/mister_p/mister_p.html#the-need-for-re-weighting",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "The Need for Re-Weighting",
    "text": "The Need for Re-Weighting\n\n\n\nCausal Inference\n\nInverse Probability Weighting\nPseudo-Population Imputation\nTreatment effect estimation\n\n\n\n\nSurvey Sample Bias\n\nNon-response\nOpt-out Sampling contracts\nIncomplete coverage\nMultilevel Regression\nPost-stratification Adjustment"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#the-data",
    "href": "talks/mister_p/mister_p.html#the-data",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "The Data",
    "text": "The Data\nWe examine a comprehensive YouGov poll on whether employers should cover abortion in their coverage plans.\nWe select a biased subsample.\n\nState Level Data"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#deliberate-bias",
    "href": "talks/mister_p/mister_p.html#deliberate-bias",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Deliberate Bias",
    "text": "Deliberate Bias\n\nIllustrated differences in vote share by demographics"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#prep-data-for-modelling",
    "href": "talks/mister_p/mister_p.html#prep-data-for-modelling",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Prep Data for Modelling",
    "text": "Prep Data for Modelling\n\nAggregate Across Strata"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#exploratory-modelling",
    "href": "talks/mister_p/mister_p.html#exploratory-modelling",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Exploratory Modelling",
    "text": "Exploratory Modelling\nWe fit a preliminary model to investigate the interactions across demographic splits using a logit model with the binomial link function.\nformula = \"\"\" p(abortion, n) ~ C(state) + C(eth) + C(edu) + male + repvote\"\"\"\n\nbase_model = bmb.Model(formula, model_df, family=\"binomial\")\n\nresult = base_model.fit(\n    random_seed=100,\n    target_accept=0.95,\n    idata_kwargs={\"log_likelihood\": True},\n)\n\nfig, ax = bmb.interpret.plot_comparisons(\n    model=base_model,\n    idata=result,\n    contrast={\"eth\": [\"Black\", \"White\"]},\n    conditional=[\"age\", \"edu\"],\n    comparison_type=\"diff\",\n    subplot_kwargs={\"main\": \"age\", \"group\": \"edu\"},\n    fig_kwargs={\"figsize\": (12, 5), \"sharey\": True},\n    legend=True,\n)"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#plotting-implications",
    "href": "talks/mister_p/mister_p.html#plotting-implications",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Plotting Implications",
    "text": "Plotting Implications\n\nExploratory Interaction Effects"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#the-full-hierarchical-interaction-model",
    "href": "talks/mister_p/mister_p.html#the-full-hierarchical-interaction-model",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "The Full Hierarchical Interaction Model",
    "text": "The Full Hierarchical Interaction Model\n\\[Pr(y_i = 1) = logit^{-1}\\Bigg(\n\\alpha_{\\rm s[i]}^{\\rm state}\n+ \\alpha_{\\rm a[i]}^{\\rm age}\n+ \\alpha_{\\rm r[i]}^{\\rm eth}\n+ \\alpha_{\\rm e[i]}^{\\rm edu} \\\\\n+ \\beta^{\\rm male} \\cdot {\\rm Male}_{\\rm i}\n+ \\alpha_{\\rm g[i], r[i]}^{\\rm male.eth}\n+ \\alpha_{\\rm e[i], a[i]}^{\\rm edu.age}\n+ \\alpha_{\\rm e[i], r[i]}^{\\rm edu.eth}\n\\Bigg)\\]\nAllowing for stratum specific intercept terms for each level of the demographic categories and their interaction effects."
  },
  {
    "objectID": "talks/mister_p/mister_p.html#the-model-in-code",
    "href": "talks/mister_p/mister_p.html#the-model-in-code",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "The Model in Code",
    "text": "The Model in Code\nFitting the model to the biased sample:\nformula = \"\"\" p(abortion, n) ~ (1 | state) + (1 | eth) + (1 | edu)\n + male + repvote  + (1 | male:eth) + (1 | edu:age) + (1 | edu:eth)\"\"\"\n\nmodel_hierarchical = bmb.Model(formula, model_df, family=\"binomial\")"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#learning-the-bias",
    "href": "talks/mister_p/mister_p.html#learning-the-bias",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Learning the Bias",
    "text": "Learning the Bias\nThe Model Derived Coefficients\n\n\n\n\n\nEstimated Coefficients\n\n\n\n\n\n\nPosterior Predictive Fit"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#preliminaries",
    "href": "talks/mister_p/mister_p.html#preliminaries",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Preliminaries",
    "text": "Preliminaries\n\n\n\n\n\n\nIntro\n\n\n\nI’m a data scientist at Personio\n\nBayesian statistician,\nReformed philosopher and logician.\n\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\n\nDisclaimer\n\n\nNone of Personio’s data was used in this presentation\n\n\n\n\nQR Website Code\n\n\n\n\n\nCode or it didn’t Happen\n\n\nThe worked examples used here can be found here"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#predicting-vote-share",
    "href": "talks/mister_p/mister_p.html#predicting-vote-share",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Predicting Vote Share",
    "text": "Predicting Vote Share\nUsing the Biased Model\nnew_data = (new_data.merge(\n    new_data.groupby(\"state\").agg({\"n\": \"sum\"})\n    .reset_index()\n    .rename({\"n\": \"state_total\"}, axis=1)\n)\n)\nnew_data[\"state_percent\"] = new_data[\"n\"] / new_data[\"state_total\"]\nnew_data.head()\n\nPopulation by Strataresult_adjust = model_hierarchical.predict(result, \ndata=new_data, inplace=False, kind=\"pps\")\nresult_adjust"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#adjusting-the-state-level-predictions",
    "href": "talks/mister_p/mister_p.html#adjusting-the-state-level-predictions",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Adjusting the State Level Predictions",
    "text": "Adjusting the State Level Predictions\nReweighting Outcomes by Strata specific Share\n\nestimates = []\n## The base model posterior fitted on biased sample\nabortion_posterior_base = az.extract(result)[\"p(abortion, n)_mean\"]\n\n## The posterior updated with national level figures\nabortion_posterior_mrp = az.extract(result_adjust)[\"p(abortion, n)_mean\"]\n\n## Adjusting the predictions on state level\nfor s in new_data[\"state\"].unique():\n    idx = new_data.index[new_data[\"state\"] == s].tolist()\n    predicted_mrp = (\n        ((abortion_posterior_mrp[idx].mean(dim=\"sample\") * \n        new_data.iloc[idx][\"state_percent\"]))\n        .sum()\n        .item()\n    )\n    predicted_mrp_lb = (\n        (\n            (\n                abortion_posterior_mrp[idx].quantile(0.025, dim=\"sample\")\n                * new_data.iloc[idx][\"state_percent\"]\n            )\n        )\n        .sum()\n        .item()\n    )\n    predicted_mrp_ub = (\n        (\n            (\n                abortion_posterior_mrp[idx].quantile(0.975, dim=\"sample\")\n                * new_data.iloc[idx][\"state_percent\"]\n            )\n        )\n        .sum()\n        .item()\n    )\n    predicted = abortion_posterior_base[idx].mean().item()\n    base_lb = abortion_posterior_base[idx].quantile(0.025).item()\n    base_ub = abortion_posterior_base[idx].quantile(0.975).item()\n\n    estimates.append(\n        [s, predicted, base_lb, base_ub, predicted_mrp, predicted_mrp_ub, predicted_mrp_lb]\n    )\n\n\nstate_predicted = pd.DataFrame(\n    estimates,\n    columns=[\"state\", \"base_expected\", \"base_lb\", \n    \"base_ub\", \"mrp_adjusted\", \"mrp_ub\", \"mrp_lb\"],\n)\n\nstate_predicted = (\n    state_predicted.merge(cces_all_df.groupby(\"state\")[[\"abortion\"]].mean().reset_index())\n    .sort_values(\"mrp_adjusted\")\n    .rename({\"abortion\": \"census_share\"}, axis=1)\n)\nstate_predicted.head()"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#comparing-adjusted-and-raw-predictions",
    "href": "talks/mister_p/mister_p.html#comparing-adjusted-and-raw-predictions",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Comparing Adjusted and Raw Predictions",
    "text": "Comparing Adjusted and Raw Predictions\nDerived state level predictions using the biased sample and the corrected values.\n\nAdjusted Predictions"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#comparing-adjusted-and-raw-predictions-1",
    "href": "talks/mister_p/mister_p.html#comparing-adjusted-and-raw-predictions-1",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Comparing Adjusted and Raw Predictions",
    "text": "Comparing Adjusted and Raw Predictions\n\nComparing Predictions to Adjusted Predictions"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#conclusion",
    "href": "talks/mister_p/mister_p.html#conclusion",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Conclusion",
    "text": "Conclusion\nThe Need for Reweighting\n\n“The IID condition is a mathematical specification of what Hume called the uniformity of nature. To say that nature is uniform means that whatever circumstances holds for the observed, the same circumstances will continue to hold for the unobserved. This is what Hume required for the possibility of inductive reasoning”\n\n\n\n\nSurvey Bias Breaks the IID condition\nInference falls apart with non-representative samples\nPrediction suffers from wild skew\n\n\n\nKnowledge about demographic representation informs priors\nHistoric rates can be used to improve sample representation\nModel recovers inferential validity. Prediction improves."
  },
  {
    "objectID": "talks/mister_p/mister_p.html#informed-sampling",
    "href": "talks/mister_p/mister_p.html#informed-sampling",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Informed Sampling",
    "text": "Informed Sampling\nModelling of Survey Outcomes with Prior information\n\n“In conventional sampling theory, the only scenario considered is essentially that of ‘drawing from an urn’, and the only probabilities that arise are those that presuppose the contents of the ‘urn’ or the ‘population’ already known, and seek to predict what ‘data’ we are likely to get as a result. Problems of this type can become arbitrarily complicated in the details, and there is a highly developed mathematical literature dealing with them…It was our use of probability theory as logic that has enabled us to do so easily what was impossible for those who thought of probability as a physical phenomenon associated with ‘randomness’. Quite the opposite; we have thought of probability distributions as carriers of information. At the same time, under the protection of Cox’s theorems, we have avoided the inconsistencies and absurdities which are generated inevitably by those who try to deal with the problems of scientific inference by inventing ad hoc devices instead of applying the rules of probability theory” - Edwin Jaynes in Probability: The Logic of Science pg88"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#modelling-of-survey-outcomes-with-prior-information",
    "href": "talks/mister_p/mister_p.html#modelling-of-survey-outcomes-with-prior-information",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Modelling of Survey Outcomes with Prior information",
    "text": "Modelling of Survey Outcomes with Prior information\nBayesian Models incorporate different sources of knowledge\n\n“In conventional sampling theory, the only scenario considered is essentially that of ‘drawing from an urn’, and the only probabilities that arise are those that presuppose the contents of the ‘urn’ or the ‘population’ already known, and seek to predict what ‘data’ we are likely to get as a result. …It was our use of probability theory as logic that has enabled us to do so easily what was impossible for those who thought of probability as a physical phenomenon associated with ‘randomness’. Quite the opposite; we have thought of probability distributions as carriers of information.” - Edwin Jaynes in Probability: The Logic of Science pg88 & p117"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#modelling-of-survey-outcomes",
    "href": "talks/mister_p/mister_p.html#modelling-of-survey-outcomes",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Modelling of Survey Outcomes",
    "text": "Modelling of Survey Outcomes\nBayesian Models incorporate different sources of knowledge\n\n“In conventional sampling theory, the only scenario considered is essentially that of ‘drawing from an urn’, and the only probabilities that arise are those that presuppose the contents of the ‘urn’ or the ‘population’ already known, and seek to predict what ‘data’ we are likely to get as a result. …It was our use of probability theory as logic that has enabled us to do so easily what was impossible for those who thought of probability as a physical phenomenon associated with ‘randomness’. Quite the opposite; we have thought of probability distributions as carriers of information.” - Edwin Jaynes in Probability: The Logic of Science pg88 & p117"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#plotting-implications-1",
    "href": "talks/mister_p/mister_p.html#plotting-implications-1",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Plotting Implications",
    "text": "Plotting Implications\n\nPosterior Predictive By Class"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#investigating-marginal-contrasts",
    "href": "talks/mister_p/mister_p.html#investigating-marginal-contrasts",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Investigating Marginal Contrasts",
    "text": "Investigating Marginal Contrasts\n\nMarginal Differences"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#preliminaries",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#preliminaries",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Preliminaries",
    "text": "Preliminaries\n\n\n\n\n\n\nProfile\n\n\n\nI’m a data scientist at Personio\n\nBayesian statistician,\nReformed philosopher and logician.\n\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\n\nDisclaimer\n\n\nNone of Personio’s data was used in this presentation\n\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\nThe worked examples used here can be found here"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#agenda",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#agenda",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Agenda",
    "text": "Agenda"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#agenda-1",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#agenda-1",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Agenda",
    "text": "Agenda\n\n\nTypology of Missing-ness\n\n\n\n\nMultivariate imputation using FIML\n\n\n\n\nBayesian Imputation by Chained Equations\n\n\n\n\nHierarchical Structures impacting Missing-ness\n\n\n\n\nImputation and Causal Narratives\n\n\n\n\nConclusion\n\nMissing Data: a Gateway to Causal Inference\n\n\n\n\n\nNon Parametric Causal Inference"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#three-acts",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#three-acts",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Three Acts",
    "text": "Three Acts\n\n\nPropensity Scores and Non-Parametric Causal Inference\n\n\n\n\nConfounding and Debiasing\n\n\n\n\nCausal Structure and Mediation"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#three-acts-1",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#three-acts-1",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Three Acts",
    "text": "Three Acts\n\n\nPropensity Scores and Non-Parametric Causal Inference\n\n\n\n\nConfounding and Debiasing\n\n\n\n\nCausal Structure and Mediation\n\n\n\n\nNon Parametric Causal Inference"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#act-one",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#act-one",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Act One",
    "text": "Act One\nPropensity Scores and Non-Parametric Causal Inference\n\n\nStrong Ignorability and Propensity Scores\n\n\n\n\nBART models and Non-Parametric Estimation\n\n\n\n\nBalance and Inverse Propensity Weighting\n\n\n\n\nRobust and Doubly Robust methods"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#act-two",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#act-two",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Act Two",
    "text": "Act Two\nConfounding and Debiasing\n\n\nPropensity Scores Miscalibrated\n\n\n\n\nBART models and Overfitting\n\n\n\n\nDebiased Machine Learning\n\n\n\n\nCATE estimation"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#act-three",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#act-three",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Act Three",
    "text": "Act Three\nCausal Structure and Mediation\n\n\nParametric Mediation\n\n\n\n\nNon-Parametric Mediation\n\n\n\n\nEscalating Structural Assumptions and Bayesian Inference"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#sources",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#sources",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Sources",
    "text": "Sources\n\n\n\n\n\nFoundations\n\n\n\n\n\n\nFoundations\n\n\n\n\n\n\nNon-Parametric"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#debts-and-sources",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#debts-and-sources",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Debts and Sources",
    "text": "Debts and Sources\n\n\n\n\n\nFoundations\n\n\n\n\n\n\nG-Computation\n\n\n\n\n\n\nNon-Parametric"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#primary-debts-and-sources",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#primary-debts-and-sources",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Primary Debts and Sources",
    "text": "Primary Debts and Sources\n\n\n\n\n\nFoundations\n\n\n\n\n\n\nDebiased ML\n\n\n\n\n\n\nNon-Parametric"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#strong-ignorability-and-propensity-scores",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#strong-ignorability-and-propensity-scores",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Strong Ignorability and Propensity Scores",
    "text": "Strong Ignorability and Propensity Scores\nDefinitions\n\nPotential Outcomes\n\n\\(Y(0)\\) and \\(Y(1)\\) under different treatment regimes \\(T \\in \\{ 0, 1\\}\\)\n\nStrong Ignorability\n\nOutcomes are independent of the treatment assignment given a covariate profile \\(X\\): \\(Y(0), Y(1) \\perp\\!\\!\\!\\perp T | X\\)\n\nPropensity Scores\n\nAn estimate of the probability for a particular treatment status conditional on the covariate profile \\(X\\): \\(0 \\leq p_{t}(X) \\leq 1\\)"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#bart-models-and-non-parametric-estimation",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#bart-models-and-non-parametric-estimation",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "BART Models and Non-Parametric Estimation",
    "text": "BART Models and Non-Parametric Estimation\n\nBART\n\nBayesian Additive Regression Trees\n“[B]lack-box method based on the sum of many trees where priors are used to regularize inference”\n\nNon-Parametric Estimation\n\nOutcomes and Propensity Scores can be estimated using non-parametric methods\nCausal estimands can be estimated using posterior predictive imputation under different treatment regimes\nBenefit of minimalist structural assumptions."
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#inverse-propensity-score-weighting",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#inverse-propensity-score-weighting",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Inverse Propensity Score Weighting",
    "text": "Inverse Propensity Score Weighting\n\nAdjustment by representative Weighting\n\nUsing the propensity scores as a summary metric for group membership, we down-weight and upweight the prevalence of high and low propensity score in each group to induce strong ignorability like conditions."
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#robust-and-doubly-robust-weighting-schemes",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#robust-and-doubly-robust-weighting-schemes",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Robust and Doubly Robust Weighting Schemes",
    "text": "Robust and Doubly Robust Weighting Schemes"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#robust-and-doubly-robust",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#robust-and-doubly-robust",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Robust and Doubly Robust",
    "text": "Robust and Doubly Robust\nWeighting Schemes"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#robust-and-doubly-robust-methods",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#robust-and-doubly-robust-methods",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Robust and Doubly Robust Methods",
    "text": "Robust and Doubly Robust Methods\nDiffering Weighting Schemes\n\nRaw\n\n\\(\\sum\\frac{1}{N}\\Big[ Y(1) \\cdot \\frac{1}{p_{T}(X)} - (Y(0)\\cdot\\frac{1}{1-p_{T}(X)}) \\Big]\\)\n\nDoubly Robust\n\n\\[ \\hat{Y(1)} = \\frac{1}{n} \\sum_{0}^{N} \\Bigg[ \\frac{T(Y - m_{1}(X))}{p_{T}(X)} + m_{1}(X) \\Bigg] \\\\ \\hat{Y(0)} = \\frac{1}{n} \\sum_{0}^{N} \\Bigg[ \\frac{(1-T)(Y - m_{0}(X))}{(1-p_{T}(X))} + m_{0}(X) \\Bigg] \\]"
  },
  {
    "objectID": "oss/pymc/bnp.html",
    "href": "oss/pymc/bnp.html",
    "title": "Bayesian Non Parametric Causal Inference in PyMC",
    "section": "",
    "text": "Bayesian Non Parametric Causal Inference\nIn this project I demonstrated how to fit and use BART propensity score models in the assessment of causal inference questions. We showed the conditions under which these methods work well and an example of where they break down due to overfit. We showed how they could be fixed using debiasing techniques and how appropriate analysis of this case could be achieved using non-parametric mediation analysis.\nThe project culminated in a publication to the official PyMC documentation that can be found online here or downloaded as notebook here\n\n\n\nConditional Average Treatment Effects of Smoking\n\n\nI also recorded a youtube modeling webinair on this topic with Alex Andorra\n\n\n\nYoutube Webinair"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#miscalibrated-propensity-scores",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#miscalibrated-propensity-scores",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Miscalibrated Propensity Scores",
    "text": "Miscalibrated Propensity Scores\nWhat is the Estimand?\n\n“Each theoretical estimand is linked to an empirical estimand involving only observable quantities (e.g. a difference in means in a population) by assumptions about the relationship between the data we observe and the data we do not.” - Lundberg et al in What is your Estimand\n\n\nQ1. What are we aiming at when we estimate propensity scores for highly granular covariate profiles?\nQ2. What happens when the sample data has no treatment data cases for a particular covariate profile?"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#overfitting",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#overfitting",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Overfitting",
    "text": "Overfitting\nBART models can achieve perfect Allocation\n\nOverfitting"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#debiasing-machine-learning",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#debiasing-machine-learning",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Debiasing Machine Learning",
    "text": "Debiasing Machine Learning\n\nK-fold Propensity Estimation"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#cate-estimation",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#cate-estimation",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "CATE Estimation",
    "text": "CATE Estimation\n\nConditional Average Treatment Effect"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#parametric-mediation",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#parametric-mediation",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Parametric Mediation",
    "text": "Parametric Mediation\n\nTraditional Model Based Mediation"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#causal-mediation-analysis",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#causal-mediation-analysis",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Causal Mediation Analysis",
    "text": "Causal Mediation Analysis\nNon-Parametric Estimation\n\nNDE: \\(E[Y(t, M(t^{*})) - Y(t^{*}, M(t^{*}))]\\)\n\nWhich is to say we’re interested in the differences in the imputed outcomes under different treatments, mediated by values for M under a specific treatment regime.\n\nNIE: \\(E[(Y(t, M(t))) - Y(t, M(t^{*}))]\\)\n\nWhich amounts to the imputed differences in the outcome Y due to differences in the treatment regimes which generated the mediation values M.\n\nTE: NDE + NIE"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#conclusion",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#conclusion",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Conclusion",
    "text": "Conclusion\n### Structural Assumption\n\n\nNon Parametric Causal Inference"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#conclusions",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#conclusions",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Conclusions",
    "text": "Conclusions\nStructural Beliefs and Bayesian Inference\n\nPropensity score adjustment reflects the belief in the need for adjustment\n\nIt reflects a belief in the adequacy of the propensity score model for achieving balance\n\nRegression based imputation of treatment effects reflects the belief that covariate controls eliminates selection effects\nDoubly Robust methods reflect the belief that either the propensity or outcome model is mispecified.\n\nBut that one ought to be adequately specified.\n\nDebiased ML methods reflect the belief that mis-specification can be corrected by cross-validation and non-parametric estimation of residuals in FWL theorem\nMediation Analysis reflects the belief that the causal influence must be interepreted with particular causal structure to avoid confounding.\nThere are no truly Agnostic statistics:In each case sound inference proceeds as we take steps to adjust our model or the conditions of its assessment as informed by our best beliefs regarding the problem to hand.\n\n\n\nNon Parametric Causal Inference"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#bayesian-non-parametrics",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#bayesian-non-parametrics",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Bayesian Non-parametrics",
    "text": "Bayesian Non-parametrics\n\nThe Pitch"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#the-pitch-1",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#the-pitch-1",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "The Pitch",
    "text": "The Pitch\n\nThe Pitch"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#obligatory-meme",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#obligatory-meme",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Obligatory Meme",
    "text": "Obligatory Meme\nBayesian Structural Modelling\n\nThe Pitch"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#propaganda",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#propaganda",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Propaganda",
    "text": "Propaganda\nFull Luxury Bayesianism\n\nPosterior Predictive Imputation of Treatment Effects"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#bayes-formula",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#bayes-formula",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Bayes Formula",
    "text": "Bayes Formula\nFull Luxury Bayesianism\n\\[ p(\\theta |D) \\propto p(D | \\theta)p(D) \\]\nwhere \\(\\theta\\) is parameter becomes\n\\[ p(G |D) \\propto p(D | G)p(D) \\]\nwhere G is a general stochastic process"
  },
  {
    "objectID": "talks/lbs_non_param_causal/non_param_causal.html#non-parametric-bayes-formula",
    "href": "talks/lbs_non_param_causal/non_param_causal.html#non-parametric-bayes-formula",
    "title": "Non Parametric Causal Inference with PyMC",
    "section": "Non Parametric Bayes Formula",
    "text": "Non Parametric Bayes Formula\nFull Luxury Bayesianism\n\\[ p(\\color{blue}{\\theta} |D) \\propto p(D | \\color{blue}{\\theta})p(D) \\]\nwhere \\(\\color{blue}{\\theta}\\) is an explict model parameter becomes\n\\[ p(\\color{blue}{G} |D) \\propto p(D | \\color{blue}{G})p(D) \\]\nwhere \\(\\color{blue}{G}\\) is a general stochastic process"
  },
  {
    "objectID": "notes/certain_things/Wedding/Ceremony.html",
    "href": "notes/certain_things/Wedding/Ceremony.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "Remind guests to turn off phones, and no social media.\nBridesmaid :Margaret Stanley Best Man : Jamie Forde Readers –  Aadil Kurji, John Stanley and Ciaran Murray Witnesses : Margaret and Jamie Music : Band the Panoramics: https://www.thepanoramicsweddingband.ie/read-me\nWelcome & Introduction\nA very warm welcome to each and every one here this afternoon to the beautiful Marco Pierre White’s here in the heart of Donnybrook, in Dublin to the wedding of Joanne and Nathaniel\nIntroduction\nMy name is Dave Russell and I am a One Spirit Interfaith Minister & Celebrant and it is an honour and a joy to have been invited here today by Joanne and Nathaniel to celebrate their marriage. Jamie Forde, Nathaniel’s brother is the best man and Margaret Stanley, Joanne’s sister is the maid of honour. Please bring any all issues to their attention and avoid bothering the bride and groom!\nJoanne and Nathaniel are delighted that so many friends and family could join them here today. Those of you have set aside your routines and commitments to be here to celebrate with them this afternoon.\nWelcome guests from UK, America, Netherlands, Germany, Austria and Dublin  You are all very welcome."
  },
  {
    "objectID": "notes/certain_things/Wedding/Ceremony.html#opening-the-ceremony-with-lighting-of-a-candle",
    "href": "notes/certain_things/Wedding/Ceremony.html#opening-the-ceremony-with-lighting-of-a-candle",
    "title": "Examined Algorithms",
    "section": "Opening the Ceremony with lighting of a Candle",
    "text": "Opening the Ceremony with lighting of a Candle\nSo as we begin…let’s take a moment of silence as we gather ourselves into this ceremonial space so that we are fully present for Joanne and Nathaniel\nAs I light this candle symbolising the kindling of their feelings for one another, that has grown into the light of love that is present between our beautiful couple.\nWe also acknowledge all the members of the Stanley and Forde families who for whatever reason cannot be with us here today.\nWe especially remember Nathaniel’s Dad Desmond He is very much in our thoughts here today, as he would have quite been annoyed to miss this occasion, the birth of his grandson and a host of other developments in the recent years.\n\nOpening words for the Couple\nWe are thankful for the gift of Joanne and Nathaniel - the gift they have been to each other and the gift they are to their families & friends.\nToday we surround them both in great love and wonderment and may they always know and feel the strength and power of their love for each other."
  },
  {
    "objectID": "notes/certain_things/Wedding/Ceremony.html#candle-ceremony",
    "href": "notes/certain_things/Wedding/Ceremony.html#candle-ceremony",
    "title": "Examined Algorithms",
    "section": "Candle ceremony",
    "text": "Candle ceremony\nJoanne and Nathaniel will light two candles, acknowledging the support and care offered them by their parents, their family and friends for the love and care they have shown over innumerable instances and interactions - too many to recount.\nYou have all played a role in teaching them about kindness, love and life. By lighting these candles they are now symbolising the light that they are to each other.\n\nMuscial Interlude here\n\nWe are here to witness and celebrate their deep love for one another; and their total acceptance of each other and the connection between life and love.\nThere are moments in our lives that are ruled not so much by time but by the heart.  This is such a moment for Joanne and Nathaniel\nThese two people have fallen in love .. so deeply, so completely … that today they make a bond, a profound covenant whereby their hearts, their bodies and their souls shall be united as one in marriage for the rest of their days.\nToday, before all of you, their most cherished family and friends, they will say the most powerful most loving words two people can say to each other.\n > They will take their wedding vows   And it is our honour and privilege to stand witness.  For this profound act, my friends, is magnificent and so tender to behold.\n\n_This is where I talk about the two of you meeting up etc, I don’t include it here with you as I want to keep it as a bit of a surprise for the day, don’t worry I won’t be adding anything that would embarrass either of you.\n\nOver the last 10 years, their love for one another has been constant, like the stars above them and the earth beneath them their love had a constant source of light and a firm foundation from which to grow.\n   >Speaking to Joanne and Nathaniel   Today you begin the next stage of your life together. Our wish for you is that you both become: - more magnificent and powerful than you have ever been - healthier than you have ever been - more full of life and love than you have ever been - more at one with yourself and all others as a direct result of the transformational power of your marriage here today.\nJoanne and Nathaniel, I am just putting in some samples of readings. You can let me know the ones which you have chosen."
  },
  {
    "objectID": "notes/certain_things/Wedding/Ceremony.html#readings",
    "href": "notes/certain_things/Wedding/Ceremony.html#readings",
    "title": "Examined Algorithms",
    "section": "Readings:",
    "text": "Readings:\n\nOur first reading is being read for us by Aadil Kurji\n\nOn Exactitude in Science\n“…In that Empire, the Art of Cartography attained such Perfection that the map of a single Province occupied the entirety of a City, and the map of the Empire, the entirety of a Province. In time, those Unconscionable Maps no longer satisfied, and the Cartographers Guilds struck a Map of the Empire whose size was that of the Empire, and which coincided point for point with it. The following Generations, who were not so fond of the Study of Cartography as their Forebears had been, saw that that vast Map was Useless, and not without some Pitilessness was it, that they delivered it up to the Inclemencies of Sun and Winters. In the Deserts of the West, still today, there are Tattered Ruins of that Map, inhabited by Animals and Beggars; in all the Land there is no other Relic of the Disciplines of Geography.” - On exactitude in Science by Borges\n\nOur Second Reading is read for us by John Stanley\n\nSonnet 115\nThose lines that I before have writ do lie, Even those that said I could not love you dearer; Yet then my judgment knew no reason why My most full flame should afterwards burn clearer. But reckoning time, whose millioned accidents Creep in ’twixt vows and change decrees of kings, Tan sacred beauty, blunt the sharp’st intents, Divert strong minds to th’ course of alt’ring things— Alas, why, fearing of time’s tyranny, Might I not then say “Now I love you best,” When I was certain o’er incertainty, Crowning the present, doubting of the rest? Love is a babe. Then might I not say so, To give full growth to that which still doth grow.” - Shakespeare\n\nOur Third Reading is read for us by Ciaran Murray\n\nLetter to Mauro - How do I not have my heart broken?\n“The surest way to avoid a broken heart is to love nothing and no-one…In short, resist love, because real love, big love, true love, fierce love, is a perilous thing, and travels surely towards its devastation. A broken heart — that grief of love — is always love’s true destination. This is the covenant of love.\nHowever, Mauro, to resist love and inoculate yourself against heartbreak is to reject life itself, for to love is your primary human function. It is your duty to love in whatever way you can, and to move boldly into that love — deeply, dangerously and recklessly — and restore the world with your awe and wonder. This world is in urgent need — desperate, crucial need — and is crying out for love, your love. It cannot survive without it.” - Nick Cave \n\nMusical Interlude Here…"
  },
  {
    "objectID": "notes/certain_things/Wedding/Ceremony.html#vow-ceremony",
    "href": "notes/certain_things/Wedding/Ceremony.html#vow-ceremony",
    "title": "Examined Algorithms",
    "section": "Vow Ceremony          ",
    "text": "Vow Ceremony          \n\nCivil Vows\nJoanne and Nathaniel you both formally attended in the presence of a civil registrar and on that occasion a declaration was signed by the two of you stating that there was no impediment of kindred or alliance or any other impediment or lawful hindrance to marriage and that the consent of every person whose consent to such marriage  is by law required has been duly obtained.  \nI Nathaniel  do solemnly and sincerely declare……. that I know not….. of any lawful impediment…. why I Nathaniel may not be joined in matrimony to Joanne.\nI.. Joanne…….do solemnly and sincerely declare…. that I know not….. of any lawful impediment why I, Joanne…. may not be joined in matrimony to Nathaniel.\nExpression of intent\nMarriage is a profound and compelling adventure. You are committing to loving each another for ever.\nNathaniel do you take Joanne to be your wife?\nNathaniel: I do\nDo you offer her your heart, mind, body and spirit? \nNathaniel: I Do \nDo you offer her your friendship and your loving care - honouring her growth and freedom as your own, cherishing and respecting her, listening to her, loving and embracing her in times of adversity and in times of joy?\nNathaniel: I Do. \nJoanne,   Do you take Nathaniel to be your husband?\nJoanne : I Do\nDo you offer him your heart, mind, body and spirit? :\nJoanne:  I Do\nDo you offer him your friendship and your loving care - honouring his growth and freedom as your own, cherishing and respecting him, listening to him, loving and embracing him in times of adversity and in times of joy?\nJoanne: I do\nVows\n\nNathaniel  please repeat after me…..\n\nToday I will marry my friend,\nThe one I will live with, ….. dream with and love.\nJoanne  I take you to be my wife.\nFrom this day forward ….. I will cherish you,\nI will look with joy …. down the path of our tomorrow’s\nKnowing we will walk it together … side by side,\nhand in hand …. and heart to heart.\n\nJoanne  please repeat after me…\n\nToday I will marry my friend,\nThe one I will live with, ….. dream with and love.\nNathaniel  I take you to be my husband.\nFrom this day forward ….. I will cherish you,\nI will look with joy …. down the path of our tomorrow’s\nKnowing we will walk it together … side by side,\nhand in hand …. and heart to heart.\nExchanging of Rings.\nThese rings represent circles of love and circles of life.  And as it is with the great mysteries of life, they have no beginning and no end.  \nWe wish for you never ending love and may your life together be strong and radiant. May these rings join you with one another in happiness, joy and fulfilment. May these rings join you with one another in honesty, trust and commitment. May these rings join you with one another in encircling and never-ending love. As you wear them, may they always remind you of this precious moment.\n\nNathaniel, take the ring, put it on Joanne’s finger and repeat after me.\n\nJoanne, with this ring, I commit my heart and my soul to you.   I promise to remain faithful and love you all the days of my life.\n\nJoanne,  take the ring, put it on Nathaniel’s finger and repeat after me.\n\nNathaniel…..With this ring, I commit my heart and my soul to you.   I promise to remain faithful and love you all the days of my life.\n\n\nPronouncement\n\nJoanne and Nathaniel \n\nToday have promised before your family and friends, to give wholly and freely to one another, and to love one another according to your vows,\nIt is my honour and delight to pronounce you husband and wife.\nYou may kiss the Bride.\nCongratulations \n\nMusical interlude here\n\nLight Marriage candle and the Signing of the Register, \nWitnesses Margaret and Jamie.\nClosing words\nThese two have found each other, and their destinies will now be woven into one and their joys shall not be known apart.   May the nourishment of the earth be yours, May the clarity of the light be yours, May the fluency of the ocean be yours, May the protection of the ancestors be yours. And so may a slow wind …. work these words of love …… around you, Like an invisible cloak ……To mind your lives now & forever………..\n\nBride and Groom then walk down the aisle together as a married couple End song Ceremony by New Order"
  },
  {
    "objectID": "notes/certain_things/Wedding/Three things.html",
    "href": "notes/certain_things/Wedding/Three things.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "I think the three things should add detail that evokes concrete imagery and raises an element of depth unexplored - hinting at a tapestry of connections between aspects of her character that only we share….\n\nPassion\nCourage\nJoy"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#preliminaries",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#preliminaries",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Preliminaries",
    "text": "Preliminaries\n\n\n\n\n\n\nI am not an Economist\n\n\n\nI’m a data scientist at Personio - where we work on cool problems ranging across themes of:\n\nrevenue optimisation\ncustomer churn\nexperimentation\nsurvey design\nproduct analytics\n\nBayesian statistician, reformed philosopher and logician.\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\n\nDisclaimer\n\n\nNone of Personio’s data was used in this presentation\n\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\nThe worked examples used here can be found here"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#agenda",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#agenda",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Agenda",
    "text": "Agenda\n\n\nHistory and Conceptual Background\n\n\n\n\nA Naive Utilty Model\n\n\n\n\nAn Augmented Model\n\n\n\n\nAdding Correlation Structure\n\n\n\n\nCounterfactual Questions\n\n\n\n\nIndividual Heterogenous Utility\n\n\n\n\nConclusion\n\nThe World in the Model"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#mcfadden-and-bart",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#mcfadden-and-bart",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "McFadden and BART",
    "text": "McFadden and BART\n\n\n\n\n“Transport projects involve sinking money in expensive capital investments, which have a long life and wide repercussions. There is no escape from the attempt both to estimate the demand for their services over twenty or thirty years and to assess their repercussions on the economy as a whole.” - Denys Munby, Transport, 1968 ”\n\n\n\n\n\n\n\nBay Area Rapid Transit\n\n\n\n\n\nDublin Metrolink"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#revealed-preference-and-predicting-demand",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#revealed-preference-and-predicting-demand",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Revealed Preference and Predicting Demand",
    "text": "Revealed Preference and Predicting Demand\nSelf Centred Utility Maximisers?\n\n\n\n\nThe assumption of revealed preference theory is that if a person chooses A over B then their latent subjective utility for A is greater than for B.\n\n\n\n\nSurvey data estimated about 15% of users would adopt the newly introduced BART system. McFadden’s random utility model estimated 6%.\n\n\n\n\nHe was right.\n\n\n\n\n\n\nCopernican Shift: He estimated utility to predict choice, rather than infer utility from stated choice."
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#general-applicability-of-choice-problems",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#general-applicability-of-choice-problems",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "General Applicability of Choice Problems",
    "text": "General Applicability of Choice Problems\n\n\nThese models offer the possibility of predicting choice in diverse domains: policy, brand, school, car and partners.\n\n\n\n\nQuestion: What are the attributes that drive these choices? How well are they measurable?\n\n\n\n\nQuestion: How do changes in these attributes influence the predicted market demand for these choices?"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#note-on-model-evaluation",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#note-on-model-evaluation",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Note on Model Evaluation",
    "text": "Note on Model Evaluation\nReplicating the Super Soldier Program"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#note-on-model-evaluation-1",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#note-on-model-evaluation-1",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Note on Model Evaluation",
    "text": "Note on Model Evaluation\nReplicating the Super Soldier Program\n\n\n\n\n\n\n\n\n\n\nBayesian Models aim to replicate the DGP holistically"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-the-data",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-the-data",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Choice: The Data",
    "text": "Choice: The Data\nGas Central Heating and Electrical Central Heating described by their cost of installation and operation.\n\n\n\nchoice_id\nchosen\nic_gc\noc_gc\n…\noc_ec\n\n\n\n\n1\ngc\n866\n200\n…\n542\n\n\n2\nec\n802\n195\n…\n510\n\n\n3\ner\n759\n203\n…\n495\n\n\n4\ngr\n789\n220\n…\n502"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-a-naive-model",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-a-naive-model",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Choice: A Naive Model",
    "text": "Choice: A Naive Model\nUnderspecified Utilities\nLet there be five goods described by their cost of installation and operation.\n\\[ \\begin{split} \\overbrace{\\begin{pmatrix}\n\\color{green}{u_{gc}}   \\\\\n\\color{green}{u_{gr}}   \\\\\n\\color{green}{u_{ec}}   \\\\\n\\color{green}{u_{er}}   \\\\\n\\color{green}{u_{hp}}   \\\\\n\\end{pmatrix}}^{utility} =  \\begin{pmatrix}\ngc_{ic} & gc_{oc}  \\\\\ngr_{ic} & gr_{oc}  \\\\\nec_{ic} & ec_{oc}  \\\\\ner_{ic} & er_{oc}  \\\\\nhp_{ic} & hp_{oc}  \\\\\n\\end{pmatrix} \\overbrace{\\begin{pmatrix}\n\\color{blue}{\\beta_{ic}}   \\\\\n\\color{blue}{\\beta_{oc}}   \\\\\n\\end{pmatrix}}^{parameters}  \\end{split}\n\\]"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-a-naive-model-1",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-a-naive-model-1",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Choice: A Naive Model",
    "text": "Choice: A Naive Model\nThe utility calculation is fundamentally comparative. \\[ \\begin{split} \\begin{pmatrix}\n\\color{green}{u_{gc}}   \\\\\n\\color{green}{u_{gr}}   \\\\\n\\color{green}{u_{ec}}   \\\\\n\\color{green}{u_{er}}   \\\\\n\\color{red}{\\overbrace{0}^{\\text{outside good}}}   \\\\\n\\end{pmatrix} =  \\begin{pmatrix}\ngc_{ic} & gc_{oc}  \\\\\ngr_{ic} & gr_{oc}  \\\\\nec_{ic} & ec_{oc}  \\\\\ner_{ic} & er_{oc}  \\\\\n\\color{red}{0} & \\color{red}{0} \\\\\n\\end{pmatrix} \\begin{pmatrix}\n\\color{blue}{\\beta_{ic}}   \\\\\n\\color{blue}{\\beta_{oc}}   \\\\\n\\end{pmatrix}  \\end{split}\n\\]\nWe zero out one category in the data set to represent the “outside good” for comparison. Similar to dummy variables in Regression, this is required for the model to be identified."
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-a-naive-model-2",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-a-naive-model-2",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Choice: A Naive Model",
    "text": "Choice: A Naive Model\nUtility determines choice probability of choice:\n\\[\\text{softmax}(\\color{green}{u})_{j} = \\frac{\\exp(\\color{green}{u_{j}})}{\\sum_{q=1}^{J}\\exp(\\color{green}{u_{q}})}\\]\nchoices determine market share where:\n\\[ s_{j}(\\mathbf{\\color{blue}{\\beta}}) = P(\\color{green}{u_{j}} > \\color{green}{u_{k}}; ∀k ̸= j) \\]"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-estimation",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-estimation",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Choice: Estimation",
    "text": "Choice: Estimation\nThe model is traditionally estimated with maximum likelihood caclulations.\n\\[  L(\\color{blue}{\\beta}) = \\prod s_{j}(\\mathbf{\\color{blue}{\\beta}}) \\]\nor taking the log:\n\\[  l(\\color{blue}{\\beta}) = \\sum log(s_{j}(\\mathbf{\\color{blue}{\\beta}})) \\] \\[ \\text{ We find: } \\underset{\\color{blue}{\\beta}}{\\mathrm{argmax}} \\text{ } l(\\color{blue}{\\beta}) \\]\nResults are often brittle!"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-bayesian-estimation",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#choice-bayesian-estimation",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Choice: Bayesian Estimation",
    "text": "Choice: Bayesian Estimation\nTo evaluate the integrals in the Bayesian model we use MCMC to estimate conditional probabilities of the joint distribution.\n\\[\\underbrace{\\color{blue}{\\beta}}_{\\text{prior draws}} \\sim Normal(0, 1) \\]\n\\[ \\underbrace{p(\\color{blue}{\\beta} | D)}_{\\text{posterior draws}} = \\frac{p(\\mathbb{\\color{blue}{\\beta}})p(D | \\color{blue}{\\beta} )}{\\int_{i}^{n} p(D | \\mathbf{\\color{blue}{\\beta_{i}}})p(\\mathbf{\\color{blue}{\\beta_{i}}}) } \\]\nPriors can be used flexibly regularise and improve reliability of estimation across structural causal models."
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#the-naive-model-in-code",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#the-naive-model-in-code",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "The Naive Model in Code",
    "text": "The Naive Model in Code\nwith pm.Model(coords=coords) as model_1:\n    ## Priors for the Beta Coefficients\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n\n    ## Construct Utility matrix and Pivot\n    u0 = beta_ic * wide_heating_df[\"ic.ec\"] + beta_oc * wide_heating_df[\"oc.ec\"]\n    u1 = beta_ic * wide_heating_df[\"ic.er\"] + beta_oc * wide_heating_df[\"oc.er\"]\n    u2 = beta_ic * wide_heating_df[\"ic.gc\"] + beta_oc * wide_heating_df[\"oc.gc\"]\n    u3 = beta_ic * wide_heating_df[\"ic.gr\"] + beta_oc * wide_heating_df[\"oc.gr\"]\n    u4 = np.zeros(N)  # Outside Good\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    ## Apply Softmax Transform\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n\n    ## Likelihood\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#interpreting-the-model-coefficients",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#interpreting-the-model-coefficients",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Interpreting the Model Coefficients",
    "text": "Interpreting the Model Coefficients\nRate of Substitution\n\n\nThe beta coefficients in the model are interpreted as weights of utility. However, the precision in these latent terms is relative to the variance of unobserved factors.\nThe utility scale is not fixed, but the ratio \\(\\frac{\\beta_{ic}}{\\beta_{oc}}\\) is invariant.\n\n\n\n\nRate of Substitution"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#model-posterior-predictive-fits",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#model-posterior-predictive-fits",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Model Posterior Predictive Fits",
    "text": "Model Posterior Predictive Fits\nThe model fit fails to recapture the observed data points\n\nModel Fit"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#augmenting-the-model-1",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#augmenting-the-model-1",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Augmenting the Model:",
    "text": "Augmenting the Model:\nProduct Specific Intercepts\n\\[\n\\begin{split}\n\\begin{pmatrix}\n\\color{red}{u_{gc}} \\\\\n\\color{purple}{u_{gr}} \\\\\n\\color{orange}{u_{ec}} \\\\\n\\color{teal}{u_{er}} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  \\color{red}{\\alpha_{gc}} + \\color{blue}{\\beta_{ic}}gc_{ic} + \\color{blue}{\\beta_{oc}}gc_{oc} \\\\\n  \\color{purple}{\\alpha_{gr}} + \\color{blue}{\\beta_{ic}}gr_{ic} + \\color{blue}{\\beta_{oc}}gr_{oc}  \\\\\n  \\color{orange}{\\alpha_{ec}} + \\color{blue}{\\beta_{ic}}ec_{ic} + \\color{blue}{\\beta_{oc}}ec_{oc}  \\\\\n  \\color{teal}{\\alpha_{er}} + \\color{blue}{\\beta_{ic}}er_{ic} + \\color{blue}{\\beta_{oc}}er_{oc}  \\\\\n   0 + 0 + 0\n\\end{pmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#augmenting-the-model-2",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#augmenting-the-model-2",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Augmenting the Model:",
    "text": "Augmenting the Model:\nwith pm.Model(coords=coords) as model_2:\n    ## Priors for the Beta Coefficients\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n    alphas = pm.Normal(\"alpha\", 0, 1, dims=\"alts_intercepts\")\n\n    ## Construct Utility matrix and Pivot using an intercept per alternative\n    u0 = alphas[0] + beta_ic * wide_heating_df[\"ic.ec\"] + beta_oc * wide_heating_df[\"oc.ec\"]\n    u1 = alphas[1] + beta_ic * wide_heating_df[\"ic.er\"] + beta_oc * wide_heating_df[\"oc.er\"]\n    u2 = alphas[2] + beta_ic * wide_heating_df[\"ic.gc\"] + beta_oc * wide_heating_df[\"oc.gc\"]\n    u3 = alphas[3] + beta_ic * wide_heating_df[\"ic.gr\"] + beta_oc * wide_heating_df[\"oc.gr\"]\n    u4 = np.zeros(N)  # Outside Good\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    ## Apply Softmax Transform\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n\n    ## Likelihood\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#augmenting-the-model-3",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#augmenting-the-model-3",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Augmenting the Model:",
    "text": "Augmenting the Model:\n\nModel Structure"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#augmenting-the-model-4",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#augmenting-the-model-4",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Augmenting the Model",
    "text": "Augmenting the Model\nPosterior Predictions\n\nModel Fit"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#independence-of-irrelevant-alternatives",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#independence-of-irrelevant-alternatives",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Independence of Irrelevant Alternatives",
    "text": "Independence of Irrelevant Alternatives\nNew Products Cannibalise Equally from all Alternatives\n\nSuppose a market choice between transport modes is determined by the above model.\nRed Bus or Car are you initial Options. Assume \\(s_{\\color{red}{bus}}(\\beta) = s_{car}(\\beta)\\). Market Share is 50% to each option.\nIntroduce the Blue Bus Option, then the Independent characteristics of the utility specification implies that \\(s_{\\color{red}{bus}}(\\beta) = s_{car}(\\beta) = s_{\\color{blue}{bus}}(\\beta)\\)\nThis implies an implausible substitution pattern for real markets.1\n\nWhat kind of Monster have we Created!?"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#adding-correlation-structure-1",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#adding-correlation-structure-1",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\nDependence in Market Share\n\\[ \\alpha_{i} \\sim Normal(\\mathbf{0}, \\color{brown}{\\Gamma}) \\]\n\\[\n\\begin{split}\n\\begin{pmatrix}\n\\color{red}{u_{gc}} \\\\\n\\color{purple}{u_{gr}} \\\\\n\\color{orange}{u_{ec}} \\\\\n\\color{teal}{u_{er}} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  \\color{red}{\\alpha_{gc}} + \\color{blue}{\\beta_{ic}}gc_{ic} + \\color{blue}{\\beta_{oc}}gc_{oc} \\\\\n  \\color{purple}{\\alpha_{gr}} + \\color{blue}{\\beta_{ic}}gr_{ic} + \\color{blue}{\\beta_{oc}}gr_{oc}  \\\\\n  \\color{orange}{\\alpha_{ec}} + \\color{blue}{\\beta_{ic}}ec_{ic} + \\color{blue}{\\beta_{oc}}ec_{oc}  \\\\\n  \\color{teal}{\\alpha_{er}} + \\color{blue}{\\beta_{ic}}er_{ic} + \\color{blue}{\\beta_{oc}}er_{oc}  \\\\\n   0 + 0 + 0\n\\end{pmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#adding-correlation-structure-2",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#adding-correlation-structure-2",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\nPriors on Parameters determine Market Structure\n\\[ \\begin{split} \\color{brown}{\\Gamma} =\n\\begin{pmatrix}\n  \\color{red}{1} , \\gamma , \\gamma , \\gamma \\\\\n  \\gamma , \\color{blue}{1} , \\gamma , \\gamma  \\\\\n   \\gamma , \\gamma  , \\color{orange}{1} , \\gamma \\\\\n  \\gamma , \\gamma , \\gamma , \\color{teal}{1}  \n\\end{pmatrix}\n\\end{split} \\]"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#adding-correlation-structure-3",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#adding-correlation-structure-3",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\nwith pm.Model(coords=coords) as model_3:\n    beta_ic = pm.Normal(\"beta_ic\", 0, 1)\n    beta_oc = pm.Normal(\"beta_oc\", 0, 1)\n\n    beta_income = pm.Normal(\"beta_income\", 0, 1 dims=\"alts_intercepts\")\n\n    chol, corr, stds = pm.LKJCholeskyCov(\n        \"chol\", n=4, eta=2.0, \n        sd_dist=pm.Exponential.dist(1.0, shape=4)\n    )\n    alphas = pm.MvNormal(\"alpha\", mu=0, chol=chol, dims=\"alts_intercepts\")\n\n    u0 = (\n        alphas[0]\n        + beta_ic * wide_heating_df[\"ic.gc\"]\n        + beta_oc * wide_heating_df[\"oc.gc\"]\n        + beta_income[0] * wide_heating_df[\"income\"]\n    )\n    u1 = (\n        alphas[1]\n        + beta_ic * wide_heating_df[\"ic.gc\"]\n        + beta_oc * wide_heating_df[\"oc.gc\"]\n        + beta_income[1] * wide_heating_df[\"income\"]\n    )\n    u2 = (\n        alphas[2]\n        + beta_ic * wide_heating_df[\"ic.gc\"]\n        + beta_oc * wide_heating_df[\"oc.gc\"]\n        + beta_income[2] * wide_heating_df[\"income\"]\n    )\n    u3 = (\n        alphas[3]\n        + beta_ic * wide_heating_df[\"ic.gr\"]\n        + beta_oc * wide_heating_df[\"oc.gr\"]\n        + beta_income[3] * wide_heating_df[\"income\"]\n    )\n    u4 = np.zeros(N)  # pivot\n    s = pm.math.stack([u0, u1, u2, u3, u4]).T\n\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#adding-correlation-structure-4",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#adding-correlation-structure-4",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Adding Correlation Structure",
    "text": "Adding Correlation Structure\n\nCorrelation Structure"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#counterfactual-reasoning",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#counterfactual-reasoning",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Counterfactual Reasoning",
    "text": "Counterfactual Reasoning\nFrom Probability to Causation\n\n“[C]ontrary to the views of a number of pessimistic statisticians and philosophers you can get from probabilities to causes after all. Not always, not even ussually - but in just the right circumstances and with just the right kind of starting information, it is in principle possible.” - Nancy Cartwright in Nature’s Capacities and their Measurement\n\n\n“One of the functions of theoretical economics is to provide fully articulated artificial economic systems that can serve as laboratories in which policies that would be prohibitively expensive to experiment with in actual economies can be tested out at a much lower cost” - Mary Morgan quoted in Hunting Causes and Using Them"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#counterfactual-reasoning-1",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#counterfactual-reasoning-1",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Counterfactual Reasoning",
    "text": "Counterfactual Reasoning\nCeteris Paribus Laws\nWith a fitted PyMC model we can counterfactually reset the values for the input data and regenerate the posterior predictive distribution holding else equal in the data generating process.\n\nWhat would the market share be like if prices for electrical systems increased 20%?\n\n # update values of predictors with new 20% \n # price increase in operating costs for electrical options\nwith model_3:\n    pm.set_data({\"oc_ec\": wide_heating_df[\"oc.ec\"] * 1.2, \n                 \"oc_er\": wide_heating_df[\"oc.er\"] * 1.2})\n    # use the updated values and predict outcomes and probabilities:\n    idata_new_policy = pm.sample_posterior_predictive(\n        idata_m3,\n        var_names=[\"p\", \"y_cat\"],\n        return_inferencedata=True,\n        predictions=True,\n        extend_inferencedata=False,\n        random_seed=100,\n    )\n\nidata_new_policy"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#counterfactual-reasoning-2",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#counterfactual-reasoning-2",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Counterfactual Reasoning",
    "text": "Counterfactual Reasoning\n\nCounterfactual Shares"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#counterfactual-reasoning-3",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#counterfactual-reasoning-3",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Counterfactual Reasoning",
    "text": "Counterfactual Reasoning\nInterventions and Conditionalisation\n\n\n\nThere is a sharp distinction between conditional probability distributions and probability under intervention\nIn PyMC you can implement the do-operator to intervene on the graph that represents your data generating process."
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#individual-heterogenous-utility",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#individual-heterogenous-utility",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\nRepeated Choice and Hierarchical Structure\n\n\n\nperson_id\nchoice_id\nchosen\nnabisco_price\nkeebler_price\n\n\n\n\n1\n1\nnabisco\n3.40\n2.00\n\n\n1\n2\nnabisco\n3.45\n2.50\n\n\n1\n3\nkeebler\n3.60\n2.70\n\n\n2\n1\nkeebler\n3.48\n2.20\n\n\n2\n2\nkeebler\n3.30\n2.25"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#individual-heterogenous-utility-1",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#individual-heterogenous-utility-1",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\n\\[\n\\begin{split}\n\\begin{pmatrix}\n\\color{red}{u_{i, nb}} \\\\\n\\color{purple}{u_{i, kb}} \\\\\n\\color{orange}{u_{i, sun}} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n  (\\color{red}{\\alpha_{nb}} + \\beta_{i}) + \\color{blue}{\\beta_{p}}p_{nb} + \\color{green}{\\beta_{disp}}d_{nb} \\\\\n  (\\color{purple}{\\alpha_{kb}} + \\beta_{i}) +  \\color{blue}{\\beta_{p}}p_{kb} + \\color{green}{\\beta_{disp}}d_{kb}  \\\\\n  (\\color{orange}{\\alpha_{sun}}  + \\beta_{i})  + \\color{blue}{\\beta_{p}}p_{sun} + \\color{green}{\\beta_{disp}}d_{sun}  \\\\\n   0 + 0 + 0\n\\end{pmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#individual-heterogenous-utility-2",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#individual-heterogenous-utility-2",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\nIn Code\n\nwith pm.Model(coords=coords) as model_4:\n    beta_feat = pm.TruncatedNormal(\"beta_feat\", 0, 1, upper=10, lower=0)\n    beta_disp = pm.TruncatedNormal(\"beta_disp\", 0, 1, upper=10, lower=0)\n    ## Stronger Prior on Price to ensure \n    ## an increase in price negatively impacts utility\n    beta_price = pm.TruncatedNormal(\"beta_price\", 0, 1, upper=0, lower=-10)\n    alphas = pm.Normal(\"alpha\", 0, 1, dims=\"alts_intercepts\")\n    beta_individual = pm.Normal(\"beta_individual\", 0, 0.05,\n     dims=(\"individuals\", \"alts_intercepts\"))\n\n    u0 = (\n        (alphas[0] + beta_individual[person_indx, 0])\n        + beta_disp * c_df[\"disp.sunshine\"]\n        + beta_feat * c_df[\"feat.sunshine\"]\n        + beta_price * c_df[\"price.sunshine\"]\n    )\n    u1 = (\n        (alphas[1] + beta_individual[person_indx, 1])\n        + beta_disp * c_df[\"disp.keebler\"]\n        + beta_feat * c_df[\"feat.keebler\"]\n        + beta_price * c_df[\"price.keebler\"]\n    )\n    u2 = (\n        (alphas[2] + beta_individual[person_indx, 2])\n        + beta_disp * c_df[\"disp.nabisco\"]\n        + beta_feat * c_df[\"feat.nabisco\"]\n        + beta_price * c_df[\"price.nabisco\"]\n    )\n    u3 = np.zeros(N)  # Outside Good\n    s = pm.math.stack([u0, u1, u2, u3]).T\n    # Reconstruct the total data\n\n    ## Apply Softmax Transform\n    p_ = pm.Deterministic(\"p\", pm.math.softmax(s, axis=1), dims=(\"obs\", \"alts_probs\"))\n\n    ## Likelihood\n    choice_obs = pm.Categorical(\"y_cat\", p=p_, observed=observed, dims=\"obs\")"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#individual-heterogenous-utility-3",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#individual-heterogenous-utility-3",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\nRecovered Posterior Predictive Distribution"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#individual-heterogenous-utility-4",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#individual-heterogenous-utility-4",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Individual Heterogenous Utility",
    "text": "Individual Heterogenous Utility\nIndividual Preference\n\n\n\n\n\nIndividual preferences can be derived from the model in this manner.\nThe relationship between preferences over the product offering can be seen too"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html#conclusion",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html#conclusion",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "Conclusion",
    "text": "Conclusion\nThe World in the Model\nWe’ve seen a series of models, each one expanding on the last.\n\nModels should articulate the relevant structure of the world.\nThey serve as microscopes. Simulation systems are tools to interrogate reality.\nBayesian Conditionalisation calibrates the system against the observed facts.\nBayesian Discrete choice models help us interrogate aspects of actors and their motivations under uncertainty.\nPyMC enables us to easily build and experiment with those models.\nCausal inference is plausible to degree that we can defend the structural assumptions. Bayesian models enforce tranparency and justification of structural commitments.\n\n\n“Models… [are] like sonnets for the poet, [a] means to express accounts of life in exact, short form using languages that may easily abstract or analogise, and involve imaginative choices and even a certain degree of playfulness in expression” - Mary Morgan in The World in the Model\n\n\n\n\nDiscrete Choice with PyMC"
  },
  {
    "objectID": "talks/bayesian_mixer/bayesian_mixer_london.html",
    "href": "talks/bayesian_mixer/bayesian_mixer_london.html",
    "title": "Model Evaluation and Discrete Choice Scenarios",
    "section": "",
    "text": "I am not an Economist\n\n\n\n\nI’m a data scientist at Personio - where we work on cool problems ranging across themes of:\n\nrevenue optimisation\ncustomer churn\nexperimentation\nsurvey design\nproduct analytics\n\nBayesian statistician, reformed philosopher and logician.\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\nDisclaimer\n\n\n\nNone of Personio’s data was used in this presentation\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\n\nThe worked examples used here can be found here"
  },
  {
    "objectID": "notes/certain_things/Wedding/Three things.html#razor-sharp",
    "href": "notes/certain_things/Wedding/Three things.html#razor-sharp",
    "title": "Examined Algorithms",
    "section": "Razor Sharp",
    "text": "Razor Sharp\nMind like a diamond, and her tongue is just as sharp.\n\nJoy\nThe cascade of smiles rising across her cheeks when watching Wyatt burgeoning pride of discovering the next new thing."
  },
  {
    "objectID": "notes/certain_things/Public/Statistics/Analogies between Missing Data and Causal Inference.html",
    "href": "notes/certain_things/Public/Statistics/Analogies between Missing Data and Causal Inference.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "The clearest framework for causal inference has a tight relationship with #missing-data imputation. However, the range of problems addressed seem to require a slew of distinct estimators. It is not always clear in which circumstances we should apply each estimation procedure.\nLike most statistical work the practical details should be worked out in code for the clearest demonstration. However, here we’ll describe (as best we can) the conceptual underpinning of important estimators. Their usage and motivation."
  },
  {
    "objectID": "notes/certain_things/Public/Statistics/Analogies between Missing Data and Causal Inference.html#taxonomies-of-missing-ness",
    "href": "notes/certain_things/Public/Statistics/Analogies between Missing Data and Causal Inference.html#taxonomies-of-missing-ness",
    "title": "Examined Algorithms",
    "section": "Taxonomies of Missing-ness",
    "text": "Taxonomies of Missing-ness\nCausal inference can be seen as a species of missing data problem where the missing data is the counterfactual situation(s) of how the world would have been were the course of the world different from the one we know. What if we used this treatment plan rather than another? What it the actors’ behaviour differed from the actions they in fact pursued?\nFrom an estimation perspective there are different species of missing-ness that matter. We won’t here go deep into the distinctions. It is enough to note that they vary in the operative source of the missing-ness: Missing-completely-at-random (MCAR), missing-at-random (MAR) and missing-not-at-random (MNAR).\n\nThe Stable Outcomes Assumption\nMechanically we can’t work with null values under any of these assumptions. The stable outcomes model is a first step procedure for imputing the missing values. Whether we choose mean imputation or an arbitrary figure - we initially assume missing values at the individual level in a stable fashion by specifying a constant value for the missing cases.\nVery crudely, estimation procedures work reasonably well under (MCAR) but require extra effort when there is assumptions if we hope to account for the (MAR) and (MNAR) cases. Under MAR we are assuming that the values are missing as a function of the observable covariates and can be imputed under proper conditionalisation.\nImputation under MAR and MCAR succeeds largely from successive applications of the law of iterated expectations. In this case the stable outcome assumption encodes the missing data as \\(-99\\) and we then average over the joint distribution of the stable outcomes model and the missingness data.\n\nThe various estimation procedures for counterfactual results trade on this property of expectation that allow for point identification of the expected value for the outcome variable.\nBut we also want to consider individual variation due to the observed covariate profiles. When we consider cases of missing data conditional the observed covariate profile, we can derive a propensity score as a one number summary for the conditional probability of missing-ness. This can be used in imputation techniques where we want to carefully attribute values to the missing data that respect the other observed properties of the individual.\n\n\nPropensity Scores\nThe propensity score has a role in a number of re-weighting schemes for the estimation of missing data. These rely on the property of expectation under (MAR). So accuracy of the propensity score is itself an important question. Because the missing-ness variable \\(R\\) is a binary random variable in \\(\\{ 0, 1 \\}\\) maximum likelihood methods for logistic regression are often used to estimate these terms.\n\\[p_{R}(\\mathbf{x}) \\sim logit(X_{i} \\beta )\\]\nThis score is a summary in some sense of the factors driving missing-ness. In the context of causal inference it is often stated “in reverse” as a probability of being treated. We will keep focus here on the case of missing data, but the generality of the notion shouldn’t be lost. The propensity score is a one number summary of the covariate profile for each individual in the data. Under the (MAR) assumption it is often sufficient to conditionalise on the propensity score for each individual for imputation of their missing data values.\n\n\nRegression Estimators\nWe might want to simply estimate the missing values of our outcome using the conditional expectation function (CEF) property of simple regression. The imputation pattern will work well when the linear properties of the regression model are a good fit for the relationship between the outcome variables and the observed covariates. Hence the estimate for:\n\\[E[Y_{i}] = \\beta_{0} + \\beta_{1}\\cdot X_{1i} ... \\beta_{n} \\cdot X_{ni} \\] where we replace all values to be prediction of our regression model for each individual and then average the predictions.\n\n\nWeighting Estimators\nAnother approach to missing data imputation, which relies on the expectation properties of our outcome variable of interest under (MAR) and the stable outcome model, is the inverse probability weighting approach to imputation.\n\\[ E[Y_{i}] = E[\\dfrac{(YR + (-99) (1-R ))R }{p_{R}(\\mathbf{x})}]\\] With this property we can express estimates of missing values as a function of the individuals’ observed data. There are variations on this theme but sophisticated imputation schemes all rely on functions of the individual’s observed covariate profile. This specificity is important too in the context heterogenous treatment effects in causal inference."
  },
  {
    "objectID": "notes/certain_things/Public/Statistics/Analogies between Missing Data and Causal Inference.html#causal-inference",
    "href": "notes/certain_things/Public/Statistics/Analogies between Missing Data and Causal Inference.html#causal-inference",
    "title": "Examined Algorithms",
    "section": "Causal Inference",
    "text": "Causal Inference\nThe above perspective on missing data has deep analogies with the potential outcomes framework of causal inference. This is well brought out in the beautiful book Foundations of Agnostic Statistics by Aronow and Miller.\n\nPotential Outcomes and SUTVA\nIn the causal context we assume the potential outcomes framework and the notation of \\(Y(1), Y(0)\\) to denote the value of the outcome under the treatment regime \\(D\\).\n\\[ Y_{i} =  \n\\left\\{\\begin{array}{lr}\n        Y_{i}(0), & \\text{for } D = 0\\\\\n        Y_{i}(1), & \\text{for } D = 1\\\\\n        \\end{array}\\right\\}\n\\] where the (S)table (U)nit (T)reatment (V)alue (A)ssumption holds i.e. the observed data under treatment or non-treatment regimes is the potential outcome for that individual. Additionally the counterfactual outcome is assumed to be stable for each individual. It is crucially this assumption that allows for statistical identification of key metrics in causal inference under randomisation.\n\n\nAverage Treatment Effects\nSimilarly, here we rely on the properties of expectation over the observed data to isolate quantities of causal effect. In particular we tend to be interested in the average treatment effects, which we can get by using the following decomposition under random assignment.\n\\[ E[\\tau] = E[Y_{i}(1) - Y_{i}(0)] = E[Y_{i}(1)] - E[Y_{i}(0)] \\] This decomposition is crucial since it allows us to move between the expectations derived from the observed data under each regime towards an estimate of the population treatment effects.\n\n\n\nsubject\n\\(Y_{i}(1)\\)\n\\(Y_{i}(0)\\)\n\\(\\tau\\)\n\n\n\n\nJoe\n?\n115\n?\n\n\nBob\n120\n?\n?\n\n\nJames\n100\n?\n?\n\n\nMary\n115\n?\n?\n\n\nSally\n120\n?\n?\n\n\nLaila\n?\n105\n?\n\n\n\\(E[Y_{i}(D)]\\)\n113.75\n110\n3.75\n\n\n\nThe missing values in this table depict the fundamental problem of causal inference as a missing data issue. So #causal-inference as a strategy is broadly related to finding ways to solve this missing data problem under different regimes of missing-ness. For instance, the reason A/B testing works to isolate the treatment effects is that under randomisation of treatment regime we are implicitly assuming that the reason missing-data is effectively a case of MCAR missing-ness. As such the expectations of the individual columns in the above table are valid estimates which can then be combined using the above decomposition to derived the treatment effect \\(\\tau\\).\nIn this case the pattern of reasoning is akin to performing mean-imputation and then taking the difference of the averages. The imputation step is redundant in A/B testing, but it is highlighted by Aronow and Miller as a useful lens on more complex causal inference tasks on observed data. We are always (under the hood) trying to impute the missing values to gain a better view of the treatment effect distribution. ### Regression Estimators\nAgain we rely on the idea of regression as an approximation to the CEF of the data generating process. The flexibility of regression modelling for automating a host of statistical test should be reasonably familiar. The point here is not to rehash the theory but just to note the similarity with the procedures used above for regression-based imputation. Regression modelling of the treatment effect proceeds on the strong ignorability assumption that - conditional on the observed covariates knowing whether or not an individual received the treatment adds no new information i.e. it is the insistence that assignment might as well be random after accounting for the background characteristics. These assumptions mirror the conditions required for imputation under the MAR regime.\nSo we can derive estimates for the ATE from the data generating model\n\\[ Y \\sim \\beta_0 + \\beta_1 D + ... \\beta_{n} \\cdot X_{n} \\] such that out quantity of interest \\(\\tau\\) is cleanly identified in expectation by the quantity: \\[ E[\\tau] = \\beta_{1}\\] But this result can also be derived by predicting the outcomes under the different treatment regimes, using a fitted regression model, and taking the differences of the averaged predictions over the cases. The equivalence between these perspectives is the insight we want to record here. We drew out this connection in the discussion of poststratification estimators\n\nThis is a neat and beautiful connection between causal-inference and missing data analysis. Simultaneously a reminder of the versatility of regression analysis.\n\n\nPropensity Functions and Reweighting Estimators.\nWe will skip the detailed elaboration of propensity score matching, a technique for creating pseudo treatment and control groups, only noting that there is a rich and detailed literature on the topic for causal inference.\nWe do want to draw out how propensity-scores can be used in the class of reweighting estimators. Where under the strong ignorability assumption we can estimate the treatment effect as a simple expectation:\n\\[E[\\tau] = E [\\dfrac{YD}{p_D(X)} - \\dfrac{Y(1-D)}{(1 - p_D (X))}] \\]\nUsing this formula we can scale each observation by the relative probabilities for the individual falling into each treatment regime. Then the expectation of the scaled differences is an estimate of our ATE. The logic of this inverse probability weighting (IPW) estimator stems from the idea that low propensity individuals are likely underrepresented in the treatment group and over represented in the control. So this estimator down weights and unweights each option accordingly to “balance” the groups before comparison.\nThis balancing operation can work but is dependent on empirical properties of the sample data. Even if the data generating process ensures that strong ignorability holds, if our sample under represents the variety of possible individual in each group then reweighting the remaining individuals is no guarantee for sound inference. This is a small sample problem recurring."
  },
  {
    "objectID": "notes/certain_things/Public/Statistics/Analogies between Missing Data and Causal Inference.html#conclusion",
    "href": "notes/certain_things/Public/Statistics/Analogies between Missing Data and Causal Inference.html#conclusion",
    "title": "Examined Algorithms",
    "section": "Conclusion",
    "text": "Conclusion\nThe sequence of complexity in missing data imputation is as follows:\n\\[ MCAR \\Rightarrow MAR \\Rightarrow MNAR \\]\nwhich mirrors the complexity of cases in causal inference. Here we have:\n\\[ Ignorability \\Rightarrow \\text{Strong Ignorability} \\Rightarrow \\text{Non Ignorability} \\] As we consider circumstances moving up the hierarchy, we require an increase in assumptions or structural commitments to offset the risk of non-identifiability bringing us back down the hierarchy. The emphasis in the book stresses how properties of good experimental design can help recover sound inference by enforcing MAR conditions in MNAR circumstances. But the crucial role of modelling in defending the strong ignorability condition is underplayed.\nYes, we need to justify our estimator but also our model! Are we including the right covariates? Have we an appropriate covariance structure? What is the functional form and why is it reasonable? Are we accounting for heterogeneity of outcome? All such questions centre the importance of domain knowledge for causal inference. This is not a criticism of boon focused on Agnostic statistics. Their focus is appropriately on the design aspects that enable inference. However it should be abundantly clear that you cannot get away with agnostic approaches in the real world. There is no way to justify stepping back down the hierarchy without substantial commitments about the world-model fit. Even if your aesthetic preferences drive you toward design based methods, this only serves to obscure the commitments. Statistics in the real world require real world commitments."
  },
  {
    "objectID": "notes/certain_things/Public/Statistics/Introduction - Statistics Topics.html",
    "href": "notes/certain_things/Public/Statistics/Introduction - Statistics Topics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "This section will serve to capture my Zettelkasten style notes on topics in Statistics."
  },
  {
    "objectID": "notes/certain_things/Public/Logic/Introduction - Logic Topics.html",
    "href": "notes/certain_things/Public/Logic/Introduction - Logic Topics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "This section will serve to capture any and all Zettelkasten notes on topics in logic -mathematical, philosophical or any style in between."
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html",
    "href": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "In this note we’ll capture reflections about Jun Otsuka’s Thinking about statistics"
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#statistical-models-and-the-problem-of-induction",
    "href": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#statistical-models-and-the-problem-of-induction",
    "title": "Examined Algorithms",
    "section": "Statistical Models and the Problem of Induction",
    "text": "Statistical Models and the Problem of Induction\nThe book begins by framing the different epistemological projects of both #Bayesian and #Frequentist patterns of inference as approaches to solving the problem of induction expressed by David Hume. #book #philosophy #statistics\nThis is a nice lens on the development of statistics and the applied work of statistical modelling. The two probabilistic frameworks are initially contrasted or compared to each other. The distinction is drawn between the epistemological process involved in both approaches. Firstly the notion of Bayesian conditionalisation which incorporates new data to derive new beliefs coherent with the observed facts is spelled out. Then we see how the frequentist approach can be considered as a species of reliablist epistemology, where the focus is on the error control of well defined processes. In both cases the problem of induction is located as one of inference i.e. if we have an appropriate set of i.i.d sample data we can warrant the inference that the future will look like the past\nHe draws out the ontological commitments to probabilistic kinds that appear required to underwrite statistical inference in both paradigms. He supplies a justification for these commitments as being instances of real patterns in the sense of Daniel Dennett’s phrasing. This is a kind of indispensability argument for the deployment of probabilistic kinds in our best science."
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#the-uniformity-of-nature",
    "href": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#the-uniformity-of-nature",
    "title": "Examined Algorithms",
    "section": "The Uniformity of Nature",
    "text": "The Uniformity of Nature\nIf probabilistic kinds inhabit the world they can exhibit different characteristics - varying fauna and flora of the natural world. One process might be well described by a Gaussian distribution, another by a Laplace distribution… this diversity is all well and good, but to go beyond descriptive statistics we need to posit more. We need the assumption that there is some stability to the processes we seek to characterise. A uniformity of nature that underwrites statistical inference and probabilistic prediction models.\nOtsuka suggests that this commitment is cashed out in contemporary statistics with the famous i.i.d assumption. This posit argues that for sound inference, we must assume that any sample data is drawn from a probability distribution that each draw is independent and identically distributed.\n\n“The IID condition is a mathematical specification of what Hume called the uniformity of nature. To say that nature is uniform means that whatever circumstances holds for the observed, the same circumstances will continue to hold for the unobserved. This is what Hume required for the possibility of inductive reasoning …” pg 25/26\n\nThis obviously is constraint on sound inference, but also implicitly an ontological assertion regarding distributional drift. This commitment is deeper than when we argue for a particular distributional characterisation of our process of interest. Our target process could be articulated as a mixture distribution or some more complicated beast, but in each case we require that (a) it is well described by some probability model and (b) the world is set up in such a way that more observations enable us to learn which particular statistical model (parametric or non-parametric) fits the data."
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#approaches-to-learning",
    "href": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#approaches-to-learning",
    "title": "Examined Algorithms",
    "section": "Approaches to Learning",
    "text": "Approaches to Learning\nWith this background Otsuka goes on to describe the manner in which the Bayesian and frequentist schools approach the task of learning from data.\n\nBayesian Machinery\nThe focus in the Bayesian setting presents conditionalisation as a logic of inductive reasoning, which expands on logical inference. These are frameworks for organising and interrogating our system of beliefs and their relationship to the achievement of knowledge. Otsuka argues that Bayesian inference plays a crucial role in justification. Moving from prior to posterior distribution is seen as a change in the weighting of our beliefs. An internalist species of the justification-relation between beliefs in light of data.\nBut founding a story about epistemological justification on bayesian inference involves a defense of priors and likelihood specifications. Priors are defended using the usual moves: (1) appeal to wash-out theorems of iteratively updating on sufficiently large data. Convergence theorems assuring Bayesian updated belief is truth conducive in the limit regardless of apparently subjective prior specification. (2) Non-informative priors and (3) Objective priors or empirical Bayes.\nWe won’t spend much time on (2) because it’s just kind of silly to justify beliefs and their manipulation by constant appeals to ignorance. On (3) there is a more interesting discussion regarding the relationship between degrees of belief and chance. We want our priors to reflect our background knowledge and update our beliefs in a way that it tracks the actual occurence of the events in question. David Lewis enshrined this requirement as the Principal Principle. But while this is an agreeable sounding tenet it cannot serve as a foundational justification for our priors within an internalist picture of justification without risk of infinite regress. This is problematic for the philosopher that seeks to establish bayesian inference as the sole source of belief generation, but seems less serious if you can tolerate primitive or foundational epistemological commitments outside those justified with inductive inference in the Bayesian loop. Justification must end somewhere (the spade eventually turns), and in-practice arguments and evidential exchanges rarely get anywhere close to an infinite series.\nAdditionally the Bayesian needs to defend the incorporation of different likelihood choices. Their shape and implications. Fortunately this can be more pragmatic in so far likelihood specifications are in effect testable hypotheses about the data generating process. They can be justified by the success of the modelling endeavour and our ability to recover data akin to our observations. The data is a fixed quantity, we use it to update our probabilstic beliefs and commitments. This is to the good because it can be shown (via Dutch book style theorems) that in strategic decision making where your beliefs adhere to the probability calculus they will strictly dominate other strategies.\nThe Bayesian machinery is a set of tools for arranging coherence amongst our beliefs, commitments - tracing out the implications. We move dynamically between prior and posterior by means of the likelihood term. This process cannot serve as as an ultimate court of appeal for basic beliefs that kick off the learning process itself. It is an abstract, highly flexible set of tools applicable to a wide range of questions. It provides a very general model of learning where the concern is justification of our beliefs in the context of what we know.\n\n\nFrequentist Consolidation\nSo far so uncontroversial. The Bayesian perspective is a natural fit for a species of internalist epistemology. The framework is abstract and characterised concisely, so relatively straightforward to incorporate in a general philosophical picture. Otsuka’s synthesis of the “classical” inferential picture is in this way more impressive.\nThe classical frequentist picture has a history of poor pedagogy and can seem disparate and ad-hoc. Jaynes is famously dismissive of the absurdities engendered by the pick-and-mix approach to statistical inference adhered to in the “classical” approach.\nThe frequentist view ties statements of probability to measures of relative frequency within a collection of observations. This makes it impossible to articulate probability statements for specific hypotheses. The epistemological perspective is quite distinct from the Bayesian view of updating individual hypotheses. Instead the focus must lie of testing statements about stochastic processes - processes which are inherently repeatable.\nAs such these processes can be described by probabilistic kinds. The question then becomes - how can the properties of these observable processes feed into knowledge gathering routines. How can we go from a statement about an observed frequency to claims of knowledge or belief regarding the data generating process?\nThe route is to go via the framework of statistical testing which has some relationship to Karl Popper’s falsification. This involves positing a statistical hypothesis with direct implications. These implications can be parsed as an explicit prediction that can be compared to future observations. In this way, the hypothesis is tested against the data. This gives us a means of arguing reductio ad unlikely against the initial hypothesis and turns statistical inference into a “process of systematically plowing out falsehood”. Through iteratively testing and refining more targeted hypotheses. We define and reject the null hypotheses as we go.\nThe constraints on the test design are built to ensure reliability over the course of repeated testing under a known null hypothesis. Sample size considerations, alpha-spending and statistical power are properties of the test. These properties need to be chosen in such a way to ensure reliability of a particular test, but there are also constraints for running repeated tests of multiple hypotheses. The entire testing enterprise is set up to minimise and control errors. The epistemological picture is one of reliablism. Whether a belief is justified is determined by the nature and defensiblity of the process that generated the claim. This approach allows us to reject the Gettier style counter examples to justified true belief analyses of knowledge. If the procedures of knowledge acquisition are not themselves well founded than the coincidence between claim and fact in the Gettier cases cannot be counted as justified. The procedures of knowledge acquisition are fixed, uncertainty stems from how we learn the shape of the data probabilistically.\nFor this method to work the reliabilism the methodology seeks should be clarified. Otsuka suggests that the reliability implicit in statistical testing needs to underwrite truth-tracking counterfactuals. In particular the claims:\n\nif P were not true, S would not believe P\nIf P were true then S would believe P.\n\nWhich is not to say that any particular statistical test will lead to the endorsement/rejection of the hypothesis in question. Statistical tests cannot directly accept the null - just fail to reject. But cumulatively over many tests a reliabilist epistemology should ultimately ascertain the falsity of the null when the null is false. This points to a tension in the focus on individual tests and specific p-values. Statistical testing as an epistemological enterprise is a wholesale endeavour and the overall success or failure of conditions which underwrite the reliability of the process are not easily discerned by mere success in our world. The procedures must be valid and truth tracking in “nearby” counterfactual worlds where the null hypothesis is not how it is in our world. The conditions under which a process is ultimately truth-tracking may depend on contextual factors which cannot be easily turned to an algorithm. This perspective nicely unites all the ad-hoc approaches to defining tests with asymptotic error control properties. The concern is inherently procedural, and procedures can (and perhaps should) vary in the context of the learning. But their adoption is founded on the commitment that they would be reliably truth-tracking in all worlds similar enough to our own."
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#when-philosophies-conflict",
    "href": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#when-philosophies-conflict",
    "title": "Examined Algorithms",
    "section": "When Philosophies Conflict",
    "text": "When Philosophies Conflict\nOtsuka’s survey of the two approaches is detailed and comprehensive. It sketches the motivations of each position well, and you might hope to reconcile the two. Apply each in their own domain where appropriate…, but unfortunately the motivating instincts can clash irreconcilably.\nThe Bayesian perspective necessitates the adoption of the Likelihood Principle which can be violated by the classical procedure of sequential testing. Recall how for the Bayesian the data is a fixed quantity, fed forward into the likelihood term to update our beliefs. This works the same whether we update our beliefs at time t1, t2 or tN. All the information is in the likelihood irrespective of how much data has accrued over time.\nWhile under the frequentist model if the data is analysed under different experimental designs (e.g. one with a stopping rule and one without) the results of test for a particular null hypothesis can differ I.e. with the same data and the same null hypothesis, one experiment can reject the null and the other will fail to reject it. This is because the result incorporates information about the design of the experiment that cannot be captured in the simple likelihood. Otsuka makes the point that this is a broad problem for all reliabilist philosophies where the fit between process/test and reality is contestable. There is no perfectly general procedure that returns true results in all circumstances. As such the frequentist methodologist must argue anew in each circumstances for the appropriateness of their methods.\nThis is a keen source of divisiveness between the two schools. Scientists historically cannot be trusted to review their methods with respect to their goals. Instead frequentist statistical methodology has been abused in practice. Adherents use the routine nature of procedure as a rubber stamp without due consideration for the reliability of the methods in the context of the question at hand. This pattern of abuse is rightly seen as a key contributor to the replication crisis in science. Ironically it is this aspect of frequentist methodology that introduces obscure subjective bias into the experimental exercise. The Bayesian (often accused of criminal subjectivity) wears their priors clearly and defends their appropriateness for each analysis.\nNeither school of thought is a self-contained epistemology. Neither method is self-sustaining. Both priors and procedures must be justified with an appeal to their apt fit for the problem at hand. Statistical methodology slots into a broader epistemological endeavour and there is no substitute for the careful and knowing application of inferential machinery if we hope to justify the achievements of science."
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#bias-and-regularisation",
    "href": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#bias-and-regularisation",
    "title": "Examined Algorithms",
    "section": "Bias and Regularisation",
    "text": "Bias and Regularisation\nIn later chapters we move towards the more pragmatist position of model selection based on predictive power. This mirrors a move away from understanding of uncertainty due to natural variation towards an model-uncertainty in the analysis.\nWe may prefer a model which does not capture the true data generating process just so long as it performs better in prediction tasks. This effectively invokes the more typical approaches to machine learning model evaluation as driving us toward pragmatic biases that work well to optimise for some measure of out-of-sample prediction. Your inferential framework can be independent of your model selection criteria, but model ranking is still conducted under uncertainty. Each measure of performance is still an estimate.\nOtsuka contrasts the ranking of models based on information criteria with the out of sample predictive performance of deep learning systems. The AIC style methodologies penalises models with too many parameters to optimise for predictive performance. Deep learning methods have explicit methods to induce regularisation like effects with: drop-out, modified loss-functions. What is striking is that while both methods are checks against overfitting of models to the data, both are pragmatic compromises that dispense with the idea of epistemological truth-tracking. They concede that the it is often better to overcome the problem of model uncertainty with suitably practical abstraction. Don’t worry about the world-model fit so long as the outcomes of the model are workable.\nBut this comes with a burden of explainability often demanded of opaque models. Predictive success in one domain does not always translate to success in another. Regulators and stakeholders need to understand when and why the predictive reliability of deep learning systems can be expected to transfer well across tasks. Appeals to the epistemological surety of beliefs derived from predictions in these black-box systems stem from a loosely held belief in the virtues of the deep learning mechanisms. These virtues are less well established than error-control rates of a statistical test and they are somewhat transient across task."
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#causal-inference-and-statistics",
    "href": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#causal-inference-and-statistics",
    "title": "Examined Algorithms",
    "section": "Causal Inference and Statistics",
    "text": "Causal Inference and Statistics\nFinally Otsuka pauses to consider the role of causal inference in contemporary statistics and how here optimising for predictive power will often fail when the task requires subtlety or insight into the data generating process. The manner in which models and measurements can be confounded by aspects of the data generating process cannot be detected automatically without knowledge of the relationships in the system. The focus here is on how tools for identification are required when we want to be sure that the causal quantities of interest are soundly derivable from the data we have to hand. Whether we use the potential outcomes framework, the do-calculus or structural estimation. The work of statistical inference using any and all of the above frameworks can only get off the ground when we have enough of a world-picture - a set of commitments or assumptions that allow our inferences to have warrant. Our priors our informed, our inferential procedures suitably well defined to sustain truth-tracking counterfactuals."
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#conclusion",
    "href": "notes/certain_things/Public/Philosophy/Thinking about Statistics.html#conclusion",
    "title": "Examined Algorithms",
    "section": "Conclusion",
    "text": "Conclusion\nThe broad picture painted in Otsuka’s survey of statistics is the central role of inferential procedures in our broader epistemological landscape. The diversity of roles it can play and different standards of rigour at play in different contexts. The links between the foundational questions and puzzles of epistemology have illuminated the structure of the debates in statistics."
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Introduction - Philosophy Topics.html",
    "href": "notes/certain_things/Public/Philosophy/Introduction - Philosophy Topics.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "This section will serve to capture the philosophical topics in my Zettelkasten notes."
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Sorites Paradox and Survival Analysis.html",
    "href": "notes/certain_things/Public/Philosophy/Sorites Paradox and Survival Analysis.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "There seems to be a plausible relationship between (a) the idea of a latent evolving hazard in #survival-analysis and (b) the accumulation effect that drives our intuitions from observations of distinct sand grains to observations of a heap. The classic philosophical puzzle put forward by Sorites.\nI’ve worked on survival analysis in the context of statistics and data science , but I’m putting together this note to arrange my thoughts on the value the perspective has on the classic philosophical puzzle.\nThe first thing to observe is how survival analysis is in general a model of the probabilities of state-transition. Moving between alive-dead, sick-well, subscribed-churned, hired-fired. The Framework is quite abstract and therefore widely applicable to the analysis of all state transitions with both clear, distinct and permeable borders between states.\nTraditionally you might see frequentist elaborations of the mechanics of #survival-analysis , but it becomes more interesting from a philosophical stand point when you phrase the model in a #Bayesian fashion.\nIn the Bayesian setting the uncertainty expressed in the model regarding measures of risk of state-change can be seen to contribute to the semantic ambiguity relevant in the Sorites setting. I will try to bring out this connection in the following."
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Sorites Paradox and Survival Analysis.html#the-sorites-paradox",
    "href": "notes/certain_things/Public/Philosophy/Sorites Paradox and Survival Analysis.html#the-sorites-paradox",
    "title": "Examined Algorithms",
    "section": "The Sorites Paradox",
    "text": "The Sorites Paradox\nThe typical presentation of the sorites issue in philosophy has been as a series of conditional predications as follows:\n\nFa_0\nFa_0 -> Fa_1\n.\n.\n.\nFa_{i-1} -> Fa_i\nTherefore: Fa_i\n\nWhere we allow that there is an indifference relation for the predication of F over all elements of the sequence preceding the last entry. Then, it is argued, the last predication fails for some entry (i) in the sequence. The predicate F is said to be suffer from #vagueness .This is purportedly a paradox due to the requirement of logical validity over conditional reasoning caused by the vagueness of F over the course of the sequence.\nOther rephrases of the logical steps truncate the sequence of conditionals by appealing to a Principle of mathematical Induction to arrive at the same conclusion. For out purposes it’s not crucial which phrasing is applied. The point is just that the the vagueness of the predicate F is said to threaten some central tenet of logical validity.\n\nApproaches to the Paradox\nThere have been advocates for acceptance of the Paradox - allowing that there is just a breakdown of logic in the case of these vague predicates. So much the worse for logic. This is quite radical as the prevalence of vague predicates in natural language commits us implicitly to the view that we cannot make distinctions.\nThe more plausible approaches to the Paradox seek to establish a reason for rejecting the validity of the conclusion by denying the premises in some way e.g. claiming that the indifference over the nth predication of F and the n-1 case fails. There is a precise point which is a genuine cut-off between F and not F. This position is called Epistemicism - which locates the causes of vagueness in predication in our degrees of ignorance.\nThe suggestion above is supported somewhat empirically by the reality of borderline cases of predication among reasonable speakers of the same language. People evince hedging behaviour and deliberate vagueness in cases where they avoid commitment to a sharp distinction. “She is sorta cool, nice-ish!”. This is the data the philosophical theory needs to explain. Paradigmatic cases of attributed state-change coupled with paradigmatic cases of hedging.\nThe theoretical question then becomes - what constitutes borderline vagueness? I think this is where we can use survival analysis to elaborate and explain cases of borderline vagueness and empirical cases of disagreement in predication. In particular Bayesian approaches to survival analysis which allow that there is a cut-off point in the sequence, but there is genuine uncertainty where we locate the cut-off point. Seeing this as a problem of Bayesian modelling allows us to locate the sources of hedging in the components and precision of our model terms through which the propagates the uncertainty in our attribution patterns.\n\n\nPerspectives on Probability\nSurvival analysis can seem intimidating because it asks us to understand time-to-event distributions from four distinct perspectives. The first familiar density function, the next cumulative density function and its inverse survival function. Additionally we can view the the cumulative hazard function as a transformation of the survival function, and the instantaneous hazard as a discretisation of over intervals of the temporal sequence ranged over by the cumulative hazard.\nIt’s important to see and understand that these quantities, while appearing to be abstract mathematical objects, can be derived from simple tables which record the subjects in the risk set which experience the event of interest at each interval of time. In other words the set of conditional probabilities instance-by-instance over the range of the temporal sequence. This is how we derive the instantaneous hazard quantity.\nDifferent families of probability distribution allow us to encode different structures in the hazards. For instance if we want hazards to peak early in the sequence and decline later in the sequence non-monotonically we can use the loglogistic distribution. If we want to ensure monotonic hazard sequences we can use Weibull distributions.\n\n\nDistinguishing Risk and Uncertainty\nWe want to explain the semantic ambiguity of Sorites phenomena by the probabilistic nature of state transitions over additive sequences. However, we won’t trade on the uncertainty between distinct models i.e. it’s not merely that your model of the sand-to-heap transition is characterised by one probability distributions and mine by another (although it could be). We are interested in divergences of opinion and semantic uncertainty that arises due to the stochastic nature of the phenomena where we share the same model of the phenomena. This reflects a difference in the view of the risk not the model uncertainty.\n\n\nCox Proportional Hazards Model\nTo make this a bit more concrete consider the cox proportional hazard model. This is a #regression model which aims to characterise the probability of state transition using a statistical model with two components. The baseline hazard:\n\\[ \\lambda_0 (t) \\]\nwhich is combined multiplicatively with an exponentiated weighted linear sum as follows \\[\\lambda_0 (t) \\cdot e^{\\beta_0 X_0 + \\beta_1 X_1 ... \\beta_n X_n}\\] In this model the baseline hazard is a function of the time intervals and we estimate a hazard term for each interval when we fit the model. There is a “free” parameter for the instantaneous hazard at each timepoint. This sequence is the baseline hazard. This latent baseline hazard is akin to an intercept term(s) in more traditional regression models. Individual predictions of the evolving hazard are then determined by how the individuen’s covariate profile modifies the baseline hazard. Estimation procedures for this model find values for the baseline hazard and for the coefficient weights \\(\\beta_i\\) in our equation.\nWith these structures in mind you might be tempted to locate the source of disagreement between people’s judgments as stemming from differences in their covariates \\(X_i\\) , or put another way… we see the probability of transition as a function of the same variables, but disagree on the values of those inputs to the function. The benefit of this perspective is that instead of seeing the Sorites Paradox as an error of logical reasoning that needs to be fixed by one or more adjustments to classical logic, we can instead view the phenomena as reflecting disagreement among latent probabilistic models.\n\n\nComplexity of the Heap\nOne additional perspective on the problem is gained by noting how the Cox proportional model is a prediction model and comes with criteria of model adequacy. How many predictor variables are required to anticipate state transition? How much variance is explained? If we can gauge the complexity of the prediction task, can the complexity itself explain disagreement?\n\n\nHierarchical or Meta Vagueness\nWe’ve seen now a few different sources of divergences. At the highest level we can appeal to the Knightian distinction between risk and uncertainty, then secondarily to differences in the data used to calibrate risk or thirdly in differences due to estimation strategies and finally in pure prediction complexity.\nIf divergences are due to complete uncertainty of the appropriate model, then we concede allot to the sceptic and the quantification of any plausible cut point is hopeless. If differences result from the other candidate sources there remains hope for arriving at intersubjective consensus.\nThis can be seen in some sense in Bayesian model development workflow with hierarchical survival models. Instead of imagining agents reasoning if-then style through a sequence of additional sand grains. Let’s picture the reasoner working with a latent survival model, negotiating a contract between reality and their linguistic usage.\nHierarchical models in the Bayesian setting are typical and interesting in their own right as they allow for the expression of heterogeneity across individuals. Broadly they involve adding one or more parameters that modify the baseline model equation. We saw earlier that the Cox Proportional hazard model is expressed as \\[\\lambda_0 (t) \\cdot e^{\\beta_0 X_0 + \\beta_1 X_1 ... \\beta_n X_n}\\]\nThis can be modified as follows:\n\\[z_{i} \\cdot \\lambda_0 (t) \\cdot e^{\\beta_0 X_0 + \\beta_1 X_1 ... \\beta_n X_n}\\] Where we allow an individual “frailty” term \\(z_{i}\\) is added to the model as a multiplicative factor for each individual in the data set. The terms are drawn from a distribution, often centred on 1, so that the average individual modifies the baseline model not at all… but modifications are expressed as a reduction or increase to the multiplicative speed of state transition. The baseline model can therefore be considered\nRecall that the Bayesian modelling exercise quantifies the probability distribution of all the parameters in the model. A well specified baseline model will mean that less explanatory work needs to be done by the individual frailty terms. A poorly specified model will locate allot of weight in these terms. This is a mechanism which helps quantify the degree of irreducible uncertainty in our attribution patterns derived from our understanding of the paradigmatic cases (our sample).\n\n\nThe Bayesian Reasoner\nAn individual reasoner working with their set of paradigmatic data points y ~ f(X | \\(\\theta\\)) may fit their model to this data. The variance in the distribution of frailty terms \\(z_{i} \\in \\theta\\) estimated represents their view of the remaining uncertainty in cases after controlling for the impact of the covariate profiles across the cases.\nThese quantities represents disagreements regarding the speed up or slow down in the survival curves…but the survival curves quantifies the probability of transition at each point of accumulation. So a survival model allows us to say precisely at each point of accumulation what is the probability of transition. For any given point in series of accumulating instances, the diversity of individual frailty terms needed to account for the predications determine the quantifiable range of uncertainty in the survival probabilities we derive from the paradigmatic cases.\nEpistemicism about existence of a cut point for vague predicates will always assume the existence of threshold. The picture of Sorites Paradox for the Bayesian reasoner sees them go from uncertainty to uncertainty updating the latent model as they go. Maybe the model converges tightly in some cases, maybe not. Incorporating more paradigmatic instances, more covariate indicators as they develop conceptual clarity on what drives the attribution of state-hood under the evolving or growing pressure to change. Finding the threshold would not and could not be a solution to the paradox. Any threshold will be context specific and also learned (with uncertainty) relative to the tolerances of the domain. Understanding that paradox as yet another instance of learning in a multifaceted world at least lets us see the problem without requiring torturous modifications to classical logic."
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport bambi as bmb\nimport seaborn as sns\nfrom pygam.datasets import mcycle\nfrom pygam import LinearGAM, s, f, GAM, l, utils\nimport numpy as np\nimport arviz as az\nimport pymc as pm\nimport scipy as sp\nfrom patsy import bs as bs_patsy, dmatrix\nimport pytensor.tensor as pt\n\n\nrandom_seed = 100\n\nimport warnings\n\nwarnings.simplefilter('ignore')"
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#generalised-additive-models",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#generalised-additive-models",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Generalised Additive models",
    "text": "Generalised Additive models\nNonlinear functions can be approximated with linearly additive combinations of component features. Before delving into the practicalities we’ll quickly note some of the theoretical background. The canonical reference for these additive models is Simon Wood’s “Generalised Additive Models: An Introduction with R” which outlines in some detail the theoretical background of splines and univariate smoothers. The book stresses the trade-offs between the flexibility of splines and the need for cross-validation and penalised estimation methods for spline based modelling. Spline models ape the complexities of esoteric functions by regression-like formulas combining subordinate functions of observed variables to to predict another:\n\\[y \\sim f(x_1) + f(x_2) ...\\]\nThe structure of the individual \\(f\\) functions is approximated. So we are approximating \\(y\\) by adding a series of input approximations \\(\\sum f_{i}\\). In R these penalised models fits can be achieved in mgcv which incorporates a Wilkinson like formula syntax for model specification: y ~ s(x) + s(x1). The closest implementation in python is available in PyGam and we will adopt this package to illustrate the spline based smoothing.\n\nPyGAM and Penalised Fits\nLet’s first look at an example data set on which to demonstrate univariate smoothing patterns using penalised splines. We’ll initially look simply at the function calls before going “under the hood”.\nSpline models can be optimised by fitting differing strength penalities over a varying number of splines. Splines are used to construct non-linear functions of the input variable which are combined additively in our model equation. The penalities used in the optimisation routines constrain how “wiggly” these constructions will be. The number of splines determine how precise our will be.\n\nX, y = mcycle(return_X_y=True)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(X, y)\nax.set_ylabel(\"Acceleration\")\nax.set_xlabel(\"Time Step\")\nax.set_title(\"Crash Test Dummy Acceleration \\n Simulated Motorcycle Crash\", fontsize=20);\n\n\n\n\nNow we fit a number of different models to account for the herky-jerky nature of the data generating processs. We vary the parameterisations to see how the numbers of splines and strength of the penalty help account for the variation in \\(y\\) over the support of \\(X\\).\n\ngam1 = LinearGAM(s(0, n_splines=5)).fit(X, y)\ngam2 = LinearGAM(s(0, n_splines=7)).fit(X, y)\ngam3 = LinearGAM(s(0, n_splines=10)).fit(X, y)\ngam4 = LinearGAM(s(0, n_splines=15)).fit(X, y)\ngam5 = LinearGAM(s(0, lam=.1)).fit(X, y)\ngam6 = LinearGAM(s(0, lam=.5)).fit(X, y)\ngam7 = LinearGAM(s(0, lam=5)).fit(X, y)\ngam8 = LinearGAM(s(0, lam=15)).fit(X, y)\n\n\ndef plot_fit(gam, X, y, ax, t, c1='b', c2='r'):\n    XX = gam.generate_X_grid(term=0, n=500)\n\n    ax.plot(XX, gam.predict(XX), color=c2, linestyle='--')\n    ax.plot(XX, gam.prediction_intervals(XX, width=.95), color=c1, ls='--')\n\n    ax.scatter(X, y, facecolor='gray', edgecolors='none')\n    ax.set_title(f\"\"\"95% prediction interval with {t} \\n LL: {gam.statistics_['loglikelihood']}\"\"\");\n\nfig, axs = plt.subplots(4,2, figsize=(10, 20))\naxs = axs.flatten()\ntitles = ['5_splines', '7_splines', '10_splines', '15_splines',\n'lam=.1', 'lam=.5', 'lam=5', 'lam=15']\ngs = [gam1, gam2, gam3, gam4, gam5, gam6, gam7, gam8]\nfor ax, g, t in zip(axs, gs, titles):\n    plot_fit(g, X, y, ax, t)\n\n\n\n\nHere we’ve seen the PyGAM package applied to fitting our model to the data. In the formula specification we see y ~ s(i) where i denotes the index of the column variable in the X data.\nOver the range of of the x-axis we can see how the conditional expectation is more or less well fit to the data depending on how the penalities and complexity of the model is specified. The art and science of developing a GAM model is finding the constraints on the approximation that help capture the observed patterns but do not excessively overfit to the observed data."
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#optimising-the-parameter-setting",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#optimising-the-parameter-setting",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Optimising The Parameter Setting",
    "text": "Optimising The Parameter Setting\nThe manner of the the linear combination achieved by GAMs is constrained by the optimisation goal in their model fit. We can see from the model summary what is going on under the hood. For a given model specification the summary will report a number of model-fit statistics such as the log-likelihood and the AIC.\n\n## Naive Model manually specified splines\ngam_raw = LinearGAM(s(0, n_splines=10), fit_intercept=False).fit(X, y)\nprint(\"log_likelihood:\", gam_raw.statistics_['loglikelihood'])\nprint(\"AIC:\", gam_raw.statistics_['AIC'])\n\nlog_likelihood: -1042.2807399926558\nAIC: 2097.7407642114244\n\n\nThe question then becomes what changes are induced in the model as we seek to optimise these model fit statistics.\n\n## model optimised\ngam = LinearGAM(s(0),  fit_intercept=False)\ngam.gridsearch(X, y)\ngam.summary()\n\n  0% (0 of 11) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--\n\n\n  9% (1 of 11) |##                       | Elapsed Time: 0:00:00 ETA:   0:00:00\n\n\n100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n\n\n\n\n\nLinearGAM                                                                                                 \n=============================================== ==========================================================\nDistribution:                        NormalDist Effective DoF:                                     11.8135\nLink Function:                     IdentityLink Log Likelihood:                                  -952.2409\nNumber of Samples:                          133 AIC:                                             1930.1088\n                                                AICc:                                            1933.0789\n                                                GCV:                                              609.3811\n                                                Scale:                                            512.7965\n                                                Pseudo R-Squared:                                   0.7984\n==========================================================================================================\nFeature Function                  Lambda               Rank         EDoF         P > x        Sig. Code   \n================================= ==================== ============ ============ ============ ============\ns(0)                              [0.2512]             20           11.8         1.11e-16     ***         \n==========================================================================================================\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n         which can cause p-values to appear significant when they are not.\n\nWARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n         are typically lower than they should be, meaning that the tests reject the null too readily.\n\n\nFortunately, this routine can be performed directly and results in the following differences between the naive and optimised model.\n\nPlot the GAM fits\n\nfig, ax = plt.subplots(figsize=(10, 6))\nplot_fit(gam_raw, X, y, ax, \"Unoptimised Fit\", c1='orange', c2='green')\nplot_fit(gam, X, y, ax, \"Optimised Fit\")\n\n\n\n\nThis is all well and good! We’ve seen an approach to modelling that can capture eccentric patterns in raw data. But how does it work and why should we care? If you’re familiar with the nomenclature of machine learning, you should think of spline modelling as a variety of feature creation. It generates “synthetic” features over the range of the observed variable. These synthetic features are the splines in question. Model building here too then relies on feature selection and principles for model-comparison.\n\n\nDigression on the usage of “Splines”\nThe history of the term “spline” is related to the history of draftmanship. Historically splines were thin strips of flexible wood or plastic that could be bent or shaped around a weight or “knot” points to express a traceable curve over the space of a “numberline”. The elastic nature of the spline material allowed it to be bent around the knot points of curvature expressing a smooth or continuous bend.\nThe mathematical technique apes these properties by defining a curve over in an analogous way. We specify “knots” to carve up the support of the random variable \\(X\\) into portions that require different weighting schemes to represent the outcome \\(y\\) in each partition of the support variable.\n\n\n\nExtracting the Splines\nReturning to our model we can extract the spline features used in the PyGAM by invoking the following commands. We first identify the knot points and create the b-spline basis appropriate for the variable \\(X\\).\n\nknot_edges=utils.gen_edge_knots(X,dtype='numerical')\nknots=np.linspace(knot_edges[0],knot_edges[-1],len(gam.coef_))\n\nsplines = utils.b_spline_basis(X, edge_knots=knot_edges, sparse=False)\n\nsplines_df = pd.DataFrame(splines, columns=[f'basis_{i}' for i in range(len(gam.coef_))])\n\nsplines_df.head(10)\n\n\n\n\n\n  \n    \n      \n      basis_0\n      basis_1\n      basis_2\n      basis_3\n      basis_4\n      basis_5\n      basis_6\n      basis_7\n      basis_8\n      basis_9\n      basis_10\n      basis_11\n      basis_12\n      basis_13\n      basis_14\n      basis_15\n      basis_16\n      basis_17\n      basis_18\n      basis_19\n    \n  \n  \n    \n      0\n      0.166667\n      0.666667\n      0.166667\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      1\n      0.132997\n      0.661606\n      0.205334\n      0.000063\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      2\n      0.059688\n      0.594827\n      0.341426\n      0.004059\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      3\n      0.030095\n      0.518726\n      0.437481\n      0.013698\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      4\n      0.012374\n      0.428013\n      0.527144\n      0.032470\n      0.000000\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      5\n      0.000000\n      0.040337\n      0.551431\n      0.399315\n      0.008917\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      6\n      0.000000\n      0.018232\n      0.465467\n      0.492630\n      0.023671\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      7\n      0.000000\n      0.011137\n      0.418489\n      0.535407\n      0.034967\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      8\n      0.000000\n      0.000014\n      0.189310\n      0.664817\n      0.145859\n      0.000000\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      9\n      0.000000\n      0.000000\n      0.120914\n      0.656897\n      0.222015\n      0.000174\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n\n\n\nThese spline features range the extent of the of covariate space \\(X\\) defining “partitions” of the space. The model “learns” to capture the shape of the outcome variable \\(y\\) by figuring out how to weight the different portion of this spline basis matrix i.e. the linear combination of this basis matrix with the derived coefficients is a model of our outcome variable.\nNote how each row is 0 everywhere except within the columns that represent a partition of \\(X\\). Additionally each row sums to unity. These properties are important because they ensure that any weighted combination of this basis represents the outcome variable in a controlled and quite granular manner. The more splines we use the more control we have of the representation, but we also risk overfit.\n\n\nPlotting the Weighted Spline\n\nax = splines_df.dot(gam.coef_).plot(title='Weighted Splines', label='Weighted Combination of Spline Basis', figsize=(10, 6))\nax.set_ylabel(\"Acceleration\")\nax.set_xlabel(\"Time Steps\")\nax.legend();\n\n\n\n\nIn this manner we can see how the specification of a spline basis can help us model eccentric curves and waves in an outcome space. Spline features are like ad-hoc joists structuring our ship’s rigging. Features filling the gaps in our theory, they act like duct-tape binding the pieces together. If we hold this part down there, and jiggle another part here, our theory stays afloat. Spline features exemplify the contorted nature of empirically informed theory construction.\nNext we’ll see how to more directly work with the specification of basis splines, passing these feature matrices into Bambi models."
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#bayesian-splines-with-bambi",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#bayesian-splines-with-bambi",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Bayesian Splines with bambi",
    "text": "Bayesian Splines with bambi\nSpline models can be built and assessed using Bayesian approaches to quantify the uncertainty in the constructed function. Under the hood Bambi makes use of the ‘formulae’ package to allow for formula-like syntax to specify spline basis terms and mixed effect model terms. We leverage this to streamline the specification of spline models and demonstrate different kinds of spline-features.\n\nknots_6 = np.linspace(0, np.max(X), 6+2)[1:-1]\nknots_10 = np.linspace(0, np.max(X), 10+2)[1:-1]\nknots_20 = np.linspace(0, np.max(X), 20+2)[1:-1]\n\ndf = pd.DataFrame({'X': X.flatten(), 'y': y})\nformula1 = 'bs(X, degree=0, knots=knots_6)'\nformula2 = 'bs(X, degree=1, knots=knots_6, intercept=False)'\nformula3 = 'bs(X, degree=3, knots=knots_6, intercept=False)'\nformula4 = 'bs(X, degree=3, knots=knots_10, intercept=False)'\nformula5 = 'bs(X, degree=3, knots=knots_20, intercept=False)'\nmodel_spline1 = bmb.Model(f\"y ~ {formula1}\", df)\nmodel_spline2 = bmb.Model(f\"y ~ {formula2}\", df)\nmodel_spline3 = bmb.Model(f\"y ~ {formula3}\", df)\nmodel_spline4 = bmb.Model(f\"y ~ {formula4}\", df)\nmodel_spline5 = bmb.Model(f\"y ~ {formula5}\", df)\nmodel_spline5\n\n       Formula: y ~ bs(X, degree=3, knots=knots_20, intercept=False)\n        Family: gaussian\n          Link: mu = identity\n  Observations: 133\n        Priors: \n    target = mu\n        Common-level effects\n            Intercept ~ Normal(mu: -25.5459, sigma: 222.6623)\n            bs(X, degree=3, knots=knots_20, intercept=False) ~ Normal(mu: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n                0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], sigma: [1407.0172 1448.7263 1394.069   877.5692\n                912.3295  679.8904  507.1974\n              696.2455  860.7155  666.7815  665.863   932.178   883.0419  740.3925\n             1010.3524  914.7406  923.9545 1218.535  1355.2211 1420.6241 1363.5314\n             2403.9764 1393.2356])\n        \n        Auxiliary parameters\n            sigma ~ HalfStudentT(nu: 4.0, sigma: 48.14)\n\n\nAs you can see here we are specifying a range of different degrees of spline basis. The different degrees corresspond to the smoothness of the overlapping splines. The degree=0 splines mean we specify a piecewise constant basis i.e. 0 or 1 within each region of the partition. But we can add more degrees to see more flexible representations of the space.\n\nmodel_spline5.build()\nmodel_spline5.graph()\n\n\n\n\nThe differences here will become clearer as we plot the various spline basis matrices below, but the main thought is that there are different degrees of “smoothness” to the linear basis of the spline features.\n\nPlot the Spline Basis\nThe below functions extract the basis specification from each model and plots the basis design for an increasingly complex series of spline basis matrices.\n\ndef plot_spline_basis(basis, X, ax, title=\"Spline Basis\"):\n    df = (\n        pd.DataFrame(basis)\n        .assign(X=X)\n        .melt(\"X\", var_name=\"basis_idx\", value_name=\"y\")\n    )\n\n\n    for idx in df.basis_idx.unique():\n        d = df[df.basis_idx == idx]\n        ax.plot(d[\"X\"], d[\"y\"])\n    \n    ax.set_title(title)\n    return ax\n\ndef plot_knots(knots, ax):\n    for knot in knots:\n        ax.axvline(knot, color=\"0.1\", alpha=0.4)\n    return ax\n\n\n\nfig, axs = plt.subplots(5, 1, figsize=(9, 20))\naxs = axs.flatten()\naxs.flatten()\nB1 = model_spline1.response_component.design.common[formula1]\nplot_spline_basis(B1, df[\"X\"].values, ax=axs[0], title=\"Piecewise Constant Basis\")\nplot_knots(knots_6, axs[0]);\n\nB2 = model_spline2.response_component.design.common[formula2]\nax = plot_spline_basis(B2, df[\"X\"].values, axs[1], \ntitle=\"Piecewise Linear Basis\")\nplot_knots(knots_6, axs[1]);\n\nB3 = model_spline3.response_component.design.common[formula3]\nax = plot_spline_basis(B3, df[\"X\"].values, axs[2], \ntitle=\"Cubic Spline Basis (6 Knots)\")\nplot_knots(knots_6, axs[2]);\n\nB4 = model_spline4.response_component.design.common[formula4]\nax = plot_spline_basis(B4, df[\"X\"].values, axs[3], \ntitle=\"Cubic Spline Basis (10 Knots)\")\nplot_knots(knots_10, axs[3]);\n\n\nB5 = model_spline5.response_component.design.common[formula5]\nax = plot_spline_basis(B5, df[\"X\"].values, axs[4], \ntitle=\"Cubic Spline Basis (20 Knots)\")\nplot_knots(knots_20, axs[4]);\n\n\n\n\nHere we’ve seen the nature of the modelling splines we’ll fit to the data. By “fit” we mean estimate a set of linear coefficients that we can use to additively combine these spline features creating a smooth linear representation of the outcome.\n\n\nFit the Individual Spline Models\nWe now combine the spline components within a linear model fit, pulling and tying them together in such a way as to ape the shape of the observed sequence.\n\nidata_spline1 = model_spline1.fit(random_seed=random_seed, idata_kwargs={\"log_likelihood\": True})\n\nidata_spline2 = model_spline2.fit(random_seed=random_seed, idata_kwargs={\"log_likelihood\": True})\n\nidata_spline3 = model_spline3.fit(random_seed=random_seed, idata_kwargs={\"log_likelihood\": True})\n\nidata_spline4 = model_spline4.fit(random_seed=random_seed, idata_kwargs={\"log_likelihood\": True})\n\nidata_spline5 = model_spline5.fit(random_seed=random_seed, idata_kwargs={\"log_likelihood\": True})\n\nFinally we can plot fits achieved by each of the models and compare how the different spline smooths contribute to approxiating the outcome variable.\n\n\nPlot the Weighted Mean\n\ndef plot_weighted_splines(B, idata, formula, ax, knots):\n    posterior_stacked = az.extract(idata)\n    wp = posterior_stacked[formula].mean(\"sample\").values\n\n    plot_spline_basis(B * wp.T, df[\"X\"].values, ax)\n    ax.plot(df.X.values, np.dot(B, wp.T), color=\"black\", lw=3, label='Weighted Splines')\n    plot_knots(knots, ax);\n    ax.legend()\n\n\n\nfig, axs = plt.subplots(5, 1, figsize=(10, 20))\naxs = axs.flatten()\naxs.flatten()\n\nplot_weighted_splines(B1, idata_spline1, formula1, axs[0], knots_6)\nplot_weighted_splines(B2, idata_spline2, formula2, axs[1], knots_6)\nplot_weighted_splines(B3, idata_spline3, formula3, axs[2], knots_6)\nplot_weighted_splines(B4, idata_spline4, formula4, axs[3], knots_10)\nplot_weighted_splines(B5, idata_spline5, formula5, axs[4], knots_20)\n\n\n\n\nHere we can see how the models with increasingly complex splines are more exactly able to fit the herky jerky trajectory of the outcome variable in each interval. The fewer the intervals, the less flexibility available to the model.\n\n\nCompare Model Fits\nAs before we can evaluate these model fits and compare them based on leave-one-out cross validation scores and information theoretic complexity measures.\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 7))\naxs = axs.flatten()\nmodels_dict = {\"piecewise_constant\": idata_spline1, \"piecewise_linear\": idata_spline2, \"cubic_bspline\": idata_spline3, \"cubic_bspline_10\": idata_spline4, \n\"cubic_bspline_20\": idata_spline5}\ndf_compare = az.compare(models_dict)\naz.plot_compare(df_compare, ax=axs[0])\naxs[0].get_legend().remove()\naz.plot_compare(az.compare(models_dict, 'waic'), ax=axs[1])\naxs[1].set_yticklabels([])\ndf_compare\n\n\n\n\n\n  \n    \n      \n      rank\n      elpd_loo\n      p_loo\n      elpd_diff\n      weight\n      se\n      dse\n      warning\n      scale\n    \n  \n  \n    \n      cubic_bspline_10\n      0\n      -612.572745\n      11.646898\n      0.000000\n      9.011087e-01\n      9.646678\n      0.000000\n      True\n      log\n    \n    \n      cubic_bspline_20\n      1\n      -620.709337\n      19.669201\n      8.136592\n      1.360171e-13\n      9.600116\n      2.074742\n      True\n      log\n    \n    \n      cubic_bspline\n      2\n      -634.647180\n      8.703519\n      22.074435\n      6.196578e-14\n      8.915728\n      6.642850\n      True\n      log\n    \n    \n      piecewise_constant\n      3\n      -643.781042\n      6.981098\n      31.208296\n      5.299668e-02\n      9.770740\n      8.728072\n      False\n      log\n    \n    \n      piecewise_linear\n      4\n      -647.016885\n      5.987587\n      34.444140\n      4.589467e-02\n      7.914787\n      8.116163\n      False\n      log\n    \n  \n\n\n\n\n\n\n\nHere we see that the extra complexity of using 20 splines leads to slightly worse performance measures than the less complex but seemingly adequate 10 splines. In other words, it’s starting to overfit to the data at 20. This is the price of flexibility and a sign of a model unlikely to generalise well on out-of-sample data.\n\nnew_data = pd.DataFrame({\"X\": np.linspace(df.X.min() - 5, df.X.max() + 5, num=500)})\n    \nmodel_spline4.predict(idata_spline4, data=new_data, \nkind='pps', inplace=True)\n\nidata_spline4\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                                               (chain: 4,\n                                                           draw: 1000,\n                                                           bs(X, degree=3, knots=knots_10, intercept=False)_dim: 13,\n                                                           y_obs: 500)\nCoordinates:\n  * chain                                                 (chain) int64 0 1 2 3\n  * draw                                                  (draw) int64 0 ... 999\n  * bs(X, degree=3, knots=knots_10, intercept=False)_dim  (bs(X, degree=3, knots=knots_10, intercept=False)_dim) int64 ...\n  * y_obs                                                 (y_obs) int64 0 ......\nData variables:\n    Intercept                                             (chain, draw) float64 ...\n    bs(X, degree=3, knots=knots_10, intercept=False)      (chain, draw, bs(X, degree=3, knots=knots_10, intercept=False)_dim) float64 ...\n    y_sigma                                               (chain, draw) float64 ...\n    y_mean                                                (chain, draw, y_obs) float64 ...\nAttributes:\n    created_at:                  2024-05-27T06:00:45.172558\n    arviz_version:               0.17.0\n    inference_library:           pymc\n    inference_library_version:   5.10.3\n    sampling_time:               2.553478956222534\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000bs(X, degree=3, knots=knots_10, intercept=False)_dim: 13y_obs: 500Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])bs(X, degree=3, knots=knots_10, intercept=False)_dim(bs(X, degree=3, knots=knots_10, intercept=False)_dim)int640 1 2 3 4 5 6 7 8 9 10 11 12array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])y_obs(y_obs)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (4)Intercept(chain, draw)float64-30.48 -9.768 17.5 ... -4.816 -3.18array([[-30.47836564,  -9.76829627,  17.49712677, ...,  17.81763441,\n         13.95495119,  -6.10602699],\n       [-16.23316086,   9.49899062,   3.70481638, ..., -25.03974245,\n         -2.91235316, -18.76784286],\n       [ -0.38968653, -23.55404923,  21.85558793, ..., -15.88911032,\n        -12.17693056, -22.799846  ],\n       [-23.78536816, -10.33769596, -18.72531118, ...,  10.30614657,\n         -4.81605641,  -3.18037091]])bs(X, degree=3, knots=knots_10, intercept=False)(chain, draw, bs(X, degree=3, knots=knots_10, intercept=False)_dim)float6460.08 -26.33 72.03 ... 26.62 -19.5array([[[ 60.08361849, -26.32632766,  72.03440963, ..., -24.69505575,\n          41.29966173, 126.0957609 ],\n        [ 42.01923744,  13.97976273, -10.66570712, ...,  37.69872884,\n          32.66180174,  56.71948798],\n        [-20.79003642, -60.86218268,   7.89670524, ..., -79.792871  ,\n          26.13817326,   5.2715798 ],\n        ...,\n        [-53.05009482, -44.90737225,  -0.53936709, ...,  11.86630968,\n         -30.56679357, -47.68641912],\n        [-31.19449725, -22.73967732,  20.06162131, ..., -28.10157326,\n           4.07136337, -21.05300466],\n        [ 35.06057706,  -9.59093606,  11.09479783, ...,  -3.88987727,\n          -8.987879  ,  27.57356096]],\n\n       [[ 33.08556413,  -1.11856386,  24.73543073, ...,  73.28686802,\n         -24.37794241,  33.15302761],\n        [-28.57730471,  -3.73441648,  -2.57506729, ..., -18.45867357,\n          -4.60293539,  -5.79920437],\n        [ -4.42415991, -12.80936972,  20.31341056, ...,   1.64870928,\n          19.07410069,  26.02601531],\n...\n        [  9.81825793,  16.07126062,  40.10272142, ..., -21.65327653,\n          34.82747494,  28.59603009],\n        [ 50.49534197, -16.63350377,  36.81576572, ...,  36.77027802,\n           8.30573837,  15.51060296],\n        [ 33.01164624,  -1.33302557,  38.41471255, ...,   3.94749231,\n          46.54871666,  43.92361804]],\n\n       [[ 18.45099645, -28.24611558,  49.26005839, ...,   4.79065054,\n          50.07280024, -17.17531704],\n        [ 24.27302084,  12.70125656,  13.48983162, ...,  23.33510231,\n         -38.49057179, 101.69703639],\n        [ 22.36020074, -13.16116681,  32.08258294, ...,  56.15103319,\n           4.60000597,  72.46523374],\n        ...,\n        [-71.35771382,  25.42919025, -19.3976876 , ..., -30.2130114 ,\n           5.95767059, -25.33043675],\n        [ 10.9586426 , -46.45830404,  45.31732601, ...,  47.8378147 ,\n         -28.12282789,  41.36265809],\n        [-31.53039023,  20.83808587, -18.46035112, ...,  -4.39658487,\n          26.61753145, -19.50485034]]])y_sigma(chain, draw)float6422.77 23.59 21.69 ... 20.98 25.07array([[22.77247029, 23.5905779 , 21.68882473, ..., 21.91856404,\n        23.1234507 , 20.69603421],\n       [23.30953138, 21.78668016, 23.3377476 , ..., 23.554608  ,\n        22.30139546, 23.25599855],\n       [25.0826106 , 24.14700624, 23.17634169, ..., 22.55796555,\n        22.00396287, 23.83528365],\n       [22.97336709, 22.34507905, 24.09241758, ..., 23.84889601,\n        20.97955574, 25.07045989]])y_mean(chain, draw, y_obs)float64-1.786e+03 -1.688e+03 ... -384.5array([[[-1785.63598357, -1688.46091242, -1594.80861303, ...,\n           471.11786457,   486.11321041,   501.38658015],\n        [-1008.56095247,  -955.43500036,  -904.13867636, ...,\n           209.04228219,   217.17041391,   225.5454136 ],\n        [  171.46343997,   166.77923669,   162.10649158, ...,\n          -298.9913081 ,  -317.81225915,  -337.30145789],\n        ...,\n        [ 1113.19763849,  1056.55963212,  1001.80503673, ...,\n           -46.36650365,   -45.39216019,   -44.30916241],\n        [  672.75237366,   638.6237798 ,   605.62977443, ...,\n          -210.41648402,  -220.83665251,  -231.5695011 ],\n        [ -981.31984966,  -927.88664463,  -876.36327872, ...,\n           245.7530471 ,   256.32233432,   267.17337954]],\n\n       [[ -892.49690627,  -844.79097031,  -798.77984615, ...,\n           543.89918243,   572.12293189,   601.25719405],\n        [  729.10432472,   690.36248691,   652.97626248, ...,\n           -29.69218582,   -31.76353623,   -33.91297753],\n        [   29.72027438,    29.25039646,    28.75480424, ...,\n            45.18641388,    45.54217006,    45.89560459],\n...\n        [ -186.24078895,  -177.60095429,  -169.24721219, ...,\n          -129.71594267,  -138.38733622,  -147.37882584],\n        [-1442.9878318 , -1364.25391604, -1288.35121975, ...,\n           100.15273049,   105.7273578 ,   111.49734282],\n        [ -904.10355066,  -855.98742694,  -809.58838617, ...,\n           -72.7318446 ,   -78.59078957,   -84.67012043]],\n\n       [[ -700.6554728 ,  -661.92143273,  -624.64606141, ...,\n          -521.29646421,  -545.21735853,  -569.83464216],\n        [ -566.28543846,  -536.82610575,  -508.37957578, ...,\n          1032.55566447,  1078.47471162,  1125.69033827],\n        [ -695.22027979,  -657.54938003,  -621.25358394, ...,\n           556.45783286,   581.91710165,   608.14063371],\n        ...,\n        [ 2030.9713329 ,  1919.9629329 ,  1812.93379418, ...,\n          -265.67005783,  -278.5688838 ,  -291.86028771],\n        [ -600.01947051,  -564.67068253,  -530.70395972, ...,\n           590.9906349 ,   619.60614144,   649.1001913 ],\n        [  954.09836044,   900.90776771,   849.64997795, ...,\n          -351.31548764,  -367.65439889,  -384.46730166]]])Indexes: (4)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))bs(X, degree=3, knots=knots_10, intercept=False)_dimPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], dtype='int64', name='bs(X, degree=3, knots=knots_10, intercept=False)_dim'))y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='y_obs', length=500))Attributes: (8)created_at :2024-05-27T06:00:45.172558arviz_version :0.17.0inference_library :pymcinference_library_version :5.10.3sampling_time :2.553478956222534tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 4, draw: 1000, y_obs: 500)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * y_obs    (y_obs) int64 0 1 2 3 4 5 6 7 8 ... 492 493 494 495 496 497 498 499\nData variables:\n    y        (chain, draw, y_obs) float64 -1.769e+03 -1.707e+03 ... -438.7\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000y_obs: 500Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])y_obs(y_obs)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (1)y(chain, draw, y_obs)float64-1.769e+03 -1.707e+03 ... -438.7array([[[-1.76850707e+03, -1.70683103e+03, -1.58503478e+03, ...,\n          4.75269803e+02,  4.71717129e+02,  4.93095980e+02],\n        [-9.78963706e+02, -9.57350338e+02, -8.94242702e+02, ...,\n          2.23703058e+02,  1.99989491e+02,  2.36217010e+02],\n        [ 1.51236792e+02,  1.44573452e+02,  1.59874803e+02, ...,\n         -2.92521983e+02, -3.39277905e+02, -3.39765298e+02],\n        ...,\n        [ 1.12245700e+03,  1.02901445e+03,  1.00163629e+03, ...,\n         -5.76818185e+01, -5.79655659e+01, -6.95232453e+01],\n        [ 7.03262674e+02,  6.89295681e+02,  5.72987411e+02, ...,\n         -2.19231936e+02, -1.91846651e+02, -1.77780985e+02],\n        [-1.00564975e+03, -9.26117757e+02, -8.32114624e+02, ...,\n          2.43447636e+02,  2.46073703e+02,  2.61526238e+02]],\n\n       [[-8.78027516e+02, -8.39940972e+02, -7.70858431e+02, ...,\n          5.62919028e+02,  5.81119362e+02,  5.98368471e+02],\n        [ 7.59822216e+02,  7.35672407e+02,  6.66036776e+02, ...,\n         -3.86148939e+01, -2.62397986e+01, -7.83702403e+01],\n        [ 6.01153527e+01, -8.09278342e-01, -4.14103639e+01, ...,\n          1.77331558e+01,  3.35647548e+01,  8.67001318e+01],\n...\n        [-2.03929257e+02, -1.59688749e+02, -1.67829907e+02, ...,\n         -1.26352656e+02, -1.58838619e+02, -1.33905356e+02],\n        [-1.42981773e+03, -1.35031319e+03, -1.30844300e+03, ...,\n          1.11580849e+02,  8.39822162e+01,  1.55903514e+02],\n        [-9.65920008e+02, -8.62270119e+02, -7.92953633e+02, ...,\n         -5.96365921e+01, -5.50070341e+01, -9.95204835e+01]],\n\n       [[-7.28155125e+02, -6.77887003e+02, -5.89951035e+02, ...,\n         -5.12375162e+02, -5.30806452e+02, -6.00515410e+02],\n        [-5.86770932e+02, -5.53033193e+02, -4.89166246e+02, ...,\n          9.92634196e+02,  1.04493361e+03,  1.12704971e+03],\n        [-7.01773152e+02, -6.43685059e+02, -5.89255386e+02, ...,\n          5.27179378e+02,  5.58208456e+02,  5.88341537e+02],\n        ...,\n        [ 2.02804805e+03,  1.90391514e+03,  1.80455354e+03, ...,\n         -2.68343325e+02, -2.78799372e+02, -2.96707748e+02],\n        [-5.94745406e+02, -5.62236707e+02, -5.62459190e+02, ...,\n          6.09916668e+02,  6.51977577e+02,  6.51319039e+02],\n        [ 9.61991994e+02,  9.05536001e+02,  8.37728249e+02, ...,\n         -3.91949049e+02, -3.49475144e+02, -4.38689017e+02]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='y_obs', length=500))Attributes: (2)modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 4, draw: 1000, y_obs: 133)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * y_obs    (y_obs) int64 0 1 2 3 4 5 6 7 8 ... 125 126 127 128 129 130 131 132\nData variables:\n    y        (chain, draw, y_obs) float64 -4.94 -4.349 -4.059 ... -4.232 -5.027\nAttributes:\n    created_at:                  2024-05-27T06:00:45.279798\n    arviz_version:               0.17.0\n    inference_library:           pymc\n    inference_library_version:   5.10.3\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000y_obs: 133Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])y_obs(y_obs)int640 1 2 3 4 5 ... 128 129 130 131 132array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132])Data variables: (1)y(chain, draw, y_obs)float64-4.94 -4.349 ... -4.232 -5.027array([[[ -4.94013004,  -4.34926962,  -4.05923397, ...,  -4.07989994,\n          -4.32526297, -10.99702367],\n        [ -4.16551546,  -4.0798865 ,  -4.34890397, ...,  -4.27847476,\n          -4.90199179,  -5.2604801 ],\n        [ -4.32114608,  -4.21922923,  -4.01523436, ...,  -4.00171403,\n          -4.46666592,  -4.15055295],\n        ...,\n        [ -4.33667694,  -4.08524041,  -4.16193486, ...,  -4.25075843,\n          -4.0367804 ,  -5.71916378],\n        [ -4.2418906 ,  -4.13815308,  -4.06475279, ...,  -4.08566333,\n          -4.13276703,  -4.35600265],\n        [ -3.99240307,  -3.95297982,  -4.22738498, ...,  -4.39489511,\n          -3.96850328,  -4.08422128]],\n\n       [[ -4.31029941,  -4.1356895 ,  -4.08943162, ...,  -4.39621813,\n          -4.11988518,  -4.10340212],\n        [ -4.09528528,  -4.02978374,  -4.0169432 , ...,  -4.09620393,\n          -4.02276603,  -4.05185642],\n        [ -4.08161109,  -4.08438013,  -4.07760521, ...,  -4.11690648,\n          -4.53977755,  -4.40149229],\n...\n        [ -4.28309395,  -4.19158751,  -4.07661987, ...,  -4.08978805,\n          -4.11622924,  -4.03898436],\n        [ -4.16328531,  -4.01182659,  -4.32670151, ...,  -4.05121559,\n          -4.0454269 ,  -4.0661975 ],\n        [ -4.54760772,  -4.2926589 ,  -4.09280683, ...,  -4.09149036,\n          -4.35227017,  -4.18573222]],\n\n       [[ -4.58924409,  -4.39837658,  -4.20729252, ...,  -4.07955443,\n          -4.09450053,  -6.58164643],\n        [ -4.13256189,  -4.0441922 ,  -4.07194408, ...,  -4.64029899,\n          -4.06235348, -10.54056155],\n        [ -4.40287769,  -4.25092797,  -4.11800136, ...,  -4.11077154,\n          -4.21757925,  -5.69653922],\n        ...,\n        [ -4.18405048,  -4.09405162,  -4.6372543 , ...,  -4.15126206,\n          -4.11639733,  -4.67240483],\n        [ -3.98883574,  -3.9655664 ,  -3.96310644, ...,  -4.21756776,\n          -3.96543135,  -4.72138657],\n        [ -4.14867518,  -4.1893545 ,  -4.35918185, ...,  -4.14286099,\n          -4.23197849,  -5.02728226]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       123, 124, 125, 126, 127, 128, 129, 130, 131, 132],\n      dtype='int64', name='y_obs', length=133))Attributes: (6)created_at :2024-05-27T06:00:45.279798arviz_version :0.17.0inference_library :pymcinference_library_version :5.10.3modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                (chain: 4, draw: 1000)\nCoordinates:\n  * chain                  (chain) int64 0 1 2 3\n  * draw                   (draw) int64 0 1 2 3 4 5 ... 994 995 996 997 998 999\nData variables: (12/17)\n    perf_counter_diff      (chain, draw) float64 0.0007404 ... 0.0007515\n    lp                     (chain, draw) float64 -721.3 -715.6 ... -710.3 -711.0\n    step_size_bar          (chain, draw) float64 0.3142 0.3142 ... 0.3369 0.3369\n    smallest_eigval        (chain, draw) float64 nan nan nan nan ... nan nan nan\n    acceptance_rate        (chain, draw) float64 0.9519 0.8589 ... 0.7224 0.9861\n    reached_max_treedepth  (chain, draw) bool False False False ... False False\n    ...                     ...\n    largest_eigval         (chain, draw) float64 nan nan nan nan ... nan nan nan\n    energy                 (chain, draw) float64 728.6 729.2 ... 725.7 714.8\n    tree_depth             (chain, draw) int64 4 4 4 4 3 4 4 5 ... 4 5 4 3 4 4 4\n    diverging              (chain, draw) bool False False False ... False False\n    index_in_trajectory    (chain, draw) int64 -7 -13 -14 -3 6 ... -3 4 -4 7 8\n    process_time_diff      (chain, draw) float64 0.000741 0.000737 ... 0.000751\nAttributes:\n    created_at:                  2024-05-27T06:00:45.180249\n    arviz_version:               0.17.0\n    inference_library:           pymc\n    inference_library_version:   5.10.3\n    sampling_time:               2.553478956222534\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (17)perf_counter_diff(chain, draw)float640.0007404 0.0007361 ... 0.0007515array([[0.00074042, 0.00073613, 0.00073075, ..., 0.00072508, 0.00071846,\n        0.00071829],\n       [0.00072588, 0.00143679, 0.00037763, ..., 0.00073321, 0.00073304,\n        0.00073225],\n       [0.00073129, 0.00073483, 0.00108604, ..., 0.00142812, 0.00037529,\n        0.00073333],\n       [0.00144608, 0.00037754, 0.0003765 , ..., 0.00075521, 0.00076121,\n        0.0007515 ]])lp(chain, draw)float64-721.3 -715.6 ... -710.3 -711.0array([[-721.32325798, -715.60693507, -712.81600649, ..., -711.63567075,\n        -711.86251963, -711.25215936],\n       [-711.83173082, -714.18292872, -708.88038704, ..., -710.81095225,\n        -707.61049434, -710.43240125],\n       [-712.7395976 , -713.40853693, -713.62383765, ..., -712.97340089,\n        -709.41442383, -710.14460595],\n       [-714.37055597, -716.70641237, -710.51320632, ..., -713.81412225,\n        -710.25741961, -710.97040762]])step_size_bar(chain, draw)float640.3142 0.3142 ... 0.3369 0.3369array([[0.31415288, 0.31415288, 0.31415288, ..., 0.31415288, 0.31415288,\n        0.31415288],\n       [0.33735905, 0.33735905, 0.33735905, ..., 0.33735905, 0.33735905,\n        0.33735905],\n       [0.35905043, 0.35905043, 0.35905043, ..., 0.35905043, 0.35905043,\n        0.35905043],\n       [0.33686986, 0.33686986, 0.33686986, ..., 0.33686986, 0.33686986,\n        0.33686986]])smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])acceptance_rate(chain, draw)float640.9519 0.8589 ... 0.7224 0.9861array([[0.95188821, 0.85891772, 0.71451549, ..., 0.8702077 , 0.50215875,\n        0.97026438],\n       [0.71509145, 0.51444105, 0.90341721, ..., 0.79993904, 0.57204968,\n        0.66470717],\n       [0.53066916, 0.66611802, 0.72800025, ..., 0.61955158, 0.92591962,\n        0.77854416],\n       [0.99982809, 0.97668472, 0.87164137, ..., 0.74461051, 0.72243678,\n        0.98614764]])reached_max_treedepth(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])step_size(chain, draw)float640.2833 0.2833 ... 0.3204 0.3204array([[0.28329617, 0.28329617, 0.28329617, ..., 0.28329617, 0.28329617,\n        0.28329617],\n       [0.23729018, 0.23729018, 0.23729018, ..., 0.23729018, 0.23729018,\n        0.23729018],\n       [0.2420878 , 0.2420878 , 0.2420878 , ..., 0.2420878 , 0.2420878 ,\n        0.2420878 ],\n       [0.32038673, 0.32038673, 0.32038673, ..., 0.32038673, 0.32038673,\n        0.32038673]])perf_counter_start(chain, draw)float642.361e+06 2.361e+06 ... 2.361e+06array([[2360978.51092729, 2360978.51171625, 2360978.51249971, ...,\n        2360979.30091483, 2360979.30168592, 2360979.30245238],\n       [2360978.47167667, 2360978.47245279, 2360978.47393825, ...,\n        2360979.25252446, 2360979.2533045 , 2360979.2540865 ],\n       [2360978.51288592, 2360978.51366746, 2360978.51445129, ...,\n        2360979.26986125, 2360979.27133717, 2360979.27175967],\n       [2360978.46373525, 2360978.46523   , 2360978.46565383, ...,\n        2360979.27246896, 2360979.27327358, 2360979.27464221]])n_steps(chain, draw)float6415.0 15.0 15.0 ... 15.0 15.0 15.0array([[15., 15., 15., ..., 15., 15., 15.],\n       [15., 31.,  7., ..., 15., 15., 15.],\n       [15., 15., 23., ..., 31.,  7., 15.],\n       [31.,  7.,  7., ..., 15., 15., 15.]])max_energy_error(chain, draw)float64-1.01 0.7776 ... 0.8487 -0.3605array([[-1.00971736,  0.77757398,  1.28280502, ...,  0.37495702,\n         1.17970654, -0.34836573],\n       [ 1.3345751 ,  2.3915387 , -1.06730429, ...,  1.15497563,\n         1.75653514,  0.89915678],\n       [ 1.40682187,  1.52508573,  1.19349047, ...,  1.84259665,\n        -1.22585309,  0.93739617],\n       [-1.54429096, -0.75846954,  0.55295383, ...,  0.76575828,\n         0.84873093, -0.36049323]])energy_error(chain, draw)float64-0.2612 -0.1061 ... -0.1596 -0.1877array([[-0.26121615, -0.10611572, -0.69771087, ...,  0.06849994,\n         0.4074744 , -0.15667011],\n       [-0.29476717,  0.28462281, -1.06730429, ..., -0.313449  ,\n        -0.58684002,  0.26092216],\n       [ 0.37035487,  0.68575505,  0.05343487, ...,  0.74127564,\n        -1.20632225,  0.29989649],\n       [-0.85058098, -0.46592943, -0.24132289, ..., -0.12983655,\n        -0.15964365, -0.18767355]])largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])energy(chain, draw)float64728.6 729.2 721.0 ... 725.7 714.8array([[728.6255778 , 729.24609198, 720.95261511, ..., 718.23864589,\n        723.51162522, 718.37013269],\n       [725.96636541, 725.37893664, 719.42266114, ..., 721.44682066,\n        717.94143164, 714.93926686],\n       [724.0278209 , 722.19477874, 721.36794365, ..., 720.83274011,\n        716.33821598, 715.0968274 ],\n       [723.0827726 , 724.98007703, 723.47496067, ..., 724.58398944,\n        725.68406617, 714.81290627]])tree_depth(chain, draw)int644 4 4 4 3 4 4 5 ... 5 4 5 4 3 4 4 4array([[4, 4, 4, ..., 4, 4, 4],\n       [4, 5, 3, ..., 4, 4, 4],\n       [4, 4, 5, ..., 5, 3, 4],\n       [5, 3, 3, ..., 4, 4, 4]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])index_in_trajectory(chain, draw)int64-7 -13 -14 -3 6 -8 ... -3 4 -4 7 8array([[ -7, -13, -14, ...,   5,   3,   5],\n       [  6,  -4,  -4, ...,  -8,  14, -13],\n       [ -7, -13, -14, ...,  10,  -5,   8],\n       [ -8,  -5,   4, ...,  -4,   7,   8]])process_time_diff(chain, draw)float640.000741 0.000737 ... 0.000751array([[0.000741, 0.000737, 0.000731, ..., 0.000724, 0.000719, 0.000718],\n       [0.000725, 0.001437, 0.000377, ..., 0.000734, 0.000732, 0.000732],\n       [0.000731, 0.000735, 0.001086, ..., 0.001429, 0.000375, 0.000734],\n       [0.001446, 0.000378, 0.000377, ..., 0.000756, 0.000762, 0.000751]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (8)created_at :2024-05-27T06:00:45.180249arviz_version :0.17.0inference_library :pymcinference_library_version :5.10.3sampling_time :2.553478956222534tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (y_obs: 133)\nCoordinates:\n  * y_obs    (y_obs) int64 0 1 2 3 4 5 6 7 8 ... 125 126 127 128 129 130 131 132\nData variables:\n    y        (y_obs) float64 0.0 -1.3 -2.7 0.0 -2.7 ... -2.7 10.7 -2.7 10.7\nAttributes:\n    created_at:                  2024-05-27T06:00:45.182737\n    arviz_version:               0.17.0\n    inference_library:           pymc\n    inference_library_version:   5.10.3\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:y_obs: 133Coordinates: (1)y_obs(y_obs)int640 1 2 3 4 5 ... 128 129 130 131 132array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132])Data variables: (1)y(y_obs)float640.0 -1.3 -2.7 ... 10.7 -2.7 10.7array([   0. ,   -1.3,   -2.7,    0. ,   -2.7,   -2.7,   -2.7,   -1.3,\n         -2.7,   -2.7,   -1.3,   -2.7,   -2.7,   -2.7,   -5.4,   -2.7,\n         -5.4,    0. ,   -2.7,   -2.7,    0. ,  -13.3,   -5.4,   -5.4,\n         -9.3,  -16. ,  -22.8,   -2.7,  -22.8,  -32.1,  -53.5,  -54.9,\n        -40.2,  -21.5,  -21.5,  -50.8,  -42.9,  -26.8,  -21.5,  -50.8,\n        -61.7,   -5.4,  -80.4,  -59. ,  -71. ,  -91.1,  -77.7,  -37.5,\n        -85.6, -123.1, -101.9,  -99.1, -104.4, -112.5,  -50.8, -123.1,\n        -85.6,  -72.3, -127.2, -123.1, -117.9, -134. , -101.9, -108.4,\n       -123.1, -123.1, -128.5, -112.5,  -95.1,  -81.8,  -53.5,  -64.4,\n        -57.6,  -72.3,  -44.3,  -26.8,   -5.4, -107.1,  -21.5,  -65.6,\n        -16. ,  -45.6,  -24.2,    9.5,    4. ,   12. ,  -21.5,   37.5,\n         46.9,  -17.4,   36.2,   75. ,    8.1,   54.9,   48.2,   46.9,\n         16. ,   45.6,    1.3,   75. ,  -16. ,  -54.9,   69.6,   34.8,\n         32.1,  -37.5,   22.8,   46.9,   10.7,    5.4,   -1.3,  -21.5,\n        -13.3,   30.8,  -10.7,   29.4,    0. ,  -10.7,   14.7,   -1.3,\n          0. ,   10.7,   10.7,  -26.8,  -14.7,  -13.3,    0. ,   10.7,\n        -14.7,   -2.7,   10.7,   -2.7,   10.7])Indexes: (1)y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       123, 124, 125, 126, 127, 128, 129, 130, 131, 132],\n      dtype='int64', name='y_obs', length=133))Attributes: (6)created_at :2024-05-27T06:00:45.182737arviz_version :0.17.0inference_library :pymcinference_library_version :5.10.3modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n              \n            \n            \n\n\nNext we plot the posterior predictive distribution of our observed variable and compare against the observed data. Additionally we plot the 89th and 50% HDI.\n\nax = az.plot_hdi(new_data['X'], idata_spline4['posterior_predictive']['y'], fill_kwargs={'alpha': 0.2, 'color':'firebrick'}, hdi_prob=0.89, figsize=(10, 8))\n\naz.plot_hdi(new_data['X'], idata_spline4['posterior_predictive']['y'], fill_kwargs={'alpha': 0.8, 'color':'firebrick'}, hdi_prob=0.5)\n\ny_mean = idata_spline4['posterior_predictive']['y'].mean(dim=('chain', 'draw'))\n\nax.plot(new_data['X'], y_mean, label='Expected posterior predictive', color='k')\nax.set_xlabel(\"Time Point\")\nax.set_ylabel(\"Acceleration\")\n\nax.scatter(df['X'], df['y'], label='Observed Datapoints')\n\nax.legend()\n\nax.set_title(\"Posterior Predictive Distribution \\n Based on 10 Knots\");\n\n\n\n\nThis represents a good a clean model fit to the observed data using univariate spline smoothers. However, it’s clear that the uncertainty spikes massively with data points even slightly out of the training data. Next we’ll see another alternative approach to model this outcome variable using approximate gaussian processes."
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#gaussian-processes",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#gaussian-processes",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Gaussian processes",
    "text": "Gaussian processes\nThe topic of gaussian processes is rich and detailed. Too rich to be fairly covered in this blog post, so we’ll just say that we’re using a method designed for function approximation that makes use of drawing samples from a multivariate normal distribution under a range of different covariance relationships.\nThese relationships can be somewhat intuitively interrogated by defining different combinations of covariance relationships with priors over the parameters governing the covariance of a sequence of points. For example consider the following parameterisations.\n\nlengthscale = 3\nsigma = 13\ncov = sigma**2 * pm.gp.cov.ExpQuad(1, lengthscale)\n\nX = np.linspace(0, 60, 200)[:, None]\nK = cov(X).eval()\n\nfig, ax = plt.subplots(figsize=(9, 7))\nax.plot(\n    X,\n    pm.draw(\n        pm.MvNormal.dist(mu=np.zeros(len(K)), cov=K, shape=K.shape[0]), draws=10, random_seed=random_seed\n    ).T,\n)\nplt.title(f\"Samples from the GP prior \\n lengthscale: {3}, sigma: {13}\")\nplt.ylabel(\"y\")\nplt.xlabel(\"X\");\n\n\n\n\nWe’ve specified the range of X to reflect the support of the acceleration example and allowed the draws to be informed by a covariance function we have parameterised using the Exponentiated Quadratic kernel:\n\\[k(x, x') = \\mathrm{exp}\\left[ -\\frac{(x - x')^2}{2 \\ell^2} \\right]\\]\nThe patterns exhibited show a good range of “wiggliness” that they should be flexible enough to capture the shape of the acceleration, if we can calibrate the posterior of parameters against the observed data.\n\nPriors on Gaussian Priors\nConsider the following specification for the priors\n\nfig, axs = plt.subplots(1, 2, figsize=(9, 6))\naxs = axs.flatten()\naxs[0].hist(pm.draw(pm.InverseGamma.dist(mu=1, sigma=1), 1000), ec='black', bins=30);\naxs[0].set_title(\"Priors for Lengthscale \\n in ExpQuad Kernel\")\naxs[1].hist(pm.draw(pm.Exponential.dist(lam=1), 1000), ec='black', bins=30);\naxs[1].set_title(\"Priors for Amplitude \\n in ExpQuad Kernel\")\n\nText(0.5, 1.0, 'Priors for Amplitude \\n in ExpQuad Kernel')\n\n\n\n\n\nWe use these to specify priors on the Hilbert space approximation of gaussian priors available in the Bambi package.\n\nprior_hsgp = {\n    \"sigma\": bmb.Prior(\"Exponential\", lam=1), # amplitude\n    \"ell\": bmb.Prior(\"InverseGamma\", mu=1, sigma=1) # lengthscale\n}\n\n# This is the dictionary we pass to Bambi\npriors = {\n    \"hsgp(X, m=10, c=1)\": prior_hsgp,\n    \"sigma\": bmb.Prior(\"HalfNormal\", sigma=4)\n}\nmodel_hsgp = bmb.Model(\"y ~ 0 + hsgp(X, m=10, c=1)\", df, priors=priors)\nmodel_hsgp\n\n       Formula: y ~ 0 + hsgp(X, m=10, c=1)\n        Family: gaussian\n          Link: mu = identity\n  Observations: 133\n        Priors: \n    target = mu\n        HSGP contributions\n            hsgp(X, m=10, c=1)\n                cov: ExpQuad\n                sigma ~ Exponential(lam: 1.0)\n                ell ~ InverseGamma(mu: 1.0, sigma: 1.0)\n        \n        Auxiliary parameters\n            sigma ~ HalfNormal(sigma: 4.0)\n\n\nHere we’ve set the m=10 to determine the number of basis vectors used in the Hilbert space approximation. The idea differs in detail from the spline based approximations we’ve seen, but it’s perhaps useful to think of the process in the same vein.\n\nidata = model_hsgp.fit(inference_method=\"nuts_numpyro\",target_accept=0.95, random_seed=121195, \nidata_kwargs={\"log_likelihood\": True})\nprint(idata.sample_stats[\"diverging\"].sum().to_numpy())\n\nCompiling...\n\n\nCompilation time = 0:00:01.030799\n\n\nSampling...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling time = 0:00:01.825560\n\n\nTransforming variables...\n\n\nTransformation time = 0:00:00.177224\n\n\nComputing Log Likelihood...\n\n\nLog Likelihood time = 0:00:00.144392\n\n\n0\n\n\nThis model fits and the sampling seems to have worked well.\n\naz.plot_trace(idata, backend_kwargs={\"layout\": \"constrained\"}, figsize=(9, 15));\n\n\n\n\nThe lengthscale and sigma parameters we have learned by calibrating our priors against the data. The degree to which these parameters are meaningful depend a little on how familar you are with covariance matrix kernels and their properties, so we won’t dwell on the point here.\n\naz.summary(idata, var_names=['hsgp(X, m=10, c=1)_ell', 'hsgp(X, m=10, c=1)_sigma', 'y_sigma', 'hsgp(X, m=10, c=1)_weights'])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      hsgp(X, m=10, c=1)_ell\n      3.301\n      0.666\n      2.099\n      4.567\n      0.016\n      0.011\n      1763.0\n      1815.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_sigma\n      23.675\n      2.679\n      18.925\n      28.847\n      0.069\n      0.049\n      1497.0\n      2167.0\n      1.0\n    \n    \n      y_sigma\n      20.589\n      1.121\n      18.590\n      22.751\n      0.019\n      0.014\n      3440.0\n      2485.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[0]\n      -131.588\n      13.784\n      -157.019\n      -105.189\n      0.216\n      0.154\n      4071.0\n      3020.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[1]\n      -94.391\n      18.010\n      -125.921\n      -58.020\n      0.363\n      0.256\n      2500.0\n      2876.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[2]\n      106.121\n      20.717\n      69.627\n      147.247\n      0.458\n      0.324\n      2046.0\n      2563.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[3]\n      149.289\n      22.154\n      106.508\n      188.107\n      0.521\n      0.368\n      1807.0\n      2467.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[4]\n      -83.399\n      22.728\n      -124.812\n      -40.406\n      0.539\n      0.383\n      1776.0\n      2517.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[5]\n      -125.322\n      23.273\n      -169.346\n      -82.187\n      0.531\n      0.377\n      1917.0\n      2940.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[6]\n      37.835\n      21.153\n      -1.712\n      77.574\n      0.512\n      0.362\n      1713.0\n      2577.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[7]\n      94.574\n      20.745\n      53.167\n      132.182\n      0.423\n      0.299\n      2393.0\n      3105.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[8]\n      -1.494\n      17.184\n      -31.835\n      33.063\n      0.361\n      0.255\n      2264.0\n      2853.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[9]\n      -45.442\n      15.955\n      -76.649\n      -16.636\n      0.289\n      0.208\n      3041.0\n      3183.0\n      1.0\n    \n  \n\n\n\n\nBut again we can sample from the posterior predictive distribution of the outcome variable\n\nmodel_hsgp.predict(idata, data=new_data, \nkind='pps', inplace=True)\n\nidata\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                         (chain: 4, draw: 1000,\n                                     hsgp(X, m=10, c=1)_weights_dim: 10,\n                                     y_obs: 500)\nCoordinates:\n  * chain                           (chain) int64 0 1 2 3\n  * draw                            (draw) int64 0 1 2 3 4 ... 996 997 998 999\n  * hsgp(X, m=10, c=1)_weights_dim  (hsgp(X, m=10, c=1)_weights_dim) int64 0 ...\n  * y_obs                           (y_obs) int64 0 1 2 3 4 ... 496 497 498 499\nData variables:\n    hsgp(X, m=10, c=1)_weights_raw  (chain, draw, hsgp(X, m=10, c=1)_weights_dim) float64 ...\n    y_sigma                         (chain, draw) float64 20.86 19.86 ... 21.17\n    hsgp(X, m=10, c=1)_sigma        (chain, draw) float64 26.85 24.0 ... 18.06\n    hsgp(X, m=10, c=1)_ell          (chain, draw) float64 3.861 3.509 ... 3.76\n    hsgp(X, m=10, c=1)_weights      (chain, draw, hsgp(X, m=10, c=1)_weights_dim) float64 ...\n    y_mean                          (chain, draw, y_obs) float64 -0.1748 ... ...\n    hsgp(X, m=10, c=1)              (chain, draw, y_obs) float64 -0.1748 ... ...\nAttributes:\n    created_at:                  2024-04-16T21:48:50.636485\n    arviz_version:               0.17.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000hsgp(X, m=10, c=1)_weights_dim: 10y_obs: 500Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])hsgp(X, m=10, c=1)_weights_dim(hsgp(X, m=10, c=1)_weights_dim)int640 1 2 3 4 5 6 7 8 9array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])y_obs(y_obs)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (7)hsgp(X, m=10, c=1)_weights_raw(chain, draw, hsgp(X, m=10, c=1)_weights_dim)float64-1.452 -0.974 ... 0.9659 -0.2453array([[[-1.45186697, -0.97401433,  1.51626692, ...,  1.84709969,\n          0.92584892, -1.19616826],\n        [-1.73799921, -1.55920285,  1.42827017, ...,  1.78787921,\n         -1.14159607, -1.66042935],\n        [-1.3059213 , -1.22687096,  2.0214052 , ...,  2.80079646,\n          0.17462599, -0.06073935],\n        ...,\n        [-2.26781832, -1.21871881,  1.80249133, ...,  2.65551316,\n          0.30040289, -0.7085128 ],\n        [-2.23372877, -1.1832454 ,  1.74365597, ...,  2.71376469,\n          0.41123845, -0.60671544],\n        [-1.96117304, -1.13093178,  2.24600086, ...,  2.3750183 ,\n         -0.00901474, -0.75147244]],\n\n       [[-1.95999601, -1.27507311,  1.52467611, ...,  3.28813978,\n         -0.1598232 , -1.10988783],\n        [-1.59314189, -1.44864255,  1.95956808, ...,  2.01431167,\n          1.0199955 , -0.8962025 ],\n        [-1.8432561 , -1.32415496,  1.80874966, ...,  3.05464118,\n          0.81105708, -1.02872434],\n...\n        [-1.94277245, -1.5433581 ,  1.78771407, ...,  2.11464836,\n         -0.62470516, -1.01229181],\n        [-1.96747626, -1.39055532,  1.24775496, ...,  2.93940091,\n          0.46989555, -1.57735663],\n        [-2.20654554, -1.042289  ,  1.46402154, ...,  3.37106106,\n         -0.11061588, -2.15605928]],\n\n       [[-1.68378706, -0.84035677,  1.41475357, ...,  2.46320953,\n          0.88793346, -0.56144583],\n        [-1.66959934, -1.01014124,  1.6874097 , ...,  2.71030445,\n          0.68597924, -0.76198643],\n        [-1.54563186, -0.89279988,  1.77115866, ...,  2.3842957 ,\n          0.08392828, -0.78058783],\n        ...,\n        [-2.17148384, -1.59167361,  1.34960308, ...,  2.34802173,\n          0.44912308, -1.0698665 ],\n        [-1.31376138, -0.63355604,  1.97759132, ...,  1.8841587 ,\n         -0.13453184, -1.36605679],\n        [-2.0018976 , -1.36470426,  2.15352253, ...,  3.17100177,\n          0.96590611, -0.24527631]]])y_sigma(chain, draw)float6420.86 19.86 21.17 ... 20.12 21.17array([[20.85613959, 19.86298062, 21.16886643, ..., 19.48940449,\n        19.31511941, 19.44304302],\n       [18.5908299 , 21.71605063, 21.22255223, ..., 19.44547467,\n        19.21042952, 22.0099345 ],\n       [20.48425625, 21.17420065, 22.11318822, ..., 18.40825711,\n        21.65013089, 22.05150211],\n       [21.08143229, 20.7215663 , 22.60954219, ..., 21.15149371,\n        20.11622356, 21.16568101]])hsgp(X, m=10, c=1)_sigma(chain, draw)float6426.85 24.0 27.81 ... 26.09 18.06array([[26.84572148, 24.00472166, 27.80754218, ..., 23.34108117,\n        23.72941745, 22.60241785],\n       [22.01917097, 24.17709343, 26.16315265, ..., 22.03189338,\n        22.17588023, 24.00832607],\n       [24.12239909, 24.36393141, 25.79692715, ..., 24.33866729,\n        20.63877848, 22.26540398],\n       [25.41092492, 25.05093181, 28.3790263 , ..., 22.30157727,\n        26.0901197 , 18.06420147]])hsgp(X, m=10, c=1)_ell(chain, draw)float643.861 3.509 3.695 ... 3.679 3.76array([[3.86057038, 3.50871714, 3.69475981, ..., 3.20961859, 3.07237787,\n        2.99194079],\n       [4.21093385, 4.11858175, 3.55744155, ..., 3.54737759, 2.43665447,\n        3.67009957],\n       [3.38790729, 3.56906294, 3.07026931, ..., 3.17401967, 4.77493695,\n        4.1757221 ],\n       [3.73124202, 3.62463311, 4.06803959, ..., 3.06030257, 3.67928011,\n        3.75995625]])hsgp(X, m=10, c=1)_weights(chain, draw, hsgp(X, m=10, c=1)_weights_dim)float64-120.2 -78.54 ... 27.36 -5.933array([[[-120.19159663,  -78.54474016,  117.04015633, ...,\n           88.13161831,   38.07210878,  -41.65695052],\n        [-122.83658383, -107.83699596,   95.27680851, ...,\n           80.15736607,  -45.26655282,  -57.3944273 ],\n        [-109.63228325, -100.55013819,  159.16260856, ...,\n          141.94233647,    7.72311493,   -2.30700494],\n        ...,\n        [-149.23661164,  -78.75788623,  113.01499747, ...,\n          119.40081633,   12.18790241,  -25.62639515],\n        [-146.28304701,  -76.21158457,  109.23906639, ...,\n          125.36332205,   17.28993874,  -22.96014529],\n        [-120.75657952,  -68.54675039,  132.60289356, ...,\n          105.03375274,   -0.36461133,  -27.50691292]],\n\n       [[-138.76216692,  -87.49701007,   99.32062774, ...,\n          120.85182044,   -4.92170357,  -28.04716014],\n        [-122.53330814, -108.14123925,  139.17924148, ...,\n           82.74939574,   35.37871319,  -25.72818479],\n        [-142.94342858, -100.42482592,  132.17632445, ...,\n          148.36712045,   34.72137014,  -38.24384896],\n...\n        [-132.58698442, -103.47676242,  116.36868708, ...,\n           99.43921353,  -26.56716678,  -38.47612061],\n        [-138.61535955,  -94.11467693,   78.98484564, ...,\n           89.13997713,   11.35087487,  -29.54951483],\n        [-157.32953055,  -72.06983662,   96.1818003 , ...,\n          126.1515611 ,   -3.47852942,  -55.82170316]],\n\n       [[-129.78695588,  -63.20664967,  102.15003445, ...,\n          113.4760498 ,   35.60103971,  -19.27405014],\n        [-125.10213961,  -73.9587159 ,  118.87369923, ...,\n          124.94562328,   27.73901676,  -26.61384578],\n        [-138.71497899,  -77.82483202,  147.07295446, ...,\n          116.05385128,    3.46342745,  -26.78443203],\n        ...,\n        [-133.39280566,  -96.17651826,   79.33895456, ...,\n          102.02385007,   17.77408526,  -38.1416893 ],\n        [-103.26872735,  -48.62806342,  145.87733997, ...,\n           89.78661037,   -5.60101795,  -48.90554932],\n        [-110.1016679 ,  -73.21183754,  110.83494684, ...,\n          103.40789151,   27.35517734,   -5.93337047]]])y_mean(chain, draw, y_obs)float64-0.1748 -0.6437 ... 4.329e-15array([[[-1.74825831e-01, -6.43722945e-01, -1.10198757e+00, ...,\n          8.81438724e-01,  4.41420533e-01,  1.00916482e-14],\n        [ 3.99710952e+00,  4.36245618e+00,  4.70113794e+00, ...,\n         -3.68931837e-01, -1.84461976e-01, -4.21486081e-15],\n        [ 1.12371919e+01,  9.85417314e+00,  8.49086985e+00, ...,\n         -4.14235314e-01, -2.07469138e-01, -4.74327547e-15],\n        ...,\n        [ 5.14480643e-01, -3.24968824e-01, -1.15248120e+00, ...,\n          2.85647398e-02,  1.43964056e-02,  3.29822057e-16],\n        [-1.15486831e+00, -2.03017733e+00, -2.88893220e+00, ...,\n         -1.21523155e-01, -6.06829841e-02, -1.38598577e-15],\n        [ 1.21777447e+01,  1.16287837e+01,  1.10821348e+01, ...,\n          1.95250577e-01,  9.77499077e-02,  2.23450097e-15]],\n\n       [[-2.44479507e+00, -2.94292536e+00, -3.43456273e+00, ...,\n         -7.42363606e-01, -3.71378424e-01, -8.48736477e-15],\n        [-3.36694245e+00, -3.93597519e+00, -4.48819398e+00, ...,\n          1.45535040e+00,  7.28337111e-01,  1.66472919e-14],\n        [ 1.86823417e-01, -1.03465399e+00, -2.23999952e+00, ...,\n          3.24627418e-01,  1.62576930e-01,  3.71683234e-15],\n...\n        [-3.54206467e+00, -3.29770384e+00, -3.05890542e+00, ...,\n         -8.07266002e-01, -4.03861987e-01, -9.22984926e-15],\n        [-8.28095921e+00, -8.58481681e+00, -8.88022655e+00, ...,\n         -1.37004285e-01, -6.83094783e-02, -1.55937910e-15],\n        [ 1.05497489e+00,  4.74317876e-01, -1.10703434e-01, ...,\n         -4.72148575e-01, -2.35887059e-01, -5.38850915e-15]],\n\n       [[-4.02175043e+00, -4.75614076e+00, -5.46523621e+00, ...,\n         -1.18443060e-01, -5.89840516e-02, -1.34595499e-15],\n        [ 3.62393761e+00,  2.72714782e+00,  1.84762910e+00, ...,\n         -9.97541423e-02, -4.96901944e-02, -1.13397909e-15],\n        [ 1.18490633e+01,  1.10655715e+01,  1.02860360e+01, ...,\n         -5.74333102e-01, -2.87269966e-01, -6.56480568e-15],\n        ...,\n        [-9.39146019e+00, -9.93167964e+00, -1.04627004e+01, ...,\n          1.23581084e-01,  6.21200724e-02,  1.42193352e-15],\n        [ 2.48719138e+01,  2.46289484e+01,  2.43707844e+01, ...,\n         -9.35532279e-02, -4.65445693e-02, -1.06175818e-15],\n        [-5.90472295e-01, -1.42675597e+00, -2.23855095e+00, ...,\n          3.78505074e-01,  1.89406640e-01,  4.32904960e-15]]])hsgp(X, m=10, c=1)(chain, draw, y_obs)float64-0.1748 -0.6437 ... 4.329e-15array([[[-1.74825831e-01, -6.43722945e-01, -1.10198757e+00, ...,\n          8.81438724e-01,  4.41420533e-01,  1.00916482e-14],\n        [ 3.99710952e+00,  4.36245618e+00,  4.70113794e+00, ...,\n         -3.68931837e-01, -1.84461976e-01, -4.21486081e-15],\n        [ 1.12371919e+01,  9.85417314e+00,  8.49086985e+00, ...,\n         -4.14235314e-01, -2.07469138e-01, -4.74327547e-15],\n        ...,\n        [ 5.14480643e-01, -3.24968824e-01, -1.15248120e+00, ...,\n          2.85647398e-02,  1.43964056e-02,  3.29822057e-16],\n        [-1.15486831e+00, -2.03017733e+00, -2.88893220e+00, ...,\n         -1.21523155e-01, -6.06829841e-02, -1.38598577e-15],\n        [ 1.21777447e+01,  1.16287837e+01,  1.10821348e+01, ...,\n          1.95250577e-01,  9.77499077e-02,  2.23450097e-15]],\n\n       [[-2.44479507e+00, -2.94292536e+00, -3.43456273e+00, ...,\n         -7.42363606e-01, -3.71378424e-01, -8.48736477e-15],\n        [-3.36694245e+00, -3.93597519e+00, -4.48819398e+00, ...,\n          1.45535040e+00,  7.28337111e-01,  1.66472919e-14],\n        [ 1.86823417e-01, -1.03465399e+00, -2.23999952e+00, ...,\n          3.24627418e-01,  1.62576930e-01,  3.71683234e-15],\n...\n        [-3.54206467e+00, -3.29770384e+00, -3.05890542e+00, ...,\n         -8.07266002e-01, -4.03861987e-01, -9.22984926e-15],\n        [-8.28095921e+00, -8.58481681e+00, -8.88022655e+00, ...,\n         -1.37004285e-01, -6.83094783e-02, -1.55937910e-15],\n        [ 1.05497489e+00,  4.74317876e-01, -1.10703434e-01, ...,\n         -4.72148575e-01, -2.35887059e-01, -5.38850915e-15]],\n\n       [[-4.02175043e+00, -4.75614076e+00, -5.46523621e+00, ...,\n         -1.18443060e-01, -5.89840516e-02, -1.34595499e-15],\n        [ 3.62393761e+00,  2.72714782e+00,  1.84762910e+00, ...,\n         -9.97541423e-02, -4.96901944e-02, -1.13397909e-15],\n        [ 1.18490633e+01,  1.10655715e+01,  1.02860360e+01, ...,\n         -5.74333102e-01, -2.87269966e-01, -6.56480568e-15],\n        ...,\n        [-9.39146019e+00, -9.93167964e+00, -1.04627004e+01, ...,\n          1.23581084e-01,  6.21200724e-02,  1.42193352e-15],\n        [ 2.48719138e+01,  2.46289484e+01,  2.43707844e+01, ...,\n         -9.35532279e-02, -4.65445693e-02, -1.06175818e-15],\n        [-5.90472295e-01, -1.42675597e+00, -2.23855095e+00, ...,\n          3.78505074e-01,  1.89406640e-01,  4.32904960e-15]]])Indexes: (4)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))hsgp(X, m=10, c=1)_weights_dimPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64', name='hsgp(X, m=10, c=1)_weights_dim'))y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='y_obs', length=500))Attributes: (4)created_at :2024-04-16T21:48:50.636485arviz_version :0.17.0modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 4, draw: 1000, y_obs: 500)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * y_obs    (y_obs) int64 0 1 2 3 4 5 6 7 8 ... 492 493 494 495 496 497 498 499\nData variables:\n    y        (chain, draw, y_obs) float64 52.29 12.0 -15.8 ... -7.277 -0.6193\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000y_obs: 500Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])y_obs(y_obs)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (1)y(chain, draw, y_obs)float6452.29 12.0 -15.8 ... -7.277 -0.6193array([[[ 5.22936097e+01,  1.19978202e+01, -1.57982224e+01, ...,\n         -2.94686487e+01, -1.15648426e+00,  3.05766096e+01],\n        [-4.76584635e+01,  1.27246989e+01,  1.32011413e+01, ...,\n         -1.46264536e+01, -3.40966683e+01,  4.27381431e+00],\n        [ 6.57604280e+01, -1.58375968e+00,  3.04687510e+01, ...,\n          9.17079025e+00, -1.31545895e+01, -3.53084183e+00],\n        ...,\n        [ 1.12540415e+01, -1.03612151e+01,  5.16925300e+00, ...,\n         -8.80372304e+00,  4.23855827e+01,  3.86672697e+01],\n        [ 1.58179199e+01,  1.81354709e+00, -1.61407921e+01, ...,\n          2.99057653e+01, -2.81156162e+01,  3.01082768e+01],\n        [ 5.00906194e+01,  1.72989035e+01,  8.72112249e+00, ...,\n         -2.18459942e+01, -1.22332010e+01,  3.47301391e+01]],\n\n       [[ 4.98456573e+01, -2.15902881e+01, -1.01091765e+01, ...,\n          2.77829573e+01, -8.31966503e+00,  4.05382474e+01],\n        [-6.18665801e+01, -1.89799539e+01, -2.18600703e+01, ...,\n         -2.46085346e+00,  3.79662766e+01,  2.12367943e+01],\n        [ 2.82531065e+00, -2.66180770e+01, -3.09302003e+00, ...,\n         -1.47884850e+01, -1.35979869e+01,  1.66919798e+01],\n...\n        [-2.78424147e+01, -2.76704484e+01, -3.40801106e+01, ...,\n         -3.34053498e-01, -3.04495384e+01,  1.28357634e+01],\n        [-7.72085676e+00, -2.97495829e+01, -2.74093551e+01, ...,\n          3.75289441e+01,  1.50098225e+01,  1.27876358e+00],\n        [ 3.90403640e+01,  1.92823746e+01, -5.97898444e+00, ...,\n         -8.27064422e-01, -8.40095524e+00,  3.04210549e+01]],\n\n       [[ 9.80420140e-01,  2.62293682e+00, -2.41407897e+00, ...,\n         -1.36930920e-02,  9.47470172e+00,  3.82376791e+00],\n        [ 2.02564956e+01, -7.67608538e+00,  2.91352132e+01, ...,\n         -1.45786727e+01, -1.40717625e+01,  1.21951915e+01],\n        [ 3.34246765e+01,  3.06645368e+01,  2.89732275e+01, ...,\n         -5.44012965e+00,  5.39713832e+00,  2.42196230e+00],\n        ...,\n        [-3.82283445e+00, -7.38678013e+00, -3.83033130e+01, ...,\n         -1.07484233e+01,  4.01568557e+00,  1.61832716e+01],\n        [ 4.19341088e+00, -1.44430520e+01,  4.93944933e+01, ...,\n          2.17531046e+00,  1.78504565e+01,  1.26495414e+01],\n        [ 3.57253266e+01, -8.19945322e+00, -2.48977977e+01, ...,\n          2.75647652e+00, -7.27697932e+00, -6.19334674e-01]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='y_obs', length=500))Attributes: (2)modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 4, draw: 1000, y_obs: 133)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * y_obs    (y_obs) int64 0 1 2 3 4 5 6 7 8 ... 125 126 127 128 129 130 131 132\nData variables:\n    y        (chain, draw, y_obs) float64 -3.957 -3.957 -3.957 ... -4.013 -4.099\nAttributes:\n    created_at:                  2024-04-16T21:48:50.640043\n    arviz_version:               0.17.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000y_obs: 133Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])y_obs(y_obs)int640 1 2 3 4 5 ... 128 129 130 131 132array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132])Data variables: (1)y(chain, draw, y_obs)float64-3.957 -3.957 ... -4.013 -4.099array([[[-3.95662204, -3.95668036, -3.95697947, ..., -3.96786393,\n         -4.06567925, -4.08819109],\n        [-3.92804385, -3.95248402, -4.00438808, ..., -4.19279911,\n         -3.90894979, -4.05289002],\n        [-4.11236339, -4.08418887, -3.99366348, ..., -4.1948162 ,\n         -3.97180293, -4.09921461],\n        ...,\n        [-3.88915792, -3.88893229, -3.89724674, ..., -4.04248536,\n         -3.89847518, -4.03951892],\n        [-3.88161419, -3.88254965, -3.9052254 , ..., -4.08668059,\n         -3.88211318, -4.03326819],\n        [-4.08257196, -4.09265666, -4.04648571, ..., -3.99108035,\n         -3.91131912, -4.03785686]],\n\n       [[-3.85025382, -3.84763163, -3.85608195, ..., -4.34165517,\n         -3.86684608, -4.00723736],\n        [-4.0090095 , -4.00707108, -4.017144  , ..., -4.01175855,\n         -4.25268066, -4.11837821],\n        [-3.97404168, -3.97456237, -4.00764182, ..., -4.0433394 ,\n         -4.00511976, -4.10110199],\n...\n        [-3.85025004, -3.83654156, -3.832685  , ..., -4.37637177,\n         -3.86484545, -4.00066997],\n        [-4.06709934, -4.05434047, -4.05493257, ..., -4.17570336,\n         -3.99476606, -4.11607839],\n        [-4.01346366, -4.01406212, -4.01257167, ..., -4.30726434,\n         -4.01839009, -4.13004222]],\n\n       [[-3.98552821, -3.98561292, -4.0079376 , ..., -4.15815157,\n         -3.96833515, -4.09613757],\n        [-3.96540632, -3.96291088, -3.95021137, ..., -4.13476712,\n         -3.95195981, -4.08343265],\n        [-4.17463712, -4.17201421, -4.11695573, ..., -4.3197009 ,\n         -4.04463544, -4.14929407],\n        ...,\n        [-4.0692212 , -4.06241625, -4.09034963, ..., -4.09469991,\n         -3.98159516, -4.09860353],\n        [-4.68482086, -4.73793167, -4.72387693, ..., -4.11913623,\n         -3.92229916, -4.06192875],\n        [-3.97170872, -3.97200671, -3.98370832, ..., -4.02417539,\n         -4.01254991, -4.09910258]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       123, 124, 125, 126, 127, 128, 129, 130, 131, 132],\n      dtype='int64', name='y_obs', length=133))Attributes: (4)created_at :2024-04-16T21:48:50.640043arviz_version :0.17.0modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:          (chain: 4, draw: 1000)\nCoordinates:\n  * chain            (chain) int64 0 1 2 3\n  * draw             (draw) int64 0 1 2 3 4 5 6 ... 993 994 995 996 997 998 999\nData variables:\n    acceptance_rate  (chain, draw) float64 0.9484 0.989 0.9772 ... 0.9905 0.9283\n    step_size        (chain, draw) float64 0.1631 0.1631 ... 0.1827 0.1827\n    diverging        (chain, draw) bool False False False ... False False False\n    energy           (chain, draw) float64 671.4 665.4 671.0 ... 670.8 674.0\n    n_steps          (chain, draw) int64 31 31 31 15 15 31 ... 15 31 31 15 15 31\n    tree_depth       (chain, draw) int64 5 5 5 4 4 5 5 4 4 ... 4 5 5 4 5 5 4 4 5\n    lp               (chain, draw) float64 661.7 661.3 665.7 ... 665.6 668.8\nAttributes:\n    created_at:                  2024-04-16T21:48:50.639029\n    arviz_version:               0.17.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (7)acceptance_rate(chain, draw)float640.9484 0.989 ... 0.9905 0.9283array([[0.94837217, 0.98900712, 0.97718268, ..., 0.99599719, 0.97974622,\n        0.93727528],\n       [0.9824305 , 0.87426419, 0.99008681, ..., 0.99942845, 0.97008375,\n        0.99555447],\n       [0.99910745, 0.98130743, 0.95550076, ..., 0.97069333, 0.9342073 ,\n        0.95033605],\n       [0.97363799, 0.97895034, 0.93808177, ..., 0.9514047 , 0.99054107,\n        0.92830969]])step_size(chain, draw)float640.1631 0.1631 ... 0.1827 0.1827array([[0.16306409, 0.16306409, 0.16306409, ..., 0.16306409, 0.16306409,\n        0.16306409],\n       [0.15671706, 0.15671706, 0.15671706, ..., 0.15671706, 0.15671706,\n        0.15671706],\n       [0.15280013, 0.15280013, 0.15280013, ..., 0.15280013, 0.15280013,\n        0.15280013],\n       [0.1826574 , 0.1826574 , 0.1826574 , ..., 0.1826574 , 0.1826574 ,\n        0.1826574 ]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])energy(chain, draw)float64671.4 665.4 671.0 ... 670.8 674.0array([[671.39512608, 665.35858794, 670.96295141, ..., 670.72923684,\n        668.42111758, 669.24601286],\n       [665.04518693, 671.40450254, 672.47055359, ..., 665.8504822 ,\n        674.75907256, 672.99992931],\n       [672.00375668, 668.16520377, 666.3166769 , ..., 671.79404347,\n        671.22701287, 671.74308362],\n       [669.46327702, 671.11966704, 667.28825365, ..., 665.11423361,\n        670.79366043, 673.9890605 ]])n_steps(chain, draw)int6431 31 31 15 15 ... 31 31 15 15 31array([[31, 31, 31, ..., 31, 15, 15],\n       [15, 31, 31, ..., 31, 31, 31],\n       [31, 31, 15, ..., 15, 31, 15],\n       [31, 15, 15, ..., 15, 15, 31]])tree_depth(chain, draw)int645 5 5 4 4 5 5 4 ... 5 5 4 5 5 4 4 5array([[5, 5, 5, ..., 5, 4, 4],\n       [4, 5, 5, ..., 5, 5, 5],\n       [5, 5, 4, ..., 4, 5, 4],\n       [5, 4, 4, ..., 4, 4, 5]])lp(chain, draw)float64661.7 661.3 665.7 ... 665.6 668.8array([[661.70967519, 661.2875458 , 665.70729556, ..., 661.40453042,\n        661.53660449, 661.51768248],\n       [662.47186408, 665.25230302, 666.28391286, ..., 662.10849958,\n        668.1822538 , 664.18500466],\n       [662.60487008, 660.69369885, 662.99303071, ..., 663.00720061,\n        664.60205279, 667.39059102],\n       [664.1010232 , 660.27378818, 664.08879552, ..., 661.26338997,\n        665.58839085, 668.77321037]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (4)created_at :2024-04-16T21:48:50.639029arviz_version :0.17.0modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (y_obs: 133)\nCoordinates:\n  * y_obs    (y_obs) int64 0 1 2 3 4 5 6 7 8 ... 125 126 127 128 129 130 131 132\nData variables:\n    y        (y_obs) float64 0.0 -1.3 -2.7 0.0 -2.7 ... -2.7 10.7 -2.7 10.7\nAttributes:\n    created_at:                  2024-04-16T21:48:50.640324\n    arviz_version:               0.17.0\n    inference_library:           numpyro\n    inference_library_version:   0.13.2\n    sampling_time:               1.82556\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:y_obs: 133Coordinates: (1)y_obs(y_obs)int640 1 2 3 4 5 ... 128 129 130 131 132array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132])Data variables: (1)y(y_obs)float640.0 -1.3 -2.7 ... 10.7 -2.7 10.7array([   0. ,   -1.3,   -2.7,    0. ,   -2.7,   -2.7,   -2.7,   -1.3,\n         -2.7,   -2.7,   -1.3,   -2.7,   -2.7,   -2.7,   -5.4,   -2.7,\n         -5.4,    0. ,   -2.7,   -2.7,    0. ,  -13.3,   -5.4,   -5.4,\n         -9.3,  -16. ,  -22.8,   -2.7,  -22.8,  -32.1,  -53.5,  -54.9,\n        -40.2,  -21.5,  -21.5,  -50.8,  -42.9,  -26.8,  -21.5,  -50.8,\n        -61.7,   -5.4,  -80.4,  -59. ,  -71. ,  -91.1,  -77.7,  -37.5,\n        -85.6, -123.1, -101.9,  -99.1, -104.4, -112.5,  -50.8, -123.1,\n        -85.6,  -72.3, -127.2, -123.1, -117.9, -134. , -101.9, -108.4,\n       -123.1, -123.1, -128.5, -112.5,  -95.1,  -81.8,  -53.5,  -64.4,\n        -57.6,  -72.3,  -44.3,  -26.8,   -5.4, -107.1,  -21.5,  -65.6,\n        -16. ,  -45.6,  -24.2,    9.5,    4. ,   12. ,  -21.5,   37.5,\n         46.9,  -17.4,   36.2,   75. ,    8.1,   54.9,   48.2,   46.9,\n         16. ,   45.6,    1.3,   75. ,  -16. ,  -54.9,   69.6,   34.8,\n         32.1,  -37.5,   22.8,   46.9,   10.7,    5.4,   -1.3,  -21.5,\n        -13.3,   30.8,  -10.7,   29.4,    0. ,  -10.7,   14.7,   -1.3,\n          0. ,   10.7,   10.7,  -26.8,  -14.7,  -13.3,    0. ,   10.7,\n        -14.7,   -2.7,   10.7,   -2.7,   10.7])Indexes: (1)y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       123, 124, 125, 126, 127, 128, 129, 130, 131, 132],\n      dtype='int64', name='y_obs', length=133))Attributes: (7)created_at :2024-04-16T21:48:50.640324arviz_version :0.17.0inference_library :numpyroinference_library_version :0.13.2sampling_time :1.82556modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n              \n            \n            \n\n\nand plot the model fit to see if it can recover the observed data.\n\nax = az.plot_hdi(new_data['X'], idata['posterior_predictive']['y'], fill_kwargs={'alpha': 0.2, 'color':'firebrick'}, figsize=(9, 8))\n\naz.plot_hdi(new_data['X'], idata['posterior_predictive']['y'], fill_kwargs={'alpha': 0.8, 'color':'firebrick'}, hdi_prob=0.5)\n\ny_mean = idata['posterior_predictive']['y'].mean(dim=('chain', 'draw'))\n\nax.plot(new_data['X'], y_mean, label='Expected posterior predictive', color='k')\n\nax.scatter(df['X'], df['y'], label='Observed Datapoints')\n\nax.legend()\n\nax.set_title(\"Posterior Predictive Distribution \\n Based on HSGP approximation\")\n\nText(0.5, 1.0, 'Posterior Predictive Distribution \\n Based on HSGP approximation')\n\n\n\n\n\nAnd we can compare versus the spline models to see that by the aggregate performance measures our HSGP model seems to come out on top.\n\nmodels_dict = {\"piecewise_constant\": idata_spline1, \"piecewise_linear\": idata_spline2, \"cubic_bspline\": idata_spline3, \"cubic_bspline_10\": idata_spline4, \n\"cubic_bspline_15\": idata_spline5, 'hsgp': idata}\ndf_compare = az.compare(models_dict)\ndf_compare\n\n\n\n\n\n  \n    \n      \n      rank\n      elpd_loo\n      p_loo\n      elpd_diff\n      weight\n      se\n      dse\n      warning\n      scale\n    \n  \n  \n    \n      hsgp\n      0\n      -608.819770\n      9.958827\n      0.000000\n      0.873213\n      12.009698\n      0.000000\n      False\n      log\n    \n    \n      cubic_bspline_10\n      1\n      -612.278059\n      11.465651\n      3.458289\n      0.000000\n      9.688433\n      2.962396\n      True\n      log\n    \n    \n      cubic_bspline_15\n      2\n      -616.252712\n      15.566839\n      7.432943\n      0.000000\n      9.833120\n      3.457579\n      True\n      log\n    \n    \n      cubic_bspline\n      3\n      -634.729956\n      8.698337\n      25.910186\n      0.000000\n      8.897139\n      7.686648\n      True\n      log\n    \n    \n      piecewise_constant\n      4\n      -643.781042\n      6.981098\n      34.961272\n      0.069506\n      9.770740\n      9.490817\n      False\n      log\n    \n    \n      piecewise_linear\n      5\n      -647.267410\n      6.170543\n      38.447641\n      0.057282\n      7.902853\n      9.200728\n      False\n      log\n    \n  \n\n\n\n\n\n\nRecap\nSo far we’ve seen how we can use splines and gauassian processes to model highly eccentric functional relationships where the function could be approximated with univariate smoothing routine. These are two distinct abstractions which seem adequately fit to the world, but demand very different ways of thinking about the underlying reality. This is no fundamental contradiction in so far as the world admits many descriptions.\n\n[K]nowledge develops in a multiplicity of theories, each with it’s limited utility … These theories overlap very considerable in their logical laws and in much else, but that they add up to an integrated and consistent whole is only a worthy ideal and not a pre-requistite of scientific progress … Let reconciliations proceed. - W.V.O Quine in Word and Object\n\nAnother observation in a similar vein is that penalised spline models are provably equivalent to hierarchical regression (random effects) models. This is striking because the character of these types of model seems diametrically opposed. With spline models you jerry-rig your features to an optimisation goal, with hierarchical model you tend to impose theoretical structure to induce shrinkage. It’s hard to see how this works - with a penalised spline model are you inducing a hierarchy of latent features you can’t name? Should you even try to translate between the two!? The abstract components of our model is less graspable than the predictive performance.\nNext we’ll step up our abstractions and show how to use hierarchical modelling over spline fits to extract insight into the data generating process over a family of curves. In particular we’ll focus on the development of insurance loss curves."
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#insurance-loss-curves-hierarchical-spline-models",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#insurance-loss-curves-hierarchical-spline-models",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Insurance Loss Curves: Hierarchical Spline Models",
    "text": "Insurance Loss Curves: Hierarchical Spline Models\nHierarchical models of spline fits allow us to characterise and sample from fits over a family of curves while improving out of sample predictive accuracy. We draw on car insurance losses data set discussed in Mick Cooney’s Stan case-study, but we simplify things for ourselves considerably by focusing on two types of loss and ensuring that each year under consideration has equal observations of the accruing losses.\n\nloss_df = pd.read_csv('ppauto_pos.csv')\nloss_df = loss_df[(loss_df['GRCODE'].isin([43, 353])) & (loss_df['DevelopmentYear'] < 1998)]\n\nloss_df = loss_df[['GRCODE', 'AccidentYear', 'DevelopmentYear', 'DevelopmentLag', 'EarnedPremDIR_B', 'CumPaidLoss_B']]\n\n\nloss_df.columns = ['grcode', 'acc_year', 'dev_year', 'dev_lag', 'premium', 'cum_loss']\nloss_df['lr'] = loss_df['cum_loss'] / loss_df['premium']\nloss_df = loss_df[(loss_df['acc_year'] <= 1992) & (loss_df['dev_lag'] <= 6)].reset_index(drop=True)\n\nloss_df['year_code'] = loss_df['acc_year'].astype(str) + '_' + loss_df['grcode'].astype(str)\nloss_df.sort_values(by=['year_code', 'acc_year', 'dev_lag'], inplace=True)\nloss_df['standardised_premium'] = (loss_df.assign(mean_premium = np.mean(loss_df['premium']))\n.assign(std_premium = np.std(loss_df['premium']))\n.apply(lambda x: (x['mean_premium'] - x['premium']) /x['std_premium'], axis=1)\n)\n\nloss_df.head(12)\n\n\n\n\n\n  \n    \n      \n      grcode\n      acc_year\n      dev_year\n      dev_lag\n      premium\n      cum_loss\n      lr\n      year_code\n      standardised_premium\n    \n  \n  \n    \n      30\n      353\n      1988\n      1988\n      1\n      18793\n      4339\n      0.230884\n      1988_353\n      -0.347453\n    \n    \n      31\n      353\n      1988\n      1989\n      2\n      18793\n      9617\n      0.511733\n      1988_353\n      -0.347453\n    \n    \n      32\n      353\n      1988\n      1990\n      3\n      18793\n      11584\n      0.616400\n      1988_353\n      -0.347453\n    \n    \n      33\n      353\n      1988\n      1991\n      4\n      18793\n      12001\n      0.638589\n      1988_353\n      -0.347453\n    \n    \n      34\n      353\n      1988\n      1992\n      5\n      18793\n      12640\n      0.672591\n      1988_353\n      -0.347453\n    \n    \n      35\n      353\n      1988\n      1993\n      6\n      18793\n      12966\n      0.689938\n      1988_353\n      -0.347453\n    \n    \n      0\n      43\n      1988\n      1988\n      1\n      957\n      133\n      0.138976\n      1988_43\n      1.722344\n    \n    \n      1\n      43\n      1988\n      1989\n      2\n      957\n      333\n      0.347962\n      1988_43\n      1.722344\n    \n    \n      2\n      43\n      1988\n      1990\n      3\n      957\n      431\n      0.450366\n      1988_43\n      1.722344\n    \n    \n      3\n      43\n      1988\n      1991\n      4\n      957\n      570\n      0.595611\n      1988_43\n      1.722344\n    \n    \n      4\n      43\n      1988\n      1992\n      5\n      957\n      615\n      0.642633\n      1988_43\n      1.722344\n    \n    \n      5\n      43\n      1988\n      1993\n      6\n      957\n      615\n      0.642633\n      1988_43\n      1.722344\n    \n  \n\n\n\n\n\nPlot the Loss Curves\nHere we have plotted the developing loss curves from two different coded insurance products.\n\npivot = loss_df.pivot(index=['dev_lag'], columns=['grcode', 'acc_year'], values='lr')\nfig, axs = plt.subplots(1, 2, figsize=(9, 7))\npivot.plot(figsize=(10, 6), ax=axs[0])\naxs[0].set_title(\"Loss Ratios by Year\");\nfor c in pivot.columns:\n    if 43 in c:\n        color='red'\n    else: \n        color='grey'\n    axs[1].plot(pivot[c], color=color, label=c)\naxs[1].legend()\naxs[1].set_title(\"Loss Ratio by Group\");\n\n\n\n\nWe want to model these curves collectively as instances of draws from a distribution of loss curves. To do so we will specify a PyMC hierarchical (mixed) spline model. To do so we will have a spline basis for the global hyper parameters beta_g and the individual parameters for each curve. Here we define a convenience function to generate the basis splines.\n\ndef make_basis_splines(num_knots=3, max_dev=7):\n    knot_list = np.linspace(0, max_dev, num_knots+2)[1:-1]\n    dev_periods = np.arange(1, max_dev, 1)\n\n    Bi = dmatrix(\n        \"bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=True) - 1\",\n        {\"dev_periods\": dev_periods, \"knots\": knot_list},\n    )\n\n    Bg = dmatrix(\n        \"bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=True) - 1\",\n        {\"dev_periods\": dev_periods, \"knots\": knot_list})\n\n\n    return Bi, Bg\n\nBi, Bg = make_basis_splines()\nBg\n\nDesignMatrix with shape (6, 7)\n  Columns:\n    ['bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=True)[0]',\n     'bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=True)[1]',\n     'bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=True)[2]',\n     'bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=True)[3]',\n     'bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=True)[4]',\n     'bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=True)[5]',\n     'bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=True)[6]']\n  Terms:\n    'bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=True)' (columns 0:7)\n  (to view full data, use np.asarray(this_obj))\n\n\nNext we specify a model maker function to create the various pooled, unpooled and hierarhical (mixed) models of the insurance curve data. Note that even though we’re specifying a hierarhical model we have not specified a hierarchy over the insurance codes, instead we have added this as a “fixed” effect feature into our regression model. The idea is that this fixed effect will capture the differences in expected baseline per group code of insurance product.\n\ndef make_model(loss_df, num_knots=3, max_dev=7, model_type='mixed'):\n    Bi, Bg = make_basis_splines(num_knots, max_dev)\n    observed = loss_df['lr'].values\n    uniques, unique_codes = pd.factorize(loss_df['year_code'])\n    coords= {'years': unique_codes, \n            'splines': list(range(Bi.shape[1])) ,\n            'measurement': list(range(6)), \n            'obs': uniques\n            }\n\n    with pm.Model(coords=coords) as sp_insur:\n        basis_g = pm.MutableData('Bg', np.asfortranarray(Bg))\n\n        tau = pm.HalfCauchy('tau', 1)\n        ## Global Hierarchical Spline Terms\n        beta_g = pm.Normal(\"beta_g\", mu=0, sigma=tau, \n        dims='splines')\n        mu_g = pm.Deterministic(\"mu_g\", pm.math.dot(basis_g, beta_g), dims='measurement')\n\n        ## Individual or Year Specific Spline Modifications\n        if model_type in ['mixed', 'unpooled']:\n            sigma = pm.HalfCauchy('sigma_i', 1)\n            basis_i = pm.MutableData('Bi', np.asfortranarray(Bi))\n            beta = pm.Normal(\"beta\", mu=0, sigma=sigma, dims=('splines', 'years'))\n            mui = pm.Deterministic(\"mui\", pm.math.dot(basis_i, beta), dims=('measurement', 'years'))\n        \n        ## Features\n        prem = pm.MutableData('prem', loss_df['standardised_premium'].values)\n        grcode = pm.MutableData('grcode', loss_df['grcode'] == 43)\n\n        beta_prem = pm.Normal('beta_prem', 0, 1)\n        beta_grcode = pm.Normal('beta_grcode', 0, 1)\n        mu_prem = beta_prem*prem\n        mu_grcode = beta_grcode*grcode\n\n        ## Likelihood\n        sigma = pm.TruncatedNormal(\"sigma\", 1, lower=0.005)\n        if model_type == 'mixed':\n            mu = pm.Deterministic('mu',  mu_grcode + mu_prem + (mu_g.T + mui.T).ravel(), dims='obs')\n            lr_likelihood = pm.Normal(\"lr\", mu, sigma, observed=observed, dims=('obs'))\n        elif model_type == 'pooled': \n             lr_likelihood = pm.Normal(\"lr\",  np.repeat(mu_g, len(unique_codes)), sigma, observed=observed, dims='obs')\n        elif model_type == 'unpooled':\n            lr_likelihood = pm.Normal(\"lr\",  mui.T.ravel(), sigma, observed=observed, dims=('obs'))\n\n\n        ## Sampling\n        idata_sp_insur = pm.sample(2000, return_inferencedata=True, target_accept=.99,\n        idata_kwargs={\"log_likelihood\": True})\n        idata_sp_insur = pm.sample_posterior_predictive(\n            idata_sp_insur,extend_inferencedata=True)\n\n    return idata_sp_insur, sp_insur\n\n\nidata_sp_insur_unpooled, sp_insur_unpooled = make_model(loss_df, model_type='unpooled')\nidata_sp_insur_pooled, sp_insur_pooled = make_model(loss_df, model_type='pooled')\nidata_sp_insur_mixed, sp_insur_mixed = make_model(loss_df, model_type='mixed')\n\nThe model structure can be seen more clearly in this graph. The important features to note are the spline components and the manner of their combination. We are postulating a kind of hierarchy of development curves. We specify the “global” curve in 7 development steps and allow that each distinct year is a realisation of a curve that can be thought of as modifying the profile of “global” curve as mui is combined with mu_g. These structures are then combined into a more traditional regression and fed into our likelihood term.\n\npm.model_to_graphviz(sp_insur_mixed)\n\n\n\n\nWe can extract the effect of the differences grcodes and examine the baseline and annual spline related coefficients.\n\nsummary = az.summary(idata_sp_insur_mixed, var_names=['sigma', 'beta_grcode', 'beta_prem', 'beta_g', 'beta'])\n\nsummary\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      sigma\n      0.014\n      0.007\n      0.005\n      0.027\n      0.001\n      0.000\n      153.0\n      348.0\n      1.04\n    \n    \n      beta_grcode\n      0.231\n      0.057\n      0.123\n      0.337\n      0.001\n      0.001\n      4479.0\n      4712.0\n      1.00\n    \n    \n      beta_prem\n      -0.031\n      0.029\n      -0.086\n      0.023\n      0.000\n      0.000\n      4884.0\n      5002.0\n      1.00\n    \n    \n      beta_g[0]\n      0.132\n      0.069\n      0.006\n      0.265\n      0.001\n      0.001\n      3185.0\n      3812.0\n      1.00\n    \n    \n      beta_g[1]\n      0.070\n      0.395\n      -0.651\n      0.841\n      0.006\n      0.006\n      4464.0\n      3851.0\n      1.00\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      beta[6, 1990_43]\n      0.547\n      0.073\n      0.412\n      0.686\n      0.001\n      0.001\n      2884.0\n      4276.0\n      1.00\n    \n    \n      beta[6, 1991_353]\n      -0.098\n      0.070\n      -0.225\n      0.034\n      0.001\n      0.001\n      2873.0\n      4230.0\n      1.00\n    \n    \n      beta[6, 1991_43]\n      0.115\n      0.073\n      -0.016\n      0.257\n      0.001\n      0.001\n      3073.0\n      4496.0\n      1.00\n    \n    \n      beta[6, 1992_353]\n      -0.115\n      0.070\n      -0.250\n      0.012\n      0.001\n      0.001\n      2861.0\n      4547.0\n      1.00\n    \n    \n      beta[6, 1992_43]\n      -0.033\n      0.091\n      -0.212\n      0.135\n      0.002\n      0.001\n      3726.0\n      4461.0\n      1.00\n    \n  \n\n80 rows × 9 columns\n\n\n\nAgain we can compare the performance metrics of the various models.\n\ncompare_df = az.compare({'unpooled': idata_sp_insur_unpooled, \n            'pooled': idata_sp_insur_pooled, \n            'mixed': idata_sp_insur_mixed})\n\naz.plot_compare(compare_df)\ncompare_df\n\n\n\n\n\n  \n    \n      \n      rank\n      elpd_loo\n      p_loo\n      elpd_diff\n      weight\n      se\n      dse\n      warning\n      scale\n    \n  \n  \n    \n      mixed\n      0\n      134.697068\n      61.393405\n      0.000000\n      1.000000e+00\n      1.382933\n      0.000000\n      True\n      log\n    \n    \n      unpooled\n      1\n      88.378689\n      73.735810\n      46.318379\n      0.000000e+00\n      1.267429\n      1.520694\n      True\n      log\n    \n    \n      pooled\n      2\n      -9.288780\n      5.995182\n      143.985848\n      6.050367e-10\n      5.414548\n      5.020442\n      False\n      log\n    \n  \n\n\n\n\n\n\n\nThe hierarchical splne model is by far the best fit to our data. What does it represent? Yes, splines are function approximation tools. Sure, extrapolation is dangerous, but in this domain where the range of the development curves is constrained they seem to offer an elegant way of articulating the structure of the risk profiles for various products.\n\n\nPlot the Posterior Predictive Checks\nWe can check how well the model can recapture the observed data.\n\ndef plot_ppc_splines(idata):\n    fig, axs = plt.subplots(2, 5, figsize=(20, 10), sharey=True)\n    axs = axs.flatten()\n    dev_periods = np.arange(1, 7, 1)\n    uniques, unique_codes = pd.factorize(loss_df['year_code'])\n    for y, c in zip(unique_codes, range(10)):\n        az.plot_hdi(dev_periods, idata['posterior_predictive']['lr'].sel(obs=c), color='firebrick', ax=axs[c], fill_kwargs={'alpha': 0.2}, hdi_prob=.89)\n        az.plot_hdi(dev_periods, idata['posterior_predictive']['lr'].sel(obs=c), color='firebrick', ax=axs[c], hdi_prob=0.5)\n        axs[c].scatter(dev_periods, loss_df[(loss_df['year_code'] == y)]['lr'], color='k', label='Actual Loss Ratio')\n        axs[c].plot(dev_periods, loss_df[(loss_df['year_code'] == y)]['lr'], color='k')\n        axs[c].set_title(f\"PPC 89% and 50% HDI: {y}\")\n        axs[c].set_ylabel(\"Loss Ratio\")\n        axs[c].legend();\n\nplot_ppc_splines(idata_sp_insur_mixed);\n\n\n\n\n\n\nPlot the Hierarchical Components\nIn the following plot we show similarly how to recapture the observed data, but additionally we can decompose the structure of the model in each case and extract baseline forecasts which would be our guide to future loss-ratio development curves in lieu of any other information.\n\nmu_g = idata_sp_insur_mixed.posterior.stack(draws=(\"chain\", \"draw\"))[\"mu_g\"]\n\nmu_i = idata_sp_insur_mixed.posterior.stack(draws=(\"chain\", \"draw\"))[\"mui\"]\n\nmu  = idata_sp_insur_mixed.posterior.stack(draws=(\"chain\", \"draw\"))['mu']\n\nbeta_grcode = idata_sp_insur_mixed.posterior.stack(draws=(\"chain\", \"draw\"))['beta_grcode']\n\ndev_periods = np.arange(1, 7, 1)\nuniques, unique_codes = pd.factorize(loss_df['year_code'])\n\nmosaic = \"\"\"\n         ABCDE\n         FGHIJ\n         KKKKK\n\"\"\"\nfig, axs = plt.subplot_mosaic(mosaic, sharey=True, \nfigsize=(20, 15))\naxs = [axs[k] for k in axs.keys()] \n\nmu_g_mean = mu_g.mean(dim='draws')\nfor y, c in zip(unique_codes, range(10)):\n    group_effect = 0\n    if '43' in y: \n        group_effect = beta_grcode.mean().item()\n    mu_i_mean = mu_i.sel(years=y).mean(dim='draws')\n    axs[c].plot(dev_periods, group_effect + mu_g_mean.values + mu_i_mean.values, label='Combined + E(grp_effect)', color='purple', linewidth=3.5)\n    axs[c].plot(dev_periods, group_effect + mu_g_mean.values, label='E(Hierarchical Baseline)', color='red', linestyle='--')\n    axs[c].plot(dev_periods,  group_effect + mu_i_mean.values, label='E(Year Specific Adjustment)', color='blue', linestyle='--')\n    axs[c].scatter(dev_periods, loss_df[(loss_df['year_code'] == y)]['lr'], color='k', label='Actual Loss Ratio')\n    az.plot_hdi(dev_periods,mu.sel(obs=c).T  , ax=axs[c], color='firebrick', fill_kwargs={'alpha': 0.2})\n    az.plot_hdi(dev_periods, mu.sel(obs=c).T , ax=axs[c], color='firebrick', fill_kwargs={'alpha': 0.5}, hdi_prob=.50)\n    axs[c].set_title(f\"Components for Year {y}\")\n    axs[c].set_ylabel(\"Loss Ratio\")\n    if (c == 0):\n         axs[c].legend()\n\naxs[10].plot(dev_periods, mu_g_mean.values, label='E(Hierarchical Baseline)', color='black')\naxs[10].plot(dev_periods, mu_g_mean.values + group_effect, label='E(Hierarchical Baseline) + E(grp_effect)', color='black', linestyle='--')\naz.plot_hdi(dev_periods, mu_g.T.values, color='slateblue', ax=axs[10], fill_kwargs={'alpha': 0.2})\naz.plot_hdi(dev_periods, mu_g.T.values + group_effect, color='magenta', ax=axs[10], fill_kwargs={'alpha': 0.2})\naz.plot_hdi(dev_periods, mu_g.T.values, color='slateblue', ax=axs[10], hdi_prob=.5)\naz.plot_hdi(dev_periods, mu_g.T.values  + group_effect, color='magenta', ax=axs[10], hdi_prob=.5)\naxs[10].set_title(\"Baseline Forecast Loss Ratio\")\naxs[10].legend();\n\n\n\n\nThis plot is a bit clunky, because we’re mixing expectations and posterior distributions over the parameters. The point is just to highlight the “compositional” structure of our model.\n\n\nPredicting Next Year’s Losses\nA better way to interrogate the implications of the model is to “push” forward different data through the posterior predictive distribution and derive a kind of ceteris paribus rule for accrual of losses.\n\nwith sp_insur_mixed: \n    pm.set_data({'grcode': np.ones(len(loss_df)), \n    })\n    idata_43 = pm.sample_posterior_predictive(idata_sp_insur_mixed, var_names=['lr'], extend_inferencedata =True)\n\nwith sp_insur_mixed: \n    pm.set_data({'grcode': np.zeros(len(loss_df))})\n    idata_353 = pm.sample_posterior_predictive(idata_sp_insur_mixed, var_names=['lr'], extend_inferencedata=True)\n\nidata_353\n\nSampling: [lr]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00<00:00]\n    \n    \n\n\nSampling: [lr]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00<00:00]\n    \n    \n\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:      (chain: 4, draw: 2000, splines: 7, years: 10, measurement: 6,\n                  obs: 60)\nCoordinates:\n  * chain        (chain) int64 0 1 2 3\n  * draw         (draw) int64 0 1 2 3 4 5 6 ... 1994 1995 1996 1997 1998 1999\n  * splines      (splines) int64 0 1 2 3 4 5 6\n  * years        (years) <U8 '1988_353' '1988_43' ... '1992_353' '1992_43'\n  * measurement  (measurement) int64 0 1 2 3 4 5\n  * obs          (obs) int64 0 0 0 0 0 0 1 1 1 1 1 1 ... 8 8 8 8 8 8 9 9 9 9 9 9\nData variables:\n    beta_g       (chain, draw, splines) float64 0.08213 0.08078 ... 0.6575\n    beta         (chain, draw, splines, years) float64 0.1395 ... -0.1751\n    beta_prem    (chain, draw) float64 -0.0156 -0.03719 ... -0.0874 -0.06236\n    beta_grcode  (chain, draw) float64 0.2377 0.2905 0.2288 ... 0.3751 0.3465\n    tau          (chain, draw) float64 0.5886 0.5384 0.4783 ... 0.6427 0.4663\n    sigma_i      (chain, draw) float64 0.2091 0.171 0.2062 ... 0.2064 0.1871\n    sigma        (chain, draw) float64 0.01148 0.009831 ... 0.005943 0.005919\n    mu_g         (chain, draw, measurement) float64 0.08213 0.3377 ... 0.6575\n    mui          (chain, draw, measurement, years) float64 0.1395 ... -0.1751\n    mu           (chain, draw, obs) float64 0.2271 0.509 ... 0.9015 0.9269\nAttributes:\n    created_at:                 2024-05-27T06:09:05.927942\n    arviz_version:              0.17.0\n    inference_library:          pymc\n    inference_library_version:  5.10.3\n    sampling_time:              227.7068202495575\n    tuning_steps:               1000xarray.DatasetDimensions:chain: 4draw: 2000splines: 7years: 10measurement: 6obs: 60Coordinates: (6)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999array([   0,    1,    2, ..., 1997, 1998, 1999])splines(splines)int640 1 2 3 4 5 6array([0, 1, 2, 3, 4, 5, 6])years(years)<U8'1988_353' '1988_43' ... '1992_43'array(['1988_353', '1988_43', '1989_353', '1989_43', '1990_353', '1990_43',\n       '1991_353', '1991_43', '1992_353', '1992_43'], dtype='<U8')measurement(measurement)int640 1 2 3 4 5array([0, 1, 2, 3, 4, 5])obs(obs)int640 0 0 0 0 0 1 1 ... 8 8 9 9 9 9 9 9array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n       4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n       8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9])Data variables: (10)beta_g(chain, draw, splines)float640.08213 0.08078 ... 0.1945 0.6575array([[[ 0.08213429,  0.08078411,  0.43921067, ...,  0.68826351,\n          0.76152486,  0.57747165],\n        [ 0.0729024 ,  0.34594954,  0.30222266, ...,  0.58894912,\n          0.70131994,  0.56389405],\n        [ 0.1296795 , -0.07301492,  0.59161604, ...,  0.89261377,\n          0.32944684,  0.70257189],\n        ...,\n        [-0.04196952,  0.17650917,  0.4217089 , ...,  0.62258926,\n          0.59316501,  0.79250849],\n        [-0.00829219, -0.1684174 ,  0.68416978, ...,  0.81418464,\n          0.38975666,  0.74581672],\n        [ 0.0817568 ,  0.21642253,  0.4364298 , ...,  0.70190257,\n          0.48913041,  0.62736766]],\n\n       [[ 0.1372587 ,  0.23309973,  0.49454757, ...,  0.65137438,\n          0.75772987,  0.61066381],\n        [ 0.01118497, -0.501921  ,  0.70257164, ...,  0.86052648,\n         -0.11635177,  0.5797538 ],\n        [ 0.19565787,  0.27753886,  0.68404036, ...,  0.96958051,\n          0.72825027,  0.73195019],\n...\n        [ 0.13938073, -0.02592998,  0.73699995, ...,  0.88935344,\n          0.379609  ,  0.81252247],\n        [ 0.11689267,  0.04623414,  0.67605048, ...,  0.90426575,\n          0.4922117 ,  0.70663987],\n        [ 0.12929436,  0.12568724,  0.61095241, ...,  0.75010102,\n          0.58792975,  0.74250617]],\n\n       [[ 0.1464027 ,  0.76694943, -0.06153482, ...,  0.15534104,\n          1.32746341,  0.66072813],\n        [ 0.14358004,  0.68940626, -0.11149118, ...,  0.17124399,\n          1.18011474,  0.71314634],\n        [ 0.05419155, -0.42883811,  1.14901708, ...,  1.26919344,\n          0.01586078,  0.63687761],\n        ...,\n        [ 0.18723226,  0.48566063,  0.23265438, ...,  0.5794988 ,\n          0.85015745,  0.86002011],\n        [ 0.03751469,  0.0086789 ,  0.55346192, ...,  0.8412314 ,\n          0.43493969,  0.65331407],\n        [ 0.05265733, -0.19628186,  0.64580651, ...,  0.90007346,\n          0.19448681,  0.65747667]]])beta(chain, draw, splines, years)float640.1395 -0.1399 ... -0.1164 -0.1751array([[[[ 1.39520480e-01, -1.39920512e-01,  1.59795716e-01, ...,\n          -4.88250518e-02,  1.49757586e-01, -7.87841938e-02],\n         [ 1.57822205e-01,  1.57221882e-01,  8.80230108e-02, ...,\n           1.84740484e-01,  1.40654109e-01,  1.05066007e-01],\n         [ 1.71863836e-01, -3.89547722e-01,  2.10933769e-01, ...,\n          -1.10459344e-02,  2.27594707e-02, -1.97138584e-01],\n         ...,\n         [-1.36108960e-01, -1.82625112e-01,  4.75130784e-02, ...,\n           4.45905407e-02, -3.94624785e-02, -1.40864962e-01],\n         [ 6.71214512e-02, -3.95470614e-01, -1.96252840e-01, ...,\n          -2.64734451e-02, -3.31353605e-01,  2.14460499e-02],\n         [ 1.11116707e-01, -1.45940962e-01,  3.79732609e-02, ...,\n           1.98087329e-01, -8.13679795e-03,  8.28432092e-02]],\n\n        [[ 1.46217147e-01, -1.45987241e-01,  1.33636350e-01, ...,\n          -1.04141339e-01,  1.56999588e-01, -1.73189166e-01],\n         [ 1.77012604e-01,  1.41028626e-02,  7.00230739e-02, ...,\n          -6.63265264e-02,  5.91449369e-02, -2.69751242e-01],\n         [ 1.41749001e-01, -3.79009870e-01,  1.25803026e-01, ...,\n           2.14529318e-02,  5.38979285e-02, -1.33607062e-01],\n...\n         [ 3.45479799e-02, -3.58625458e-01, -2.18827643e-01, ...,\n          -1.90020093e-01, -2.01458145e-01, -3.92093686e-01],\n         [-1.46373265e-01, -5.99317086e-02,  1.30108020e-01, ...,\n           9.35956673e-02, -9.62921164e-02, -1.04324354e-01],\n         [ 1.15647935e-02, -2.22568566e-01, -4.62102677e-02, ...,\n          -9.27173884e-03, -1.22732622e-01, -2.49213829e-01]],\n\n        [[ 1.63621902e-01, -1.51455454e-01,  1.65518502e-01, ...,\n          -1.56291791e-01,  1.56621714e-01, -2.49188309e-01],\n         [-1.31496227e-02,  1.33838754e-01, -1.48588957e-01, ...,\n          -8.37641665e-02,  9.22397066e-02, -3.70879910e-01],\n         [ 2.55943473e-01, -4.43680242e-01,  2.87918409e-01, ...,\n          -6.67540255e-02, -1.11330929e-03, -1.87938351e-01],\n         ...,\n         [ 1.05025544e-01, -2.90129799e-01,  1.05270857e-01, ...,\n          -4.30757899e-02, -1.14967846e-01, -1.59326571e-01],\n         [-3.26459908e-02, -8.45911318e-02, -1.55795510e-01, ...,\n           1.08681450e-01, -7.15263058e-02, -1.45134288e-01],\n         [ 8.68094467e-03, -2.62496397e-01, -4.86758721e-02, ...,\n           9.04772502e-03, -1.16437070e-01, -1.75097569e-01]]]])beta_prem(chain, draw)float64-0.0156 -0.03719 ... -0.06236array([[-0.01560439, -0.03719103, -0.04590153, ..., -0.04335828,\n        -0.06037419,  0.00715623],\n       [-0.08052324, -0.01663825, -0.07448925, ..., -0.04856727,\n        -0.00828554, -0.03699675],\n       [-0.01992275, -0.04479077, -0.05495835, ...,  0.03346062,\n        -0.00432382, -0.03984384],\n       [-0.06903017, -0.03561944, -0.06378486, ..., -0.04559814,\n        -0.08739882, -0.06236288]])beta_grcode(chain, draw)float640.2377 0.2905 ... 0.3751 0.3465array([[0.23773592, 0.29052938, 0.22882446, ..., 0.22874581, 0.27521708,\n        0.20095271],\n       [0.31476531, 0.33120918, 0.10765162, ..., 0.31997823, 0.19133539,\n        0.27806806],\n       [0.21980155, 0.17918741, 0.25618125, ..., 0.16353416, 0.13860823,\n        0.19789005],\n       [0.26856768, 0.26101595, 0.17110103, ..., 0.2398666 , 0.37505705,\n        0.34646743]])tau(chain, draw)float640.5886 0.5384 ... 0.6427 0.4663array([[0.58855469, 0.5383688 , 0.47833408, ..., 0.35456915, 1.00747773,\n        0.63732983],\n       [0.37739955, 0.53458212, 0.66920479, ..., 0.48638153, 0.59084796,\n        0.76687933],\n       [0.79058091, 0.78629887, 0.47156563, ..., 0.6155269 , 0.84141259,\n        0.90084556],\n       [0.74634296, 0.56718238, 0.63635355, ..., 0.53907184, 0.64268993,\n        0.46630978]])sigma_i(chain, draw)float640.2091 0.171 ... 0.2064 0.1871array([[0.20909747, 0.17103069, 0.20618882, ..., 0.211108  , 0.19893185,\n        0.21397901],\n       [0.19959592, 0.21750556, 0.23931261, ..., 0.20211561, 0.18453507,\n        0.20412235],\n       [0.19160775, 0.19335863, 0.1767455 , ..., 0.19974999, 0.19592067,\n        0.2032291 ],\n       [0.20263706, 0.19317583, 0.18770624, ..., 0.19822884, 0.20642422,\n        0.18713983]])sigma(chain, draw)float640.01148 0.009831 ... 0.005919array([[0.0114768 , 0.0098313 , 0.01093488, ..., 0.00679955, 0.00660215,\n        0.00630935],\n       [0.00763042, 0.00790286, 0.00785762, ..., 0.01620113, 0.01155728,\n        0.01278793],\n       [0.01091791, 0.01269563, 0.0112426 , ..., 0.0160019 , 0.00890353,\n        0.00816588],\n       [0.01126429, 0.01476696, 0.01066087, ..., 0.00525937, 0.00594278,\n        0.00591885]])mu_g(chain, draw, measurement)float640.08213 0.3377 ... 0.6124 0.6575array([[[ 0.08213429,  0.33768236,  0.49079526,  0.58195553,\n          0.6893857 ,  0.57747165],\n        [ 0.0729024 ,  0.3523693 ,  0.47549742,  0.57555192,\n          0.62509911,  0.56389405],\n        [ 0.1296795 ,  0.36973404,  0.53621521,  0.64158577,\n          0.66511154,  0.70257189],\n        ...,\n        [-0.04196952,  0.37081521,  0.53617268,  0.60818718,\n          0.61363189,  0.79250849],\n        [-0.00829219,  0.38796381,  0.54652576,  0.59643256,\n          0.63414497,  0.74581672],\n        [ 0.0817568 ,  0.38371482,  0.51608403,  0.6080783 ,\n          0.61884953,  0.62736766]],\n\n       [[ 0.1372587 ,  0.40331188,  0.4591502 ,  0.51765001,\n          0.65439136,  0.61066381],\n        [ 0.01118497,  0.26250702,  0.41669178,  0.47397997,\n          0.47131783,  0.5797538 ],\n        [ 0.19565787,  0.55795127,  0.69739504,  0.79814199,\n          0.8594259 ,  0.73195019],\n...\n        [ 0.13938073,  0.47041618,  0.61011331,  0.66575445,\n          0.68220337,  0.81252247],\n        [ 0.11689267,  0.45871305,  0.59006843,  0.67156949,\n          0.72612402,  0.70663987],\n        [ 0.12929436,  0.45995572,  0.6102857 ,  0.66215402,\n          0.68172986,  0.74250617]],\n\n       [[ 0.1464027 ,  0.32512369,  0.50687362,  0.58588734,\n          0.62142815,  0.66072813],\n        [ 0.14358004,  0.26546548,  0.4564482 ,  0.55671318,\n          0.57768676,  0.71314634],\n        [ 0.05419155,  0.53789895,  0.62458472,  0.66990082,\n          0.74346936,  0.63687761],\n        ...,\n        [ 0.18723226,  0.35784665,  0.45608484,  0.57637053,\n          0.66758408,  0.86002011],\n        [ 0.03751469,  0.38496915,  0.56650101,  0.66771488,\n          0.68016691,  0.65331407],\n        [ 0.05265733,  0.34718215,  0.49383633,  0.58342827,\n          0.61237385,  0.65747667]]])mui(chain, draw, measurement, years)float640.1395 -0.1399 ... -0.1164 -0.1751array([[[[ 1.39520480e-01, -1.39920512e-01,  1.59795716e-01, ...,\n          -4.88250518e-02,  1.49757586e-01, -7.87841938e-02],\n         [ 1.65873605e-01, -1.93497148e-01,  1.47566232e-01, ...,\n           7.87630258e-02,  5.26650556e-02, -5.69859310e-02],\n         [ 1.42490699e-01, -2.54958783e-01,  9.21905855e-02, ...,\n           1.17105881e-01, -7.24684266e-03,  4.86017675e-03],\n         [ 3.83479319e-02, -1.91999665e-01,  3.42301192e-02, ...,\n           1.33318539e-01, -3.34724977e-02,  2.27443612e-02],\n         [-3.72703490e-02, -2.46357200e-01, -3.30969747e-02, ...,\n           4.52311029e-02, -1.28371119e-01, -5.07836293e-02],\n         [ 1.11116707e-01, -1.45940962e-01,  3.79732609e-02, ...,\n           1.98087329e-01, -8.13679795e-03,  8.28432092e-02]],\n\n        [[ 1.46217147e-01, -1.45987241e-01,  1.33636350e-01, ...,\n          -1.04141339e-01,  1.56999588e-01, -1.73189166e-01],\n         [ 1.50806947e-01, -2.30291464e-01,  1.09203466e-01, ...,\n           9.20703581e-03,  4.54642291e-02, -1.56726472e-01],\n         [ 1.22668583e-01, -2.49806973e-01,  1.13781261e-01, ...,\n           8.35633149e-02,  1.22118628e-03, -5.88513761e-02],\n         [ 6.62855094e-02, -2.01179440e-01,  4.96226930e-02, ...,\n...\n          -9.33073690e-02, -1.27232209e-01, -3.13057795e-01],\n         [-5.68682820e-02, -2.92825044e-01, -7.16499541e-02, ...,\n          -9.83126153e-02, -1.67003142e-01, -3.19813169e-01],\n         [-4.65304179e-02, -2.48490531e-01, -8.00679145e-02, ...,\n          -8.15331960e-02, -1.62924280e-01, -2.87153932e-01],\n         [ 1.15647935e-02, -2.22568566e-01, -4.62102677e-02, ...,\n          -9.27173884e-03, -1.22732622e-01, -2.49213829e-01]],\n\n        [[ 1.63621902e-01, -1.51455454e-01,  1.65518502e-01, ...,\n          -1.56291791e-01,  1.56621714e-01, -2.49188309e-01],\n         [ 1.36287416e-01, -2.29995142e-01,  1.06986574e-01, ...,\n          -5.57859181e-02,  2.19595602e-02, -2.41298686e-01],\n         [ 9.00781502e-02, -2.78835663e-01,  7.79656367e-02, ...,\n           2.00867494e-03, -3.20768389e-02, -1.75140835e-01],\n         [ 3.93295341e-02, -2.29924865e-01,  1.67347175e-02, ...,\n           1.21352879e-02, -7.20658012e-02, -1.62981950e-01],\n         [ 4.44384071e-02, -2.10066162e-01,  8.83961075e-04, ...,\n           1.70640485e-02, -9.33275239e-02, -1.55367387e-01],\n         [ 8.68094467e-03, -2.62496397e-01, -4.86758721e-02, ...,\n           9.04772502e-03, -1.16437070e-01, -1.75097569e-01]]]])mu(chain, draw, obs)float640.2271 0.509 ... 0.9015 0.9269array([[[0.22707658, 0.50897777, 0.63870776, ..., 0.86695823,\n         0.9008604 , 0.9225732 ],\n        [0.2320417 , 0.5160984 , 0.61108815, ..., 0.85957629,\n         0.88786053, 0.91962846],\n        [0.22591164, 0.49834019, 0.63366749, ..., 0.87302862,\n         0.8952962 , 0.91718248],\n        ...,\n        [0.21752702, 0.50863783, 0.62085732, ..., 0.85667073,\n         0.90717039, 0.91667823],\n        [0.24668158, 0.50946944, 0.61496311, ..., 0.85950557,\n         0.91499729, 0.93394897],\n        [0.22803856, 0.51225596, 0.60431136, ..., 0.85384508,\n         0.91546043, 0.92367727]],\n\n       [[0.20798215, 0.49653525, 0.61163146, ..., 0.84536501,\n         0.90679466, 0.9304019 ],\n        [0.21296999, 0.52098307, 0.6079544 , ..., 0.85788457,\n         0.90174194, 0.91528221],\n        [0.21005812, 0.50465123, 0.59585638, ..., 0.85754494,\n         0.90510048, 0.92520922],\n...\n        [0.23575948, 0.52517243, 0.61730376, ..., 0.86419416,\n         0.88562497, 0.92776999],\n        [0.23289817, 0.51858843, 0.61101066, ..., 0.86935524,\n         0.89006191, 0.92026702],\n        [0.24012795, 0.50146092, 0.611026  , ..., 0.85900662,\n         0.9094241 , 0.916668  ]],\n\n       [[0.24430336, 0.49530459, 0.60862876, ..., 0.86711487,\n         0.89899689, 0.89876914],\n        [0.23968997, 0.50137434, 0.61292425, ..., 0.87145524,\n         0.90762533, 0.91330743],\n        [0.22874435, 0.5038266 , 0.63214802, ..., 0.86415242,\n         0.900888  , 0.92221532],\n        ...,\n        [0.22737739, 0.50688812, 0.61785909, ..., 0.86241171,\n         0.90291879, 0.93063882],\n        [0.2253554 , 0.52076927, 0.61201075, ..., 0.86030662,\n         0.90541789, 0.91650515],\n        [0.23794743, 0.50513776, 0.60558268, ..., 0.86491745,\n         0.9014776 , 0.92685024]]])Indexes: (6)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999],\n      dtype='int64', name='draw', length=2000))splinesPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6], dtype='int64', name='splines'))yearsPandasIndexPandasIndex(Index(['1988_353', '1988_43', '1989_353', '1989_43', '1990_353', '1990_43',\n       '1991_353', '1991_43', '1992_353', '1992_43'],\n      dtype='object', name='years'))measurementPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5], dtype='int64', name='measurement'))obsPandasIndexPandasIndex(Index([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n       4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n       8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9],\n      dtype='int64', name='obs'))Attributes: (6)created_at :2024-05-27T06:09:05.927942arviz_version :0.17.0inference_library :pymcinference_library_version :5.10.3sampling_time :227.7068202495575tuning_steps :1000\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 4, draw: 2000, obs: 60)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 ... 1993 1994 1995 1996 1997 1998 1999\n  * obs      (obs) int64 0 0 0 0 0 0 1 1 1 1 1 1 2 ... 7 8 8 8 8 8 8 9 9 9 9 9 9\nData variables:\n    lr       (chain, draw, obs) float64 0.233 0.5237 0.6346 ... 0.8981 0.9297\nAttributes:\n    created_at:                 2024-05-27T06:09:08.031307\n    arviz_version:              0.17.0\n    inference_library:          pymc\n    inference_library_version:  5.10.3xarray.DatasetDimensions:chain: 4draw: 2000obs: 60Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999array([   0,    1,    2, ..., 1997, 1998, 1999])obs(obs)int640 0 0 0 0 0 1 1 ... 8 8 9 9 9 9 9 9array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n       4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n       8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9])Data variables: (1)lr(chain, draw, obs)float640.233 0.5237 ... 0.8981 0.9297array([[[0.2329779 , 0.52369396, 0.63459651, ..., 0.88727935,\n         0.89708185, 0.92159448],\n        [0.22917617, 0.52664668, 0.59261549, ..., 0.86452905,\n         0.90716367, 0.92848945],\n        [0.21781397, 0.4993653 , 0.62570146, ..., 0.87851744,\n         0.91024927, 0.90860314],\n        ...,\n        [0.21017767, 0.50642437, 0.61274687, ..., 0.86717561,\n         0.90019066, 0.92583528],\n        [0.25375665, 0.50843402, 0.60506852, ..., 0.85534182,\n         0.91866865, 0.94337612],\n        [0.22315311, 0.50562829, 0.60910443, ..., 0.85065712,\n         0.91298724, 0.92813007]],\n\n       [[0.21910589, 0.51068439, 0.60243355, ..., 0.82541821,\n         0.91008109, 0.93325851],\n        [0.20831853, 0.51436674, 0.61926284, ..., 0.85784094,\n         0.91192378, 0.91751401],\n        [0.211794  , 0.50545555, 0.59703336, ..., 0.84964635,\n         0.90139699, 0.93754076],\n...\n        [0.20342139, 0.53073673, 0.58714448, ..., 0.85629361,\n         0.86931632, 0.93170111],\n        [0.22936355, 0.51296337, 0.62169933, ..., 0.86757595,\n         0.87222132, 0.91036641],\n        [0.23170526, 0.50702719, 0.615165  , ..., 0.86935216,\n         0.89592565, 0.9289633 ]],\n\n       [[0.23720474, 0.49880944, 0.60052934, ..., 0.86819616,\n         0.89829526, 0.89245385],\n        [0.21610787, 0.49419633, 0.59675283, ..., 0.87135881,\n         0.91189684, 0.92522446],\n        [0.21011217, 0.48862193, 0.62340207, ..., 0.85734297,\n         0.90236241, 0.90941639],\n        ...,\n        [0.22319361, 0.50040966, 0.62049363, ..., 0.85878946,\n         0.90237754, 0.93678927],\n        [0.21677194, 0.51807386, 0.61236904, ..., 0.85692388,\n         0.90624593, 0.91623522],\n        [0.23988418, 0.5116527 , 0.59962956, ..., 0.86614767,\n         0.89813962, 0.92968554]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999],\n      dtype='int64', name='draw', length=2000))obsPandasIndexPandasIndex(Index([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n       4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n       8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9],\n      dtype='int64', name='obs'))Attributes: (4)created_at :2024-05-27T06:09:08.031307arviz_version :0.17.0inference_library :pymcinference_library_version :5.10.3\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 4, draw: 2000, obs: 60)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 ... 1993 1994 1995 1996 1997 1998 1999\n  * obs      (obs) int64 0 0 0 0 0 0 1 1 1 1 1 1 2 ... 7 8 8 8 8 8 8 9 9 9 9 9 9\nData variables:\n    lr       (chain, draw, obs) float64 3.493 3.52 1.659 ... 3.711 4.137 4.128\nAttributes:\n    created_at:                 2024-05-27T06:09:06.204725\n    arviz_version:              0.17.0\n    inference_library:          pymc\n    inference_library_version:  5.10.3xarray.DatasetDimensions:chain: 4draw: 2000obs: 60Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999array([   0,    1,    2, ..., 1997, 1998, 1999])obs(obs)int640 0 0 0 0 0 1 1 ... 8 8 9 9 9 9 9 9array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n       4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n       8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9])Data variables: (1)lr(chain, draw, obs)float643.493 3.52 1.659 ... 4.137 4.128array([[[ 3.49346525,  3.51967093,  1.65940483, ...,  3.3082412 ,\n          3.51674037,  3.53525774],\n        [ 3.69631071,  3.6046685 ,  3.55729917, ...,  3.70154431,\n          2.3967742 ,  3.58347456],\n        [ 3.4934784 ,  2.84680694,  2.35000913, ...,  2.77423593,\n          3.29784217,  3.37659525],\n        ...,\n        [ 2.14258977,  3.96834899,  3.85707249, ...,  4.01314636,\n          3.94561888,  3.42040059],\n        [ 1.23863774,  4.04264235,  4.07774621, ...,  4.09852065,\n          2.65095731,  3.06425839],\n        [ 4.04510066,  4.14335   ,  2.31136224, ...,  3.81265832,\n          2.42504945,  4.13947279]],\n\n       [[-0.54742785,  1.97315188,  3.76142259, ...,  2.35947554,\n          3.87719424,  3.6514542 ],\n        [ 1.35250133,  3.23660373,  3.35059476, ...,  3.91158231,\n          3.88923252,  3.25015959],\n        [ 0.41505901,  3.52118601,  0.50965773, ...,  3.91012279,\n          3.91261702,  3.9225444 ],\n...\n        [ 3.16969071,  2.86342788,  3.21451326, ...,  3.16348367,\n          2.57445527,  3.19445904],\n        [ 3.77677685,  3.50595189,  3.61919133, ...,  3.12638761,\n          2.62018425,  3.69252388],\n        [ 3.24809316,  3.09764755,  3.67232438, ...,  3.88885252,\n          3.64764734,  3.43590068]],\n\n       [[ 2.85754207,  2.50362844,  3.32921447, ...,  3.30786291,\n          3.47806162,  0.97031193],\n        [ 3.11861345,  3.05038605,  3.26872831, ...,  2.94087519,\n          3.26203195,  3.01224386],\n        [ 3.60209919,  3.34722452,  2.5311683 , ...,  3.50557154,\n          3.58614045,  3.60045959],\n        ...,\n        [ 4.10655831,  3.90449352,  4.29030831, ...,  4.11874516,\n          4.31624387,  3.63427395],\n        [ 3.77393089,  3.05063185,  3.93392071, ...,  4.18257176,\n          4.16737028,  3.31520253],\n        [ 3.49856554,  3.58985088,  2.54068726, ...,  3.71137697,\n          4.136818  ,  4.12777749]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999],\n      dtype='int64', name='draw', length=2000))obsPandasIndexPandasIndex(Index([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n       4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n       8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9],\n      dtype='int64', name='obs'))Attributes: (4)created_at :2024-05-27T06:09:06.204725arviz_version :0.17.0inference_library :pymcinference_library_version :5.10.3\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                (chain: 4, draw: 2000)\nCoordinates:\n  * chain                  (chain) int64 0 1 2 3\n  * draw                   (draw) int64 0 1 2 3 4 5 ... 1995 1996 1997 1998 1999\nData variables: (12/17)\n    perf_counter_diff      (chain, draw) float64 0.08216 0.0801 ... 0.08181\n    lp                     (chain, draw) float64 184.4 196.5 ... 216.8 224.4\n    step_size_bar          (chain, draw) float64 0.005908 0.005908 ... 0.005495\n    smallest_eigval        (chain, draw) float64 nan nan nan nan ... nan nan nan\n    acceptance_rate        (chain, draw) float64 0.9998 0.9855 ... 0.944 0.9537\n    reached_max_treedepth  (chain, draw) bool False False False ... True False\n    ...                     ...\n    largest_eigval         (chain, draw) float64 nan nan nan nan ... nan nan nan\n    energy                 (chain, draw) float64 -141.4 -154.8 ... -177.6 -175.2\n    tree_depth             (chain, draw) int64 10 10 10 10 10 ... 10 10 10 10 10\n    diverging              (chain, draw) bool False False False ... False False\n    index_in_trajectory    (chain, draw) int64 -352 387 -740 ... 790 521 -683\n    process_time_diff      (chain, draw) float64 0.08216 0.08007 ... 0.0818\nAttributes:\n    created_at:                 2024-05-27T06:09:05.940582\n    arviz_version:              0.17.0\n    inference_library:          pymc\n    inference_library_version:  5.10.3\n    sampling_time:              227.7068202495575\n    tuning_steps:               1000xarray.DatasetDimensions:chain: 4draw: 2000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999array([   0,    1,    2, ..., 1997, 1998, 1999])Data variables: (17)perf_counter_diff(chain, draw)float640.08216 0.0801 ... 0.08166 0.08181array([[0.08215638, 0.08010133, 0.08184954, ..., 0.08156783, 0.08167758,\n        0.08158583],\n       [0.08136029, 0.08029138, 0.08061071, ..., 0.08166729, 0.08090975,\n        0.08182967],\n       [0.08155483, 0.08204283, 0.08018262, ..., 0.08148587, 0.08159787,\n        0.08199237],\n       [0.080971  , 0.08061054, 0.07909571, ..., 0.08166879, 0.08165904,\n        0.08180558]])lp(chain, draw)float64184.4 196.5 191.3 ... 216.8 224.4array([[184.36486414, 196.53933231, 191.27157196, ..., 200.00169387,\n        204.82818005, 214.33525947],\n       [192.7863688 , 187.12040812, 189.2459854 , ..., 155.91231505,\n        178.6049379 , 187.65005752],\n       [188.86778753, 183.74271399, 179.99640544, ..., 167.53100342,\n        193.17434736, 201.62651467],\n       [178.50910416, 177.79847515, 184.59234725, ..., 220.75603875,\n        216.79913757, 224.43676879]])step_size_bar(chain, draw)float640.005908 0.005908 ... 0.005495array([[0.0059085 , 0.0059085 , 0.0059085 , ..., 0.0059085 , 0.0059085 ,\n        0.0059085 ],\n       [0.00804049, 0.00804049, 0.00804049, ..., 0.00804049, 0.00804049,\n        0.00804049],\n       [0.00597803, 0.00597803, 0.00597803, ..., 0.00597803, 0.00597803,\n        0.00597803],\n       [0.00549452, 0.00549452, 0.00549452, ..., 0.00549452, 0.00549452,\n        0.00549452]])smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])acceptance_rate(chain, draw)float640.9998 0.9855 ... 0.944 0.9537array([[0.99979105, 0.98550281, 0.97845882, ..., 0.98798177, 0.99585789,\n        0.91300196],\n       [0.88604944, 0.92966456, 0.9978866 , ..., 0.97677582, 0.99334874,\n        0.9967715 ],\n       [0.99347033, 0.99333916, 0.95811576, ..., 0.97819394, 0.99938068,\n        0.99843286],\n       [0.99895433, 0.99905734, 0.97817478, ..., 0.94058163, 0.94399245,\n        0.95368363]])reached_max_treedepth(chain, draw)boolFalse False False ... True Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [ True, False, False, ...,  True, False,  True],\n       [False, False, False, ..., False,  True, False]])step_size(chain, draw)float640.004065 0.004065 ... 0.007016array([[0.00406536, 0.00406536, 0.00406536, ..., 0.00406536, 0.00406536,\n        0.00406536],\n       [0.00551699, 0.00551699, 0.00551699, ..., 0.00551699, 0.00551699,\n        0.00551699],\n       [0.00537033, 0.00537033, 0.00537033, ..., 0.00537033, 0.00537033,\n        0.00537033],\n       [0.00701583, 0.00701583, 0.00701583, ..., 0.00701583, 0.00701583,\n        0.00701583]])perf_counter_start(chain, draw)float642.361e+06 2.361e+06 ... 2.361e+06array([[2361318.79279567, 2361318.87508063, 2361318.95529054, ...,\n        2361479.04572258, 2361479.12740954, 2361479.20918579],\n       [2361312.089392  , 2361312.17086767, 2361312.25125425, ...,\n        2361457.72276992, 2361457.80452825, 2361457.88554038],\n       [2361312.71388937, 2361312.79556071, 2361312.87769787, ...,\n        2361473.83453225, 2361473.91610921, 2361473.99781033],\n       [2361315.01769779, 2361315.098796  , 2361315.17949475, ...,\n        2361475.50722583, 2361475.58900379, 2361475.67075933]])n_steps(chain, draw)float641.023e+03 1.023e+03 ... 1.023e+03array([[1023., 1023., 1023., ..., 1023., 1023., 1023.],\n       [1023., 1023., 1023., ..., 1023., 1023., 1023.],\n       [1023., 1023., 1023., ..., 1023., 1023., 1023.],\n       [1023., 1023., 1023., ..., 1023., 1023., 1023.]])max_energy_error(chain, draw)float64-0.03173 -0.06478 ... 0.1507 0.15array([[-0.03173012, -0.06477837, -0.2075868 , ..., -0.24468182,\n        -0.11603018,  0.29430899],\n       [-0.84910791, -0.52014748, -0.76125186, ..., -0.32416101,\n        -0.36622319, -0.5667363 ],\n       [-0.15657836, -0.25783916, -0.18139475, ...,  0.06635242,\n        -0.05819053, -0.06931609],\n       [-0.13684815, -0.08643352, -0.15030015, ...,  0.17884379,\n         0.15065315,  0.14999317]])energy_error(chain, draw)float64-0.02417 0.02804 ... -0.06934array([[-0.02417412,  0.02804133, -0.01108127, ..., -0.03346141,\n        -0.00129006,  0.24695244],\n       [ 0.05038906,  0.22945603, -0.36579941, ...,  0.04978283,\n         0.07012965, -0.27605762],\n       [ 0.00575441, -0.04686598, -0.03784106, ...,  0.00566893,\n         0.00287121, -0.0143361 ],\n       [-0.00074252, -0.00241762, -0.06416166, ..., -0.02266332,\n         0.07892408, -0.06933777]])largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])energy(chain, draw)float64-141.4 -154.8 ... -177.6 -175.2array([[-141.35781495, -154.82042184, -158.50830829, ..., -169.51286638,\n        -161.92045136, -167.16629184],\n       [-161.91301831, -144.81534601, -149.29314566, ..., -124.68284739,\n        -126.4725391 , -140.03003424],\n       [-142.80921945, -146.5387865 , -146.8058751 , ..., -129.37532231,\n        -140.67959809, -156.69836175],\n       [-139.25837416, -135.14965861, -136.52022668, ..., -188.92859374,\n        -177.63265783, -175.21581418]])tree_depth(chain, draw)int6410 10 10 10 10 ... 10 10 10 10 10array([[10, 10, 10, ..., 10, 10, 10],\n       [10, 10, 10, ..., 10, 10, 10],\n       [10, 10, 10, ..., 10, 10, 10],\n       [10, 10, 10, ..., 10, 10, 10]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])index_in_trajectory(chain, draw)int64-352 387 -740 273 ... 790 521 -683array([[-352,  387, -740, ...,  643,  428, -650],\n       [ 453,  487,  609, ...,  749, -425,  216],\n       [-596,  472,  590, ..., -161,  452,  263],\n       [-220,  272, -830, ...,  790,  521, -683]])process_time_diff(chain, draw)float640.08216 0.08007 ... 0.08164 0.0818array([[0.082157, 0.080075, 0.081789, ..., 0.081559, 0.081672, 0.08158 ],\n       [0.08135 , 0.080263, 0.080593, ..., 0.081654, 0.080874, 0.081816],\n       [0.081545, 0.082021, 0.080146, ..., 0.081464, 0.081571, 0.081977],\n       [0.080926, 0.080607, 0.079093, ..., 0.081597, 0.081638, 0.0818  ]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999],\n      dtype='int64', name='draw', length=2000))Attributes: (6)created_at :2024-05-27T06:09:05.940582arviz_version :0.17.0inference_library :pymcinference_library_version :5.10.3sampling_time :227.7068202495575tuning_steps :1000\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (obs: 60)\nCoordinates:\n  * obs      (obs) int64 0 0 0 0 0 0 1 1 1 1 1 1 2 ... 7 8 8 8 8 8 8 9 9 9 9 9 9\nData variables:\n    lr       (obs) float64 0.2309 0.5117 0.6164 0.6386 ... 0.859 0.9038 0.9244\nAttributes:\n    created_at:                 2024-05-27T06:09:05.943015\n    arviz_version:              0.17.0\n    inference_library:          pymc\n    inference_library_version:  5.10.3xarray.DatasetDimensions:obs: 60Coordinates: (1)obs(obs)int640 0 0 0 0 0 1 1 ... 8 8 9 9 9 9 9 9array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n       4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n       8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9])Data variables: (1)lr(obs)float640.2309 0.5117 ... 0.9038 0.9244array([0.23088384, 0.51173309, 0.61639972, 0.63858884, 0.67259086,\n       0.68993774, 0.13897597, 0.34796238, 0.45036573, 0.59561129,\n       0.64263323, 0.64263323, 0.2345894 , 0.47973401, 0.58713321,\n       0.62354866, 0.63473718, 0.63742875, 0.25277402, 0.47253045,\n       0.64005413, 0.69797023, 0.74776725, 0.80270636, 0.29829006,\n       0.51463926, 0.59872363, 0.66151898, 0.68509768, 0.70209967,\n       0.33072662, 0.79244053, 1.12088628, 1.31753014, 1.40029326,\n       1.42440534, 0.25679105, 0.44820942, 0.53957139, 0.57970674,\n       0.59681361, 0.59784754, 0.25876918, 0.65744596, 0.86254492,\n       0.94998004, 0.98790852, 1.03097017, 0.24606574, 0.41036624,\n       0.49781029, 0.54771644, 0.56508975, 0.57923865, 0.25779626,\n       0.54739102, 0.76565216, 0.85900276, 0.90375243, 0.9244402 ])Indexes: (1)obsPandasIndexPandasIndex(Index([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n       4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n       8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9],\n      dtype='int64', name='obs'))Attributes: (4)created_at :2024-05-27T06:09:05.943015arviz_version :0.17.0inference_library :pymcinference_library_version :5.10.3\n                      \n                  \n            \n            \n            \n                  \n                  constant_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:       (Bg_dim_0: 6, Bg_dim_1: 7, Bi_dim_0: 6, Bi_dim_1: 7,\n                   prem_dim_0: 60, grcode_dim_0: 60)\nCoordinates:\n  * Bg_dim_0      (Bg_dim_0) int64 0 1 2 3 4 5\n  * Bg_dim_1      (Bg_dim_1) int64 0 1 2 3 4 5 6\n  * Bi_dim_0      (Bi_dim_0) int64 0 1 2 3 4 5\n  * Bi_dim_1      (Bi_dim_1) int64 0 1 2 3 4 5 6\n  * prem_dim_0    (prem_dim_0) int64 0 1 2 3 4 5 6 7 ... 52 53 54 55 56 57 58 59\n  * grcode_dim_0  (grcode_dim_0) int64 0 1 2 3 4 5 6 7 ... 53 54 55 56 57 58 59\nData variables:\n    Bg            (Bg_dim_0, Bg_dim_1) float64 1.0 0.0 0.0 0.0 ... 0.0 0.0 1.0\n    Bi            (Bi_dim_0, Bi_dim_1) float64 1.0 0.0 0.0 0.0 ... 0.0 0.0 1.0\n    prem          (prem_dim_0) float64 -0.3475 -0.3475 -0.3475 ... -1.572 -1.572\n    grcode        (grcode_dim_0) float64 0.0 0.0 0.0 0.0 0.0 ... 1.0 1.0 1.0 1.0\nAttributes:\n    created_at:                 2024-05-27T06:09:05.943907\n    arviz_version:              0.17.0\n    inference_library:          pymc\n    inference_library_version:  5.10.3xarray.DatasetDimensions:Bg_dim_0: 6Bg_dim_1: 7Bi_dim_0: 6Bi_dim_1: 7prem_dim_0: 60grcode_dim_0: 60Coordinates: (6)Bg_dim_0(Bg_dim_0)int640 1 2 3 4 5array([0, 1, 2, 3, 4, 5])Bg_dim_1(Bg_dim_1)int640 1 2 3 4 5 6array([0, 1, 2, 3, 4, 5, 6])Bi_dim_0(Bi_dim_0)int640 1 2 3 4 5array([0, 1, 2, 3, 4, 5])Bi_dim_1(Bi_dim_1)int640 1 2 3 4 5 6array([0, 1, 2, 3, 4, 5, 6])prem_dim_0(prem_dim_0)int640 1 2 3 4 5 6 ... 54 55 56 57 58 59array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59])grcode_dim_0(grcode_dim_0)int640 1 2 3 4 5 6 ... 54 55 56 57 58 59array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59])Data variables: (4)Bg(Bg_dim_0, Bg_dim_1)float641.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 1.0array([[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 3.08571429e-01, 5.69339736e-01, 1.21488595e-01,\n        6.00240096e-04, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.14285714e-02, 4.09819928e-01, 5.03721489e-01,\n        7.50300120e-02, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 7.50300120e-02, 5.03721489e-01,\n        4.09819928e-01, 1.14285714e-02, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 6.00240096e-04, 1.21488595e-01,\n        5.69339736e-01, 3.08571429e-01, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n        0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])Bi(Bi_dim_0, Bi_dim_1)float641.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 1.0array([[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 3.08571429e-01, 5.69339736e-01, 1.21488595e-01,\n        6.00240096e-04, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.14285714e-02, 4.09819928e-01, 5.03721489e-01,\n        7.50300120e-02, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 7.50300120e-02, 5.03721489e-01,\n        4.09819928e-01, 1.14285714e-02, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 6.00240096e-04, 1.21488595e-01,\n        5.69339736e-01, 3.08571429e-01, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n        0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])prem(prem_dim_0)float64-0.3475 -0.3475 ... -1.572 -1.572array([-0.34745346, -0.34745346, -0.34745346, -0.34745346, -0.34745346,\n       -0.34745346,  1.72234378,  1.72234378,  1.72234378,  1.72234378,\n        1.72234378,  1.72234378, -0.3654406 , -0.3654406 , -0.3654406 ,\n       -0.3654406 , -0.3654406 , -0.3654406 ,  1.40460971,  1.40460971,\n        1.40460971,  1.40460971,  1.40460971,  1.40460971, -0.5486773 ,\n       -0.5486773 , -0.5486773 , -0.5486773 , -0.5486773 , -0.5486773 ,\n        1.12110923,  1.12110923,  1.12110923,  1.12110923,  1.12110923,\n        1.12110923, -0.63582788, -0.63582788, -0.63582788, -0.63582788,\n       -0.63582788, -0.63582788, -0.20123544, -0.20123544, -0.20123544,\n       -0.20123544, -0.20123544, -0.20123544, -0.5779209 , -0.5779209 ,\n       -0.5779209 , -0.5779209 , -0.5779209 , -0.5779209 , -1.57150713,\n       -1.57150713, -1.57150713, -1.57150713, -1.57150713, -1.57150713])grcode(grcode_dim_0)float640.0 0.0 0.0 0.0 ... 1.0 1.0 1.0 1.0array([0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n       0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n       1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n       0., 0., 0., 1., 1., 1., 1., 1., 1.])Indexes: (6)Bg_dim_0PandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5], dtype='int64', name='Bg_dim_0'))Bg_dim_1PandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6], dtype='int64', name='Bg_dim_1'))Bi_dim_0PandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5], dtype='int64', name='Bi_dim_0'))Bi_dim_1PandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6], dtype='int64', name='Bi_dim_1'))prem_dim_0PandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59],\n      dtype='int64', name='prem_dim_0'))grcode_dim_0PandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59],\n      dtype='int64', name='grcode_dim_0'))Attributes: (4)created_at :2024-05-27T06:09:05.943907arviz_version :0.17.0inference_library :pymcinference_library_version :5.10.3\n                      \n                  \n            \n            \n              \n            \n            \n\n\nEven here though we want to average the curves over the specific years in the data and abstract a view of the model implications under different counterfactual settings. Or put another way, we want a view of the average development curve between years, not within a year. If we want to predict the novel insurance loss for next year we need some way to aggregate between the annual trajectories. Here we define a helper function to effect this step.\n\ndef get_posterior_predictive_curve(idata, prem=2, grcode=1):\n    weighted_splines = [np.dot(np.asfortranarray(Bi), az.extract(idata['posterior']['beta'])['beta'].values[:, :, i]) for i in range(2000)]\n\n    weighted_splines_1 = [np.dot(np.asfortranarray(Bg), az.extract(idata['posterior']['beta_g'])['beta_g'].values[:, i]) for i in range(2000)]\n\n    beta_grcode = az.extract(idata['posterior']['beta_grcode'])['beta_grcode']\n\n    beta_prem = az.extract(idata['posterior']['beta_prem'])['beta_prem']\n    df1 = pd.DataFrame([beta_prem.values[i]*prem + beta_grcode.values[i]*grcode for i in range(2000)]).T\n\n\n    ## How do we averaging over the years to get\n    ## a view of a new development period?\n    #df = pd.concat([pd.DataFrame(weighted_splines_1[i].T + weighted_splines[i].T).mean() for i in range(1000)], axis=1)\n\n    ## Sample random group each draw from posterior\n    df = pd.concat([pd.DataFrame((weighted_splines_1[i].T + weighted_splines[i].T)[np.random.choice(list(range(10))), :]) for i in range(2000)], axis=1)\n\n    ## Average random subset of of groups\n    #df = pd.concat([pd.DataFrame((weighted_splines_1[i].T + weighted_splines[i].T)[np.random.choice(list(range(10)), 5), :]).mean() for i in range(2000)], axis=1)\n\n    df = df1.iloc[0].values + df\n\n    return df\n\npred_df_1 = get_posterior_predictive_curve(idata_43, prem=1, grcode=1)\npred_df_0 = get_posterior_predictive_curve(idata_353, prem=1, grcode=0) \n\nfig, ax = plt.subplots(figsize=(9, 7), sharey=True)\n\naz.plot_hdi(range(6), pred_df_0.values.T, ax=ax, color='slateblue', smooth=False, fill_kwargs={'alpha': 0.2})\naz.plot_hdi(range(6), pred_df_1.values.T, ax=ax, color='firebrick', smooth=False, fill_kwargs={'alpha': 0.2})\naz.plot_hdi(range(6), pred_df_0.values.T, ax=ax, color='slateblue', smooth=False, fill_kwargs={'alpha': 0.5}, hdi_prob=.5)\naz.plot_hdi(range(6), pred_df_1.values.T, ax=ax, color='firebrick', smooth=False, fill_kwargs={'alpha': 0.5}, hdi_prob=.5)\nax.plot(pred_df_0.mean(axis=1), linestyle='-', color='k', linewidth=4, label='grcode 353 prem 1')\n\nax.plot(pred_df_1.mean(axis=1), linestyle='--', color='grey', linewidth=4, label='grcode 43 prem 1')\n\nax.set_title(\"Posterior Samples of the Trajectories \\n Under different Counterfactual settings\")\nax.set_ylabel(\"Loss Ratio\")\nax.set_xlabel(\"Development Period\")\nax.legend();\n\n\n\n\nNote how we’ve varied the predictor inputs to showcase the expected realisation under different product codes. In this manner we can construct a hierarchical spline model with informed by the historic trajectories of the development curves but with predictions modulated by more recent information."
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#conclusion",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#conclusion",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve seen the application of splines as univariate smoothers to approximate wiggly curves of arbitrary shape. We’ve also tried gaussian process approximations of the same univariate functions. The suggested flexibility of both methods is a strength, but we need to be careful where splines have a tendency to over-fit to individual curves. As such we have tried to show that we can incorporate spline basis modelling in a hierarchical bayesian model and recover more compelling posterior predictive checks and additionally derive predictiions from the mixed variant of the hierarhical model which helps us understand the implications of the data for generic forecasts of insurance loss curves. Finally we showed how splines can be used additively to model non-linear functions of multiple covariates. These are a powerful tool to interrogate complex non-linear relationships, but they offer interpolation of functions within a well understood domain. Applealing to these models for extrapolation needs to be done carefully.\nWe can, I think, draw a broad moral from this presentation. Observed phenomena admit description with a wide variety of mathematical abstraction. The “correctness” of the abstraction is never the right question. The manner in which one abstraction distils the structure of reality is not fundamentally any better than the manner in which an alternative scheme will do so. Predictive success achieved applying one abstraction can be traded off for interpretive tractability in another. What matters is the task at hand - solving the problem. Sometimes problems can be solved with new information, but mostly they’re resolved by re-arranging assumptions and phrasing the problem anew - seeing through a different lens, pulling the door handle where we previously pushed. Splines, hierarhical splines and gaussian process models all attempt to approximate an answer by rephrasing the question. The more of these tools we have, the better we are prepared to tackle novel problems and discern the pertinent structure in the world."
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#multiple-smoother-regression-models",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#multiple-smoother-regression-models",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Multiple Smoother Regression Models",
    "text": "Multiple Smoother Regression Models\nNon-linear spline relationships can be additively combined across multiple variables in simple and hierarchical regressions. To demonstrate how spline modelling can be further adapted to the multiple regression like cases we use the PISA data set discussed in this case study from Michael Clark. We’ll see how crucial it is to carefully assess the implications of these model fits.\nThe data set has been constructed using average Science scores by country from the Programme for International Student Assessment (PISA) 2006, along with GNI per capita (Purchasing Power Parity, 2005 dollars), Educational Index, Health Index, and Human Development Index from UN data. We want to model the overall outcome score as a function of these broad demographic features.\n\npisa_df = pd.read_csv(\"https://raw.githubusercontent.com/m-clark/generalized-additive-models/master/data/pisasci2006.csv\")\n\npisa_df.head()\n\n\n\n\n\n  \n    \n      \n      Country\n      Overall\n      Issues\n      Explain\n      Evidence\n      Interest\n      Support\n      Income\n      Health\n      Edu\n      HDI\n    \n  \n  \n    \n      0\n      Albania\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.599\n      0.886\n      0.716\n      0.724\n    \n    \n      1\n      Argentina\n      391.0\n      395.0\n      386.0\n      385.0\n      567.0\n      506.0\n      0.678\n      0.868\n      0.786\n      0.773\n    \n    \n      2\n      Australia\n      527.0\n      535.0\n      520.0\n      531.0\n      465.0\n      487.0\n      0.826\n      0.965\n      0.978\n      0.920\n    \n    \n      3\n      Austria\n      511.0\n      505.0\n      516.0\n      505.0\n      507.0\n      515.0\n      0.835\n      0.944\n      0.824\n      0.866\n    \n    \n      4\n      Azerbaijan\n      382.0\n      353.0\n      412.0\n      344.0\n      612.0\n      542.0\n      0.566\n      0.780\n      NaN\n      NaN\n    \n  \n\n\n\n\nThe relationships displayed between each of these measures is not obviously linear, and as such could plausibly benefit from being modelled with splines.\n\ng = sns.pairplot(pisa_df[['Overall', 'Income', 'Support', 'Health', 'Edu']],  kind=\"reg\", height=1.5)\ng.fig.suptitle(\"Pair Plot of Complex Relations\", y=1.05);\n\n\n\n\nWe define three models for contrasting the implications. Note here how we have to define a seperate spline basis for each of the covariates. Here we create the knots for defining our basis on each covariate.\n\nknots_income = np.linspace(np.min(pisa_df['Income']), np.max(pisa_df['Income']), 5+2)[1:-1]\n\nknots_edu = np.linspace(np.min(pisa_df['Edu']), np.max(pisa_df['Edu']), 5+2)[1:-1]\n\nknots_health = np.linspace(np.min(pisa_df['Health']), np.max(pisa_df['Health']), 5+2)[1:-1]\n\nknots_income1 = np.linspace(np.min(pisa_df['Income']), np.max(pisa_df['Income']), 3+2)[1:-1]\n\nknots_edu1 = np.linspace(np.min(pisa_df['Edu']), np.max(pisa_df['Edu']), 3+2)[1:-1]\n\nknots_health1 = np.linspace(np.min(pisa_df['Health']), np.max(pisa_df['Health']), 3+2)[1:-1]\n\nNow we initialise these models\n\nformula = \"Overall ~ Income + Edu + Health\"\nbase_model = bmb.Model(formula, pisa_df, dropna=True)\n\nformula_spline = \"\"\"Overall ~ bs(Income, degree=3, knots=knots_income) + bs(Edu, degree=3, knots=knots_edu) + bs(Health, degree=3, knots=knots_health) \"\"\"\n\nformula_spline1 = \"\"\"Overall ~ (1 | Country) + bs(Income, degree=3, knots=knots_income1) + bs(Edu, degree=3, knots=knots_edu1) + bs(Health, degree=3, knots=knots_health1) \"\"\"\n\nspline_model = bmb.Model(formula_spline, pisa_df, dropna=True)\n\nspline_model1 = bmb.Model(formula_spline1, pisa_df, dropna=True)\n\nspline_model1\n\nAutomatically removing 13/65 rows from the dataset.\n\n\nAutomatically removing 13/65 rows from the dataset.\n\n\nAutomatically removing 13/65 rows from the dataset.\n\n\n       Formula: Overall ~ (1 | Country) + bs(Income, degree=3, knots=knots_income1) + bs(Edu, degree=3, knots=knots_edu1) + bs(Health, degree=3, knots=knots_health1) \n        Family: gaussian\n          Link: mu = identity\n  Observations: 52\n        Priors: \n    target = mu\n        Common-level effects\n            Intercept ~ Normal(mu: 471.1538, sigma: 517.9979)\n            bs(Income, degree=3, knots=knots_income1) ~ Normal(mu: [0. 0. 0. 0. 0. 0.], sigma: [1630.0781\n                872.587   532.485   674.4864  570.5563  779.4582])\n            bs(Edu, degree=3, knots=knots_edu1) ~ Normal(mu: [0. 0. 0. 0. 0. 0.], sigma: [965.2606 734.261\n                559.679  590.0573 684.2515 649.6795])\n            bs(Health, degree=3, knots=knots_health1) ~ Normal(mu: [0. 0. 0. 0. 0. 0.], sigma: [1842.3891\n                815.7797  534.7546  671.1905  544.161   802.929 ])\n        \n        \n        Group-level effects\n            1|Country ~ Normal(mu: 0.0, sigma: HalfNormal(sigma: 517.9979))\n        \n        Auxiliary parameters\n            sigma ~ HalfStudentT(nu: 4.0, sigma: 53.4654)\n\n\n\nspline_model.build()\nspline_model.graph()\n\n\n\n\n\nbase_idata = base_model.fit(random_seed=random_seed, idata_kwargs={\"log_likelihood\": True})\nspline_idata = spline_model.fit(random_seed=random_seed, idata_kwargs={\"log_likelihood\": True}, target_accept=.95)\nspline_idata1 = spline_model1.fit(random_seed=random_seed, idata_kwargs={\"log_likelihood\": True}, target_accept=.99)\n\nWe can compare the simple regression approach to the spline based regression in the now usual way.\n\ncompare_df = az.compare({'spline': spline_idata, 'raw': base_idata, 'spline_hierarchy':spline_idata1})\n\ncompare_df\n\n\n\n\n\n  \n    \n      \n      rank\n      elpd_loo\n      p_loo\n      elpd_diff\n      weight\n      se\n      dse\n      warning\n      scale\n    \n  \n  \n    \n      spline_hierarchy\n      0\n      -244.691895\n      38.487159\n      0.000000\n      1.000000e+00\n      3.901855\n      0.000000\n      True\n      log\n    \n    \n      spline\n      1\n      -254.101306\n      20.218465\n      9.409411\n      0.000000e+00\n      4.724572\n      2.840914\n      True\n      log\n    \n    \n      raw\n      2\n      -263.061455\n      9.617888\n      18.369559\n      1.054712e-13\n      10.083921\n      9.376156\n      True\n      log\n    \n  \n\n\n\n\nThe coefficients comparisons are harder\n\naz.summary(base_idata)\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      118.438\n      81.996\n      -41.166\n      266.033\n      1.522\n      1.086\n      2915.0\n      2986.0\n      1.0\n    \n    \n      Income\n      181.955\n      89.547\n      17.413\n      353.598\n      1.913\n      1.353\n      2196.0\n      2432.0\n      1.0\n    \n    \n      Edu\n      234.125\n      59.153\n      120.739\n      340.343\n      1.017\n      0.740\n      3414.0\n      2518.0\n      1.0\n    \n    \n      Health\n      30.395\n      140.966\n      -223.699\n      298.258\n      2.852\n      2.153\n      2450.0\n      2598.0\n      1.0\n    \n    \n      Overall_sigma\n      34.234\n      3.620\n      27.882\n      41.175\n      0.060\n      0.043\n      3757.0\n      2879.0\n      1.0\n    \n  \n\n\n\n\nsince it’s less clear what the spline coefficient terms mean.\n\naz.summary(spline_idata)\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      225.689\n      205.515\n      -133.222\n      630.021\n      6.036\n      4.292\n      1163.0\n      1653.0\n      1.0\n    \n    \n      bs(Income, degree=3, knots=knots_income)[0]\n      -355.159\n      1613.343\n      -3460.788\n      2642.535\n      56.357\n      39.865\n      819.0\n      1504.0\n      1.0\n    \n    \n      bs(Income, degree=3, knots=knots_income)[1]\n      194.905\n      122.719\n      -42.478\n      423.960\n      2.343\n      1.728\n      2748.0\n      2719.0\n      1.0\n    \n    \n      bs(Income, degree=3, knots=knots_income)[2]\n      48.748\n      123.638\n      -182.012\n      272.863\n      4.065\n      2.875\n      923.0\n      1431.0\n      1.0\n    \n    \n      bs(Income, degree=3, knots=knots_income)[3]\n      68.533\n      98.242\n      -118.858\n      250.496\n      3.369\n      2.383\n      853.0\n      1416.0\n      1.0\n    \n    \n      bs(Income, degree=3, knots=knots_income)[4]\n      210.290\n      103.892\n      17.347\n      406.753\n      3.626\n      2.565\n      825.0\n      1404.0\n      1.0\n    \n    \n      bs(Income, degree=3, knots=knots_income)[5]\n      222.546\n      105.553\n      31.163\n      427.878\n      3.589\n      2.539\n      866.0\n      1442.0\n      1.0\n    \n    \n      bs(Income, degree=3, knots=knots_income)[6]\n      202.395\n      105.402\n      0.453\n      394.110\n      3.629\n      2.567\n      846.0\n      1507.0\n      1.0\n    \n    \n      bs(Income, degree=3, knots=knots_income)[7]\n      81.592\n      103.313\n      -106.154\n      279.937\n      3.641\n      2.575\n      805.0\n      1390.0\n      1.0\n    \n    \n      bs(Edu, degree=3, knots=knots_edu)[0]\n      145.558\n      287.518\n      -410.399\n      654.146\n      8.525\n      6.029\n      1142.0\n      1509.0\n      1.0\n    \n    \n      bs(Edu, degree=3, knots=knots_edu)[1]\n      47.574\n      193.029\n      -323.504\n      392.406\n      5.595\n      3.957\n      1197.0\n      1656.0\n      1.0\n    \n    \n      bs(Edu, degree=3, knots=knots_edu)[2]\n      71.806\n      211.848\n      -312.889\n      469.050\n      6.246\n      4.417\n      1156.0\n      1647.0\n      1.0\n    \n    \n      bs(Edu, degree=3, knots=knots_edu)[3]\n      147.390\n      200.622\n      -224.182\n      518.214\n      5.972\n      4.224\n      1134.0\n      1603.0\n      1.0\n    \n    \n      bs(Edu, degree=3, knots=knots_edu)[4]\n      70.472\n      203.119\n      -314.815\n      427.662\n      6.018\n      4.256\n      1144.0\n      1535.0\n      1.0\n    \n    \n      bs(Edu, degree=3, knots=knots_edu)[5]\n      121.297\n      202.750\n      -264.164\n      485.547\n      5.998\n      4.243\n      1147.0\n      1575.0\n      1.0\n    \n    \n      bs(Edu, degree=3, knots=knots_edu)[6]\n      78.372\n      206.089\n      -296.582\n      457.669\n      6.012\n      4.252\n      1178.0\n      1713.0\n      1.0\n    \n    \n      bs(Edu, degree=3, knots=knots_edu)[7]\n      124.157\n      203.070\n      -258.062\n      490.727\n      6.008\n      4.249\n      1148.0\n      1532.0\n      1.0\n    \n    \n      bs(Health, degree=3, knots=knots_health)[0]\n      235.831\n      885.656\n      -1490.479\n      1862.011\n      31.104\n      22.002\n      813.0\n      1482.0\n      1.0\n    \n    \n      bs(Health, degree=3, knots=knots_health)[1]\n      162.805\n      389.636\n      -552.198\n      913.839\n      7.756\n      6.545\n      2529.0\n      2422.0\n      1.0\n    \n    \n      bs(Health, degree=3, knots=knots_health)[2]\n      14.967\n      120.574\n      -202.130\n      248.327\n      3.854\n      2.726\n      979.0\n      1657.0\n      1.0\n    \n    \n      bs(Health, degree=3, knots=knots_health)[3]\n      25.213\n      100.498\n      -170.867\n      204.686\n      3.411\n      2.413\n      871.0\n      1555.0\n      1.0\n    \n    \n      bs(Health, degree=3, knots=knots_health)[4]\n      -21.315\n      102.473\n      -223.005\n      158.665\n      3.577\n      2.530\n      822.0\n      1468.0\n      1.0\n    \n    \n      bs(Health, degree=3, knots=knots_health)[5]\n      -19.449\n      101.418\n      -213.704\n      165.809\n      3.541\n      2.505\n      823.0\n      1444.0\n      1.0\n    \n    \n      bs(Health, degree=3, knots=knots_health)[6]\n      -55.820\n      100.895\n      -252.216\n      126.193\n      3.547\n      2.509\n      811.0\n      1347.0\n      1.0\n    \n    \n      bs(Health, degree=3, knots=knots_health)[7]\n      0.840\n      101.892\n      -196.002\n      186.216\n      3.543\n      2.506\n      828.0\n      1469.0\n      1.0\n    \n    \n      Overall_sigma\n      24.766\n      3.303\n      18.816\n      30.884\n      0.085\n      0.060\n      1491.0\n      2068.0\n      1.0\n    \n  \n\n\n\n\nWe can also check that the model seems to recover the observed data well.\n\nPlotting Posterior Predictive Checks\n\nbase_model.predict(base_idata, kind='pps')\nspline_model.predict(spline_idata, kind='pps')\nspline_model1.predict(spline_idata1, kind='pps')\n\n\nfig, axs = plt.subplots(1, 3, figsize=(20, 6))\naxs = axs.flatten()\naz.plot_ppc(base_idata, ax=axs[0])\naz.plot_ppc(spline_idata, ax=axs[1]);\naz.plot_ppc(spline_idata1, ax=axs[2]);\naxs[0].set_xlabel('')\naxs[1].set_title(\"PPC: Spline Model\");\naxs[0].set_title(\"PPC: Regression Model\");\naxs[2].set_title(\"PPC: Hierarchical Spline Model\");\n\n\n\n\nNext we’ll dig into the spline basis features and decompose the predicted outcome and show how the outcome varies for levels in the inputs variables. The Bambi interpret package offers some functionality to assess the conditional predictions for varying values of the input variables.\n\n\nModel Interpretability Plots in Bambi\nWe want to highlight the differences between the hierarchical and non-hierarchical multivariable spline models here. The shrinkage effects of hierarchically modelling the country intercepts is clearly evident. They helpfully constrain the poor extrapolation effects of the simpler spline model.\n\nfig, axs = plt.subplots(3, 2, figsize=(9, 14))\naxs = axs.flatten()\nbmb.interpret.plot_predictions(\n    spline_model, spline_idata, \"Income\", ax=axs[0]\n)\n\naxs[0].set_title(\"Non-Hierarchical Income\")\naxs[2].set_title(\"Non-Hierarchical Edu\")\naxs[4].set_title(\"Non-Hierarchical Health\")\n\naxs[1].set_title(\"Hierarchical Income\")\naxs[3].set_title(\"Hierarchical Edu\")\naxs[5].set_title(\"Hierarchical Health\")\n\n\nbmb.interpret.plot_predictions(\n    spline_model, spline_idata, \"Edu\", ax=axs[2]\n)\n\nbmb.interpret.plot_predictions(\n    spline_model, spline_idata, \"Health\", ax=axs[4]\n);\n\nbmb.interpret.plot_predictions(\n    spline_model1, spline_idata1, \"Income\", \n    sample_new_groups=True, ax=axs[1]\n)\n\nbmb.interpret.plot_predictions(\n    spline_model1, spline_idata1, \"Edu\", sample_new_groups=True, ax=axs[3]\n)\n\nbmb.interpret.plot_predictions(\n    spline_model1, spline_idata1, \"Health\",sample_new_groups=True, ax=axs[5]\n);\n\n\n\n\nWe can pull these types of values out into a table. Note here how we ask for predictions based on varying values of the Edu and Income variables where we keep the Health variable fixed at the mean value.\n\nsummary_df = bmb.interpret.predictions(\n    spline_model,\n    spline_idata,\n    conditional={\n        \"Edu\": list(np.linspace(0.5, .95, 10)),\n        \"Income\": list(np.linspace(0.4, .95, 10)),\n        },\n)\nsummary_df\n\n\nsummary_df1 = bmb.interpret.predictions(\n    spline_model1,\n    spline_idata1,\n    conditional={\n        \"Edu\": list(np.linspace(0.5, .95, 10)),\n        \"Income\": list(np.linspace(0.4, .95, 10)),\n        },\n    sample_new_groups=True\n)\nsummary_df1\n\n\n\n\n\n  \n    \n      \n      Edu\n      Income\n      Country\n      Health\n      estimate\n      lower_3.0%\n      upper_97.0%\n    \n  \n  \n    \n      0\n      0.50\n      0.400000\n      Albania\n      0.885672\n      102.686176\n      -587.761967\n      750.373783\n    \n    \n      1\n      0.50\n      0.461111\n      Albania\n      0.885672\n      451.070276\n      -37.688251\n      905.442110\n    \n    \n      2\n      0.50\n      0.522222\n      Albania\n      0.885672\n      376.852378\n      -161.138067\n      945.914437\n    \n    \n      3\n      0.50\n      0.583333\n      Albania\n      0.885672\n      283.859330\n      -357.146695\n      933.659941\n    \n    \n      4\n      0.50\n      0.644444\n      Albania\n      0.885672\n      288.608418\n      -345.493652\n      920.643943\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      0.95\n      0.705556\n      Albania\n      0.885672\n      460.351293\n      413.197965\n      508.270287\n    \n    \n      96\n      0.95\n      0.766667\n      Albania\n      0.885672\n      516.885593\n      474.706704\n      561.845324\n    \n    \n      97\n      0.95\n      0.827778\n      Albania\n      0.885672\n      552.375825\n      506.285465\n      603.102537\n    \n    \n      98\n      0.95\n      0.888889\n      Albania\n      0.885672\n      499.921356\n      455.457677\n      550.896029\n    \n    \n      99\n      0.95\n      0.950000\n      Albania\n      0.885672\n      229.655207\n      49.386349\n      412.317948\n    \n  \n\n100 rows × 7 columns\n\n\n\nWe can then plot these results based on the conditional realisations and see some interesting behaviour at the implausilble reaslisations. This should make us somewhat wary of using this model to extrapolate too far beyond the observable range of data\n\ng = sns.relplot(data=summary_df, x=\"Income\", y=\"estimate\", hue=\"Edu\")\n\ng.fig.suptitle(\"Marginal Predictions of the Outcome Variable \\n conditional on counterfactual values for Edu and Income\", y=1.05);\n\n\ng = sns.relplot(data=summary_df1, x=\"Income\", y=\"estimate\", hue=\"Edu\")\n\ng.fig.suptitle(\"Hierarchical Marginal Predictions of the Outcome Variable \\n conditional on counterfactual values for Edu and Income\", y=1.05);\n\n\n\n\n\n\n\nHowever, we can see here how the hierarchical component shrinks the predicted values back towards a reasonable range! Constraining the poor extrapolation of the more naive spline model. This demonstrates something of the interplay between in-sample approximation and out of sample generalisation we saw above in the case of the insurance curve development. The additional structure insists on commensurate realisations under different counterfactual settings reflective of the fact that countries do not vary so radically.\n\n\nThe Spline Component Contributions\nFinally, we’ll pull out the component contributions of each variable and see how they combine additively. This will also serve as a kind of posterior predictive check for each country as we can show the degree which posterior draws from each component sum to achieve a plausible mirror of the observed data.\n\nBincome = spline_model.response_component.design.common['bs(Income, degree=3, knots=knots_income)']\n\nincome_coefs = az.extract(spline_idata['posterior']['bs(Income, degree=3, knots=knots_income)'])['bs(Income, degree=3, knots=knots_income)']\n\nBedu = spline_model.response_component.design.common['bs(Edu, degree=3, knots=knots_edu)']\n\nedu_coefs = az.extract(spline_idata['posterior']['bs(Edu, degree=3, knots=knots_edu)'])['bs(Edu, degree=3, knots=knots_edu)']\n\n\nBhealth = spline_model.response_component.design.common['bs(Health, degree=3, knots=knots_health)']\n\nhealth_coefs = az.extract(spline_idata['posterior']['bs(Health, degree=3, knots=knots_health)'])['bs(Health, degree=3, knots=knots_health)']\n\nincome = np.dot(Bincome, income_coefs).T \nedu = np.dot(Bedu, edu_coefs).T\nhealth = np.dot(Bhealth, health_coefs).T\n\nintercept = az.extract(spline_idata['posterior']['Intercept'])['Intercept'].values\n\nfig, ax = plt.subplots(figsize=(10, 7))\nfor i in range(100):\n    if i == 1:\n        ax.plot(income[i], label='Income Component', color='red')\n        ax.plot(edu[i], label='Edu Component', color='blue')\n        ax.plot(health[i], label='Health Component', color='darkgreen')\n        ax.plot(intercept[i] + income[i] + edu[i] + health[i], label='Combined Components', color='purple')\n    else: \n        ax.plot(income[i], alpha=0.1, color='red')\n        ax.plot(edu[i], alpha=0.1, color='blue')\n        ax.plot(health[i], alpha=0.1, color='darkgreen')\n        ax.plot(intercept[i] + income[i] + edu[i] + health[i], color='purple', alpha=0.3)\n\nax.scatter(range(len(spline_idata['observed_data']['Overall'])), spline_idata['observed_data']['Overall'], label='Observed', color='grey', s=56, ec='black')\nax.set_title(\"PISA Outcomes \\n Additive Spline Components\", fontsize=20)\nax.legend();\nax.set_ylabel(\"Overall Score\", fontsize=12)\nax.set_xticklabels(pisa_df.dropna(axis=0).reset_index()['Country'], fontsize=12);\n\n\n\n\nWe can see here how the combined components borrow the structure of the outcome variable primarily from the income variable component. The health measures have closer to zero additive contribution while the uncertainty in the educational component varies wildly. But the blue educational component here is used primarily as a scaling contribution which adds to the level of the outcome variable rather than distorting the shape. These initially opaque synthetic features of splines can offer deep insight into the structure of our data generating process when seen correctly."
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#theoretical-posits-and-science",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#theoretical-posits-and-science",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Theoretical Posits and Science",
    "text": "Theoretical Posits and Science\n\nThere is no way to establish fully secured, neat protocol statements as starting points of the sciences. There is no tabula rasa. We are like sailors who have to rebuild their ship on the open sea, without ever being able to dismantle it in dry-dock and reconstruct it from its best components. - Otto Neurath\n\nIn this blog post we’re going to dive into modelling of non-linear functions and explore some of the tooling available in the python eco-system. We’ll start by looking into Generalised Additive Models with splines in pyGAM before preceding to look Bayesian versions of spline modelling comparing the splines to Gaussian processes in Bambi and PyMC. Before showing how hierarchical bayesian models avoid some of the issues of overfit in simpler spline models.\nOur interest in these models stems from their flexibility to approximate functions of arbitrary complexity. We’ll see how the methods work in the case of relatively straightforward toy example and then we’ll apply each of the methods to deriving insights into the functional form of insurance loss curves. In this application we adapt a data set discussed in Mick Cooney’s Stan case study to demonstrate the power of hierarchical spline models. Throughout we’ll draw on the discussion of these methods in Osvaldo Martin’s “Bayesian Analysis with Python” for practical details implementing these models.\nAll of these methods need to be assessed with respect to their in-sample model fit and their out of sample performance. How can we best calibrate the model fits to perform reasonably well out of sample?"
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#theoretical-posits-and-scientific-commitments",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#theoretical-posits-and-scientific-commitments",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Theoretical Posits and Scientific Commitments",
    "text": "Theoretical Posits and Scientific Commitments\nThe scientific enterprise must abstract away details of reality to provide insight into the structure of reality. Statistical models of real mechanisms are cobbled together from a variety of abstractions; probability distributions and functional relations cojoined, are then calibrated against observed data. We are building on disparate foundations. Good abstractions aim at modelling the structures or real patterns that matter. It doesn’t matter if those abstractions are themselves reflective of reality.\n\n[T]here is no way to establish [a] fully secured, … starting point of the sciences. There is no tabula rasa. We are like sailors who have to rebuild their ship on the open sea, without ever being able to dismantle it in dry-dock and reconstruct it from its best components. - Otto Neurath\n\nThis is an apt metaphor for spline models in particular - like these ships built at sea, spline models are constructed from ad-hoc materials fished from the vast ocean of possible models. They are linear approxiations invoked to ride the next wave form.\nIn this blog post we’re going to dive into modelling of non-linear functions and explore some of the tooling available in the python eco-system. We’ll start by looking into Generalised Additive Models with splines in pyGAM before proceeding to look at how Bayesian versions of spline modelling compares to Gaussian processes in Bambi and PyMC. Finally we will show how hierarchical bayesian models avoid some of the issues of overfit in simpler spline models.\nOur interest in these models stems from their flexibility to approximate functions of arbitrary complexity. We’ll see how the methods work in the case of relatively straightforward toy example and then we’ll apply each of the methods to deriving insights into the functional form of insurance loss curves. In this application we adapt a data set discussed in Mick Cooney’s Stan case study to demonstrate the power of hierarchical spline models. Throughout we’ll draw on the discussion of these methods in Osvaldo Martin’s “Bayesian Analysis with Python” for practical details implementing these models."
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#approximate-gaussian-processes",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#approximate-gaussian-processes",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Approximate Gaussian processes",
    "text": "Approximate Gaussian processes\nGaussian processes models allow us to express our beliefs about the structure of a function and calibrate these beliefs against the observed data. The topic of gaussian processes is rich and complex. Too rich to be fairly covered in this blog post, so we’ll just say that we’re using a method designed for function approximation that makes use of drawing samples from a multivariate normal distribution under a range of different covariance relationships. The properties of these covariance relations determine the shape and fluidity of the function realisations. We will use the HSGP approximation of gaussian processes models.\nThe gaussian process models can be somewhat interrogated by examining the different combinations of covariance relationships with priors over the parameters governing the covariance of a sequence of points. For example consider the following parameterisations.\n\nfig, axs = plt.subplots(2, 2, figsize=(9, 10))\naxs = axs.flatten()\n\ndef plot_cov_draws(ax1, ax2, lengthscale=3, sigma=13):\n    cov = sigma**2 * pm.gp.cov.ExpQuad(1, lengthscale)\n    X = np.linspace(0, 60, 200)[:, None]\n    K = cov(X).eval()\n\n    sns.heatmap(pd.DataFrame(K), center=0, xticklabels=[], yticklabels=[], ax=ax1, cmap='crest');\n    ax1.set_title(f\"Covariance Length Scale {lengthscale}\")\n    ax2.plot(\n        X,\n        pm.draw(\n            pm.MvNormal.dist(mu=np.zeros(len(K)), cov=K, shape=K.shape[0]), draws=10, random_seed=random_seed\n        ).T, color='blue', alpha=0.5\n    )\n    ax2.set_title(f\"Samples from the GP prior \\n lengthscale: {lengthscale}, sigma: {sigma}\")\n    plt.ylabel(\"y\")\n    plt.xlabel(\"X\");\n\nplot_cov_draws(axs[0], axs[1])\nplot_cov_draws(axs[2], axs[3], lengthscale=10, sigma=13)\n\n\n\n\nWe’ve specified the range of X to reflect the support of the acceleration example and allowed the draws to be informed by a covariance function we have parameterised using the Exponentiated Quadratic kernel. We’ve additionally tweaked the lengthscale to demonstrate how proximity in the sequence informs the kernel and determines the shape of the covariance structure.\n\\[k(x, x') = \\mathrm{exp}\\left[ -\\frac{(x - x')^2}{2 \\ell^2} \\right]\\]\nThe plot on the left highlights the importance of the lengthscale parameter over the sequence. The wider the central shading is the more pronounced is the correlation among more points in the sequence. The patterns to the right show a good range of “wiggliness” that they should be flexible enough to capture the shape of the acceleration. These are the constraints of structure we can impose of the theoretical realisations of our model. If we can calibrate the model and derive posterior parameters against the observed data, we can learn the probable shape of our functional relationship.\n\nPriors on Gaussian Processes\nConsider the following specification for the priors\n\nfig, axs = plt.subplots(1, 2, figsize=(9, 6))\naxs = axs.flatten()\naxs[0].hist(pm.draw(pm.InverseGamma.dist(mu=1, sigma=1), 1000), ec='black', bins=30);\naxs[0].set_title(\"Priors for Lengthscale \\n in ExpQuad Kernel\")\naxs[1].hist(pm.draw(pm.Exponential.dist(lam=1), 1000), ec='black', bins=30);\naxs[1].set_title(\"Priors for Amplitude \\n in ExpQuad Kernel\")\n\nText(0.5, 1.0, 'Priors for Amplitude \\n in ExpQuad Kernel')\n\n\n\n\n\nWe use these to specify priors on the Hilbert space approximation of gaussian priors available in the Bambi package.\n\nprior_hsgp = {\n    \"sigma\": bmb.Prior(\"Exponential\", lam=1), # amplitude\n    \"ell\": bmb.Prior(\"InverseGamma\", mu=1, sigma=1) # lengthscale\n}\n\n# This is the dictionary we pass to Bambi\npriors = {\n    \"hsgp(X, m=10, c=1)\": prior_hsgp,\n    \"sigma\": bmb.Prior(\"HalfNormal\", sigma=4)\n}\nmodel_hsgp = bmb.Model(\"y ~ 0 + hsgp(X, m=10, c=1)\", df, priors=priors)\nmodel_hsgp\n\n       Formula: y ~ 0 + hsgp(X, m=10, c=1)\n        Family: gaussian\n          Link: mu = identity\n  Observations: 133\n        Priors: \n    target = mu\n        HSGP contributions\n            hsgp(X, m=10, c=1)\n                cov: ExpQuad\n                sigma ~ Exponential(lam: 1.0)\n                ell ~ InverseGamma(mu: 1.0, sigma: 1.0)\n        \n        Auxiliary parameters\n            sigma ~ HalfNormal(sigma: 4.0)\n\n\nHere we’ve set the m=10 to determine the number of basis vectors used in the Hilbert space approximation. The idea differs in detail from the spline based approximations we’ve seen, but it’s perhaps useful to think of the process in the same vein. Here again we have a theory of the world expressed as a function of opaque components jerry-rigged for modelling some phenomena.\n\nidata_hsgp = model_hsgp.fit(inference_method=\"nuts_numpyro\",target_accept=0.95, random_seed=121195, \nidata_kwargs={\"log_likelihood\": True})\nprint(idata_hsgp.sample_stats[\"diverging\"].sum().to_numpy())\n\nCompiling...\n\n\nCompilation time = 0:00:01.138923\n\n\nSampling...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling time = 0:00:01.870798\n\n\nTransforming variables...\n\n\nTransformation time = 0:00:00.176689\n\n\nComputing Log Likelihood...\n\n\nLog Likelihood time = 0:00:00.144039\n\n\n0\n\n\nThis model fits and the sampling seems to have worked well.\n\naz.plot_trace(idata_hsgp, backend_kwargs={\"layout\": \"constrained\"}, figsize=(9, 15));\n\n\n\n\nThe lengthscale and sigma parameters we have learned by calibrating our priors against the data. The degree to which these parameters are meaningful depend a little on how familar you are with covariance matrix kernels and their properties, so we won’t dwell on the point here.\n\naz.summary(idata_hsgp, var_names=['hsgp(X, m=10, c=1)_ell', 'hsgp(X, m=10, c=1)_sigma', 'y_sigma', 'hsgp(X, m=10, c=1)_weights'])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      hsgp(X, m=10, c=1)_ell\n      3.301\n      0.666\n      2.099\n      4.567\n      0.016\n      0.011\n      1763.0\n      1815.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_sigma\n      23.675\n      2.679\n      18.925\n      28.847\n      0.069\n      0.049\n      1497.0\n      2167.0\n      1.0\n    \n    \n      y_sigma\n      20.589\n      1.121\n      18.590\n      22.751\n      0.019\n      0.014\n      3440.0\n      2485.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[0]\n      -131.588\n      13.784\n      -157.019\n      -105.189\n      0.216\n      0.154\n      4071.0\n      3020.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[1]\n      -94.391\n      18.010\n      -125.921\n      -58.020\n      0.363\n      0.256\n      2500.0\n      2876.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[2]\n      106.121\n      20.717\n      69.627\n      147.247\n      0.458\n      0.324\n      2046.0\n      2563.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[3]\n      149.289\n      22.154\n      106.508\n      188.107\n      0.521\n      0.368\n      1807.0\n      2467.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[4]\n      -83.399\n      22.728\n      -124.812\n      -40.406\n      0.539\n      0.383\n      1776.0\n      2517.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[5]\n      -125.322\n      23.273\n      -169.346\n      -82.187\n      0.531\n      0.377\n      1917.0\n      2940.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[6]\n      37.835\n      21.153\n      -1.712\n      77.574\n      0.512\n      0.362\n      1713.0\n      2577.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[7]\n      94.574\n      20.745\n      53.167\n      132.182\n      0.423\n      0.299\n      2393.0\n      3105.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[8]\n      -1.494\n      17.184\n      -31.835\n      33.063\n      0.361\n      0.255\n      2264.0\n      2853.0\n      1.0\n    \n    \n      hsgp(X, m=10, c=1)_weights[9]\n      -45.442\n      15.955\n      -76.649\n      -16.636\n      0.289\n      0.208\n      3041.0\n      3183.0\n      1.0\n    \n  \n\n\n\n\nBut again we can sample from the posterior predictive distribution of the outcome variable\n\nmodel_hsgp.predict(idata_hsgp, data=new_data, \nkind='pps', inplace=True)\n\nidata_hsgp\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                         (chain: 4, draw: 1000,\n                                     hsgp(X, m=10, c=1)_weights_dim: 10,\n                                     y_obs: 500)\nCoordinates:\n  * chain                           (chain) int64 0 1 2 3\n  * draw                            (draw) int64 0 1 2 3 4 ... 996 997 998 999\n  * hsgp(X, m=10, c=1)_weights_dim  (hsgp(X, m=10, c=1)_weights_dim) int64 0 ...\n  * y_obs                           (y_obs) int64 0 1 2 3 4 ... 496 497 498 499\nData variables:\n    hsgp(X, m=10, c=1)_weights_raw  (chain, draw, hsgp(X, m=10, c=1)_weights_dim) float64 ...\n    y_sigma                         (chain, draw) float64 20.86 19.86 ... 21.17\n    hsgp(X, m=10, c=1)_sigma        (chain, draw) float64 26.85 24.0 ... 18.06\n    hsgp(X, m=10, c=1)_ell          (chain, draw) float64 3.861 3.509 ... 3.76\n    hsgp(X, m=10, c=1)_weights      (chain, draw, hsgp(X, m=10, c=1)_weights_dim) float64 ...\n    y_mean                          (chain, draw, y_obs) float64 15.74 ... -4...\n    hsgp(X, m=10, c=1)              (chain, draw, y_obs) float64 15.74 ... -4...\nAttributes:\n    created_at:                  2024-05-27T06:01:00.623094\n    arviz_version:               0.17.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000hsgp(X, m=10, c=1)_weights_dim: 10y_obs: 500Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])hsgp(X, m=10, c=1)_weights_dim(hsgp(X, m=10, c=1)_weights_dim)int640 1 2 3 4 5 6 7 8 9array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])y_obs(y_obs)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (7)hsgp(X, m=10, c=1)_weights_raw(chain, draw, hsgp(X, m=10, c=1)_weights_dim)float64-1.452 -0.974 ... 0.9659 -0.2453array([[[-1.45186697, -0.97401433,  1.51626692, ...,  1.84709969,\n          0.92584892, -1.19616826],\n        [-1.73799921, -1.55920285,  1.42827017, ...,  1.78787921,\n         -1.14159607, -1.66042935],\n        [-1.3059213 , -1.22687096,  2.0214052 , ...,  2.80079646,\n          0.17462599, -0.06073935],\n        ...,\n        [-2.26781832, -1.21871881,  1.80249133, ...,  2.65551316,\n          0.30040289, -0.7085128 ],\n        [-2.23372877, -1.1832454 ,  1.74365597, ...,  2.71376469,\n          0.41123845, -0.60671544],\n        [-1.96117304, -1.13093178,  2.24600086, ...,  2.3750183 ,\n         -0.00901474, -0.75147244]],\n\n       [[-1.95999601, -1.27507311,  1.52467611, ...,  3.28813978,\n         -0.1598232 , -1.10988783],\n        [-1.59314189, -1.44864255,  1.95956808, ...,  2.01431167,\n          1.0199955 , -0.8962025 ],\n        [-1.8432561 , -1.32415496,  1.80874966, ...,  3.05464118,\n          0.81105708, -1.02872434],\n...\n        [-1.94277245, -1.5433581 ,  1.78771407, ...,  2.11464836,\n         -0.62470516, -1.01229181],\n        [-1.96747626, -1.39055532,  1.24775496, ...,  2.93940091,\n          0.46989555, -1.57735663],\n        [-2.20654554, -1.042289  ,  1.46402154, ...,  3.37106106,\n         -0.11061588, -2.15605928]],\n\n       [[-1.68378706, -0.84035677,  1.41475357, ...,  2.46320953,\n          0.88793346, -0.56144583],\n        [-1.66959934, -1.01014124,  1.6874097 , ...,  2.71030445,\n          0.68597924, -0.76198643],\n        [-1.54563186, -0.89279988,  1.77115866, ...,  2.3842957 ,\n          0.08392828, -0.78058783],\n        ...,\n        [-2.17148384, -1.59167361,  1.34960308, ...,  2.34802173,\n          0.44912308, -1.0698665 ],\n        [-1.31376138, -0.63355604,  1.97759132, ...,  1.8841587 ,\n         -0.13453184, -1.36605679],\n        [-2.0018976 , -1.36470426,  2.15352253, ...,  3.17100177,\n          0.96590611, -0.24527631]]])y_sigma(chain, draw)float6420.86 19.86 21.17 ... 20.12 21.17array([[20.85613959, 19.86298062, 21.16886643, ..., 19.48940449,\n        19.31511941, 19.44304302],\n       [18.5908299 , 21.71605063, 21.22255223, ..., 19.44547467,\n        19.21042952, 22.0099345 ],\n       [20.48425625, 21.17420065, 22.11318822, ..., 18.40825711,\n        21.65013089, 22.05150211],\n       [21.08143229, 20.7215663 , 22.60954219, ..., 21.15149371,\n        20.11622356, 21.16568101]])hsgp(X, m=10, c=1)_sigma(chain, draw)float6426.85 24.0 27.81 ... 26.09 18.06array([[26.84572148, 24.00472166, 27.80754218, ..., 23.34108117,\n        23.72941745, 22.60241785],\n       [22.01917097, 24.17709343, 26.16315265, ..., 22.03189338,\n        22.17588023, 24.00832607],\n       [24.12239909, 24.36393141, 25.79692715, ..., 24.33866729,\n        20.63877848, 22.26540398],\n       [25.41092492, 25.05093181, 28.3790263 , ..., 22.30157727,\n        26.0901197 , 18.06420147]])hsgp(X, m=10, c=1)_ell(chain, draw)float643.861 3.509 3.695 ... 3.679 3.76array([[3.86057038, 3.50871714, 3.69475981, ..., 3.20961859, 3.07237787,\n        2.99194079],\n       [4.21093385, 4.11858175, 3.55744155, ..., 3.54737759, 2.43665447,\n        3.67009957],\n       [3.38790729, 3.56906294, 3.07026931, ..., 3.17401967, 4.77493695,\n        4.1757221 ],\n       [3.73124202, 3.62463311, 4.06803959, ..., 3.06030257, 3.67928011,\n        3.75995625]])hsgp(X, m=10, c=1)_weights(chain, draw, hsgp(X, m=10, c=1)_weights_dim)float64-120.2 -78.54 ... 27.36 -5.933array([[[-120.19159663,  -78.54474016,  117.04015633, ...,\n           88.13161831,   38.07210878,  -41.65695052],\n        [-122.83658383, -107.83699596,   95.27680851, ...,\n           80.15736607,  -45.26655282,  -57.3944273 ],\n        [-109.63228325, -100.55013819,  159.16260856, ...,\n          141.94233647,    7.72311493,   -2.30700494],\n        ...,\n        [-149.23661164,  -78.75788623,  113.01499747, ...,\n          119.40081633,   12.18790241,  -25.62639515],\n        [-146.28304701,  -76.21158457,  109.23906639, ...,\n          125.36332205,   17.28993874,  -22.96014529],\n        [-120.75657952,  -68.54675039,  132.60289356, ...,\n          105.03375274,   -0.36461133,  -27.50691292]],\n\n       [[-138.76216692,  -87.49701007,   99.32062774, ...,\n          120.85182044,   -4.92170357,  -28.04716014],\n        [-122.53330814, -108.14123925,  139.17924148, ...,\n           82.74939574,   35.37871319,  -25.72818479],\n        [-142.94342858, -100.42482592,  132.17632445, ...,\n          148.36712045,   34.72137014,  -38.24384896],\n...\n        [-132.58698442, -103.47676242,  116.36868708, ...,\n           99.43921353,  -26.56716678,  -38.47612061],\n        [-138.61535955,  -94.11467693,   78.98484564, ...,\n           89.13997713,   11.35087487,  -29.54951483],\n        [-157.32953055,  -72.06983662,   96.1818003 , ...,\n          126.1515611 ,   -3.47852942,  -55.82170316]],\n\n       [[-129.78695588,  -63.20664967,  102.15003445, ...,\n          113.4760498 ,   35.60103971,  -19.27405014],\n        [-125.10213961,  -73.9587159 ,  118.87369923, ...,\n          124.94562328,   27.73901676,  -26.61384578],\n        [-138.71497899,  -77.82483202,  147.07295446, ...,\n          116.05385128,    3.46342745,  -26.78443203],\n        ...,\n        [-133.39280566,  -96.17651826,   79.33895456, ...,\n          102.02385007,   17.77408526,  -38.1416893 ],\n        [-103.26872735,  -48.62806342,  145.87733997, ...,\n           89.78661037,   -5.60101795,  -48.90554932],\n        [-110.1016679 ,  -73.21183754,  110.83494684, ...,\n          103.40789151,   27.35517734,   -5.93337047]]])y_mean(chain, draw, y_obs)float6415.74 15.74 15.71 ... -4.651 -4.606array([[[ 15.73717823,  15.74095015,  15.71442577, ...,  -4.67876513,\n          -4.20969038,  -3.71327448],\n        [-22.08564087, -21.76667383, -21.39772149, ...,   7.03009177,\n           7.12845194,   7.21535709],\n        [ 56.02093901,  56.10232981,  56.08787761, ...,   1.40193661,\n           1.10510341,   0.79052751],\n        ...,\n        [ 27.64916801,  27.65115973,  27.60142293, ...,   1.4750735 ,\n           1.59074503,   1.70839567],\n        [ 29.49505409,  29.44086994,  29.3291777 , ...,   3.89804528,\n           4.04221731,   4.18596665],\n        [ 24.70348988,  24.9191706 ,  25.10055256, ...,  -1.75167732,\n          -1.70197532,  -1.64814065]],\n\n       [[ 13.09479226,  13.08797832,  13.05446772, ...,  11.0508337 ,\n          11.0972292 ,  11.12522822],\n        [ 19.45360075,  19.35303968,  19.21116795, ..., -17.11306852,\n         -16.95029643, -16.75310125],\n        [ 39.40925935,  39.40667898,  39.33002811, ...,  -2.12983751,\n          -2.01625076,  -1.89843614],\n...\n        [-14.4322126 , -14.38059677, -14.30401941, ...,  11.55147703,\n          11.55940569,  11.5447161 ],\n        [  3.99064388,   3.88036701,   3.75439771, ...,   6.42065763,\n           6.71826106,   7.01812801],\n        [ 11.85726202,  12.01889355,  12.16180986, ...,  12.90196389,\n          13.3121751 ,  13.717131  ]],\n\n       [[ 27.47986715,  27.31475262,  27.09142621, ...,   7.10292311,\n           7.47525977,   7.85365034],\n        [ 34.24033269,  34.24902789,  34.19561616, ...,   5.51087296,\n           5.77912687,   6.04952041],\n        [ 31.73947173,  31.94613931,  32.10509971, ...,   9.39499668,\n           9.48056436,   9.55191877],\n        ...,\n        [  9.46004312,   9.35384251,   9.22108586, ...,   3.61236621,\n           3.94961437,   4.29454081],\n        [ 17.75725777,  18.27924573,  18.79439922, ...,   5.95699839,\n           6.24094799,   6.5246295 ],\n        [ 33.41206521,  33.28138834,  33.08465457, ...,  -4.68577545,\n          -4.65087068,  -4.60586526]]])hsgp(X, m=10, c=1)(chain, draw, y_obs)float6415.74 15.74 15.71 ... -4.651 -4.606array([[[ 15.73717823,  15.74095015,  15.71442577, ...,  -4.67876513,\n          -4.20969038,  -3.71327448],\n        [-22.08564087, -21.76667383, -21.39772149, ...,   7.03009177,\n           7.12845194,   7.21535709],\n        [ 56.02093901,  56.10232981,  56.08787761, ...,   1.40193661,\n           1.10510341,   0.79052751],\n        ...,\n        [ 27.64916801,  27.65115973,  27.60142293, ...,   1.4750735 ,\n           1.59074503,   1.70839567],\n        [ 29.49505409,  29.44086994,  29.3291777 , ...,   3.89804528,\n           4.04221731,   4.18596665],\n        [ 24.70348988,  24.9191706 ,  25.10055256, ...,  -1.75167732,\n          -1.70197532,  -1.64814065]],\n\n       [[ 13.09479226,  13.08797832,  13.05446772, ...,  11.0508337 ,\n          11.0972292 ,  11.12522822],\n        [ 19.45360075,  19.35303968,  19.21116795, ..., -17.11306852,\n         -16.95029643, -16.75310125],\n        [ 39.40925935,  39.40667898,  39.33002811, ...,  -2.12983751,\n          -2.01625076,  -1.89843614],\n...\n        [-14.4322126 , -14.38059677, -14.30401941, ...,  11.55147703,\n          11.55940569,  11.5447161 ],\n        [  3.99064388,   3.88036701,   3.75439771, ...,   6.42065763,\n           6.71826106,   7.01812801],\n        [ 11.85726202,  12.01889355,  12.16180986, ...,  12.90196389,\n          13.3121751 ,  13.717131  ]],\n\n       [[ 27.47986715,  27.31475262,  27.09142621, ...,   7.10292311,\n           7.47525977,   7.85365034],\n        [ 34.24033269,  34.24902789,  34.19561616, ...,   5.51087296,\n           5.77912687,   6.04952041],\n        [ 31.73947173,  31.94613931,  32.10509971, ...,   9.39499668,\n           9.48056436,   9.55191877],\n        ...,\n        [  9.46004312,   9.35384251,   9.22108586, ...,   3.61236621,\n           3.94961437,   4.29454081],\n        [ 17.75725777,  18.27924573,  18.79439922, ...,   5.95699839,\n           6.24094799,   6.5246295 ],\n        [ 33.41206521,  33.28138834,  33.08465457, ...,  -4.68577545,\n          -4.65087068,  -4.60586526]]])Indexes: (4)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))hsgp(X, m=10, c=1)_weights_dimPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64', name='hsgp(X, m=10, c=1)_weights_dim'))y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='y_obs', length=500))Attributes: (4)created_at :2024-05-27T06:01:00.623094arviz_version :0.17.0modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 4, draw: 1000, y_obs: 500)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * y_obs    (y_obs) int64 0 1 2 3 4 5 6 7 8 ... 492 493 494 495 496 497 498 499\nData variables:\n    y        (chain, draw, y_obs) float64 76.04 21.77 64.12 ... -41.12 -26.02\nAttributes:\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000y_obs: 500Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])y_obs(y_obs)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (1)y(chain, draw, y_obs)float6476.04 21.77 64.12 ... -41.12 -26.02array([[[ 7.60446220e+01,  2.17746429e+01,  6.41213079e+01, ...,\n          2.88184766e+01,  5.35607407e+00,  8.28161473e+00],\n        [-3.71916430e+01, -1.02105981e+01, -3.75556355e+01, ...,\n          6.13483186e+00,  5.17152558e+00, -7.14534907e+00],\n        [ 4.18037444e+01,  8.94677926e+01,  6.01499750e+01, ...,\n          5.15362586e+00,  1.50220973e+01,  5.10623210e+00],\n        ...,\n        [ 1.17455216e+01,  7.18088614e+00,  2.27295340e+01, ...,\n         -1.40562367e+01, -1.55645500e+01, -1.59305317e+01],\n        [ 4.57996940e+01,  4.09450669e+01,  6.17203837e+01, ...,\n          5.43347241e+00,  2.16723522e+01, -1.34623303e+01],\n        [-1.99298911e+01, -7.31053097e+00,  2.33680338e+01, ...,\n         -1.86693562e+01, -3.45100068e+01, -4.37085484e-02]],\n\n       [[ 1.68744925e+01,  4.86269717e+01, -3.54535158e-01, ...,\n          6.97092074e+00,  1.38401427e+01,  4.41897478e+01],\n        [ 3.80673611e+01,  5.08088452e+01,  3.24135895e+01, ...,\n         -1.64036397e+00,  9.20252673e-01, -2.28191503e+01],\n        [ 2.45419863e+01,  3.67156591e+01,  8.06594166e+01, ...,\n          2.35165419e+01, -2.27769611e+00,  3.19786177e+01],\n...\n        [ 7.65492507e+00, -2.52651612e+01, -1.14319746e+01, ...,\n          2.10783876e+00,  6.51461474e+01,  1.39449220e+01],\n        [-1.52884615e+01, -3.81858568e+01,  3.83976350e+01, ...,\n          6.70649060e+00, -4.19015833e+00,  3.10102382e+01],\n        [ 3.22122816e+01,  1.40541501e-01,  1.39868866e+01, ...,\n          1.39995757e+00,  1.40413480e+01,  2.15327900e+01]],\n\n       [[ 6.45006934e+01,  1.89982303e+01,  4.92943955e+01, ...,\n          4.48989355e+01,  6.00626370e+00, -4.11335459e+00],\n        [ 7.69567142e+01,  3.85217722e+01,  6.00470265e+01, ...,\n         -2.85540970e+01,  1.17829549e+00,  2.88735960e+01],\n        [ 3.82736929e+01,  1.35754696e+01,  3.18636391e+01, ...,\n          1.30573340e+01,  2.58695675e+01,  3.63775721e+00],\n        ...,\n        [ 2.64333429e+01,  2.96956055e+01,  9.65696351e+00, ...,\n         -8.66072179e+00, -1.22879325e+00, -1.74010423e+01],\n        [-3.67585411e+01, -3.45711874e+00,  1.22087765e+01, ...,\n          2.78805963e+01, -2.67630198e+01, -1.17549578e+01],\n        [ 6.74946544e+01,  2.73016469e+01,  1.89342532e+01, ...,\n         -1.80490133e+01, -4.11156817e+01, -2.60158793e+01]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='y_obs', length=500))Attributes: (2)modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (chain: 4, draw: 1000, y_obs: 133)\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 8 ... 992 993 994 995 996 997 998 999\n  * y_obs    (y_obs) int64 0 1 2 3 4 5 6 7 8 ... 125 126 127 128 129 130 131 132\nData variables:\n    y        (chain, draw, y_obs) float64 -3.957 -3.957 -3.957 ... -4.013 -4.099\nAttributes:\n    created_at:                  2024-05-27T06:01:00.626582\n    arviz_version:               0.17.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000y_obs: 133Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])y_obs(y_obs)int640 1 2 3 4 5 ... 128 129 130 131 132array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132])Data variables: (1)y(chain, draw, y_obs)float64-3.957 -3.957 ... -4.013 -4.099array([[[-3.95662204, -3.95668036, -3.95697947, ..., -3.96786393,\n         -4.06567925, -4.08819109],\n        [-3.92804385, -3.95248402, -4.00438808, ..., -4.19279911,\n         -3.90894979, -4.05289002],\n        [-4.11236339, -4.08418887, -3.99366348, ..., -4.1948162 ,\n         -3.97180293, -4.09921461],\n        ...,\n        [-3.88915792, -3.88893229, -3.89724674, ..., -4.04248536,\n         -3.89847518, -4.03951892],\n        [-3.88161419, -3.88254965, -3.9052254 , ..., -4.08668059,\n         -3.88211318, -4.03326819],\n        [-4.08257196, -4.09265666, -4.04648571, ..., -3.99108035,\n         -3.91131912, -4.03785686]],\n\n       [[-3.85025382, -3.84763163, -3.85608195, ..., -4.34165517,\n         -3.86684608, -4.00723736],\n        [-4.0090095 , -4.00707108, -4.017144  , ..., -4.01175855,\n         -4.25268066, -4.11837821],\n        [-3.97404168, -3.97456237, -4.00764182, ..., -4.0433394 ,\n         -4.00511976, -4.10110199],\n...\n        [-3.85025004, -3.83654156, -3.832685  , ..., -4.37637177,\n         -3.86484545, -4.00066997],\n        [-4.06709934, -4.05434047, -4.05493257, ..., -4.17570336,\n         -3.99476606, -4.11607839],\n        [-4.01346366, -4.01406212, -4.01257167, ..., -4.30726434,\n         -4.01839009, -4.13004222]],\n\n       [[-3.98552821, -3.98561292, -4.0079376 , ..., -4.15815157,\n         -3.96833515, -4.09613757],\n        [-3.96540632, -3.96291088, -3.95021137, ..., -4.13476712,\n         -3.95195981, -4.08343265],\n        [-4.17463712, -4.17201421, -4.11695573, ..., -4.3197009 ,\n         -4.04463544, -4.14929407],\n        ...,\n        [-4.0692212 , -4.06241625, -4.09034963, ..., -4.09469991,\n         -3.98159516, -4.09860353],\n        [-4.68482086, -4.73793167, -4.72387693, ..., -4.11913623,\n         -3.92229916, -4.06192875],\n        [-3.97170872, -3.97200671, -3.98370832, ..., -4.02417539,\n         -4.01254991, -4.09910258]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       123, 124, 125, 126, 127, 128, 129, 130, 131, 132],\n      dtype='int64', name='y_obs', length=133))Attributes: (4)created_at :2024-05-27T06:01:00.626582arviz_version :0.17.0modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:          (chain: 4, draw: 1000)\nCoordinates:\n  * chain            (chain) int64 0 1 2 3\n  * draw             (draw) int64 0 1 2 3 4 5 6 ... 993 994 995 996 997 998 999\nData variables:\n    acceptance_rate  (chain, draw) float64 0.9484 0.989 0.9772 ... 0.9905 0.9283\n    step_size        (chain, draw) float64 0.1631 0.1631 ... 0.1827 0.1827\n    diverging        (chain, draw) bool False False False ... False False False\n    energy           (chain, draw) float64 671.4 665.4 671.0 ... 670.8 674.0\n    n_steps          (chain, draw) int64 31 31 31 15 15 31 ... 15 31 31 15 15 31\n    tree_depth       (chain, draw) int64 5 5 5 4 4 5 5 4 4 ... 4 5 5 4 5 5 4 4 5\n    lp               (chain, draw) float64 661.7 661.3 665.7 ... 665.6 668.8\nAttributes:\n    created_at:                  2024-05-27T06:01:00.625568\n    arviz_version:               0.17.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (7)acceptance_rate(chain, draw)float640.9484 0.989 ... 0.9905 0.9283array([[0.94837217, 0.98900712, 0.97718268, ..., 0.99599719, 0.97974622,\n        0.93727528],\n       [0.9824305 , 0.87426419, 0.99008681, ..., 0.99942845, 0.97008375,\n        0.99555447],\n       [0.99910745, 0.98130743, 0.95550076, ..., 0.97069333, 0.9342073 ,\n        0.95033605],\n       [0.97363799, 0.97895034, 0.93808177, ..., 0.9514047 , 0.99054107,\n        0.92830969]])step_size(chain, draw)float640.1631 0.1631 ... 0.1827 0.1827array([[0.16306409, 0.16306409, 0.16306409, ..., 0.16306409, 0.16306409,\n        0.16306409],\n       [0.15671706, 0.15671706, 0.15671706, ..., 0.15671706, 0.15671706,\n        0.15671706],\n       [0.15280013, 0.15280013, 0.15280013, ..., 0.15280013, 0.15280013,\n        0.15280013],\n       [0.1826574 , 0.1826574 , 0.1826574 , ..., 0.1826574 , 0.1826574 ,\n        0.1826574 ]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])energy(chain, draw)float64671.4 665.4 671.0 ... 670.8 674.0array([[671.39512608, 665.35858794, 670.96295141, ..., 670.72923684,\n        668.42111758, 669.24601286],\n       [665.04518693, 671.40450254, 672.47055359, ..., 665.8504822 ,\n        674.75907256, 672.99992931],\n       [672.00375668, 668.16520377, 666.3166769 , ..., 671.79404347,\n        671.22701287, 671.74308362],\n       [669.46327702, 671.11966704, 667.28825365, ..., 665.11423361,\n        670.79366043, 673.9890605 ]])n_steps(chain, draw)int6431 31 31 15 15 ... 31 31 15 15 31array([[31, 31, 31, ..., 31, 15, 15],\n       [15, 31, 31, ..., 31, 31, 31],\n       [31, 31, 15, ..., 15, 31, 15],\n       [31, 15, 15, ..., 15, 15, 31]])tree_depth(chain, draw)int645 5 5 4 4 5 5 4 ... 5 5 4 5 5 4 4 5array([[5, 5, 5, ..., 5, 4, 4],\n       [4, 5, 5, ..., 5, 5, 5],\n       [5, 5, 4, ..., 4, 5, 4],\n       [5, 4, 4, ..., 4, 4, 5]])lp(chain, draw)float64661.7 661.3 665.7 ... 665.6 668.8array([[661.70967519, 661.2875458 , 665.70729556, ..., 661.40453042,\n        661.53660449, 661.51768248],\n       [662.47186408, 665.25230302, 666.28391286, ..., 662.10849958,\n        668.1822538 , 664.18500466],\n       [662.60487008, 660.69369885, 662.99303071, ..., 663.00720061,\n        664.60205279, 667.39059102],\n       [664.1010232 , 660.27378818, 664.08879552, ..., 661.26338997,\n        665.58839085, 668.77321037]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (4)created_at :2024-05-27T06:01:00.625568arviz_version :0.17.0modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:  (y_obs: 133)\nCoordinates:\n  * y_obs    (y_obs) int64 0 1 2 3 4 5 6 7 8 ... 125 126 127 128 129 130 131 132\nData variables:\n    y        (y_obs) float64 0.0 -1.3 -2.7 0.0 -2.7 ... -2.7 10.7 -2.7 10.7\nAttributes:\n    created_at:                  2024-05-27T06:01:00.626851\n    arviz_version:               0.17.0\n    inference_library:           numpyro\n    inference_library_version:   0.13.2\n    sampling_time:               1.870798\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0xarray.DatasetDimensions:y_obs: 133Coordinates: (1)y_obs(y_obs)int640 1 2 3 4 5 ... 128 129 130 131 132array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132])Data variables: (1)y(y_obs)float640.0 -1.3 -2.7 ... 10.7 -2.7 10.7array([   0. ,   -1.3,   -2.7,    0. ,   -2.7,   -2.7,   -2.7,   -1.3,\n         -2.7,   -2.7,   -1.3,   -2.7,   -2.7,   -2.7,   -5.4,   -2.7,\n         -5.4,    0. ,   -2.7,   -2.7,    0. ,  -13.3,   -5.4,   -5.4,\n         -9.3,  -16. ,  -22.8,   -2.7,  -22.8,  -32.1,  -53.5,  -54.9,\n        -40.2,  -21.5,  -21.5,  -50.8,  -42.9,  -26.8,  -21.5,  -50.8,\n        -61.7,   -5.4,  -80.4,  -59. ,  -71. ,  -91.1,  -77.7,  -37.5,\n        -85.6, -123.1, -101.9,  -99.1, -104.4, -112.5,  -50.8, -123.1,\n        -85.6,  -72.3, -127.2, -123.1, -117.9, -134. , -101.9, -108.4,\n       -123.1, -123.1, -128.5, -112.5,  -95.1,  -81.8,  -53.5,  -64.4,\n        -57.6,  -72.3,  -44.3,  -26.8,   -5.4, -107.1,  -21.5,  -65.6,\n        -16. ,  -45.6,  -24.2,    9.5,    4. ,   12. ,  -21.5,   37.5,\n         46.9,  -17.4,   36.2,   75. ,    8.1,   54.9,   48.2,   46.9,\n         16. ,   45.6,    1.3,   75. ,  -16. ,  -54.9,   69.6,   34.8,\n         32.1,  -37.5,   22.8,   46.9,   10.7,    5.4,   -1.3,  -21.5,\n        -13.3,   30.8,  -10.7,   29.4,    0. ,  -10.7,   14.7,   -1.3,\n          0. ,   10.7,   10.7,  -26.8,  -14.7,  -13.3,    0. ,   10.7,\n        -14.7,   -2.7,   10.7,   -2.7,   10.7])Indexes: (1)y_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       123, 124, 125, 126, 127, 128, 129, 130, 131, 132],\n      dtype='int64', name='y_obs', length=133))Attributes: (7)created_at :2024-05-27T06:01:00.626851arviz_version :0.17.0inference_library :numpyroinference_library_version :0.13.2sampling_time :1.870798modeling_interface :bambimodeling_interface_version :0.13.0\n                      \n                  \n            \n            \n              \n            \n            \n\n\nand plot the model fit to see if it can recover the observed data.\n\nax = az.plot_hdi(new_data['X'], idata_hsgp['posterior_predictive']['y'], fill_kwargs={'alpha': 0.2, 'color':'firebrick'}, figsize=(9, 8))\n\naz.plot_hdi(new_data['X'], idata_hsgp['posterior_predictive']['y'], fill_kwargs={'alpha': 0.8, 'color':'firebrick'}, hdi_prob=0.5)\n\ny_mean = idata_hsgp['posterior_predictive']['y'].mean(dim=('chain', 'draw'))\n\nax.plot(new_data['X'], y_mean, label='Expected posterior predictive', color='k')\n\nax.scatter(df['X'], df['y'], label='Observed Datapoints')\n\nax.legend()\n\nax.set_title(\"Posterior Predictive Distribution \\n Based on HSGP approximation\");\n\n\n\n\nHere again we’ve allowed the model to extrapolate beyond the observed data but we achieve far more reasonable extrapolation than with the Spline model. And we can compare other performance metrics versus the spline models to see that by the aggregate performance measures our HSGP model seems to come out on top too.\n\nmodels_dict = {\"piecewise_constant\": idata_spline1, \"piecewise_linear\": idata_spline2, \"cubic_bspline\": idata_spline3, \"cubic_bspline_10\": idata_spline4, \n\"cubic_bspline_15\": idata_spline5, 'hsgp': idata_hsgp}\ndf_compare = az.compare(models_dict)\ndf_compare\n\n\n\n\n\n  \n    \n      \n      rank\n      elpd_loo\n      p_loo\n      elpd_diff\n      weight\n      se\n      dse\n      warning\n      scale\n    \n  \n  \n    \n      hsgp\n      0\n      -608.819807\n      9.958864\n      0.000000\n      0.872617\n      12.009745\n      0.000000\n      False\n      log\n    \n    \n      cubic_bspline_10\n      1\n      -612.578311\n      11.652463\n      3.758504\n      0.000000\n      9.646544\n      3.125929\n      True\n      log\n    \n    \n      cubic_bspline_15\n      2\n      -620.709337\n      19.669201\n      11.889531\n      0.000000\n      9.600116\n      3.836813\n      True\n      log\n    \n    \n      cubic_bspline\n      3\n      -634.647180\n      8.703519\n      25.827373\n      0.000000\n      8.915728\n      7.666315\n      True\n      log\n    \n    \n      piecewise_constant\n      4\n      -643.781042\n      6.981098\n      34.961235\n      0.069309\n      9.770740\n      9.490854\n      False\n      log\n    \n    \n      piecewise_linear\n      5\n      -647.016885\n      5.987587\n      38.197078\n      0.058073\n      7.914787\n      9.204140\n      False\n      log\n    \n  \n\n\n\n\nFor a deeper dive on HSGP you might consult Juan Orduz’s work here"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#industry-questions-are-causal",
    "href": "talks/missing_data/missing_data_causal.html#industry-questions-are-causal",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Industry Questions are Causal",
    "text": "Industry Questions are Causal\nOn the Importance of Theory Construction\n\n\n\n\n\nID\n\\(Y_{i}(0)\\)\n\\(Y_{i}(1)\\)\n\n\n\n\n1\n?\n1\n\n\n2\n1\n?\n\n\n3\n?\n0\n\n\n4\n?\n1\n\n\n5\n0\n?\n\n\n\nThe Fundamental problem of Causal Inference as Missing Data\n\n\n\nThe heart of causal inference is understanding the risk of confounding influence.\n\n\n\n\nNaively optimising for some in-sample predictive benchmark does not protect your model from confounding bias.\n\n\n\n\nCausal models with deliberate and careful construction of the dependence mechanism are your best hope for genuine insight and robust predictive performance\n\n\n\n\nThis is crucial for model explainability in the human-centric domains, where the decisions need to be justifiable.\n\n\n\n\nUsed to answer Counterfactuals\n\nHow do patterns of Employee empowerment change if we can intervene on their relationship with their manager?"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#hierarchies-and-human-relations",
    "href": "talks/missing_data/missing_data_causal.html#hierarchies-and-human-relations",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Hierarchies and Human Relations",
    "text": "Hierarchies and Human Relations\nPower Structures Interactions\n\n\n\n\n\nMilitary Hierarchy\n\n\n\n\n\n\nCapitalism"
  },
  {
    "objectID": "talks/missing_data/missing_data_causal.html#recap-and-conclusion",
    "href": "talks/missing_data/missing_data_causal.html#recap-and-conclusion",
    "title": "Missing Data and Bayesian Imputation with PyMC",
    "section": "Recap and Conclusion",
    "text": "Recap and Conclusion\nWe’ve seen the application of missing data analysis to survey data in the context of People Analytics.\n\nMultivariate approaches are effective but cannot help address confounding bias,\nThe flexibility of the Bayesian approach can be tailored to the appropriate complexity of our theory about why our data is missing.\nHierarchical structures pervade business - conduits for leadership influence/communication channels. Hierarchical modelling can isolate estimates of this impact and control for biases of naive aggregates.\n\nReveal inefficiencies and mismatches between team and management.\n\nImputation gives “voice” to the missing. Inverse Propensity weighting corrects mis-representative samples. Both are correctives for selection effect bias.\n\n\n\n\nMissing Data with PyMC"
  },
  {
    "objectID": "notes/certain_things/Wedding/Speech.html",
    "href": "notes/certain_things/Wedding/Speech.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "You will have to forgive me, but I’m going to try and say something about love.\nAbout loving Joanne.\nMany of you probably love her too, but I suspect we got to the same destination by different routes.\nLoving Joanne has crystallised a few things for me. Cleared up some utterly naive confusions of mine. I don’t think these confusions were mine alone.\nLove is an overloaded concept. Obscured by layers of cliche. Efforts at defining the thing tend to be premature. Good definitions are battle tested, cover edge cases and disbar near imposters. I’m not going to offer you a definition. But I want to say why it’s important to reckon with the demands of love.\n\nShow , don’t tell.\nI left Joanne in 2011.\nToo dramatic!? More realistically, I fumbled, flailed and tried to impress Joanne in 2010.\nThere is a clarity that comes with absence. Travelling reveals this. (Prison too I imagine.) When you travel, you know what you miss at home. Sure, there is a mundane sense of loss, you don’t know where the teabags are, or how to navigate public transport… but the profound, deep loss is the absence of her. When you find someone whose absence is a glaring abyss in every room, you’ve found a good one.\nThere were other signs too though… Less apocalyptic signs. Hints, that maybe we were a good fit. Hints that we’d complement one another. Hints that she’d put up with me. That last one being about as rare a trait as you’d guess.\nI first emailed Joanne about the usefulness of baby wipes in 2014. I could not over-emphasise how central they were for my cleanliness while traveling in Mongolia. In a follow up series of densely overwritten emails about baked beans and desert landscapes, I wooed this woman. That is a statement as much about her character as it is about mine.\nWe have been together now for more than 10 years. We bought a house, we had a son. We lived through a pandemic. In sickness and health, good humour and fierce tempers. We have shared more than I imagined possible at the beginning. There was no good map, no rote formula to trace out this particular trajectory.\nThat was a problem for me. By temperament I’m the type of person who will use Wikipedia to spoil the ending of a thriller, if I think I’ve guessed where the plot is going. And I had some inclination that we were trending towards “Happily ever after.” The urge to extrapolate is overwhelming.\n‐————–\nI had thought we were on a date.\nI had such grand plans.\nJoanne and I were in the Grand Social opposite the Haypenny bridge. Things were going really well! We were somewhere between horror tropes and the third pint of Guinness, ascending the heights of some tangent. I had thought it was a date, but, ladies and gentlemen, it was not a date.\nMy grand plans were gently but decisively rebuffed.\nWe did eventually go on a date. A few years later we even got engaged. Now 6 years after that, 1 child and 1 pandemic later we’re finally getting married.\nThis should give you a sense of how my plans tend to go.\nThere is a resonant trope in detective fiction - the hapless hero uncovers clue after clue, building an elaborate trap for the villain. Of course things are not as they seem. Soon the villain reveals how she was pulling the strings all along…\nI would like to believe this kind of thing happens. I think just for the narrative neatness of it all.\n\nThis was supposed to be short speech. It’s a short-ish speech.\nThe best I could manage with so many things to encompass. Joanne and I have built a life together and this feels less like a beginning and more like a celebration of that fact. But with a life, a whole of mess details that seem urgent and important to touch on. For one, there are too many of you, too many people to thank. Too many variations on the theme of gratitude. But briefly, my Mam and Margaret have been pivotal over the last few years as we became parents. We would be hollow wrecks of people were it not for their support. This is generally true of all of you, we would be worse without you. Whether you put manners on us, or just picked us up after a hard day we owe you a debt. A debt we will never seek to pay, if only to keep you knocking on our door.\nThis wedding was supposed to happen years ago. Instead we had a pandemic and a baby.\nCurse and blessing. This is just our most egregious example of how Gods laugh when men make plans. We have been engaged a long time. This day seems both overdue and hardwon. And there is no one i’d rather have at my side, than Joanne as we shake our fist at the heavens and the dare the Gods to try their worst. Let the frogs rain, we bought umbrellas years ago!\nIn 2014 we met again. Our reunion was supposed to be a date. I thought it was a date.\nJoanne and I were in the Grand Social opposite the Haypenny bridge. We were somewhere between horror tropes and the third pint of Guinness, ascending the heights of some tangent. Things were going welI. I had plans. These plans were gently rebuffed. It was decisively not a date.\nJoanne and I originally met at some point in UCD. For those of you who don’t know me, it’s relevant to know I studied philosophy and would be described kindly as arrogant. At the time I was enamoured with two things - Joanne Stanley and the idea that the world could be carved up into sensible statements and nonsensical ones. This view boiled down to the idea that wherever something appeared unintelligible it was meaningless.\nThis is, I’m convinced today, the symptom of a profound psychopathology and moral cowardice. For it means that you cannot speak meaningfully of love. You cannot cleanly encompass the sentiment in necessary and sufficient conditions. You cannot fence off the feeling into schema\n….\nSo I’d invite you to raise a glass to the idea of inadequate maps"
  },
  {
    "objectID": "oss/causalpy/inv_prop_weights.html",
    "href": "oss/causalpy/inv_prop_weights.html",
    "title": "Inverse Propensity Score Weighting in CausalPy",
    "section": "",
    "text": "Inverse Propensity Score Weighting\nIn this project I sought to add the functionality for bayesian inverse propensity weighting functionality to the CausalPy package. I adapted earlier work with PyMC to contribute the base classes to the package and demonstrated how these classes can be used to esitmate various treatment effect estimands. The demonstration can be seen here"
  },
  {
    "objectID": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#bayesian-penalised-splines-smoothing-priors",
    "href": "posts/post-with-code/GAMs_and_GPs/gams_and_gps.html#bayesian-penalised-splines-smoothing-priors",
    "title": "GAMs and GPs: Flexibility and Calibration",
    "section": "Bayesian Penalised Splines (Smoothing Priors)",
    "text": "Bayesian Penalised Splines (Smoothing Priors)\nPriors can be used in the same way to that regularisation techniques are deployed in machine learning. They serve as a kind of extra model parameter used to effect a variable selection routine. In the case of splines we are creating these synthetic features and this can be seen as a generating a feature selection problem. So far we have discussed priors as a means of imposing structure based on our knowledge of the problem but we can also use priors to automate feature selection over spline features.\nHere we fit a two spline models to the speed-test data set but we use 60 knot points to generate our spline features. This is wildy excessive for the data set in question. We show how to use non-centered parameterisation with a “smoothing” trick to induce a good fit despite excessive features. In this way we’re removing the burden of manually specifying the spline structure. This is akin to penalisation methods used in PyGam but with a Bayesian flavour.\n\nmax_dev= len(y)\nnum_knots=60\nknot_list = np.linspace(0, np.max(X), num_knots)[2:-2]\ndev_periods = np.linspace(0, np.max(X), len(y))\n\nBx = dmatrix(\n    \"bs_patsy(dev_periods, knots=knots, degree=3, include_intercept=False)\",\n    {\"dev_periods\": dev_periods, \"knots\": knot_list},\n)\nBx\n\ndef penalised_model(Bx, penalised=True):\n    with pm.Model() as model:\n        basis = pm.MutableData('basis', np.asfortranarray(Bx))\n\n        if penalised:\n            sigma_a = pm.HalfCauchy('sigma_a', 5.)\n            a0 = pm.Normal('a0', 0., 10.)\n            delta_a = pm.Normal('delta_a', 0., 1.,shape=num_knots)\n            a = pm.Deterministic('a', a0 + (sigma_a * delta_a).cumsum())\n        else: \n            b = pm.Normal('delta_a', 0., 25,\n            shape=num_knots)\n            a0 = pm.Normal('a0', 0, 10)\n            a = pm.Deterministic('a', a0 + b)\n        \n        sigma = pm.HalfCauchy('sigma', 5.)\n        obs = pm.Normal('obs', pm.math.dot(basis, a), sigma, observed=y)\n        idata = pm.sample(target_accept=.95, idata_kwargs={\"log_likelihood\": True})\n    \n    return model, idata\n\nmodel_raw, idata_raw = penalised_model(Bx, penalised=False)\nmodel_penalised, idata_penalised = penalised_model(Bx, penalised=True)\n\nThe model structure is as follows:\n\npm.model_to_graphviz(model_penalised)\n\n\n\n\nThis parameterisation trick is owed to work by Austin Rochford and Adrian Seyboldt discussed here. We can see how the two parameterisations induce quite distinct values on the coefficients for splines.\n\nfig, ax = plt.subplots(figsize=(9, 20))\naz.plot_forest([idata_raw, idata_penalised], var_names=['a'], combined=True, ax=ax, model_names=['raw', 'penalised'])\nax.fill_betweenx(range(160), -25, 25, alpha=0.2, color='red');\n\n\n\n\nThis results in much different posterior predictive fits. The un-smoothed spline fit overfits to the small partitions of the spline features, whereas the smoothing spline model achieves a reasonable fit recovering the smooth structure of the data generating process.\n\nB_df = pd.DataFrame(Bx)\nweights = az.summary(idata_penalised, var_names='a')['mean']\nfig, ax = plt.subplots(figsize=(8, 7))\nprocessed = np.array([np.dot(B_df, az.extract(idata_penalised['posterior']['a'])['a'].to_numpy()[:, i]) for i in range(4000)])\naz.plot_hdi(range(len(y)), processed, ax=ax,  fill_kwargs={'alpha': 0.4, 'color':'firebrick'})\naz.plot_hdi(range(len(y)), processed, ax=ax,  fill_kwargs={'alpha': 0.8, 'color':'firebrick'}, hdi_prob=.50)\nweights_raw = az.summary(idata_raw, var_names='a')['mean']\nax.plot(range(len(y)), np.dot(B_df, weights), color='black', label='Cubic_Bsplines_60_Penalised')\nax.plot(range(len(y)), np.dot(B_df, weights_raw), color='blue', linestyle='--', label='Cubic_Bsplines_60_unpenalised')\nax.set_title(\"Bayesian Automatic Regularisation \\n Smoothing Priors over 60 Knots\")\nax.legend();\n\n\n\n\nThis is reflected in the model performance measures as well.\n\nmodels_dict = {\"piecewise_constant\": idata_spline1, \"piecewise_linear\": idata_spline2, \"cubic_bspline\": idata_spline3, \"cubic_bspline_10\": idata_spline4, \n\"cubic_bspline_20\": idata_spline5, 'hsgp': idata_hsgp, 'spline_penalised_bayes': idata_penalised, 'cubic_bspline_60': idata_raw}\ndf_compare = az.compare(models_dict)\ndf_compare\n\n\n\n\n\n  \n    \n      \n      rank\n      elpd_loo\n      p_loo\n      elpd_diff\n      weight\n      se\n      dse\n      warning\n      scale\n    \n  \n  \n    \n      hsgp\n      0\n      -608.819807\n      9.958864\n      0.000000\n      0.764385\n      12.009745\n      0.000000\n      False\n      log\n    \n    \n      spline_penalised_bayes\n      1\n      -610.258217\n      19.573938\n      1.438411\n      0.115567\n      11.444307\n      2.822797\n      True\n      log\n    \n    \n      cubic_bspline_10\n      2\n      -612.578311\n      11.652463\n      3.758504\n      0.000000\n      9.646544\n      3.125929\n      True\n      log\n    \n    \n      cubic_bspline_20\n      3\n      -620.709337\n      19.669201\n      11.889531\n      0.000000\n      9.600116\n      3.836813\n      True\n      log\n    \n    \n      cubic_bspline\n      4\n      -634.647180\n      8.703519\n      25.827373\n      0.000000\n      8.915728\n      7.666315\n      True\n      log\n    \n    \n      cubic_bspline_60\n      5\n      -642.070674\n      24.951194\n      33.250867\n      0.000667\n      9.135971\n      8.313174\n      False\n      log\n    \n    \n      piecewise_constant\n      6\n      -643.781042\n      6.981098\n      34.961235\n      0.061876\n      9.770740\n      9.490854\n      False\n      log\n    \n    \n      piecewise_linear\n      7\n      -647.016885\n      5.987587\n      38.197078\n      0.057506\n      7.914787\n      9.204140\n      False\n      log\n    \n  \n\n\n\n\nThis functionality of Bayesian priors is not unique to spline models. We can mirror Lasso regression models with double-exponential prior specifications. Ridge regrssion is provably a species of regression models with Normal priors pulled toward 0. Other alternatives such horse-shoe and slab and spike specifications offer a rich range of bayesian approaches to variable selection methods with sparse data.\n\nRecap\nSo far we’ve seen how we can use splines and gauassian processes to model highly eccentric functional relationships where the function could be approximated with univariate smoothing routine. These are two distinct abstractions which seem adequately fit to the world, but demand very different ways of thinking about the underlying reality. This is no fundamental contradiction in so far as the world admits many descriptions.\n\n[K]nowledge develops in a multiplicity of theories, each with it’s limited utility … These theories overlap very considerable in their logical laws and in much else, but that they add up to an integrated and consistent whole is only a worthy ideal and not a pre-requistite of scientific progress … Let reconciliations proceed. - W.V.O Quine in Word and Object\n\nAnother observation in a similar vein is that penalised spline models are provably equivalent to hierarchical regression (random effects) models. This is striking because the character of these types of model seems diametrically opposed. With spline models you jerry-rig your features to an optimisation goal, with hierarchical model you tend to impose theoretical structure to induce shrinkage. It’s hard to see how this would work? With a penalised spline model are you inducing a hierarchy of latent features you can’t name? Should you even try to translate between the two!? How can we leverage our prior knowledge of the data generating process to add structure to spline model predictions?\nThe abstract components of our model are less graspable than their predictive performance yet the qualitative character of theory buiding differs markedly between these abstractions and aesthetics and explainability feature heavily in model preference. Next we’ll add another layer to our abstractions and show how to use hierarchical modelling over spline fits can be used to extract insight into the data generating process over a family of curves. In particular we’ll focus on the development of insurance loss curves and how to constrain our predictions within reasonable bounds. This is an apt problem for spline modelling in that the process is repeatable annually, but the domain of the support i.e. development in time is fixed year-on-year so the extrapolation problem is muted."
  },
  {
    "objectID": "oss/causalpy/justifying_instruments.html",
    "href": "oss/causalpy/justifying_instruments.html",
    "title": "Justifying Instruments in CausalPy",
    "section": "",
    "text": "The Strength of Instruments\nIn this project I sought to outline the process of justifying instrumental variable regressions in CausalPy using the Bayesian modelling workflow. We emphasised the role of exploratory data analysis and plotting in how we come to understand the causal dynamics at play in our data generating process. The focus was on the manner of justification and role of comparative assessment in credible inference. The demonstration can be seen here.\n\n\n\nComparison with OLS"
  },
  {
    "objectID": "posts/post-with-code/multilevel_confounding/multilevel_models.html",
    "href": "posts/post-with-code/multilevel_confounding/multilevel_models.html",
    "title": "Freedom, Hierarchies and Confounded Estimates",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport bambi as bmb\nimport pymc as pm\nimport arviz as az\nimport matplotlib.pyplot as plt\nfrom matplotlib import transforms\nfrom itertools import product\nimport pyfixest as pf\nimport statsmodels.formula.api as smf\nfrom patsy import dmatrices, dmatrix\nimport seaborn as sns\nimport networkx as nx\nfrom pyfixest.did.estimation import did2s, lpdid\nfrom matplotlib.lines import Line2D\nfrom matplotlib.patches import Patch\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\npd.options.mode.chained_assignment = None \n\nnp.random.seed(100)"
  },
  {
    "objectID": "posts/post-with-code/multilevel_confounding/multilevel_models.html#estimation-and-group-effects",
    "href": "posts/post-with-code/multilevel_confounding/multilevel_models.html#estimation-and-group-effects",
    "title": "Freedom, Hierarchies and Saturated Regression (WIP)",
    "section": "Estimation and Group Effects",
    "text": "Estimation and Group Effects\nFirst consider an estimation example due to Richard McElreath’s lecture series where we examine the various parameter recovery options available in the case of group level confounding. Define a data generating process determined by group level effects:\n\ndef inv_logit(p):\n    return np.exp(p) / (1 + np.exp(p))\n\nN_groups = 30\nN_id = 3000\na0 = -2\nbXY = 1\nbZY = -0.5\ng = np.random.choice(range(N_groups), size=N_id, replace=True)\nUg = np.random.normal(1.5, size=N_groups)\n\nX = np.random.normal(Ug[g], size=N_id)\nZ = np.random.normal(size=N_groups)\n\ns = a0 + bXY*X + Ug[g] + bZY*Z[g]\np = inv_logit(s)\nY = np.random.binomial(n=1, p=p)\n\n\nsim_df = pd.DataFrame({'Y': Y, 'p': p, 's': s, 'g': g, 'X': X, 'Z': Z[g]})\nsim_df\n\nThis is a bernoulli outcome with group level confounds. If we model this relationship the confounding effects will bias naive parameter estimates on the covariates \\(X\\), \\(Z\\). Seeing different results as we explore different ways of parameterising the relationships.\n\n\n\n\n\n\nWarning\n\n\n\nThere is a huge degree of confusion over the meaing of the terms “Fixed Effects” and “Random Effects”. Within this blog post we will mean the population level parameters. \\[\\beta X\\]\nfor an individual variable \\(X\\) when we refer to fixed effects. Corresspondingly we will refer to group-level parameters \\(\\beta_{g}\\)\n\\[\\Big(\\underbrace{\\beta}_{pop} + \\underbrace{\\beta_{g}}_{group}\\Big)X\\]\nwhich are incorporated into our model equation modifying population level parameters as random effects. We will generally use Wilkison notation to specify these choices where random effects for modifying a population are denoted with a conditional bar over the variable ( X | group) and fixed effects are specified by just including the variable in the equation i.e. y ~ X + (Z | group) where X has a fixed effect parameterisation and Z a random effects parameterisation. We can also create indicator variables for group membership using this syntax with y ~ C(group) + X + Z where under the hood we pivot the group category into a zero-one variables indicating group membership. This parameterisation means each indicator variable (one for each level of the grouping variable) will receive a fixed effects population parameter.\n\n\n\nNaive Model\nThe formula syntax for Bambi also enables us to specify generalised linear models with bernoulli outcomes.\n\nnaive_model = bmb.Model(f\"Y['>.5'] ~ X + Z \", sim_df, \nfamily=\"bernoulli\")\nnaive_idata = naive_model.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True},)\n\n\naz.summary(naive_idata, var_names=['Intercept', 'X', 'Z'])\n\nHere we see that all three parameter estimates are biased away from their true values. Let’s try a simple fixed effects approach that adds indicator variables for all but one of the group levels.\n\n\nFixed Effects Model\nThe additional syntax for creating the group level indicator variables is specified here.\n\nfixed_effects_model = bmb.Model(f\"Y['>.5'] ~ C(g) + X + Z\", sim_df, \nfamily=\"bernoulli\")\nfixed_effects_idata = fixed_effects_model.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True},)\n\n\naz.summary(fixed_effects_idata, var_names=['Intercept', 'X', 'Z'])\n\nNow we see that the coefficient on the \\(X\\) variable seem correct, but the coefficient on \\(Z\\) is wildly wrong. Indeed the uncertainty interval on the \\(Z\\) coefficient is huge. The fixed effect model was unable to learn anything about the correct parameter. Whereas the naive model seems to learn the correct \\(Z\\) parameter but over estimates the \\(X\\) coefficient. These are the kinds of trade-offs we need to be wary of as we account for the complexities of extensive group interactions in our model’s functional form.\n\n\nCode\nfig, axs = plt.subplots(2, 1, figsize=(10, 9))\naxs = axs.flatten()\n\naz.plot_posterior(naive_idata, var_names=['X'], ax=axs[0], \npoint_estimate=None, color='red', label='Naive Model')\naxs[0].axvline(1)\n\naz.plot_posterior(fixed_effects_idata , var_names=['X'], ax=axs[0], point_estimate=None, hdi_prob='hide', label='Fixed Effect Models')\naxs[0].set_title(\"Naive/Fixed Model X Coefficient\")\n\naz.plot_posterior(naive_idata, var_names=['Z'], ax=axs[1], point_estimate=None,  color='red', ref_val_color='black')\n\n\naz.plot_posterior(fixed_effects_idata , var_names=['Z'], ax=axs[1], point_estimate=None, hdi_prob='hide')\n\naxs[1].set_title(\"Naive/Fixed Effect Model Z Coefficient\")\naxs[1].axvline(-0.5);\n\n\nWe now want to try another approach to handle to the group confounding that involves a hierarchical approach to add group level effects to the intercept term.\n\n\nMultilevel Model\nThe syntax for the random effects model is similar.\n\nmultilevel_model = bmb.Model(f\"Y['>.5'] ~ (1 | g) + X + Z\", sim_df, \nfamily=\"bernoulli\")\nmultilevel_model_idata = multilevel_model.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True},)\n\nThis method posits a view that there is a shared underlying process generating each instance of group-behaviour i.e. the reliased values of the outcome within each group. We replace the indivdual fixed effects indicator columns with additional parameters modifying the intercept.\n\naz.summary(multilevel_model_idata, var_names=['X', 'Z', 'Intercept'])\n\nNext we’ll apply the Mundlak device method which adds the group mean back to each observation as a covariate.\n\n\nMundlak Model\nFor this technique we supply group aggregate mean values as additional columns. This somewhat absolves of the requirement to include an extra multiplicity of parameters. It’s more akin to feature creation than model specification, but it serves the same purpose - it accounts for group level variation and provides a mechanism for the model to learn the appropriate weights to accord each group in the final calculation.\n\nsim_df['group_mean'] = sim_df.groupby('g')['X'].transform(np.mean)\n\nsim_df['group_mean_Z'] = sim_df.groupby('g')['Z'].transform(np.mean)\n\nmundlak_model = bmb.Model(f\"Y['>.5'] ~ (1 | g) + X + Z + group_mean\", sim_df, \nfamily=\"bernoulli\")\nmundlak_idata = mundlak_model.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True},)\n\n\naz.summary(mundlak_idata, var_names=['Intercept', 'X', 'Z'])\n\nWe can now plot all the parameter recovery models together and we’ll see that there are some trade-offs between the fixed effects and random effects varieties of the modelling.\n\n\nPlotting the Comparisons\nThe parameter recovery exercise shows striking differences across each of the models\n\n\nCode\nfig, axs = plt.subplots(3, 1, figsize=(10, 11))\naxs = axs.flatten()\n\naz.plot_posterior(naive_idata, var_names=['X'], ax=axs[0], \npoint_estimate=None, color='red', label='Naive', hdi_prob='hide')\naxs[0].axvline(1, color='k', linestyle='--', label='True value')\n\naz.plot_posterior(fixed_effects_idata , var_names=['X'], ax=axs[0], point_estimate=None, hdi_prob='hide', label='Fixed')\n\naz.plot_posterior(multilevel_model_idata, var_names=['X'], ax=axs[0], point_estimate=None, hdi_prob='hide', color='green', label='Hierarchical')\n\naz.plot_posterior(mundlak_idata, var_names=['X'], ax=axs[0], point_estimate=None, hdi_prob='hide', color='purple', label='Mundlak')\n\n\naxs[0].set_title(\"X Coefficient\")\n\naz.plot_posterior(naive_idata, var_names=['Z'], ax=axs[1], point_estimate=None,  color='red', ref_val_color='black', hdi_prob='hide')\n\n\naz.plot_posterior(fixed_effects_idata , var_names=['Z'], ax=axs[1], point_estimate=None, hdi_prob='hide')\n\naz.plot_posterior(multilevel_model_idata, var_names=['Z'], ax=axs[1], point_estimate=None, hdi_prob='hide', color='green')\n\naz.plot_posterior(mundlak_idata, var_names=['Z'], ax=axs[1], point_estimate=None, hdi_prob='hide', color='purple')\n\naxs[1].set_title(\"Z Coefficient\")\naxs[1].axvline(-0.5, color='k', linestyle='--');\n\naz.plot_posterior(naive_idata, var_names=['Intercept'], ax=axs[2], point_estimate=None,  color='red', ref_val_color='black', hdi_prob='hide')\n\n\naz.plot_posterior(fixed_effects_idata , var_names=['Intercept'], ax=axs[2], point_estimate=None, hdi_prob='hide')\n\naz.plot_posterior(multilevel_model_idata, var_names=['Intercept'], ax=axs[2], point_estimate=None, hdi_prob='hide', color='green')\n\naz.plot_posterior(mundlak_idata, var_names=['Intercept'], ax=axs[2], point_estimate=None, hdi_prob='hide', color='purple')\n\naxs[2].axvline(-2, color='k', linestyle='--');\n\n\nImportantly, we see that while the fixed effects model is focused on recovering the treatment effect on the \\(X\\) covariate, it does so somewhat at the expense of accuracy on the other systematic components of the model. This focus renders the model less predictively accurate. Compare the models on the cross-validation score and we see how the hierarchical mundlak model is to be preferred.\n\ncompare_df = az.compare({'naive': naive_idata, 'fixed': fixed_effects_idata, 'hierarchical': multilevel_model_idata, \n'mundlak': mundlak_idata})\ncompare_df\n\n\naz.plot_compare(compare_df);\n\nThis is not the only way to assess viability of the model’s functional form but it’s not a bad way.\n\n\nFull Luxury Bayesian Mundlak Machine\nAs good Bayesians we might be worry about the false precision of adding simple point estimates for the group mean covariates in the Mundlak model. We can remedy this by explicitly incorporating these values as an extra parameter and adding uncertainty to the draws on these parameters.\n\nid_indx, unique_ids = pd.factorize(sim_df[\"g\"])\n\ncoords = {'ids': list(range(N_groups))}\nwith pm.Model(coords=coords) as model: \n\n    x_data = pm.Data('X_data', sim_df['X'])\n    z_data = pm.Data('Z_data', sim_df['Z'])\n    y_data = pm.Data('Y_data', sim_df['Y'])\n\n    alpha0 = pm.Normal('Intercept', 0, 1)\n    alpha_j = pm.Normal('alpha_j', 0, 1, dims='ids')\n    beta_xy = pm.Normal('X', 0, 1)\n    beta_zy = pm.Normal('Z', 0, 1)\n\n    group_means = pm.Normal('group_means', sim_df.groupby('g')['X'].mean().values, .1, dims='ids')\n\n    mu = pm.Deterministic('mu', (alpha0 + alpha_j[id_indx]) + beta_xy*x_data + beta_zy*z_data + group_means[id_indx])\n    p = pm.Deterministic(\"p\", pm.math.invlogit(mu))\n    # likelihood\n    pm.Binomial(\"y\", n=1, p=p, observed=y_data)\n\n    idata = pm.sample(idata_kwargs={\"log_likelihood\": True})\n\n\naz.summary(idata, var_names=['Intercept', 'X', 'Z'])\n\nThis model bakes more uncertainty into the process assuming a kind of measurement-error model which may be more or less apt depending on how much data you’ve acquired and your view of the underlying process. We’ll now examine how these considerations play out when there are multiple group-level influences."
  },
  {
    "objectID": "posts/post-with-code/multilevel_confounding/multilevel_models.html#nested-groups-and-fixed-effects",
    "href": "posts/post-with-code/multilevel_confounding/multilevel_models.html#nested-groups-and-fixed-effects",
    "title": "Freedom, Hierarchies and Saturated Regression (WIP)",
    "section": "Nested Groups and Fixed Effects",
    "text": "Nested Groups and Fixed Effects\nWe’ve seen how various attempts to account for the group effects can more or less recover the parameters of a complex data generating process with group confounding. Now we want to look at a case where we can have interacting group effects at multiple levels.\n\nPupils within Class Rooms within Schools\nA natural three level group hierarchy occurs in the context of educational organisations and business org-charts. We can use this fact to interrogate briefly how inferential statements about treatment effects vary as a function of what and how we control for group level variation. We draw the following data set from Linear Mixed Models: A Practical Guide Using Statistical Software.\n\ndf = pd.read_csv('classroom.csv')\ndf['class_mean'] = df.groupby(['classid'])['mathprep'].transform(np.mean)\ndf['school_mean'] = df.groupby(['schoolid'])['mathprep'].transform(np.mean)\ndf.head()\n\nThe data has three distinct levels: (1) the child or pupil and their demographic attributes and outcome variable mathgain, (2) the classroom and the teacher level attributes such as their experience yearstea and a record of their mathematics courses taken mathprep, (3) school and neighbourhood level with features describing poverty measures in the vicinity housepov.\nWe’ll plot the child’s outcome mathgain against the mathprep and distinguish the patterns by school.\n\n\nCode\ndef rand_jitter(arr):\n    stdev = .01 * (max(arr) - min(arr))\n    return arr + np.random.randn(len(arr)) * stdev\n\n\nschools = df['schoolid'].unique()\nschools_10 = [schools[i:i+10] for i in range(0, len(schools), 10)]\nfig, axs = plt.subplots(3,4, figsize=(20, 10), \nsharey=True, sharex=True)\naxs = axs.flatten()\nfor s, ax in zip(schools_10, axs):\n    temp = df[df['schoolid'].isin(s)]\n    ax.scatter(rand_jitter(temp['mathprep']), temp['mathgain'], \n    c=temp['schoolid'], cmap='tab10')\n    ax.set_title(f\"Schools \\n {s}\");\n\n\nThere is a small number of observed students per school so the individual school level distributions show some extreme outliers but the overall distribution nicely converges to an approximately normal symmetric shape.\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 6))\naxs = axs.flatten()\nfor school in schools:\n    temp = df[df['schoolid'] ==school]\n    axs[0].hist(temp['mathgain'], color='grey', alpha=0.3, density=True, histtype='step', cumulative=True)\n\naxs[0].hist(df['mathgain'], bins=30, ec='black', density=True, cumulative=True, histtype='step')\naxs[1].hist(df['mathgain'], bins=30, ec='black', density=True, cumulative=False)\naxs[0].set_title(\"Cumulative Distribution Function by School\")\naxs[1].set_title(\"Overall Distribution\");\n\nWith these kinds of structures we need to be careful in how we evaluate any treatment effects when there are reasons to believe in group-level effects that impact the outcome variable. Imagine the true treatment effect is a sprinter running too fast to cleanly measure - each group interaction effect is added to his load as a weight. Enough weights absorb enough of the variation in the treatment that he is dragged to a crawl. Whatever movement he can make while dragging this burden is the effect we attribute to the treatment. Or put another way - the effects of various group interactions will modify the treatment effectiveness in some way and unless we account for this impact in our model, then the inferences regarding the treatment will be clouded.\n\n\nInteraction Effects and Nuisance Parameters\nNot all possible interactions will be present in the data. But if we specify the model to account for various interactions we may explode the number of parameters beyond the number of data points. This can cause issues in estimation routines and requires consideration about what the group parameters are aimed at capturing.\nWhy add covariates for group-membership when no observation reflects outcomes in that group? We cannot learn anything about these cases. At least if we add the group effects as a hierarchical random effect we induce shrinkage on the parameter estimates towards the mean of the hierarchical parameter in the model. This means that when predicting on “new” data with examples of these missing cases we can predict a sensible default. The distinction rests in the role we have in mind for these tools. If we seriously commit to the idea that group variation reflects a real “common” process in the larger population and we want to learn about that over-arching process then we deploy a random effects model. But if we only see these as tools for accounting to variation in the sample, allowing us to pin down an alternative focal estimate then the group indicator covariates are just “nuisance” parameters and missing cases are irrelevant.\n\n\n\n\n\n\nPhilosophical Digression\n\n\n\nThis last point skips a little quickly over a fundamental feature of interpreting these models. If we aim to interpret these models as reflecting a common process across these groups that exists in a “population”, then we’re endorsing an inferential view that extends beyond the sample. We’re actively seeking to learn a general truth about the data generating process which we deem to be adequately expressed in our model. If we seek to “soak up” the variation due to group effects, we’re treating these group effects as noise in the sample data and making inferential commitments only about the focal parameter in the model. This approach to learning differentiates approaches to credible causal inference. On the one hand, fixing your estimand and designing estimators to specifically capture that estimate seems like a modest and compelling strategy. On the other hand if your model ignores aspects of underlying phenomena or fails to retrodict the observable data, it’s dubious as to why anyone would trust its output.\n\n\nTo see the extent of redundany we can examine the dimensions of the covariate matrices that result from including more or less interaction terms.\n\ny, X = dmatrices(\"mathgain ~ mathprep + C(schoolid)+ C(classid)\", df, return_type=\"dataframe\")\nprint(X.shape)\n\ny, X1 = dmatrices(\"mathgain ~ mathprep + C(schoolid)/C(classid)\", df, return_type=\"dataframe\")\nprint(X1.shape)\n\n\ny, X2 = dmatrices(\"mathgain ~ mathprep + C(schoolid):C(childid)\", df, return_type=\"dataframe\")\nprint(X2.shape)\n\nWe see here how different ways in which to account for group level variation and interaction effects lead to vastly inflated feature matrices. However not all interaction terms matter, or put another way… nor all the possible interactions feature in the data. So we have likely inflated the data matrix beyond necessity.\nHere we define a helper function to parse a complex interaction formula, remove the columns entirely composed of zeros and return a new formula and dataframe which has a suitable range of features to capture the variation structures in the data.\n\ndef make_interactions_df(formula, df):\n    y, X = dmatrices(formula, df, return_type=\"dataframe\")\n    n = X.shape[1]\n    X = X[X.columns[~(np.abs(X) < 1e-12).all()]]\n    n1 = X.shape[1]\n    target_name = y.columns[0]\n    d = pd.concat([y, X], axis=1)\n    d.drop(['Intercept'], axis=1, inplace=True)\n    d.columns = [c.replace('[', '').replace(']','').replace('C(', '').replace(')', '').replace('.', '_').replace(':', '_') for c in d.columns]\n    cols = ' + '.join([col for col in d.columns if col != target_name])\n    formula = f\"{target_name} ~ {cols}\"\n    print(f\"\"\"Size of original interaction features: {n} \\nSize of reduced feature set: {n1}\"\"\")\n    return formula, d\n\nformula, interaction_df = make_interactions_df(\"mathgain ~ mathprep + C(schoolid):C(childid)\", df)\n\ninteraction_df.head()\n\nWe have reduced the number of interactions by an order of magnitude! We can now fit a regression model to the revised feature matrix.\n\n\nComparing Interaction Models\nConsider the variation in the coefficient values estimated for mathprep as we add more and more interaction effects. The addition of interaction effects generates a large number of completely 0 interaction terms which we remove here.\n\nformulas = [\"\"\"mathgain ~ mathprep + C(schoolid)\"\"\",\n\"\"\" mathgain ~ mathprep + school_mean*class_mean\"\"\" , \n\"\"\"mathgain ~ mathprep + C(schoolid) + C(classid)\"\"\", \n\"\"\"mathgain ~ mathprep + C(schoolid)*C(classid)\"\"\",\n\"\"\"mathgain ~ mathprep + C(classid):C(childid)\"\"\", \n]\n\nestimates_df = []\nfor f in formulas:\n    formula, interaction_df = make_interactions_df(f, df)\n    result = smf.ols(formula, interaction_df).fit()\n    estimates = [[result.params['mathprep']], list(result.conf_int().loc['mathprep', :]), [formula]]\n    estimates = [e for est in estimates for e in est]\n    estimates_df.append(estimates)\n\nestimates_df = pd.DataFrame(estimates_df, columns=['mathprep_estimate', 'lower bound', 'upper bound', 'formula'])\n\nestimates_df\n\nThe point here (perhap obvious) is that the estimate of treatment effects due to some policy or programme can be differently understood when the regression model is able to account for increasing aspects of individual variation. Choice of the right way to “saturate” your regression specification are at the heart of causal inference. We will consider a number of specifications below that incorporate these group effects in a hierarchical model which nests the effect of class-membership within school membership. This choice allows us to control for group specific interactions without worrying about over-indexing on the observed interaction effects in the sample data requiring that we handle more fixed effects parameters than we have data points.\nIn what follows we’ll specify a nested approach to the parameter specifcation using a random effects model. The idea here is that classes are already implicitly nested in schools and so we don’t need to add parameters for classes at multiple schools. Additionally we’re positing that there is independent interest in the effectiveness school/class effects i.e. the degree to which variation in a school/class nest can account for variation in the outcome.\n\nMinimal Model\n\nmodel = bmb.Model(f\"mathgain ~ mathprep + (1 | schoolid / classid)\", df)\nidata = model.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\nThe model specification here is deliberately minimalist we want to observe how much of the variation in the outcome can be accounted for by solely adding extensive controls for interactions of group level effects and the treatment but ignoring all else.\n\nmodel.graph()\n\nWe can see the derived sigma parameters here which can be understood as partialling out the variance of the outcome into components due to those group level effects and the unexplained residuals.\n\naz.summary(idata, var_names=['Intercept', '1|schoolid_sigma', '1|schoolid:classid_sigma', 'mathgain_sigma', 'mathprep'])\n\nNote here the relative proportion of the school specifc variances 1|schoolid_sigma to the overall variance of the residuals mathgain_sigma.\n\n\n\nCalculating the IntraClass Correlation Coefficient\nThese models faciliate the calculation of the ICC statistics which is a measure of “explained variance”. The thought is to gauge the proportion of variance ascribed to one set of random effects over and above the total estimated variance in the baseline model, including the residuals mathgain_sigma.\n\na = idata['posterior']['1|schoolid_sigma']**2\n\nb = (idata['posterior']['1|schoolid:classid_sigma']**2 + idata['posterior']['1|schoolid_sigma']**2)\n\nc = (idata['posterior']['1|schoolid:classid_sigma']**2 + idata['posterior']['1|schoolid_sigma']**2 + idata['posterior']['mathgain_sigma']**2)\n\n(a / c).mean().item() \n\n\n((a + b) / c).mean().item()\n\nWe can see here that the interaction terms do seem to account for a goodly portion of the variance in the outcome and we ought to consider retaining their inclusion in our modelling work. The structure of the problem drives us towards their inclusion. Class/school effects are going to absorb a sufficient portion of the variation. So they merit study in their own right, lest the individual class/school dynamics obscure the effectiveness of the mathprep treatment. Similarly, it’s likely valuable to consider the efficacy of the average class/school in a wider policy conversation.\n\n\nAugmenting the Models\nNext we augment our model with more pupil level control variables aiming to pin down some of the aspects of the variation in the outcome.\n\nAdding Pupil Fixed Effects\nHere we add these fixed effects population parameters. But note they are not merely devices for controlling variance in the outcome, they’re interpretation is likely of independent interest.\n\nmodel_fixed = bmb.Model(f\"mathgain ~ mathkind + sex + minority + ses + mathprep + (1 | schoolid / classid)\", df)\nidata_fixed = model_fixed.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\n\naz.summary(idata_fixed, var_names=['Intercept', \n'mathkind', 'sex', 'minority', 'ses', 'mathprep',\n'1|schoolid_sigma', '1|schoolid:classid_sigma', 'mathgain_sigma'])\n\nNow we add a further class level control.\n\n\nAdding Class Level Fixed Effects\n\nmodel_fixed_1 = bmb.Model(f\"mathgain ~ mathkind + sex + minority + ses + yearstea + mathknow + mathprep + (1 | schoolid / classid)\", df.dropna())\nidata_fixed_1 = model_fixed_1.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\n\naz.summary(idata_fixed_1, var_names=['Intercept', \n'mathkind', 'sex', 'minority', 'ses', 'yearstea', 'mathknow', 'mathprep','1|schoolid_sigma', '1|schoolid:classid_sigma', 'mathgain_sigma'])\n\n\nfig, ax = plt.subplots(figsize=(10, 7))\naz.plot_forest([idata, idata_fixed, idata_fixed_1], combined=True, var_names=['mathprep', '1|schoolid_sigma', '1|schoolid:classid_sigma', 'mathgain_sigma'], ax=ax)\nax.axvline(1)\n\nWe now make use of bambis model interpretation module to plot the marginal effect on the outcome due to changes in the treatment intensity.\n\n\nCode\nfig, axs = plt.subplots(3, 1, figsize=(10, 14), \ndpi=120, sharey=True, sharex=True)\naxs = axs.flatten()\naxs[0].axhline(50, color='red', label='Reference Line', \nlinestyle='--')\naxs[1].axhline(50, color='red', label='Reference Line', \nlinestyle='--')\naxs[2].axhline(50, color='red', label='Reference Line', \nlinestyle='--')\nbmb.interpret.plot_predictions(model, idata, \"mathprep\", ax=axs[0]);\nbmb.interpret.plot_predictions(model_fixed, idata_fixed, \"mathprep\", ax=axs[1]);\nbmb.interpret.plot_predictions(model_fixed_1, idata_fixed_1, \"mathprep\", ax=axs[2]);\naxs[0].set_title(\"Variation in Implied Outcome by Treatment \\n mathgain ~ mathprep + (1 | schoolid / classid) \")\naxs[1].set_title(\"Variation in Implied Outcome by Treatment \\n mathgain ~ mathkind + sex + minority + ses + mathprep + (1 | schoolid / classid) \")\naxs[2].set_title(\"Variation in Implied Outcome by Treatment \\n mathgain ~ mathkind + sex + minority + ses + yearstea + \\n mathknow + mathprep + (1 | schoolid / classid\")\naxs[0].set_xlabel('')\naxs[1].set_xlabel('')\naxs[0].legend();\n\n\nAs we can see here across all the different model specifications we see quite modest effects of treatment with very wide uncertainty. So far, so what?! You might be sceptical that teacher training has any real discernible impact on child outcomes? Maybe you believe other interventions are more important to fund? These kinds of questions determine policy. So misguided policy interventions on child-hood education can have radical consequences. It’s, therefore, vital that we have robust and justifiable approaches to the analysis of these policy questions in the face of group level confounding. This last point is crucial - when model complexity balloons due extensive interaction effects, then we need efficient tools to interrogate the outcome level differences implied by the model choices."
  },
  {
    "objectID": "posts/post-with-code/multilevel_confounding/multilevel_models.html#two-way-fixed-effects-and-temporal-confounding",
    "href": "posts/post-with-code/multilevel_confounding/multilevel_models.html#two-way-fixed-effects-and-temporal-confounding",
    "title": "Freedom, Hierarchies and Confounded Estimates",
    "section": "Two Way Fixed Effects and Temporal Confounding",
    "text": "Two Way Fixed Effects and Temporal Confounding\nDifference in Differences designs are the overworked donkeys of social science. Many, many studies stand or fall by the assumptions baked into DiD designs. There are at least two aspects to these assumptions (i) the substantive commitments about the data generating process (e.g. parrallel trends) and (ii) the appropriateness of the functional form used to model (i). We will look first at a case where all the assumptions can be met, and then examine how things break-down under challenging staggered treatment regimes.\n\nEvent Studies and Change in Time\nEvent studies or dynamic treatment effects are an approach to measuring the gradual treatment effect as it evolves in time. We take this panel data set from the pyfixest package to demonstrate.\n\nurl = \"https://raw.githubusercontent.com/py-econometrics/pyfixest/master/pyfixest/did/data/df_het.csv\"\ndf_het = pd.read_csv(url)\ndf_het.head()\n\n\n\n\n\n  \n    \n      \n      unit\n      state\n      group\n      unit_fe\n      g\n      year\n      year_fe\n      treat\n      rel_year\n      rel_year_binned\n      error\n      te\n      te_dynamic\n      dep_var\n    \n  \n  \n    \n      0\n      1\n      33\n      Group 2\n      7.043016\n      2010\n      1990\n      0.066159\n      False\n      -20.0\n      -6\n      -0.086466\n      0\n      0.0\n      7.022709\n    \n    \n      1\n      1\n      33\n      Group 2\n      7.043016\n      2010\n      1991\n      -0.030980\n      False\n      -19.0\n      -6\n      0.766593\n      0\n      0.0\n      7.778628\n    \n    \n      2\n      1\n      33\n      Group 2\n      7.043016\n      2010\n      1992\n      -0.119607\n      False\n      -18.0\n      -6\n      1.512968\n      0\n      0.0\n      8.436377\n    \n    \n      3\n      1\n      33\n      Group 2\n      7.043016\n      2010\n      1993\n      0.126321\n      False\n      -17.0\n      -6\n      0.021870\n      0\n      0.0\n      7.191207\n    \n    \n      4\n      1\n      33\n      Group 2\n      7.043016\n      2010\n      1994\n      -0.106921\n      False\n      -16.0\n      -6\n      -0.017603\n      0\n      0.0\n      6.918492\n    \n  \n\n\n\n\nPanel data of this kind of structure is difficult to intuit unless visualised. The key structures on which this DiD estimator works are the averages across state and time. We depict these here. Note how we define a causal contrast as the difference between the treated and control groups averages over time. In particular our estimator has to account for the fact that we have a “dynamic” control group. As each cohort is treated our treatment group expands and the causal contrast of interest is differently defined. To gain a view of the time-evolution of treatment we want to marginalise over the specifc cohorts and extract a view of the how the effects evolve after the introduction of the treatment in expectation.\n\n\nCode\nfig, axs = plt.subplots(2, 1, figsize=(10, 12))\naxs = axs.flatten()\nfor u in df_het['unit'].unique():\n    temp = df_het[df_het['unit']==u]\n    axs[0].plot(temp['year'], temp['dep_var'], color='grey', alpha=0.01)\n    axs[1].plot(temp['year'], temp['dep_var'], color='grey', alpha=0.01)\ndf_het.groupby(['year', 'state'])[['dep_var']].mean().reset_index().pivot(index='year', columns='state', values='dep_var').plot(ax=axs[0], legend=False, color='blue', \nalpha=0.4)\n\ndf_het.groupby(['year', 'g'])[['dep_var']].mean().reset_index().pivot(index='year', columns='g', values='dep_var').plot(ax=axs[1], legend=False)\n\naxs[0].set_title(\"Difference in Differences \\n State Mean Change an Individual Trajectories\")\naxs[1].set_title(\"Difference in Differences \\n Mean Change an Individual Trajectories\");\n\n\n\n\n\nNote how the blue line represents a cohort that never undergoes a treatment and is maintained as a coherent control group throughout the sequence even though we have two other cohorts. This ensures we have a constant contrast case allowing us to identify the causal estimand of interest. It’s also worth calling out that we have heterogenous patterns in the treatment effects across the cohorts i.e. the the intensity of the treatment effect differs across the cohorts. We’ll apply the vanilla TWFE estimator and then dig into how it works.\n\n\nTWFE in pyfixest\nA natural question is to wonder how the treatment effect evolves over time? How does policy shift behaviours? Is it initially impactful converging to a quick plateau or a slowly building pattern of consistent growth?\n\nfit_twfe_event = pf.feols(\n    \"dep_var ~ i(rel_year, ref=-1.0) | state + year \",\n    df_het,\n    vcov={\"CRV1\": \"state\"},\n)\n\nfit_twfe_event.tidy()\n\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n\n\n\n\n\n\n  \n    \n      \n      Estimate\n      Std. Error\n      t value\n      Pr(>|t|)\n      2.5%\n      97.5%\n    \n    \n      Coefficient\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-20.0]\n      -0.099445\n      0.081699\n      -1.217202\n      0.230842\n      -0.264697\n      0.065808\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-19.0]\n      0.000624\n      0.083213\n      0.007493\n      0.994060\n      -0.167690\n      0.168937\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-18.0]\n      0.004125\n      0.089719\n      0.045974\n      0.963565\n      -0.177349\n      0.185599\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-17.0]\n      0.021899\n      0.085686\n      0.255573\n      0.799624\n      -0.151418\n      0.195216\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-16.0]\n      -0.036933\n      0.096921\n      -0.381067\n      0.705221\n      -0.232975\n      0.159108\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-15.0]\n      0.069578\n      0.081289\n      0.855936\n      0.397262\n      -0.094844\n      0.234000\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-14.0]\n      0.037734\n      0.086618\n      0.435641\n      0.665499\n      -0.137467\n      0.212936\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-13.0]\n      0.061779\n      0.083362\n      0.741098\n      0.463073\n      -0.106836\n      0.230394\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-12.0]\n      0.089913\n      0.084900\n      1.059044\n      0.296095\n      -0.081814\n      0.261639\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-11.0]\n      0.000982\n      0.079104\n      0.012417\n      0.990156\n      -0.159021\n      0.160986\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-10.0]\n      -0.113033\n      0.064922\n      -1.741047\n      0.089560\n      -0.244351\n      0.018285\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-9.0]\n      -0.069225\n      0.057046\n      -1.213481\n      0.232244\n      -0.184612\n      0.046162\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-8.0]\n      -0.061290\n      0.060362\n      -1.015370\n      0.316188\n      -0.183383\n      0.060804\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-7.0]\n      -0.002022\n      0.064602\n      -0.031306\n      0.975185\n      -0.132693\n      0.128648\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-6.0]\n      -0.055810\n      0.064674\n      -0.862938\n      0.393447\n      -0.186626\n      0.075006\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-5.0]\n      -0.065009\n      0.064810\n      -1.003066\n      0.322012\n      -0.196099\n      0.066082\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-4.0]\n      -0.009850\n      0.053098\n      -0.185505\n      0.853794\n      -0.117251\n      0.097551\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-3.0]\n      0.046338\n      0.062950\n      0.736104\n      0.466072\n      -0.080991\n      0.173667\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.-2.0]\n      0.015947\n      0.065442\n      0.243687\n      0.808751\n      -0.116421\n      0.148316\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.0.0]\n      1.406404\n      0.055641\n      25.276422\n      0.000000\n      1.293860\n      1.518949\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.1.0]\n      1.660552\n      0.065033\n      25.533859\n      0.000000\n      1.529010\n      1.792094\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.2.0]\n      1.728790\n      0.056793\n      30.440100\n      0.000000\n      1.613915\n      1.843665\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.3.0]\n      1.854839\n      0.058315\n      31.807288\n      0.000000\n      1.736886\n      1.972792\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.4.0]\n      1.958676\n      0.071144\n      27.531232\n      0.000000\n      1.814774\n      2.102578\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.5.0]\n      2.082161\n      0.063855\n      32.607651\n      0.000000\n      1.953002\n      2.211320\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.6.0]\n      2.191062\n      0.068510\n      31.981462\n      0.000000\n      2.052487\n      2.329637\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.7.0]\n      2.279073\n      0.075014\n      30.381921\n      0.000000\n      2.127342\n      2.430803\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.8.0]\n      2.364593\n      0.058598\n      40.352477\n      0.000000\n      2.246067\n      2.483120\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.9.0]\n      2.372163\n      0.056560\n      41.940696\n      0.000000\n      2.257759\n      2.486566\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.10.0]\n      2.649271\n      0.056177\n      47.158942\n      0.000000\n      2.535641\n      2.762901\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.11.0]\n      2.753591\n      0.075526\n      36.458951\n      0.000000\n      2.600826\n      2.906356\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.12.0]\n      2.813935\n      0.079320\n      35.475900\n      0.000000\n      2.653496\n      2.974375\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.13.0]\n      2.756070\n      0.078368\n      35.168251\n      0.000000\n      2.597555\n      2.914584\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.14.0]\n      2.863427\n      0.098389\n      29.103072\n      0.000000\n      2.664416\n      3.062438\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.15.0]\n      2.986652\n      0.093309\n      32.008066\n      0.000000\n      2.797916\n      3.175388\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.16.0]\n      2.963032\n      0.085427\n      34.684870\n      0.000000\n      2.790239\n      3.135825\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.17.0]\n      2.972596\n      0.092390\n      32.174553\n      0.000000\n      2.785721\n      3.159472\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.18.0]\n      2.935051\n      0.094126\n      31.182203\n      0.000000\n      2.744664\n      3.125439\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.19.0]\n      2.918707\n      0.084193\n      34.667036\n      0.000000\n      2.748411\n      3.089002\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.20.0]\n      2.979701\n      0.087777\n      33.946394\n      0.000000\n      2.802156\n      3.157246\n    \n    \n      C(rel_year, contr.treatment(base=-1.0))[T.inf]\n      0.128053\n      0.078513\n      1.630976\n      0.110946\n      -0.030755\n      0.286861\n    \n  \n\n\n\n\nThe model specification defines indicator variables for the relative years before and after the penultimate year before treatment is applied, it then incorporates the fixed effects for state and year. The coefficient values ascribed the the relative year indicators are used to plot the event-study trajectories. This is a two-way fixed effects estimation routine where the fixed effects for state and year indicators absorb the variance due to those groupings. It is often estimated using a de-meaning technique which we will demonstrate below.\n\nfigsize = [1200, 600]\nfit_twfe_event.iplot(\n    coord_flip=False,\n    title=\"TWFE-Estimator\",\n    figsize=figsize,\n    xintercept=18.5,\n    yintercept=0,\n).show()\n\n   \n   \n\n\nWe can also aim to marginalise over the details of the temporal trajectories by defining the similar estimation routine on the individuals and their treatment indicator.\n\nfit_twfe = pf.feols(\n    \"dep_var ~ i(treat) | unit + year\",\n    df_het,\n    vcov={\"CRV1\": \"state\"},\n)\n\nfit_twfe.tidy()\n\n\n\n\n\n  \n    \n      \n      Estimate\n      Std. Error\n      t value\n      Pr(>|t|)\n      2.5%\n      97.5%\n    \n    \n      Coefficient\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      C(treat)[T.True]\n      1.98254\n      0.019331\n      102.55618\n      0.0\n      1.943439\n      2.021642\n    \n  \n\n\n\n\n\n\nDe-meaning and TWFEs\nWe’ve seen above that the fixed-effect estimators in these DiD designs involve a lot of indicator variables. These are largely not the focus on the question at hand but are used exlusively to absorb the noise that takes away from our understanding of the treatment effect. We can achieve similar results with less parameters required if we “de-mean” the focus variables by the group averages of the control factors of state and year or unit. This operation makes for more efficient TWFE estimation routines is provably a variety of mundlak regression as shown by Woolridge 2021.\nWe implement the de-meaning technique and show “equivalence by python” as follows:\n\ndef demean(df, col_to_demean, group):\n    return df.assign(**{col_to_demean: (df[col_to_demean]\n                                        - df.groupby(group)[col_to_demean].transform(\"mean\")\n                                        )})\n\n\ndef apply_demeaning(df_het, by=['state', 'year'], event=True):\n    if event: \n        d = pd.get_dummies(df_het['rel_year']).drop(-1, axis=1) \n        d.columns = ['rel_year_' +str(c).replace('-', 'minus_') for c in d.columns]\n    else:\n        d = df_het[['treat']]\n    d[by[0]] = df_het[by[0]]\n    d[by[1]] = df_het[by[1]]\n    for col in d.columns: \n        if col in by:\n            pass\n        else: \n            for c in by:\n                d = demean(d, col, c)\n    d = d.drop(by, axis=1)\n    d['dep_var'] = df_het['dep_var']\n    return d\n## Demean the relative years features for event studies\nd_event = apply_demeaning(df_het, by=['state', 'year'], event=True)\n\n## Demean the treatment indicator for ATT estimation\nd = apply_demeaning(df_het, by=['unit', 'year'], event=False)\n\nd_event.head()\n\n\n\n\n\n  \n    \n      \n      rel_year_minus_20.0\n      rel_year_minus_19.0\n      rel_year_minus_18.0\n      rel_year_minus_17.0\n      rel_year_minus_16.0\n      rel_year_minus_15.0\n      rel_year_minus_14.0\n      rel_year_minus_13.0\n      rel_year_minus_12.0\n      rel_year_minus_11.0\n      ...\n      rel_year_13.0\n      rel_year_14.0\n      rel_year_15.0\n      rel_year_16.0\n      rel_year_17.0\n      rel_year_18.0\n      rel_year_19.0\n      rel_year_20.0\n      rel_year_inf\n      dep_var\n    \n  \n  \n    \n      0\n      0.651694\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      ...\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      -0.28125\n      7.022709\n    \n    \n      1\n      -0.002973\n      0.651694\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      ...\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      -0.28125\n      7.778628\n    \n    \n      2\n      -0.002973\n      -0.002973\n      0.651694\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      ...\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      -0.28125\n      8.436377\n    \n    \n      3\n      -0.002973\n      -0.002973\n      -0.002973\n      0.651694\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      ...\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      -0.28125\n      7.191207\n    \n    \n      4\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      0.651694\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      -0.002973\n      ...\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      0.000734\n      -0.28125\n      6.918492\n    \n  \n\n5 rows × 42 columns\n\n\n\nWe now have a data set with 42 columns focused on the treatment structures, but implicitly controls for the variation due to state and time. We’ll see below that this representation of the data will correctly estimate the treatment effects.\n\n\nEvent Study and De-Meaning\nNow we’ll use the de-meaned data structure above to estimate an event study using Bambi.\n\nx_cols = ' + '.join([c for c in d_event.columns if c != 'dep_var'])\nmodel_twfe_event = bmb.Model(f\"dep_var ~ + {x_cols}\", d_event)\nidata_twfe_event = model_twfe_event.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True},)\n\n\nmodel_twfe_event\n\n       Formula: dep_var ~ + rel_year_minus_20.0 + rel_year_minus_19.0 + rel_year_minus_18.0 + rel_year_minus_17.0 + rel_year_minus_16.0 + rel_year_minus_15.0 + rel_year_minus_14.0 + rel_year_minus_13.0 + rel_year_minus_12.0 + rel_year_minus_11.0 + rel_year_minus_10.0 + rel_year_minus_9.0 + rel_year_minus_8.0 + rel_year_minus_7.0 + rel_year_minus_6.0 + rel_year_minus_5.0 + rel_year_minus_4.0 + rel_year_minus_3.0 + rel_year_minus_2.0 + rel_year_0.0 + rel_year_1.0 + rel_year_2.0 + rel_year_3.0 + rel_year_4.0 + rel_year_5.0 + rel_year_6.0 + rel_year_7.0 + rel_year_8.0 + rel_year_9.0 + rel_year_10.0 + rel_year_11.0 + rel_year_12.0 + rel_year_13.0 + rel_year_14.0 + rel_year_15.0 + rel_year_16.0 + rel_year_17.0 + rel_year_18.0 + rel_year_19.0 + rel_year_20.0 + rel_year_inf\n        Family: gaussian\n          Link: mu = identity\n  Observations: 46500\n        Priors: \n    target = mu\n        Common-level effects\n            Intercept ~ Normal(mu: 4.7062, sigma: 7.1549)\n            rel_year_minus_20.0 ~ Normal(mu: 0.0, sigma: 83.8218)\n            rel_year_minus_19.0 ~ Normal(mu: 0.0, sigma: 83.8218)\n            rel_year_minus_18.0 ~ Normal(mu: 0.0, sigma: 83.8218)\n            rel_year_minus_17.0 ~ Normal(mu: 0.0, sigma: 83.8218)\n            rel_year_minus_16.0 ~ Normal(mu: 0.0, sigma: 83.8218)\n            rel_year_minus_15.0 ~ Normal(mu: 0.0, sigma: 83.8218)\n            rel_year_minus_14.0 ~ Normal(mu: 0.0, sigma: 83.8218)\n            rel_year_minus_13.0 ~ Normal(mu: 0.0, sigma: 83.8218)\n            rel_year_minus_12.0 ~ Normal(mu: 0.0, sigma: 83.8218)\n            rel_year_minus_11.0 ~ Normal(mu: 0.0, sigma: 83.8218)\n            rel_year_minus_10.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_minus_9.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_minus_8.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_minus_7.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_minus_6.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_minus_5.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_minus_4.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_minus_3.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_minus_2.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_0.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_1.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_2.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_3.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_4.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_5.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_6.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_7.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_8.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_9.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_10.0 ~ Normal(mu: 0.0, sigma: 60.2319)\n            rel_year_11.0 ~ Normal(mu: 0.0, sigma: 86.6805)\n            rel_year_12.0 ~ Normal(mu: 0.0, sigma: 86.6805)\n            rel_year_13.0 ~ Normal(mu: 0.0, sigma: 86.6805)\n            rel_year_14.0 ~ Normal(mu: 0.0, sigma: 86.6805)\n            rel_year_15.0 ~ Normal(mu: 0.0, sigma: 86.6805)\n            rel_year_16.0 ~ Normal(mu: 0.0, sigma: 86.6805)\n            rel_year_17.0 ~ Normal(mu: 0.0, sigma: 86.6805)\n            rel_year_18.0 ~ Normal(mu: 0.0, sigma: 86.6805)\n            rel_year_19.0 ~ Normal(mu: 0.0, sigma: 86.6805)\n            rel_year_20.0 ~ Normal(mu: 0.0, sigma: 86.6805)\n            rel_year_inf ~ Normal(mu: 0.0, sigma: 15.2312)\n        \n        Auxiliary parameters\n            sigma ~ HalfStudentT(nu: 4.0, sigma: 2.862)\n------\n* To see a plot of the priors call the .plot_priors() method.\n* To see a summary or plot of the posterior pass the object returned by .fit() to az.summary() or az.plot_trace()\n\n\nWe can then plot the event-study and observe a similar pattern to the one observed with pyfixest.\n\ndef plot_event_study(idata, ax, color='blue', model='demeaned'):\n    summary_df = az.summary(idata)\n    cols = [i for i in summary_df.index if 'rel' in i]\n    summary_df = summary_df[summary_df.index.isin(cols)]\n    x = range(len(summary_df))\n    ax.scatter(x, summary_df['mean'], label=model, color=color)\n    ax.plot([x, x], [summary_df['hdi_3%'],summary_df['hdi_97%']],   color=color)\n    ax.set_title(\"Event Study\")\n    return ax\n\nfig, ax = plt.subplots(figsize=(10, 5))\nplot_event_study(idata_twfe_event, ax)\nax.legend();\n\n\n\n\nSimilarly, we can de-mean the simple treatment indicator using the group means and marginalise over time periods to find a single treatment effect estimate.\n\nmodel_twfe_trt_demean = bmb.Model(f\"dep_var ~ treat\", d)\nidata_twfe_trt_demean = model_twfe_trt_demean.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True},)\n\n\naz.summary(idata_twfe_trt_demean)\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      4.706\n      0.013\n      4.681\n      4.731\n      0.000\n      0.000\n      3435.0\n      2692.0\n      1.0\n    \n    \n      treat\n      1.982\n      0.049\n      1.893\n      2.077\n      0.001\n      0.001\n      3203.0\n      2855.0\n      1.0\n    \n    \n      dep_var_sigma\n      2.811\n      0.009\n      2.793\n      2.828\n      0.000\n      0.000\n      5532.0\n      2855.0\n      1.0\n    \n  \n\n\n\n\nWhich again accords with the reported values from pyfixest. This is equivalent to using a Mundlak device as we can see below:\n\n\nTWFE by Mundlak Device\nWoolridge recounts that the TWFE is equivalent to a Mundlak regression where:\n\n[The Mundlak devices] \\(\\bar{X_{i}}\\) and \\(\\bar{X_{t}}\\) effectively act as sufficient statistics in accounting for any unit-specific heterogeneity and time-specific heterogeneity that is correlated with \\(X_{it}\\). Rather than having to include (N − 1) + (T − 1) control variables, it suffices to include 2K control variables, \\(\\bar{X_{i}}\\), \\(\\bar{X_{t}}\\) - Two-Way Fixed Effects, the Two-Way Mundlak Regression, and Difference-in-Differences Estimators\n\nWe will see an example of this equivalence here.\n\ndf_het['unit_mean'] = df_het.groupby('unit')['treat'].transform(np.mean)\ndf_het['time_mean'] = df_het.groupby('year')['treat'].transform(np.mean)\n\nmodel_twfe_trt = bmb.Model(f\"dep_var ~ treat\", df_het)\nidata_twfe_trt = model_twfe_trt.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\nmodel_twfe_trt_mundlak = bmb.Model(f\"dep_var ~ treat + unit_mean + time_mean\", df_het)\nidata_twfe_trt_mundlak = model_twfe_trt_mundlak.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\n\naz.plot_forest([idata_twfe_trt_demean, idata_twfe_trt_mundlak, idata_twfe_trt], combined=True, var_names=['treat'], model_names=['De-meaned', 'Mundlak', 'Simple']);\n\n\n\n\nThe de-meaned TWFE estimator and the Mundlak specification result in identical estimates and differ from the naive estimate that fails to control group level confounds. The Mundlak specification is far easier to implement and offers altnerative ways to parameterise a model capable of adjusting for the group level confounding.\n\n\nFunctional Form and Richly Parameterised Regressions\nThe vanilla TWFE estimator can successfully recover the treatment effects in DiD designs and facilitate event studies of the same. However, the details of the estimation matter because this functional form is not always robust. Here we’ll see other options that can recover substantially the same inferences and may prove more robust as we’ll see below. The key to each is to articulate enough structural features that allow the model to modify effects based on the suspected group level confounds. Most crucially, the model needs to express (or account for) variation due to relevant data generating process.\n\ndf_het['state_mean'] = df_het.groupby('state')['treat'].transform(np.mean)\ndf_het['time_mean'] = df_het.groupby('year')['treat'].transform(np.mean)\ndf_het['cohort_mean'] = df_het.groupby('group')['treat'].transform(np.mean)\n\nmodel_twfe_event_1 = bmb.Model(f\"dep_var ~ 1 + C(year) + state_mean + C(rel_year, Treatment(reference=-1)) \", df_het)\nidata_twfe_event_1 = model_twfe_event_1.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\n\nformula = \"\"\" dep_var ~ 1 + time_mean:state_mean + C(rel_year, Treatment(reference=-1))\"\"\"\ntwfe_model_ols = smf.ols(formula, data=df_het).fit()\ntwfe_model_ols.summary()\nparam_est = pd.DataFrame(twfe_model_ols.params, columns=['estimate']).iloc[1:-1]\nparam_est['index_number'] = list(range(len(param_est)))\ntemp = (param_est.reset_index()\n)\nparam_est = temp[(~temp['index'].str.contains(':')) & (temp['index'].str.contains('rel'))]\nparam_est.reset_index(inplace=True)\n\n\nmodel_twfe_event_2 = bmb.Model(f\"dep_var ~ (1 | year) + state_mean + C(rel_year, Treatment(reference=-1)) \", df_het)\nidata_twfe_event_2 = model_twfe_event_2.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\nHaving estimated the various alternatives model specifications we compare each against our baseline de-meaned event-study.\n\n\nCode\nfig, axs = plt.subplots(2, 2, figsize=(20, 10))\naxs = axs.flatten()\nplot_event_study(idata_twfe_event, axs[0], model='Manual DeMeaned')\nplot_event_study(idata_twfe_event, axs[1], model='Manual DeMeaned')\nplot_event_study(idata_twfe_event, axs[2], model='Manual DeMeaned')\nplot_event_study(idata_twfe_event, axs[3], model='Manual DeMeaned')\nplot_event_study(idata_twfe_event_1, axs[0], color='green', model='Fixed Effects Saturated Bayes')\nplot_event_study(idata_twfe_event_2, axs[1], color='purple', model='Hierarchical Effects Saturated Bayes')\naxs[2].scatter(param_est['index'], param_est['estimate'], color='red', label='Mundlak Interaction Features OLS')\ntidy = fit_twfe_event.tidy()\nxs = range(len(tidy))\ntidy.reset_index(inplace=True)\naxs[3].scatter(xs, tidy['Estimate'], color='orange', label='pyfixest TWFE')\naxs[3].plot([xs, xs], [tidy['2.5%'],tidy['97.5%']], color='orange')\naxs[2].set_xticks([])\naxs[0].set_title(\"dep_var ~ 1 + C(year) + state_mean + C(rel_year, Treatment(reference=-1))\")\naxs[1].set_title(\"dep_var ~ (1 | year) + state_mean + C(rel_year, Treatment(reference=-1))\")\naxs[2].set_title(\"dep_var ~ 1 + time_mean:state_mean + C(rel_year, Treatment(reference=-1))\")\naxs[3].set_title(\"dep_var ~ i(rel_year, ref=-1.0) | state + year\")\naxs[0].legend()\naxs[1].legend()\naxs[2].legend()\naxs[3].legend();\n\n\n\n\n\n`\nThis suggests that there are a variety of functional forms even just using regression specifications that seek to control from different types of group level confounding. In this example data most of the functional forms that seek to control for time and state level effects seem to converge in their answers. We will now switch to an example where the vanilla TWFE breaks down."
  },
  {
    "objectID": "posts/post-with-code/multilevel_confounding/multilevel_models.html#issues-with-twfe-and-richly-parameterised-linear-models",
    "href": "posts/post-with-code/multilevel_confounding/multilevel_models.html#issues-with-twfe-and-richly-parameterised-linear-models",
    "title": "Freedom, Hierarchies and Confounded Estimates",
    "section": "Issues with TWFE and Richly Parameterised Linear Models",
    "text": "Issues with TWFE and Richly Parameterised Linear Models\nWe draw on the example from Pedro Sant’Anna here where it is demonstrated that the vanilla TWFE estimator breaks down under various conditions. These conditions are often related to staggered roll out of a treatment. This staggered roll out induces dynamic changes in the composition of treatment group over time. Appropriate inference needs to carefully control for the interaction effects due to staggered treatment. In particular we need our model of the situation to reflect this aspect of the data generating process.\nLet’s generate some data.\n\ntrue_mu = 1\n\ndef make_data(nobs = 1000, nstates = 40):\n    ids = list(range(nobs))\n    states = np.random.choice(range(nstates), size=nobs, replace=True)\n    unit = pd.DataFrame({'unit': ids, \n                        'state': states, \n                        'unit_fe': np.random.normal(states/5, 1, size=nobs),\n                        'mu': true_mu})\n    \n    year = pd.DataFrame({'year': pd.date_range('1980-01-01', '2010-01-01', freq='y'), \n    'year_fe': np.random.normal(0, 1, 30) })\n    year['year'] = year['year'].dt.year\n\n    treat_taus = pd.DataFrame({'state': np.random.choice(range(nstates), size=nstates, replace=False),\n    'cohort_year': np.sort([1986, 1992, 1998, 2004]*10)\n    })\n\n    cross_join = pd.DataFrame([row for row in product(range(nobs), year['year'].unique())], columns =['unit', 'year'])\n    cross_join = cross_join.merge(unit, how='left', left_on='unit', \n    right_on='unit')\n    cross_join = cross_join.merge(year, how='left', left_on='year', \n    right_on='year')\n    cross_join = cross_join.merge(treat_taus, how='left', left_on='state', right_on='state')\n    cross_join = cross_join.assign(\n        error = np.random.normal(0, 1, len(cross_join)),\n        treat = lambda x: np.where(x['year'] >= x['cohort_year'], 1, 0)\n    )\n    cross_join = cross_join.assign(tau = np.where(cross_join['treat'] == 1, cross_join['mu'], 0), \n    ).assign(year_fe = lambda x: x['year_fe'] + 0.1*(x['year']-x['cohort_year']))\n\n    cross_join['tau_cum'] = cross_join.groupby('unit')['tau'].transform(np.cumsum)\n    cross_join = cross_join.assign(dep_var = lambda x: 2010-x['cohort_year'] + \n    x['unit_fe'] + x['year_fe'] + x['tau_cum'] + x['error'])\n    cross_join['rel_year'] =  cross_join['year'] - cross_join['cohort_year']\n\n    \n    return cross_join\n\nsim_df = make_data(500, 40)\nsim_df.head()\n\n\n\n\n\n  \n    \n      \n      unit\n      year\n      state\n      unit_fe\n      mu\n      year_fe\n      cohort_year\n      error\n      treat\n      tau\n      tau_cum\n      dep_var\n      rel_year\n    \n  \n  \n    \n      0\n      0\n      1980\n      12\n      2.770007\n      1\n      -1.443538\n      1998\n      -0.883541\n      0\n      0\n      0\n      12.442927\n      -18\n    \n    \n      1\n      0\n      1981\n      12\n      2.770007\n      1\n      -3.140326\n      1998\n      1.044140\n      0\n      0\n      0\n      12.673821\n      -17\n    \n    \n      2\n      0\n      1982\n      12\n      2.770007\n      1\n      -2.430673\n      1998\n      0.276021\n      0\n      0\n      0\n      12.615355\n      -16\n    \n    \n      3\n      0\n      1983\n      12\n      2.770007\n      1\n      -4.253994\n      1998\n      0.181049\n      0\n      0\n      0\n      10.697061\n      -15\n    \n    \n      4\n      0\n      1984\n      12\n      2.770007\n      1\n      -3.347429\n      1998\n      -2.104055\n      0\n      0\n      0\n      9.318523\n      -14\n    \n  \n\n\n\n\nWe can now plot the staggered nature of the imagined treatment regime.\n\n\nCode\nfig, ax = plt.subplots(figsize=(10, 6))\n\nfor unit in sim_df['unit'].unique()[0:100]:\n    temp = sim_df[sim_df['unit'] == unit]\n    ax.plot(temp['year'], temp['dep_var'], alpha=0.1, color='grey')\n\nsim_df.groupby(['cohort_year', 'year'])[['dep_var']].mean().reset_index().pivot(index='year', columns='cohort_year', values='dep_var').plot(ax=ax)\nax.axvline(1986)\nax.axvline(1992, color='orange')\nax.axvline(1998, color='green')\nax.axvline(2004, color='red')\nax.set_title(\"Simulated Cohorts Homogenous Treatment Effects \\n All Eventually Treated\", fontsize=20)\nax.legend()\n\n\n<matplotlib.legend.Legend at 0x2a273b2d0>\n\n\n\n\n\nThis data will present problems for the vanilla TWFE estimator. The issues stems from how each cohort receives a treatment and there are periods in the data when no group is in the “control” group. We can see how this plays out with de-meaning TWFE strategy.\n\nfit_twfe = pf.feols(\n    \"dep_var ~ i(rel_year, ref=-1.0) | state + year\",\n    sim_df,\n    vcov={\"CRV1\": \"state\"},\n)\n\n\nfigsize = [1200, 400]\nfit_twfe.iplot(\n    coord_flip=False,\n    title=\"TWFE-Estimator\",\n    figsize=figsize,\n    xintercept=18.5,\n    yintercept=0,\n).show()\n\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/pyfixest/estimation/feols_.py:1987: UserWarning: \n            The following variables are collinear: ['C(rel_year, contr.treatment(base=-1.0))[T.18]', 'C(rel_year, contr.treatment(base=-1.0))[T.19]', 'C(rel_year, contr.treatment(base=-1.0))[T.20]', 'C(rel_year, contr.treatment(base=-1.0))[T.21]', 'C(rel_year, contr.treatment(base=-1.0))[T.22]', 'C(rel_year, contr.treatment(base=-1.0))[T.23]'].\n            The variables are dropped from the model.\n            \n  warnings.warn(\n\n\n   \n   \n\n\nThis is not remotely close to the expected pattern. For contrast, consider an alternative estimator.\n\nfit_lpdid = lpdid(\n    data=sim_df,\n    yname=\"dep_var\",\n    gname=\"cohort_year\",\n    tname=\"year\",\n    idname=\"unit\",\n    vcov={\"CRV1\": \"state\"},\n    pre_window=-17,\n    post_window=17,\n    att=False,\n)\n\nfit_lpdid.iplot(\n    coord_flip=False,\n    title=\"Local-Projections-Estimator\",\n    figsize=figsize,\n    yintercept=0,\n    xintercept=18.5,\n).show()\n\n   \n   \n\n\nThe initial TWFE estimate is utterly skewed and entirely incorrect. Something dreadful has gone wrong under the hood. For contrast, we’ve included the Local Projections estimator from the pyfixest to show that we can recover the actual treatment effect under this event study with alternative strategies. However, there is more machinary involved in the local-projections estimator.\n\n\n\n\n\n\nWarning\n\n\n\nNote there has been a large volume of literature diagnosing precisely how and where TWFE estimator breakdown. Goodman-Bacon’s “Difference-in-differences with variation in treatment timing” in particular provides a decomposition of the causal estimand that can be recovered under varying treatment regimes. We won’t cover this rich area of research here. But we note that it’s a lively area of research.\n\n\nInstead we want show how to use mundlak devices to recover more reasonable estimates. No fancy estimators, just more regressions.\n\nWoolridge argues for this approach when he states that\n\n[T]hat there is nothing inherently wrong with TWFE, which is an estimation method. The problem with how TWFE is implemented in DiD settings is that it is applied to a restrictive model… - Two-Way Fixed Effects, the Two-Way Mundlak Regression, and Difference-in-Differences Estimators\n\nWe want to explore different ways of isolating the treatment effects under a variety of model specifications that controls for time-constant treatment intensities, covariates, and interactions between them. That is we want to better express the data generating process in our model of the situation.\n\nFitting a Variety of Models\nConsider the following model specifications.\n\nsim_df['unit_mean'] = sim_df.groupby('unit')['treat'].transform(np.mean)\n\nsim_df['state_mean'] = sim_df.groupby('state')['treat'].transform(np.mean)\n\nsim_df['cohort_mean'] = sim_df.groupby('cohort_year')['treat'].transform(np.mean)\n\nsim_df['time_mean'] = sim_df.groupby('year')['treat'].transform(np.mean)\n\n\nmodel_twfe = bmb.Model(f\"\"\"dep_var ~ 1  + time_mean:state_mean + C(rel_year, Treatment(reference=-1))\"\"\", sim_df)\n\nidata_twfe = model_twfe.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\n\nmodel_twfe1 = bmb.Model(f\"\"\"dep_var ~ 1  + time_mean* state_mean + C(cohort_year) + C(rel_year, Treatment(reference=-1))\"\"\", sim_df)\n\nidata_twfe1 = model_twfe1.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\n\nmodel_twfe2 = bmb.Model(f\"\"\"dep_var ~ 1  + cohort_mean: state_mean + C(rel_year, Treatment(reference=-1))\"\"\", sim_df)\n\nidata_twfe2 = model_twfe2.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\nmodel_twfe3 = bmb.Model(f\"\"\"dep_var ~ (1| year)  + state_mean + C(rel_year, Treatment(reference=-1))\"\"\", sim_df)\nidata_twfe3 = model_twfe3.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\nThese latter models will recover the appropriate treatment effects with slight variations due to the adapted functional form. The initial models will fail to capture the pattern correctly.\n\n\nCode\nfig, axs = plt.subplots(4, 1, figsize=(10, 15), sharey=True)\naxs = axs.flatten()\nplot_event_study(idata_twfe, ax=axs[0], model='Additive Mundlak')\nplot_event_study(idata_twfe1, ax=axs[1], color='red', model='Mundlak State & Time Interactions')\nplot_event_study(idata_twfe2, ax=axs[2], color='green', model='Mundlak Cohort & State Interactions')\nplot_event_study(idata_twfe3, ax=axs[3], color='purple', model='Hierarchical Time + Mundlak State')\naxs[0].set_title(str(model_twfe.formula))\naxs[1].set_title(str(model_twfe1.formula))\naxs[2].set_title(str(model_twfe2.formula))\naxs[3].set_title(str(model_twfe3.formula))\nxs = np.linspace(0, 45, 45)\nxs_centered = xs - 22\ntrue_effect = np.where(xs_centered <= 0, 0, (xs_centered +1))\naxs[0].plot(xs, true_effect, color='k', alpha=0.6, label='True effect')\naxs[1].plot(xs, true_effect, color='k', alpha=0.6, label='True effect')\naxs[2].plot(xs, true_effect, color='k', alpha=0.6, \nlabel='True effect')\naxs[3].plot(xs, true_effect, color='k', alpha=0.6,\nlabel='True effect')\naxs[0].legend()\naxs[1].legend()\naxs[2].legend()\naxs[3].legend();\n\n\n\n\n\nNote how the naive mundlak approach also exhibits odd behaviour as we saw in the TWFE estimation routine above. Adding additional interactions and controlling for the staggered launch dates seems to help isolate the real pattern in the data. We’ve seen approximate success in a number of these richly parameterised versions of simple event study regressions. But the idiosyncracies of any one sample will contort and distort the estimates away from the true values. We might hope for more stability in expectation over repeated draws from this kind of data-generating process. Consider the following Bootstrapping estimation routine.\n\n\nEvaluating Robustness of Functional Form\n\n\nCode\nfig, axs = plt.subplots(2, 1, figsize=(10, 15))\naxs = axs.flatten()\nevent_coefs = []\nevent_coefs1 = []\n\ndef fit_ols(formula, df_het):\n    twfe_model_ols = smf.ols(formula, data=df_het).fit()\n    twfe_model_ols.summary()\n    param_est = pd.DataFrame(twfe_model_ols.params, columns=['estimate']).iloc[1:-1]\n    param_est['index_number'] = list(range(len(param_est)))\n    temp = (param_est.reset_index())\n    param_est = temp[(~temp['index'].str.contains(':')) & (temp['index'].str.contains('rel'))]\n    param_est.reset_index(inplace=True)\n    try:\n        param_est['rel_year'] =(param_est['index'].str.split('[', expand=True)[1].str.replace('T.', '')\n        .str.replace(']', ''))\n    except Exception as e:\n        param_est['rel_year'] = param_est['level_0'] - 22\n    param_est['rel_year'] = param_est['rel_year'].astype(int)\n    param_est.set_index('rel_year', inplace=True)\n    return param_est\n\nfor i in range(100):\n    df_het = make_data(500, 40)\n    df_het['state_mean'] = df_het.groupby('state')['treat'].transform(np.mean)\n    df_het['time_mean'] = df_het.groupby('year')['treat'].transform(np.mean)\n    df_het['cohort_mean'] = df_het.groupby('cohort_year')['treat'].transform(np.mean)\n    df_het['unit_mean'] = df_het.groupby('unit')['treat'].transform(np.mean)\n\n    formula = \"\"\" dep_var ~ 1 + time_mean + state_mean + cohort_mean  + C(rel_year, Treatment(reference=-1)) \"\"\"\n    formula1 = \"\"\" dep_var ~ 1 + C(cohort_year) + time_mean:unit_mean + C(rel_year, Treatment(reference=-1))\"\"\"\n\n    param_est = fit_ols(formula, df_het)\n    axs[0].plot(param_est['estimate'], color='blue', alpha=0.3)\n    event_coefs.append(param_est['estimate'])\n    param_est1 = fit_ols(formula1, df_het)\n    axs[1].plot(param_est1['estimate'], color='blue', alpha=0.3)\n    event_coefs1.append(param_est1['estimate'])\n\nbootstrap_df = pd.DataFrame(event_coefs)\nmean_df = bootstrap_df.mean(axis=0)\nmean_df.index = param_est.index\nmean_df.plot(ax=axs[0], color='red')\naxs[0].set_title(f\"\"\"Bootstrapped Estimates of Demean Event Study DGP \\n {formula}\"\"\")\n\nbootstrap_df1 = pd.DataFrame(event_coefs1)\nmean_df = bootstrap_df1.mean(axis=0)\nmean_df.index = param_est1.index\nmean_df.plot(ax=axs[1], color='red')\naxs[1].set_title(f\"\"\"Bootstrapped Estimates of Event Study DGP \\n\n{formula1}\"\"\");\n\n\n\n\n\nHere we see the importance of robustness tests in estimation techniques. In the above plot we see something like the correct pattern emerging in expectation, but absurd estimates occuring on individual cases. Whereas the bottom plot appears much more consistent across multiple draws from the data generating process. This points to something about the justification required for a credible result. Asymptotic accuracy is generally not sufficient for inducing credibility in any particular analysis of finite data.\nWe can also compare these models on standard adequacy measures.\n\naz.compare({'fe_mundlak_naive': idata_twfe, \n            'mundlak_state_time_interactions_cohort': idata_twfe1, \n            'mundlak_cohort_state_interactions': idata_twfe2, \n            'mundlak_state_hierarchical_year': idata_twfe3})\n\n\n\n\n\n  \n    \n      \n      rank\n      elpd_loo\n      p_loo\n      elpd_diff\n      weight\n      se\n      dse\n      warning\n      scale\n    \n  \n  \n    \n      mundlak_state_hierarchical_year\n      0\n      -35912.100213\n      71.825690\n      0.000000\n      0.801341\n      72.022818\n      0.000000\n      False\n      log\n    \n    \n      mundlak_state_time_interactions_cohort\n      1\n      -36427.310049\n      53.130023\n      515.209836\n      0.198691\n      74.337703\n      41.178348\n      False\n      log\n    \n    \n      mundlak_cohort_state_interactions\n      2\n      -38254.123775\n      47.009205\n      2342.023562\n      0.000023\n      78.060258\n      49.592305\n      False\n      log\n    \n    \n      fe_mundlak_naive\n      3\n      -46324.432551\n      38.486619\n      10412.332337\n      0.000000\n      86.756424\n      97.412077\n      False\n      log\n    \n  \n\n\n\n\nAgain, we can plot the predictive performance of these models. Here we compare two via posterior predictive checks. Notable we see how the model with the better predictive performance also correctly estimates the treatement effects in the above plots.\n\nmodel_twfe3.predict(idata_twfe3, kind='pps')\nmodel_twfe.predict(idata_twfe, kind='pps')\nfig, axs = plt.subplots(2, 1, figsize=(10, 12))\naxs = axs.flatten()\naz.plot_ppc(idata_twfe3, ax=axs[0])\naxs[0].set_title(\"PPC checks for Mundlak State Hierarchical Year\")\naxs[1].set_title(\"PPC checks for FE Mundlak Naive\")\naz.plot_ppc(idata_twfe, ax=axs[1]);\n\n\n\n\nIt’s certainly possible that parameter recovery accuracy and predictive performance come apart. But in lieu of knowledge of the true data generating processes, the route to credible models is via their calibration against real data."
  },
  {
    "objectID": "posts/post-with-code/multilevel_confounding/multilevel_models.html#conclusion",
    "href": "posts/post-with-code/multilevel_confounding/multilevel_models.html#conclusion",
    "title": "Freedom, Hierarchies and Confounded Estimates",
    "section": "Conclusion",
    "text": "Conclusion\n\n“A schema may be transported almost anywhere. The choice of territory for invasion is arbitrary; but the operation within that territory is almost never completely so.” - Nelson Goodman in Languages of Art\n\nWe’ve now seen various cases of group-level confounding and strategies to address these issues. We’ve seen how they give rise to non-trivial questions about the effects of policy, and what can be learned in the face of group-level confounds. The case of TWFE’s estimation is especially instructive - here we have an estimation technique applied well in limited circumstances, which ran into trouble when applied in contexts with a diverging data generating process. We all lose our way some of the time, but this misstep led us off the map - we’d followed the trail well past the intended territory.\nStatistical models do not mirror the world, they project patterns in the data. Each rupture between model and world is a gift that highlights deficits in our understanding - an impropriety of projections. It is therefore a delightful reversal to see, in Woolridge’s proof, that the issue is not with the tooling but the thoughtless application of the tool. The unreasonable effectiveness of regression modelling is surely driven by projections of linearity on the world, but it remains compelling in approximation when used well. There are architectural constraints in causal inference - hard unyielding facts, that shape the contours of our design, and structure the steps we can make. The degree to which we can flexibly encode those structures in our models, the more compelling our models will be. It is this recognition of dual fit requirements between world and model - the focus on the conditions for identifiability - that lends causal inference any kind of credibility. The expressive capacity of generalised linear modelling enables us to articulate too many candidate pictures of the world. It’s the business of science to cull those pictures, to find only the plausible representations of reality."
  },
  {
    "objectID": "posts/post-with-code/multilevel_confounding/multilevel_models.html#estimation-strategies-and-group-effects",
    "href": "posts/post-with-code/multilevel_confounding/multilevel_models.html#estimation-strategies-and-group-effects",
    "title": "Freedom, Hierarchies and Confounded Estimates",
    "section": "Estimation Strategies and Group Effects",
    "text": "Estimation Strategies and Group Effects\nFirst consider an estimation example due to Richard McElreath’s lecture series where we examine the various parameter recovery options available in the case of group level confounding. We want to move through these parameterisations to highlight the usefulness of Mundlak regressions. The source of these models is a paper Mundlak’s 1978 paper “On the pooling of time series and cross section data” in which he shows a method for adjusting for group-levels which proves to be a viable alternative to richly specified fixed effects model.\nFirst define a data generating process determined by group level effects:\n\ndef inv_logit(p):\n    return np.exp(p) / (1 + np.exp(p))\n\nN_groups = 30\nN_id = 3000\na0 = -2\nbXY = 1\nbZY = -0.5\ng = np.random.choice(range(N_groups), size=N_id, replace=True)\nUg = np.random.normal(1.5, size=N_groups)\n\nX = np.random.normal(Ug[g], size=N_id)\nZ = np.random.normal(size=N_groups)\n\ns = a0 + bXY*X + Ug[g] + bZY*Z[g]\np = inv_logit(s)\nY = np.random.binomial(n=1, p=p)\n\n\nsim_df = pd.DataFrame({'Y': Y, 'p': p, 's': s, 'g': g, 'X': X, 'Z': Z[g]})\nsim_df.head()\n\n\n\n\n\n  \n    \n      \n      Y\n      p\n      s\n      g\n      X\n      Z\n    \n  \n  \n    \n      0\n      1\n      0.943243\n      2.810542\n      8\n      1.384072\n      -1.818302\n    \n    \n      1\n      0\n      0.593872\n      0.379996\n      24\n      1.047128\n      -0.242817\n    \n    \n      2\n      1\n      0.874945\n      1.945406\n      3\n      2.340418\n      -0.650122\n    \n    \n      3\n      0\n      0.317089\n      -0.767181\n      7\n      0.550504\n      0.795680\n    \n    \n      4\n      1\n      0.699030\n      0.842684\n      23\n      1.183011\n      -0.111692\n    \n  \n\n\n\n\nThis data generating process is characterised by the bernoulli outcome with group level confounds on the \\(X\\) variable. If we model this relationship the confounding effects will bias naive parameter estimates on the covariates \\(X\\), \\(Z\\). We can imagine this is a class room setting with group-confounding due to teacher effects. In a DAG we see a picture something like this:\n\n\nCode\ngraph = nx.DiGraph()\ngraph.add_edges_from([(\"classroom \\n G\", \"scores \\n y\"), (\"classroom \\n G\", \"student prep \\n X\"), (\"student prep \\n X\", \"scores \\n y\"), (\"class temp \\n Z\", \"scores \\n y\")])\nfig, ax = plt.subplots(figsize=(10,6))\nnx.draw_networkx(graph, arrows=True, ax=ax, \nnode_size = 6000, font_color=\"whitesmoke\",pos={'classroom \\n G': [0, 0], 'scores \\n y': [0.5, 1], 'student prep \\n X': [0, 2], \n'class temp \\n Z': [1, 1]})\nax.set_title(\"Group Confounding in a DAG\", fontsize=12);\n\n\n\n\n\nWe can plot the data here to visualise the shares of the binary outcomes.\n\ng = sns.pairplot(sim_df[['Y', 'X', 'Z']], diag_kind=\"kde\", corner=True)\ng;\n\n\n\n\nWe will see different estimation results as we explore different ways of parameterising the relationships, and the trade-offs for model specifications will become clearer.\n\n\n\n\n\n\nWarning\n\n\n\nThere is a huge degree of confusion over the meaning of the terms “Fixed Effects” and “Random Effects”. Within this blog post when we refer to fixed effects we will mean the population level parameters. \\[\\beta X\\]\nIn contrast we will refer to group-level parameters \\(\\beta_{g}\\)\n\\[\\Big(\\underbrace{\\beta}_{pop} + \\underbrace{\\beta_{g}}_{group}\\Big)X\\]\nwhich are incorporated into our model equation modifying population level parameters as random effects. We will generally use Wilkinson notation to specify these choices. Random effects for modifying a population parameter X are denoted with a conditional bar over the variable ( X | group) and fixed effects are specified by just including the variable in the equation i.e. y ~ X + (Z | group) where X has a fixed effect parameterisation and Z a random effects parameterisation. We can also create indicator variables for group membership using this syntax with y ~ C(group) + X + Z where under the hood we pivot the group category into a zero-one variables indicating group membership. This parameterisation means each indicator variable (one for each level of the grouping variable) will receive a fixed effects population parameter.\n\n\n\nNaive Model\nWe will use Bambi to specify the majority of these generalised linear models throughout this post. The formula syntax for Bambi enables us to specify generalised linear models with bernoulli outcomes.\n\nnaive_model = bmb.Model(f\"Y['>.5'] ~ X + Z \", sim_df, \nfamily=\"bernoulli\")\nnaive_idata = naive_model.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True},)\n\n\naz.summary(naive_idata, var_names=['Intercept', 'X', 'Z'])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      -0.991\n      0.076\n      -1.129\n      -0.841\n      0.001\n      0.001\n      4859.0\n      3143.0\n      1.0\n    \n    \n      X\n      1.308\n      0.053\n      1.210\n      1.410\n      0.001\n      0.001\n      3449.0\n      2770.0\n      1.0\n    \n    \n      Z\n      -0.614\n      0.048\n      -0.702\n      -0.522\n      0.001\n      0.001\n      3822.0\n      3033.0\n      1.0\n    \n  \n\n\n\n\nHere we see that all three parameter estimates are biased away from their true values. Let’s try a simple fixed effects approach that adds indicator variables for all but one of the group levels.\n\n\nFixed Effects Model\nThe additional syntax for creating the group level indicator variables is specified here. This will create a host of indicator variables for membership in each level of the group variable.\n\nfixed_effects_model = bmb.Model(f\"Y['>.5'] ~ C(g) + X + Z\", sim_df, \nfamily=\"bernoulli\")\nfixed_effects_idata = fixed_effects_model.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True},)\n\n\naz.summary(fixed_effects_idata, var_names=['Intercept', 'X', 'Z'])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      -0.386\n      1.505\n      -3.181\n      2.308\n      0.086\n      0.061\n      307.0\n      552.0\n      1.02\n    \n    \n      X\n      0.977\n      0.059\n      0.872\n      1.097\n      0.001\n      0.001\n      2635.0\n      2189.0\n      1.00\n    \n    \n      Z\n      -0.008\n      1.408\n      -2.684\n      2.501\n      0.079\n      0.056\n      314.0\n      565.0\n      1.01\n    \n  \n\n\n\n\nNow we see that the coefficient on the \\(X\\) variable seems correct, but the coefficient on \\(Z\\) is wildly wrong. Indeed the uncertainty interval on the \\(Z\\) coefficient is huge. The fixed effect model was unable to learn anything about the correct parameter. Whereas the naive model seems to learn the correct \\(Z\\) parameter but over estimates the \\(X\\) coefficient. These are the kinds of trade-offs we need to be wary of as we account for the complexities of extensive group interactions in our model’s functional form.\n\n\nCode\nfig, axs = plt.subplots(2, 1, figsize=(10, 9))\naxs = axs.flatten()\n\naz.plot_posterior(naive_idata, var_names=['X'], ax=axs[0], \npoint_estimate=None,  hdi_prob='hide', color='red', label='Naive Model')\naxs[0].axvline(1, color='k')\n\naz.plot_posterior(fixed_effects_idata , var_names=['X'], ax=axs[0], point_estimate=None, hdi_prob='hide', label='Fixed Effect Models')\naxs[0].set_title(\"Naive/Fixed Model X Coefficient\")\n\naz.plot_posterior(naive_idata, var_names=['Z'], ax=axs[1], point_estimate=None,  hdi_prob='hide', color='red', ref_val_color='black')\n\n\naz.plot_posterior(fixed_effects_idata , var_names=['Z'], ax=axs[1], point_estimate=None, hdi_prob='hide')\n\naxs[1].set_title(\"Naive/Fixed Effect Model Z Coefficient\")\naxs[1].axvline(-0.5, color='k');\n\n\n\n\n\nWe now want to try another approach to handle to the group confounding that involves a hierarchical approach - adding group level effects to the population intercept term.\n\n\nMultilevel Model\nThe syntax for the random effects model is easily expressed in bambi too. These group level effects modify the intercept additively and are collectively drawn from a shared distribution implicit in the model.\n\nmultilevel_model = bmb.Model(f\"Y['>.5'] ~ (1 | g) + X + Z\", sim_df, \nfamily=\"bernoulli\")\nmultilevel_model_idata = multilevel_model.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True},)\n\nThis method posits a view that there is a shared underlying process generating each instance of group-behaviour i.e. the realised values of the outcome within each group. In the random effects multilevel model we replace the indivdual fixed effects indicator columns with additional parameters modifying the intercept term. This is the “hierarchy” or multi-level aspect of the specification.\n\naz.summary(multilevel_model_idata, var_names=['X', 'Z', 'Intercept'])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      X\n      1.023\n      0.059\n      0.914\n      1.134\n      0.001\n      0.001\n      4641.0\n      3115.0\n      1.0\n    \n    \n      Z\n      -0.606\n      0.169\n      -0.918\n      -0.277\n      0.006\n      0.004\n      868.0\n      1672.0\n      1.0\n    \n    \n      Intercept\n      -0.573\n      0.201\n      -0.940\n      -0.180\n      0.007\n      0.005\n      781.0\n      1053.0\n      1.0\n    \n  \n\n\n\n\nNext we’ll apply the Mundlak device method which adds the group mean back to each observation as a single additional covariate.\n\n\nMundlak Model\nFor this technique we augment the model by supplying group an additional column. This column records the group-level means of the “confounded variable” i.e. the variable which is realised under group level influences. Adding the group mean(s) to the model in this sense somewhat absolves of the requirement to include an extra multiplicity of parameters. It’s more akin to feature creation than model specification, but it serves the same purpose - it accounts for group level variation and provides a mechanism for the model to learn the appropriate weights to accord each group in the final calculation.\n\nsim_df['group_mean'] = sim_df.groupby('g')['X'].transform(np.mean)\n\nsim_df['group_mean_Z'] = sim_df.groupby('g')['Z'].transform(np.mean)\n\nmundlak_model = bmb.Model(f\"Y['>.5'] ~ (1 | g) + X + Z + group_mean\", sim_df, \nfamily=\"bernoulli\")\nmundlak_idata = mundlak_model.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True},)\n\nMundlak’s insight is that this mechanism is akin to adding a whole slew of fixed-effect indicator variables for each group. It is a corrective mechanism for group confounding in our focal variable. We’ll see how it allows for more flexible model specifications and has a role in making adjustments to aid identification in causal inference.\n\naz.summary(mundlak_idata, var_names=['Intercept', 'X', 'Z'])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      -1.906\n      0.141\n      -2.174\n      -1.649\n      0.002\n      0.002\n      4017.0\n      2950.0\n      1.0\n    \n    \n      X\n      0.952\n      0.059\n      0.842\n      1.062\n      0.001\n      0.001\n      4442.0\n      3188.0\n      1.0\n    \n    \n      Z\n      -0.443\n      0.064\n      -0.569\n      -0.330\n      0.001\n      0.001\n      3206.0\n      2967.0\n      1.0\n    \n  \n\n\n\n\nWe can now plot all the parameter recovery models together and we’ll see that there are some trade-offs between the fixed effects and random effects varieties of the modelling.\n\n\nPlotting the Comparisons\nThe parameter recovery exercise shows striking differences across each of the models\n\n\nCode\nfig, axs = plt.subplots(3, 1, figsize=(10, 11))\naxs = axs.flatten()\n\naz.plot_posterior(naive_idata, var_names=['X'], ax=axs[0], \npoint_estimate=None, color='red', label='Naive', hdi_prob='hide')\naxs[0].axvline(1, color='k', linestyle='--', label='True value')\n\naz.plot_posterior(fixed_effects_idata , var_names=['X'], ax=axs[0], point_estimate=None, hdi_prob='hide', label='Fixed')\n\naz.plot_posterior(multilevel_model_idata, var_names=['X'], ax=axs[0], point_estimate=None, hdi_prob='hide', color='green', label='Hierarchical')\n\naz.plot_posterior(mundlak_idata, var_names=['X'], ax=axs[0], point_estimate=None, hdi_prob='hide', color='purple', label='Mundlak')\n\n\naxs[0].set_title(\"X Coefficient\")\n\naz.plot_posterior(naive_idata, var_names=['Z'], ax=axs[1], point_estimate=None, hdi_prob='hide',  color='red', ref_val_color='black')\n\n\naz.plot_posterior(fixed_effects_idata , var_names=['Z'], ax=axs[1], point_estimate=None, hdi_prob='hide')\n\naz.plot_posterior(multilevel_model_idata, var_names=['Z'], ax=axs[1], point_estimate=None, hdi_prob='hide', color='green')\n\naz.plot_posterior(mundlak_idata, var_names=['Z'], ax=axs[1], point_estimate=None, hdi_prob='hide', color='purple')\n\naxs[1].set_title(\"Z Coefficient\")\naxs[1].axvline(-0.5, color='k', linestyle='--');\n\naz.plot_posterior(naive_idata, var_names=['Intercept'], ax=axs[2], point_estimate=None,  color='red', ref_val_color='black', hdi_prob='hide')\n\n\naz.plot_posterior(fixed_effects_idata , var_names=['Intercept'], ax=axs[2], point_estimate=None, hdi_prob='hide')\n\naz.plot_posterior(multilevel_model_idata, var_names=['Intercept'], ax=axs[2], point_estimate=None, hdi_prob='hide', color='green')\n\naz.plot_posterior(mundlak_idata, var_names=['Intercept'], ax=axs[2], point_estimate=None, hdi_prob='hide', color='purple')\n\naxs[2].axvline(-2, color='k', linestyle='--');\n\n\n\n\n\nImportantly, we see that while the fixed effects model is focused on recovering the treatment effect on the \\(X\\) covariate, it does so somewhat at the expense of accuracy on the other systematic components of the model. This focus renders the model less predictively accurate. If we compare the models on the cross-validation score, we see how the hierarchical mundlak model is to be preferred.\n\ncompare_df = az.compare({'naive': naive_idata, 'fixed': fixed_effects_idata, 'hierarchical': multilevel_model_idata, \n'mundlak': mundlak_idata})\ncompare_df\n\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/stats.py:805: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n\n\n\n\n\n\n  \n    \n      \n      rank\n      elpd_loo\n      p_loo\n      elpd_diff\n      weight\n      se\n      dse\n      warning\n      scale\n    \n  \n  \n    \n      mundlak\n      0\n      -1209.532733\n      12.255178\n      0.000000\n      0.951092\n      30.213512\n      0.000000\n      False\n      log\n    \n    \n      hierarchical\n      1\n      -1218.446608\n      27.713536\n      8.913876\n      0.025741\n      30.404197\n      4.443081\n      False\n      log\n    \n    \n      fixed\n      2\n      -1221.467513\n      33.823574\n      11.934780\n      0.000000\n      31.743164\n      5.184940\n      True\n      log\n    \n    \n      naive\n      3\n      -1295.325039\n      2.808180\n      85.792306\n      0.023167\n      29.474596\n      12.972671\n      False\n      log\n    \n  \n\n\n\n\n\naz.plot_compare(compare_df);\n\n\n\n\nThis is not the only way to assess viability of the model’s functional form but it’s not a bad way.\n\n\nFull Luxury Bayesian Mundlak Machine\nAs good Bayesians we might be worry about the false precision of adding simple point estimates for the group mean covariates in the Mundlak model. We can remedy this by explicitly incorporating these values as an extra parameter and adding uncertainty to the draws on these parameters.\n\nid_indx, unique_ids = pd.factorize(sim_df[\"g\"])\n\ncoords = {'ids': list(range(N_groups))}\nwith pm.Model(coords=coords) as model: \n\n    x_data = pm.Data('X_data', sim_df['X'])\n    z_data = pm.Data('Z_data', sim_df['Z'])\n    y_data = pm.Data('Y_data', sim_df['Y'])\n\n    alpha0 = pm.Normal('Intercept', 0, 1)\n    alpha_j = pm.Normal('alpha_j', 0, 1, dims='ids')\n    beta_xy = pm.Normal('X', 0, 1)\n    beta_zy = pm.Normal('Z', 0, 1)\n\n    group_means = pm.Normal('group_means', sim_df.groupby('g')['X'].mean().values, .1, dims='ids')\n    group_sigma = pm.HalfNormal('group_sigma', 0.1)\n    \n    group_mu = pm.Normal('group_mu', (group_means[id_indx]), group_sigma, observed=sim_df['X'].values)\n\n    mu = pm.Deterministic('mu', (alpha0 + alpha_j[id_indx]) + beta_xy*x_data + beta_zy*z_data + group_means[id_indx])\n    p = pm.Deterministic(\"p\", pm.math.invlogit(mu))\n    # likelihood\n    pm.Binomial(\"y\", n=1, p=p, observed=y_data)\n\n    idata = pm.sample(idata_kwargs={\"log_likelihood\": True})\n\nInstead of merely adding a new feature to a regression we’ve added a second likelihood term for the predictor \\(X\\) and seek to model this outcome as a function of the group means. In other words, we incorporate the idea that \\(X\\) is confounded by some aspect of the group membership, and seek to estimate the nature of that confounding as it influences both the realisations of \\(X\\) and \\(y\\). Calibrating the group_mean parameter that is also included in our likelihood term for the \\(Y\\) outcome.\n\npm.model_to_graphviz(model)\n\n\n\n\nThis model bakes more uncertainty into the process assuming a kind of measurement-error model, but again, the focus here is that we need to estimate a corrective adjustment factor for confounding effects that impact the estimation of impact due to our focal variable \\(X\\). It’s perhaps easier to see in this parameter recovery exercise than it is in real cases, but wherever we suspect plausible group level confounding you should consider Mundlak adjustments alongside your go-to fixed effects or random effects controls.\n\naz.summary(idata, var_names=['Intercept', 'X', 'Z'])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      -1.876\n      0.204\n      -2.248\n      -1.481\n      0.007\n      0.005\n      866.0\n      1519.0\n      1.0\n    \n    \n      X\n      0.990\n      0.060\n      0.871\n      1.099\n      0.001\n      0.001\n      6210.0\n      3050.0\n      1.0\n    \n    \n      Z\n      -0.523\n      0.175\n      -0.840\n      -0.185\n      0.005\n      0.004\n      1091.0\n      1951.0\n      1.0\n    \n  \n\n\n\n\nYou can see it also recovers the correct parameter specifcation well.\n\n\nRecap\nWe’ve just seen a number of different modelling specifications that attempt to deal with the threat of group level confounding. Some varieties of model result in entirely saturated regression models with more parameters than observations. These models can quickly become unwieldy. We’ll now look at a case-study where group level confounding remains highly plausible and the choice of estimation routine is not trivial. This will highlight aspects of how what we seek learn determines our model choice."
  },
  {
    "objectID": "posts/post-with-code/multilevel_confounding/multilevel_models.html#nested-groups-hierarchies-and-fixed-effects",
    "href": "posts/post-with-code/multilevel_confounding/multilevel_models.html#nested-groups-hierarchies-and-fixed-effects",
    "title": "Freedom, Hierarchies and Confounded Estimates",
    "section": "Nested Groups, Hierarchies and Fixed Effects",
    "text": "Nested Groups, Hierarchies and Fixed Effects\nWe’ve seen how various attempts to account for the group effects can more or less recover the parameters of a complex data generating process with group confounding. Now we want to look at a case where we can have interacting group effects at multiple levels.\n\n\n\nStructure\n\n\n\nPupils within Class Rooms within Schools\nA natural three level group hierarchy occurs in the context of educational organisations and business org-charts. We can use this fact to interrogate briefly how inferential statements about treatment effects vary as a function of what and how we control for group level variation. We draw the following data set from Linear Mixed Models: A Practical Guide Using Statistical Software.\n\ndf = pd.read_csv('classroom.csv')\ndf['class_mean'] = df.groupby(['classid'])['mathprep'].transform(np.mean)\ndf['school_mean'] = df.groupby(['schoolid'])['mathprep'].transform(np.mean)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      sex\n      minority\n      mathkind\n      mathgain\n      ses\n      yearstea\n      mathknow\n      housepov\n      mathprep\n      classid\n      schoolid\n      childid\n      class_mean\n      school_mean\n    \n  \n  \n    \n      0\n      1\n      1\n      448\n      32\n      0.46\n      1.0\n      NaN\n      0.082\n      2.00\n      160\n      1\n      1\n      2.00\n      2.909091\n    \n    \n      1\n      0\n      1\n      460\n      109\n      -0.27\n      1.0\n      NaN\n      0.082\n      2.00\n      160\n      1\n      2\n      2.00\n      2.909091\n    \n    \n      2\n      1\n      1\n      511\n      56\n      -0.03\n      1.0\n      NaN\n      0.082\n      2.00\n      160\n      1\n      3\n      2.00\n      2.909091\n    \n    \n      3\n      0\n      1\n      449\n      83\n      -0.38\n      2.0\n      -0.11\n      0.082\n      3.25\n      217\n      1\n      4\n      3.25\n      2.909091\n    \n    \n      4\n      0\n      1\n      425\n      53\n      -0.03\n      2.0\n      -0.11\n      0.082\n      3.25\n      217\n      1\n      5\n      3.25\n      2.909091\n    \n  \n\n\n\n\nThe data has three distinct levels: (1) the child or pupil and their demographic attributes and outcome variable mathgain, (2) the classroom and the teacher level attributes such as their experience yearstea and a record of their mathematics courses taken mathprep, (3) school and neighbourhood level with features describing poverty measures in the vicinity housepov.\nWe’ll plot the child’s outcome mathgain against the mathprep and distinguish the patterns by school.\n\n\nCode\ndef rand_jitter(arr):\n    stdev = .01 * (max(arr) - min(arr))\n    return arr + np.random.randn(len(arr)) * stdev\n\nlegend_elements = [Line2D([0], [0], marker='x', label='Minority', markerfacecolor='k'),\n                   Line2D([0], [0], marker='x', label='Minority + Poverty', markersize=14, markerfacecolor='k'), \n                   ]\n\nschools = df['schoolid'].unique()\nschools_10 = [schools[i:i+10] for i in range(0, len(schools), 10)]\nfig, axs = plt.subplots(3,4, figsize=(15, 13), \nsharey=True, sharex=True)\naxs = axs.flatten()\nmkr_dict = {1: 'x', 0: '+'}\nfor s, ax in zip(schools_10, axs):\n    temp = df[df['schoolid'].isin(s)]\n    temp['m'] = temp['minority'].map(mkr_dict)\n    for m in temp['m'].unique():\n        temp1 = temp[temp['m'] == m]\n        ax.scatter(rand_jitter(temp1['mathprep']), \n        temp1['mathgain'], \n        c=temp1['schoolid'], cmap='tab10', \n        s=temp1['housepov']*100, \n        marker = m)\n    ax.set_title(f\"Schools \\n {s}\");\n    ax.set_xlabel(\"MathPrep\")\n    ax.set_ylabel(\"MathGain\")\n\naxs[0].legend(handles=legend_elements);\n\n\n\n\n\nWe’ve plotted here the individual student outcomes. We’ve sized the dots by the poverty in their neighbourhoods and differentiated the markers by whether the student was in minority group. There are, in short, reasons here to worry about group-level confounding. There is a small number of observed students per school so the individual school level distributions show some extreme outliers but the overall distribution nicely converges to an approximately normal symmetric shape.\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 6))\naxs = axs.flatten()\nfor school in schools:\n    temp = df[df['schoolid'] ==school]\n    axs[0].hist(temp['mathgain'], color='grey', alpha=0.3, density=True, histtype='step', cumulative=True)\n\naxs[0].hist(df['mathgain'], bins=30, ec='black', density=True, cumulative=True, histtype='step')\naxs[1].hist(df['mathgain'], bins=30, ec='black', density=True, cumulative=False)\naxs[0].set_title(\"Cumulative Distribution Function by School\")\naxs[1].set_title(\"Overall Distribution\");\n\n\n\n\nWith these kinds of structures we need to be careful in how we evaluate any treatment effects when there are reasons to believe in group-level effects that impact the outcome variable. Imagine the true treatment effect is a sprinter running too fast to cleanly measure - each group interaction effect is added to his load as a weight. Enough weights absorb enough of the variation in the treatment that he is dragged to a crawl. Whatever movement he can make while dragging this burden is the effect we attribute to the treatment. Or put another way - the effects of various group interactions will modify the treatment effectiveness in some way and unless we account for this impact in our model, then the inferences regarding the treatment will be clouded.\n\n\nInteraction Effects and Nuisance Parameters\nNot all possible interactions will be present in the data. But if we specify the model to account for various interactions we may explode the number of parameters beyond the number of data points. This can cause issues in estimation routines and requires consideration about what the group parameters are aimed at capturing.\nWhy add covariates for group-membership when no observation reflects outcomes in that group? We cannot learn anything about these cases. At least if we add the group effects as a hierarchical random effect we induce shrinkage on the parameter estimates towards the mean of the hierarchical parameter in the model. This means that when predicting on “new” data with examples of these missing cases we can predict a sensible default. The distinction rests in the role we have in mind for these tools. If we seriously commit to the idea that group variation reflects a real “common” process in the larger population and we want to learn about that over-arching process then we deploy a random effects model. But if we only see these as tools for accounting to variation in the sample, allowing us to pin down an alternative focal estimate then the group indicator covariates are just “nuisance” parameters and missing cases are irrelevant.\n\n\n\n\n\n\nPhilosophical Digression\n\n\n\nThis last point skips a little quickly over a fundamental feature of interpreting these models. If we aim to interpret these models as reflecting a common process across these groups that exists in a “population”, then we’re endorsing an inferential view that extends beyond the sample. We’re actively seeking to learn a general truth about the data generating process which we deem to be adequately expressed in our model. If we seek to “soak up” the variation due to group effects, we’re treating these group effects as noise in the sample data and making inferential commitments only about the focal parameter in the model. This approach to learning differentiates approaches to credible causal inference. On the one hand, fixing your estimand and designing estimators to specifically capture that estimate seems like a modest and compelling strategy. On the other hand if your model ignores aspects of underlying phenomena or fails to retrodict the observable data, it’s dubious as to why anyone would trust its output.\n\n\nTo see the extent of redundany we can examine the dimensions of the covariate matrices that result from including more or less interaction terms.\n\ny, X = dmatrices(\"mathgain ~ mathprep + C(schoolid)+ C(classid)\", df, return_type=\"dataframe\")\nprint(X.shape)\n\ny, X1 = dmatrices(\"mathgain ~ mathprep + C(schoolid)/C(classid)\", df, return_type=\"dataframe\")\nprint(X1.shape)\n\n\ny, X2 = dmatrices(\"mathgain ~ mathprep + C(schoolid):C(childid)\", df, return_type=\"dataframe\")\nprint(X2.shape)\n\n(1190, 419)\n\n\n(1190, 33385)\n\n\n(1190, 127331)\n\n\nWe see here how different ways in which to account for group level variation and interaction effects lead to vastly inflated feature matrices. However not all interaction terms matter, or put another way… nor all the possible interactions feature in the data. So we have likely inflated the data matrix beyond necessity.\nHere we define a helper function to parse a complex interaction formula, remove the columns entirely composed of zeros and return a new formula and dataframe which has a suitable range of features to capture the variation structures in the data.\n\ndef make_interactions_df(formula, df):\n    y, X = dmatrices(formula, df, return_type=\"dataframe\")\n    n = X.shape[1]\n    X = X[X.columns[~(np.abs(X) < 1e-12).all()]]\n    n1 = X.shape[1]\n    target_name = y.columns[0]\n    d = pd.concat([y, X], axis=1)\n    d.drop(['Intercept'], axis=1, inplace=True)\n    d.columns = [c.replace('[', '').replace(']','').replace('C(', '').replace(')', '').replace('.', '_').replace(':', '_') for c in d.columns]\n    cols = ' + '.join([col for col in d.columns if col != target_name])\n    formula = f\"{target_name} ~ {cols}\"\n    print(f\"\"\"Size of original interaction features: {n} \\nSize of reduced feature set: {n1}\"\"\")\n    return formula, d\n\nformula, interaction_df = make_interactions_df(\"mathgain ~ mathprep + C(schoolid):C(childid)\", df)\n\ninteraction_df.head()\n\nSize of original interaction features: 127331 \nSize of reduced feature set: 2370\n\n\n\n\n\n\n  \n    \n      \n      mathgain\n      childidT_2\n      childidT_3\n      childidT_4\n      childidT_5\n      childidT_6\n      childidT_7\n      childidT_8\n      childidT_9\n      childidT_10\n      ...\n      schoolidT_107_childid1182\n      schoolidT_107_childid1183\n      schoolidT_107_childid1184\n      schoolidT_107_childid1185\n      schoolidT_107_childid1186\n      schoolidT_107_childid1187\n      schoolidT_107_childid1188\n      schoolidT_107_childid1189\n      schoolidT_107_childid1190\n      mathprep\n    \n  \n  \n    \n      0\n      32.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2.00\n    \n    \n      1\n      109.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2.00\n    \n    \n      2\n      56.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2.00\n    \n    \n      3\n      83.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      3.25\n    \n    \n      4\n      53.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      3.25\n    \n  \n\n5 rows × 2370 columns\n\n\n\nWe have reduced the number of interactions by an order of magnitude! We can now fit a regression model to the revised feature matrix.\n\n\nComparing Interaction Models\nConsider the variation in the coefficient values estimated for mathprep as we add more and more interaction effects. The addition of interaction effects generates a large number of completely 0 interaction terms which we remove here.\n\nformulas = [\"\"\"mathgain ~ mathprep + C(schoolid)\"\"\",\n\"\"\" mathgain ~ mathprep + school_mean*class_mean\"\"\" , \n\"\"\" mathgain ~ mathprep + mathkind + sex + school_mean*class_mean\"\"\" , \n\"\"\"mathgain ~ mathprep + C(schoolid) + C(classid)\"\"\", \n\"\"\"mathgain ~ mathprep + C(schoolid)*C(classid)\"\"\",\n\"\"\"mathgain ~ mathprep + C(classid):C(childid)\"\"\", \n]\n\nestimates_df = []\nfor f in formulas:\n    formula, interaction_df = make_interactions_df(f, df)\n    result = smf.ols(formula, interaction_df).fit()\n    estimates = [[result.params['mathprep']], list(result.conf_int().loc['mathprep', :]), [formula]]\n    estimates = [e for est in estimates for e in est]\n    estimates_df.append(estimates)\n\nestimates_df = pd.DataFrame(estimates_df, columns=['mathprep_estimate', 'lower bound', 'upper bound', 'formula'])\n\nestimates_df\n\nSize of original interaction features: 108 \nSize of reduced feature set: 108\nSize of original interaction features: 5 \nSize of reduced feature set: 5\nSize of original interaction features: 7 \nSize of reduced feature set: 7\nSize of original interaction features: 419 \nSize of reduced feature set: 419\n\n\nSize of original interaction features: 33385 \nSize of reduced feature set: 728\n\n\nSize of original interaction features: 371281 \nSize of reduced feature set: 2376\n\n\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:1717: RuntimeWarning: divide by zero encountered in scalar divide\n  return np.dot(wresid, wresid) / self.df_resid\n\n\n\n\n\n\n  \n    \n      \n      mathprep_estimate\n      lower bound\n      upper bound\n      formula\n    \n  \n  \n    \n      0\n      1.060768\n      -1.435118\n      3.556655\n      mathgain ~ schoolidT_2 + schoolidT_3 + schooli...\n    \n    \n      1\n      1.789413\n      -1.587051\n      5.165876\n      mathgain ~ mathprep + school_mean + class_mean...\n    \n    \n      2\n      0.904018\n      -2.056505\n      3.864541\n      mathgain ~ mathprep + mathkind + sex + school_...\n    \n    \n      3\n      2.948546\n      0.601150\n      5.295942\n      mathgain ~ schoolidT_2 + schoolidT_3 + schooli...\n    \n    \n      4\n      3.545931\n      1.359793\n      5.732068\n      mathgain ~ schoolidT_2 + schoolidT_3 + schooli...\n    \n    \n      5\n      2.303187\n      NaN\n      NaN\n      mathgain ~ childidT_2 + childidT_3 + childidT_...\n    \n  \n\n\n\n\nThe point here (perhap obvious) is that the estimate of treatment effects due to some policy or programme can be differently understood when the regression model is able to account for increasing aspects of individual variation. Choice of the right way to “saturate” your regression specification are at the heart of causal inference. The right control structures determine the freedom available to vary your focal treatment effect parameter.\nWe will consider a number of specifications below that incorporate these group effects in a hierarchical model which nests the effect of class-membership within school membership. This choice allows us to control for group specific interactions without worrying about over-indexing on the observed interaction effects in the sample data requiring that we handle more fixed effects parameters than we have data points.\nIn what follows we’ll specify a nested approach to the parameter specifcation using a random effects model. The idea here is that classes are already implicitly nested in schools and so we don’t need to add parameters for classes at multiple schools. Additionally we’re positing that there is independent interest in the effectiveness school/class effects i.e. the degree to which variation in a school/class nest can account for variation in the outcome.\n\nMinimal Model\n\nmodel = bmb.Model(f\"mathgain ~ mathprep + (1 | schoolid / classid)\", df)\nidata = model.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\nThe model specification here is deliberately minimalist we want to observe how much of the variation in the outcome can be accounted for by solely adding extensive controls for interactions of group level effects and the treatment but ignoring all else.\n\nmodel.graph()\n\n\n\n\nWe can see the derived sigma parameters here which can be understood as partialling out the variance of the outcome into components due to those group level effects and the unexplained residuals.\n\naz.summary(idata, var_names=['Intercept', '1|schoolid_sigma', '1|schoolid:classid_sigma', 'mathgain_sigma', 'mathprep'])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      52.561\n      3.542\n      46.010\n      59.516\n      0.066\n      0.047\n      2839.0\n      2731.0\n      1.00\n    \n    \n      1|schoolid_sigma\n      8.319\n      2.265\n      4.099\n      12.590\n      0.124\n      0.087\n      399.0\n      285.0\n      1.01\n    \n    \n      1|schoolid:classid_sigma\n      9.877\n      2.459\n      5.283\n      14.369\n      0.161\n      0.114\n      256.0\n      313.0\n      1.02\n    \n    \n      mathgain_sigma\n      32.136\n      0.786\n      30.652\n      33.620\n      0.020\n      0.014\n      1537.0\n      2143.0\n      1.00\n    \n    \n      mathprep\n      1.889\n      1.242\n      -0.563\n      4.092\n      0.023\n      0.017\n      2933.0\n      2596.0\n      1.00\n    \n  \n\n\n\n\nNote here the relative proportion of the school specifc variances 1|schoolid_sigma to the overall variance of the residuals mathgain_sigma.\n\n\n\nCalculating the IntraClass Correlation Coefficient\nThese models faciliate the calculation of the ICC statistics which is a measure of “explained variance”. The thought is to gauge the proportion of variance ascribed to one set of random effects over and above the total estimated variance in the baseline model, including the residuals mathgain_sigma.\n\na = idata['posterior']['1|schoolid_sigma']**2\n\nb = (idata['posterior']['1|schoolid:classid_sigma']**2 + idata['posterior']['1|schoolid_sigma']**2)\n\nc = (idata['posterior']['1|schoolid:classid_sigma']**2 + idata['posterior']['1|schoolid_sigma']**2 + idata['posterior']['mathgain_sigma']**2)\n\n(a / c).mean().item() \n\n0.061101339125402415\n\n\n\n((a + b) / c).mean().item()\n\n0.20748893817579722\n\n\nWe can see here that the interaction terms do seem to account for a goodly portion of the variance in the outcome and we ought to consider retaining their inclusion in our modelling work. The structure of the problem drives us towards their inclusion. Class/school effects are going to absorb a sufficient portion of the variation. So they merit study in their own right, lest the individual class/school dynamics obscure the effectiveness of the mathprep treatment. Similarly, it’s likely valuable to consider the efficacy of the average class/school in a wider policy conversation.\n\n\nAugmenting the Models\nNext we augment our model with more pupil level control variables aiming to pin down some of the aspects of the variation in the outcome.\n\nAdding Pupil Fixed Effects\nHere we add these fixed effects population parameters. But note they are not merely devices for controlling variance in the outcome, they’re interpretation is likely of independent interest.\n\nmodel_fixed = bmb.Model(f\"mathgain ~  sex + minority + ses + mathprep + (1 | schoolid / classid)\", df)\nidata_fixed = model_fixed.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\n\naz.summary(idata_fixed, var_names=['Intercept', 'sex', 'minority', 'ses', 'mathprep',\n'1|schoolid_sigma', '1|schoolid:classid_sigma', 'mathgain_sigma'])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      53.364\n      4.052\n      45.853\n      61.036\n      0.075\n      0.053\n      2911.0\n      2702.0\n      1.00\n    \n    \n      sex\n      -2.320\n      1.980\n      -6.060\n      1.392\n      0.029\n      0.024\n      4785.0\n      2927.0\n      1.00\n    \n    \n      minority\n      0.285\n      2.666\n      -4.744\n      5.240\n      0.046\n      0.038\n      3342.0\n      2993.0\n      1.00\n    \n    \n      ses\n      0.916\n      1.442\n      -1.713\n      3.641\n      0.024\n      0.020\n      3726.0\n      2813.0\n      1.00\n    \n    \n      mathprep\n      1.933\n      1.207\n      -0.444\n      4.032\n      0.024\n      0.017\n      2571.0\n      2478.0\n      1.00\n    \n    \n      1|schoolid_sigma\n      8.575\n      2.000\n      4.787\n      12.441\n      0.082\n      0.058\n      632.0\n      623.0\n      1.01\n    \n    \n      1|schoolid:classid_sigma\n      9.857\n      2.345\n      5.219\n      14.041\n      0.120\n      0.085\n      402.0\n      537.0\n      1.01\n    \n    \n      mathgain_sigma\n      32.155\n      0.768\n      30.760\n      33.633\n      0.020\n      0.014\n      1425.0\n      2531.0\n      1.00\n    \n  \n\n\n\n\nNow we add a further class level control.\n\n\nAdding More Class and Pupil Level Fixed Effects\n\nmodel_fixed_1 = bmb.Model(f\"mathgain ~ mathkind + sex + minority + ses + yearstea + mathknow + mathprep + (1 | schoolid / classid)\", df.dropna())\nidata_fixed_1 = model_fixed_1.fit( inference_method=\"nuts_numpyro\",\n    idata_kwargs={\"log_likelihood\": True})\n\n\naz.summary(idata_fixed_1, var_names=['Intercept', \n'mathkind', 'sex', 'minority', 'ses', 'yearstea', 'mathknow', 'mathprep','1|schoolid_sigma', '1|schoolid:classid_sigma', 'mathgain_sigma'])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      Intercept\n      281.822\n      11.832\n      259.430\n      303.680\n      0.204\n      0.144\n      3361.0\n      2999.0\n      1.00\n    \n    \n      mathkind\n      -0.475\n      0.023\n      -0.520\n      -0.434\n      0.000\n      0.000\n      3596.0\n      2843.0\n      1.00\n    \n    \n      sex\n      -1.413\n      1.677\n      -4.369\n      1.928\n      0.026\n      0.022\n      4234.0\n      2940.0\n      1.00\n    \n    \n      minority\n      -7.866\n      2.428\n      -12.438\n      -3.209\n      0.043\n      0.030\n      3230.0\n      3086.0\n      1.00\n    \n    \n      ses\n      5.414\n      1.256\n      3.165\n      7.864\n      0.020\n      0.014\n      4041.0\n      2963.0\n      1.00\n    \n    \n      yearstea\n      0.042\n      0.118\n      -0.178\n      0.253\n      0.002\n      0.002\n      2778.0\n      2847.0\n      1.00\n    \n    \n      mathknow\n      1.861\n      1.211\n      -0.236\n      4.339\n      0.028\n      0.020\n      1915.0\n      2552.0\n      1.00\n    \n    \n      mathprep\n      1.119\n      1.142\n      -1.145\n      3.189\n      0.022\n      0.016\n      2700.0\n      2959.0\n      1.00\n    \n    \n      1|schoolid_sigma\n      8.603\n      1.701\n      5.402\n      11.758\n      0.070\n      0.050\n      606.0\n      871.0\n      1.01\n    \n    \n      1|schoolid:classid_sigma\n      9.348\n      1.745\n      5.827\n      12.490\n      0.092\n      0.068\n      353.0\n      899.0\n      1.02\n    \n    \n      mathgain_sigma\n      26.766\n      0.675\n      25.578\n      28.073\n      0.015\n      0.011\n      1995.0\n      2470.0\n      1.00\n    \n  \n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 7))\naz.plot_forest([idata, idata_fixed, idata_fixed_1], combined=True, var_names=['mathprep', '1|schoolid_sigma', '1|schoolid:classid_sigma', 'mathgain_sigma'], ax=ax)\nax.axvline(1)\n\n<matplotlib.lines.Line2D at 0x29f11b890>\n\n\n\n\n\nWe now make use of bambis model interpretation module to plot the marginal effect on the outcome due to changes in the treatment intensity.\n\n\nCode\nfig, axs = plt.subplots(1, 3, figsize=(20, 6), \ndpi=120, sharey=True, sharex=True)\naxs = axs.flatten()\naxs[0].axhline(50, color='red', label='Reference Line', \nlinestyle='--')\naxs[1].axhline(50, color='red', label='Reference Line', \nlinestyle='--')\naxs[2].axhline(50, color='red', label='Reference Line', \nlinestyle='--')\nbmb.interpret.plot_predictions(model, idata, \"mathprep\", ax=axs[0]);\nbmb.interpret.plot_predictions(model_fixed, idata_fixed, \"mathprep\", ax=axs[1]);\nbmb.interpret.plot_predictions(model_fixed_1, idata_fixed_1, \"mathprep\", ax=axs[2]);\naxs[0].set_title(\"Variation in Implied Outcome by Treatment \\n mathgain ~ mathprep + (1 | schoolid / classid) \")\naxs[1].set_title(\"Variation in Implied Outcome by Treatment \\n mathgain ~ sex + minority + ses + mathprep + (1 | schoolid / classid) \")\naxs[2].set_title(\"Variation in Implied Outcome by Treatment \\n mathgain ~ mathkind + sex + minority + ses + yearstea + \\n mathknow + mathprep + (1 | schoolid / classid\")\naxs[0].set_xlabel('')\naxs[1].set_xlabel('')\naxs[0].legend();\n\n\n\n\n\nAs we can see here across all the different model specifications we see quite modest effects of treatment with very wide uncertainty. You might therefore be sceptical that teacher training has any real discernible impact on child outcomes? Maybe you believe other interventions are more important to fund? These kinds of questions determine policy. So misguided policy interventions on child-hood education can have radical consequences. It’s, therefore, vital that we have robust and justifiable approaches and tooling for the analysis of these policy questions in the face of group level confounding.\nThis last point is crucial - when model complexity balloons due extensive interaction effects, then we need efficient tools to interrogate the outcome level differences implied by the model choices. Your intuition and understanding of a process is keyed to the observable effects of that process. Your understanding of the internal mechanics of a model is likely less than your concrete expectations of observable behaviour. As such, credibility requires that we be able to assess models on the variation produced in the outcome variable rather merely on the parameters values."
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "",
    "text": "library(lavaan)\nlibrary(dplyr)\nlibrary(reticulate)\nlibrary(marginaleffects)\nlibrary(modelsummary)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(egg)\nlibrary(lme4)\nlibrary(semPlot)\nlibrary(tinytable)\nlibrary(kableExtra)\nlibrary(reshape2)\nreticulate::py_run_string(\"import pymc as pm\")\n\noptions(rstudio.python.installationPath = \"/Users/nathanielforde/mambaforge/envs\")\noptions(\"modelsummary_factory_default\" = \"tinytable\")\noptions(repr.plot.width=15, repr.plot.height=8)\n\nknitr::knit_engines$set(python = reticulate::eng_python)\noptions(scipen=999)\nset.seed(130)"
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#measurment-and-measurment-constructs",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#measurment-and-measurment-constructs",
    "title": "Measurement, Latent Factors and Theory Construction",
    "section": "Measurment and Measurment Constructs",
    "text": "Measurment and Measurment Constructs\n\ndf = read.csv('sem_data.csv')\ninflate <- df$region == 'west'\nnoise <- rnorm(sum(inflate), 0.5, 1) # generate the noise to add\ndf$ls_p3[inflate] <- df$ls_p3[inflate] + noise\ndf$ls_sum <- df$ls_p1 + df$ls_p2 + df$ls_p3\ndf$ls_mean <- rowMeans(df[ c('ls_p1', 'ls_p2', 'ls_p3')])\n\nhead(df) |> kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nregion\ngender\nage\nse_acad_p1\nse_acad_p2\nse_acad_p3\nse_social_p1\nse_social_p2\nse_social_p3\nsup_friends_p1\nsup_friends_p2\nsup_friends_p3\nsup_parents_p1\nsup_parents_p2\nsup_parents_p3\nls_p1\nls_p2\nls_p3\nls_sum\nls_mean\n\n\n\n\n1\nwest\nfemale\n13\n4.857143\n5.571429\n4.500000\n5.80\n5.500000\n5.40\n6.5\n6.5\n7.0\n7.0\n7.0\n6.0\n5.333333\n6.75\n7.647112\n19.73044\n6.576815\n\n\n2\nwest\nmale\n14\n4.571429\n4.285714\n4.666667\n5.00\n5.500000\n4.80\n4.5\n4.5\n5.5\n5.0\n6.0\n4.5\n4.333333\n5.00\n4.469623\n13.80296\n4.600985\n\n\n10\nwest\nfemale\n14\n4.142857\n6.142857\n5.333333\n5.20\n4.666667\n6.00\n4.0\n4.5\n3.5\n7.0\n7.0\n6.5\n6.333333\n5.50\n4.710020\n16.54335\n5.514451\n\n\n11\nwest\nfemale\n14\n5.000000\n5.428571\n4.833333\n6.40\n5.833333\n6.40\n7.0\n7.0\n7.0\n7.0\n7.0\n7.0\n4.333333\n6.50\n5.636198\n16.46953\n5.489844\n\n\n12\nwest\nfemale\n14\n5.166667\n5.600000\n4.800000\n5.25\n5.400000\n5.25\n7.0\n7.0\n7.0\n6.5\n6.5\n7.0\n5.666667\n6.00\n5.266592\n16.93326\n5.644419\n\n\n14\nwest\nmale\n14\n4.857143\n4.857143\n4.166667\n5.20\n5.000000\n4.20\n5.5\n6.5\n7.0\n6.5\n6.5\n6.5\n5.000000\n5.50\n5.913709\n16.41371\n5.471236\n\n\n\n\n\n\n\n\nCandidate Structure\n\n\n\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\ndatasummary_skim(df)|> \n style_tt(\n   i = 15:17,\n   j = 1:1,\n   background = \"#20AACC\",\n   color = \"white\",\n   italic = TRUE) |> \n style_tt(\n   i = 18:19,\n   j = 1:1,\n   background = \"#2888A0\",\n   color = \"white\",\n   italic = TRUE) |> \n style_tt(\n   i = 2:14,\n   j = 1:1,\n   background = \"#17C2AD\",\n   color = \"white\",\n   italic = TRUE)\n\n\n \n\n  \n    \n    \n    tinytable_ftyu5q15n5ibjk84ch05\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                Unique\n                Missing Pct.\n                Mean\n                SD\n                Min\n                Median\n                Max\n                Histogram\n              \n        \n        \n        \n                \n                  ID\n                  283\n                  0\n                  187.9\n                  106.3\n                  1.0\n                  201.0\n                  367.0\n                  \n                \n                \n                  age\n                  5\n                  0\n                  14.7\n                  0.8\n                  13.0\n                  15.0\n                  17.0\n                  \n                \n                \n                  se_acad_p1\n                  32\n                  0\n                  5.2\n                  0.8\n                  3.1\n                  5.1\n                  7.0\n                  \n                \n                \n                  se_acad_p2\n                  36\n                  0\n                  5.3\n                  0.7\n                  3.1\n                  5.4\n                  7.0\n                  \n                \n                \n                  se_acad_p3\n                  29\n                  0\n                  5.2\n                  0.8\n                  2.8\n                  5.2\n                  7.0\n                  \n                \n                \n                  se_social_p1\n                  24\n                  0\n                  5.3\n                  0.8\n                  1.8\n                  5.4\n                  7.0\n                  \n                \n                \n                  se_social_p2\n                  27\n                  0\n                  5.5\n                  0.7\n                  2.7\n                  5.5\n                  7.0\n                  \n                \n                \n                  se_social_p3\n                  31\n                  0\n                  5.4\n                  0.8\n                  3.0\n                  5.5\n                  7.0\n                  \n                \n                \n                  sup_friends_p1\n                  13\n                  0\n                  5.8\n                  1.1\n                  1.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_friends_p2\n                  10\n                  0\n                  6.0\n                  0.9\n                  2.5\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_friends_p3\n                  13\n                  0\n                  6.0\n                  1.1\n                  1.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_parents_p1\n                  11\n                  0\n                  6.0\n                  1.1\n                  1.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_parents_p2\n                  11\n                  0\n                  5.9\n                  1.1\n                  2.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_parents_p3\n                  13\n                  0\n                  5.7\n                  1.1\n                  1.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  ls_p1\n                  15\n                  0\n                  5.2\n                  0.9\n                  2.0\n                  5.3\n                  7.0\n                  \n                \n                \n                  ls_p2\n                  21\n                  0\n                  5.8\n                  0.7\n                  2.5\n                  5.8\n                  7.0\n                  \n                \n                \n                  ls_p3\n                  161\n                  0\n                  5.5\n                  1.1\n                  1.7\n                  5.6\n                  9.6\n                  \n                \n                \n                  ls_sum\n                  218\n                  0\n                  16.5\n                  2.2\n                  8.2\n                  16.7\n                  21.0\n                  \n                \n                \n                  ls_mean\n                  217\n                  0\n                  5.5\n                  0.7\n                  2.7\n                  5.6\n                  7.0\n                  \n                \n                \n                   \n                    \n                  N\n                  %\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  region\n                  east\n                  142\n                  50.2\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  \n                  west\n                  141\n                  49.8\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  gender\n                  female\n                  132\n                  46.6\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  \n                  male\n                  151\n                  53.4\n                  \n                  \n                  \n                  \n                  \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\ndrivers = c('se_acad_p1', 'se_acad_p2', 'se_acad_p3', 'se_social_p1', 'se_social_p2', 'se_social_p3', 'sup_friends_p1','sup_friends_p2', 'sup_friends_p3', 'sup_parents_p1' , 'sup_parents_p2' , 'sup_parents_p3', 'ls_p1', 'ls_p2', 'ls_p3')\n\n\n\nplot_heatmap <- function(df, title=\"Sample Covariances\", subtitle=\"Observed Measures\") {\n  heat_df = df |> as.matrix() |> melt()\n  colnames(heat_df) <- c(\"x\", \"y\", \"value\")\n  g <- heat_df |> mutate(value = round(value, 2)) |>  ggplot(aes(x = x, y = y, fill = value)) +\n    geom_tile() + geom_text(aes(label = value), color = \"black\", size = 4) +\n   scale_fill_gradient2(\n      high = 'dodgerblue4',\n      mid = 'white',\n      low = 'firebrick2'\n    ) + theme(axis.text.x = element_text(angle=45)) + ggtitle(title, subtitle)\n  \n  g\n}\n\ng1 = plot_heatmap(cov(df[,  drivers]))\n\ng2 = plot_heatmap(cor(df[,  drivers]), \"Sample Correlations\")\n\nplot <- ggarrange(g1,g2, ncol=1, nrow=2);"
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#fit-initial-regression-models",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#fit-initial-regression-models",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Fit Initial Regression Models",
    "text": "Fit Initial Regression Models\nTo model these effects we make use of the aggregated sum and mean scores to express the relationships between these indicator metrics and life-satisfaction. We fit a variety of models each one escalating in the number of indicator metrics we incorporate into our model of the life-satisfaction outcome. This side-steps the multivariate nature of hypothesised constructs and crudely amalgamates the indicator metrics. This may be more or less justified depending on how similar in theme the three outcome questions ls_p1, ls_p2, ls_p3 are in nature. We’ve dodged the question of thematic unity as all our metrics “load equally” on the outcome variable here and we let our regression model sort out the weighting.\n\nformula_sum_1st = \" ls_sum ~ se_acad_p1  + se_social_p1 +  sup_friends_p1  + sup_parents_p1\"\nformula_mean_1st = \" ls_mean ~ se_acad_p1  + se_social_p1 +  sup_friends_p1  + sup_parents_p1\"\n\nformula_sum_12 = \" ls_sum ~ se_acad_p1  + se_acad_p2 +  se_social_p1 + se_social_p2 + \nsup_friends_p1 + sup_friends_p2  + sup_parents_p1 + sup_parents_p2\"\nformula_mean_12 = \" ls_mean ~ se_acad_p1  + se_acad_p2 +  se_social_p1 + se_social_p2 + \nsup_friends_p1 + sup_friends_p2  + sup_parents_p1 + sup_parents_p2\"\n\n\nformula_sum = \" ls_sum ~ se_acad_p1 + se_acad_p2 + se_acad_p3 + se_social_p1 +  se_social_p2 + se_social_p3 +  sup_friends_p1 + sup_friends_p2 + sup_friends_p3 + sup_parents_p1 + sup_parents_p2 + sup_parents_p3\"\nformula_mean = \" ls_mean ~ se_acad_p1 + se_acad_p2 + se_acad_p3 + se_social_p1 +  se_social_p2 + se_social_p3 +  sup_friends_p1 + sup_friends_p2 + sup_friends_p3 + sup_parents_p1 + sup_parents_p2 + sup_parents_p3\"\nmod_sum = lm(formula_sum, df)\nmod_sum_1st = lm(formula_sum_1st, df)\nmod_sum_12 = lm(formula_sum_12, df)\nmod_mean = lm(formula_mean, df)\nmod_mean_1st = lm(formula_mean_1st, df)\nmod_mean_12 = lm(formula_mean_12, df)\n\nnorm <- function(x) {\n    (x - mean(x)) / sd(x)\n}\n\ndf_norm <- as.data.frame(lapply(df[c(5:19)], norm))\n\ndf_norm$ls_sum <- df$ls_sum\ndf_norm$ls_mean <- df$ls_mean\n\nmod_sum_norm = lm(formula_sum, df_norm)\nmod_mean_norm = lm(formula_mean, df_norm)\n\nmodels = list(\n    \"Outcome: sum_score\" = list(\"model_sum_1st_factors\" = mod_sum_1st,\n     \"model_sum_1st_2nd_factors\" = mod_sum_12,\n     \"model_sum_score\" = mod_sum,\n     \"model_sum_score_norm\" = mod_sum_norm\n     ),\n    \"Outcome: mean_score\" = list(\n      \"model_mean_1st_factors\" = mod_mean_1st,\n     \"model_mean_1st_2nd_factors\" = mod_mean_12,\n     \"model_mean_score\"= mod_mean, \n     \"model_mean_score_norm\" = mod_mean_norm\n    )\n    )\n\nThe classical presentation of regression models reports the coefficient weights accorded to each of the input variables. We present these models to highlight that the manner in which we represent our theoretical constructs has ramifications for the interpretation of the data generating process. In particular, note how different degrees of significance are accorded to the different variables depending on which alternate variables are included.\n\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\nmodelsummary(models, stars=TRUE, shape =\"cbind\") |> \n style_tt(\n   i = 2:25,\n   j = 1:1,\n   background = \"#17C2AD\",\n   color = \"white\",\n   italic = TRUE)\n\n\n \n\n  \n    \n    \n    tinytable_pclqteqllnuzsr3iakja\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n\n \nOutcome: sum_score\nOutcome: mean_score\n\n        \n              \n                 \n                model_sum_1st_factors\n                model_sum_1st_2nd_factors\n                model_sum_score\n                model_sum_score_norm\n                model_mean_1st_factors\n                model_mean_1st_2nd_factors\n                model_mean_score\n                model_mean_score_norm\n              \n        \n        + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n        \n                \n                  (Intercept)   \n                  5.118***\n                  2.644** \n                  2.094*  \n                  16.192***\n                  1.706***\n                  0.881** \n                  0.698*  \n                  5.397***\n                \n                \n                                \n                  (0.907) \n                  (0.985) \n                  (0.954) \n                  (0.089)  \n                  (0.302) \n                  (0.328) \n                  (0.318) \n                  (0.030) \n                \n                \n                  se_acad_p1    \n                  0.242   \n                  -0.034  \n                  -0.208  \n                  -0.165   \n                  0.081   \n                  -0.011  \n                  -0.069  \n                  -0.055  \n                \n                \n                                \n                  (0.147) \n                  (0.180) \n                  (0.192) \n                  (0.153)  \n                  (0.049) \n                  (0.060) \n                  (0.064) \n                  (0.051) \n                \n                \n                  se_social_p1  \n                  1.088***\n                  0.501*  \n                  0.355+  \n                  0.269+   \n                  0.363***\n                  0.167*  \n                  0.118+  \n                  0.090+  \n                \n                \n                                \n                  (0.162) \n                  (0.204) \n                  (0.200) \n                  (0.152)  \n                  (0.054) \n                  (0.068) \n                  (0.067) \n                  (0.051) \n                \n                \n                  sup_friends_p1\n                  0.125   \n                  -0.224+ \n                  -0.272+ \n                  -0.307+  \n                  0.042   \n                  -0.075+ \n                  -0.091+ \n                  -0.102+ \n                \n                \n                                \n                  (0.088) \n                  (0.133) \n                  (0.150) \n                  (0.169)  \n                  (0.029) \n                  (0.044) \n                  (0.050) \n                  (0.056) \n                \n                \n                  sup_parents_p1\n                  0.561***\n                  0.238+  \n                  0.072   \n                  0.077    \n                  0.187***\n                  0.079+  \n                  0.024   \n                  0.026   \n                \n                \n                                \n                  (0.100) \n                  (0.141) \n                  (0.143) \n                  (0.154)  \n                  (0.033) \n                  (0.047) \n                  (0.048) \n                  (0.051) \n                \n                \n                  se_acad_p2    \n                          \n                  0.448*  \n                  0.327   \n                  0.224    \n                          \n                  0.149*  \n                  0.109   \n                  0.075   \n                \n                \n                                \n                          \n                  (0.197) \n                  (0.202) \n                  (0.139)  \n                          \n                  (0.066) \n                  (0.067) \n                  (0.046) \n                \n                \n                  se_social_p2  \n                          \n                  0.756***\n                  0.509*  \n                  0.356*   \n                          \n                  0.252***\n                  0.170*  \n                  0.119*  \n                \n                \n                                \n                          \n                  (0.213) \n                  (0.219) \n                  (0.153)  \n                          \n                  (0.071) \n                  (0.073) \n                  (0.051) \n                \n                \n                  sup_friends_p2\n                          \n                  0.369*  \n                  0.331*  \n                  0.313*   \n                          \n                  0.123*  \n                  0.110*  \n                  0.104*  \n                \n                \n                                \n                          \n                  (0.157) \n                  (0.160) \n                  (0.151)  \n                          \n                  (0.052) \n                  (0.053) \n                  (0.050) \n                \n                \n                  sup_parents_p2\n                          \n                  0.370** \n                  0.118   \n                  0.129    \n                          \n                  0.123** \n                  0.039   \n                  0.043   \n                \n                \n                                \n                          \n                  (0.138) \n                  (0.144) \n                  (0.157)  \n                          \n                  (0.046) \n                  (0.048) \n                  (0.052) \n                \n                \n                  se_acad_p3    \n                          \n                          \n                  0.153   \n                  0.123    \n                          \n                          \n                  0.051   \n                  0.041   \n                \n                \n                                \n                          \n                          \n                  (0.174) \n                  (0.140)  \n                          \n                          \n                  (0.058) \n                  (0.047) \n                \n                \n                  se_social_p3  \n                          \n                          \n                  0.443** \n                  0.354**  \n                          \n                          \n                  0.148** \n                  0.118** \n                \n                \n                                \n                          \n                          \n                  (0.161) \n                  (0.129)  \n                          \n                          \n                  (0.054) \n                  (0.043) \n                \n                \n                  sup_friends_p3\n                          \n                          \n                  0.165   \n                  0.181    \n                          \n                          \n                  0.055   \n                  0.060   \n                \n                \n                                \n                          \n                          \n                  (0.130) \n                  (0.143)  \n                          \n                          \n                  (0.043) \n                  (0.048) \n                \n                \n                  sup_parents_p3\n                          \n                          \n                  0.526***\n                  0.602*** \n                          \n                          \n                  0.175***\n                  0.201***\n                \n                \n                                \n                          \n                          \n                  (0.126) \n                  (0.144)  \n                          \n                          \n                  (0.042) \n                  (0.048) \n                \n                \n                  Num.Obs.      \n                  283     \n                  283     \n                  283     \n                  283      \n                  283     \n                  283     \n                  283     \n                  283     \n                \n                \n                  R2            \n                  0.399   \n                  0.467   \n                  0.517   \n                  0.517    \n                  0.399   \n                  0.467   \n                  0.517   \n                  0.517   \n                \n                \n                  R2 Adj.       \n                  0.390   \n                  0.451   \n                  0.496   \n                  0.496    \n                  0.390   \n                  0.451   \n                  0.496   \n                  0.496   \n                \n                \n                  AIC           \n                  1090.9  \n                  1064.7  \n                  1044.7  \n                  1044.7   \n                  469.1   \n                  442.9   \n                  422.9   \n                  422.9   \n                \n                \n                  BIC           \n                  1112.8  \n                  1101.2  \n                  1095.7  \n                  1095.7   \n                  491.0   \n                  479.4   \n                  473.9   \n                  473.9   \n                \n                \n                  Log.Lik.      \n                  -539.455\n                  -522.373\n                  -508.341\n                  -508.341 \n                  -228.548\n                  -211.466\n                  -197.434\n                  -197.434\n                \n                \n                  RMSE          \n                  1.63    \n                  1.53    \n                  1.46    \n                  1.46     \n                  0.54    \n                  0.51    \n                  0.49    \n                  0.49    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nWe can similarly plot the coefficient values and their uncertainty highlighting how the representation or scaling of the variables impact the scale of the coefficient weights and therefore the surety of any subsequent claims.\n\nmodels = list(\n     \"model_sum_1st_factors\" = mod_sum_1st,\n     \"model_sum_1st_2nd_factors\" = mod_sum_12,\n     \"model_sum_score\" = mod_sum,\n     \"model_sum_score_norm\" = mod_sum_norm,\n     \"model_mean_1st_factors\" = mod_mean_1st,\n     \"model_mean_1st_2nd_factors\" = mod_mean_12,\n     \"model_mean_score\"= mod_mean,\n     \"model_mean_score_norm\" = mod_mean_norm\n    )\n\nmodelplot(models, coef_omit = 'Intercept') + geom_vline(xintercept = 0, linetype=\"dotted\", \n                color = \"black\") + ggtitle(\"Comparing Model Parameter Estimates\", \"Across Covariates\")\n\n\n\n\n\nSignificant Coefficients?\nAn alternative lens on these figures highlights the statistical significance of the coefficients. But again, these criteria are much abused. Significance at what level? Conditional on which representation? Which composite outcome score?\n\ng1 = modelplot(mod_mean, coef_omit = 'Intercept') +  aes(color = ifelse(p.value < 0.001, \"Significant at 0.001\", \"Not significant at 0.001\")) +\n  scale_color_manual(values = c(\"grey\", \"blue\")) + ggtitle(\"Significance of Coefficient Values\", \"Mean Model at Different Levels\")\n\n\ng2 = modelplot(mod_mean, coef_omit = 'Intercept') +  aes(color = ifelse(p.value < 0.05, \"Significant at 0.05\", \"Not significant at 0.05\")) +\n  scale_color_manual(values = c(\"grey\", \"black\"))\n\nplot <- ggarrange(g1,g2, ncol=2, nrow=1);\n\n\n\n\n\n\nAggregate Driver Scores\nPerhaps we play with the feature representation and increase the proportion of significant indicators. Can we now tell a more definitive story about how parental support and social self-efficact are determinants of self-reported life-satisfaction scores? Let’s focus here on the sum score representation and add interaction effects.\n\ndf$se_acad_mean <- rowMeans(df[c('se_acad_p1', 'se_acad_p2', 'se_acad_p3')])\ndf$se_social_mean <- rowMeans(df[c('se_social_p1', 'se_social_p2', 'se_social_p3')])\ndf$sup_friends_mean <- rowMeans(df[c('sup_friends_p1', 'sup_friends_p2', 'sup_friends_p3')])\ndf$sup_parents_mean <- rowMeans(df[c('sup_parents_p1', 'sup_parents_p2', 'sup_parents_p3')])\n\n\nformula_parcel_sum = \"ls_sum ~ se_acad_mean + se_social_mean +\nsup_friends_mean + sup_parents_mean \" #sup_parents_mean*se_social_mean\"\n\nformula_parcel_sum_inter = \"ls_sum ~ se_acad_mean + se_social_mean + \nsup_friends_mean + sup_parents_mean + sup_parents_mean*se_social_mean\"\n\nmod_sum_parcel = lm(formula_parcel_sum, df)\nmod_sum_inter_parcel = lm(formula_parcel_sum_inter, df)\n\nmodels_parcel = list(\"model_sum_score\" = mod_sum_parcel,\n     \"model_sum_inter_score\"= mod_sum_inter_parcel\n     )\n\nmodelsummary(models_parcel, stars=TRUE)\n\n\n \n\n  \n    \n    \n    tinytable_jxxnaq7a8a1a04s6zo8g\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                model_sum_score\n                model_sum_inter_score\n              \n        \n        + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n        \n                \n                  (Intercept)                      \n                  2.728** \n                  -6.103+ \n                \n                \n                                                   \n                  (0.931) \n                  (3.356) \n                \n                \n                  se_acad_mean                     \n                  0.307+  \n                  0.370*  \n                \n                \n                                                   \n                  (0.158) \n                  (0.158) \n                \n                \n                  se_social_mean                   \n                  1.269***\n                  2.859***\n                \n                \n                                                   \n                  (0.175) \n                  (0.606) \n                \n                \n                  sup_friends_mean                 \n                  0.124   \n                  0.183+  \n                \n                \n                                                   \n                  (0.097) \n                  (0.099) \n                \n                \n                  sup_parents_mean                 \n                  0.726***\n                  2.242***\n                \n                \n                                                   \n                  (0.099) \n                  (0.562) \n                \n                \n                  se_social_mean × sup_parents_mean\n                          \n                  -0.292**\n                \n                \n                                                   \n                          \n                  (0.107) \n                \n                \n                  Num.Obs.                         \n                  283     \n                  283     \n                \n                \n                  R2                               \n                  0.489   \n                  0.503   \n                \n                \n                  R2 Adj.                          \n                  0.482   \n                  0.494   \n                \n                \n                  AIC                              \n                  1044.6  \n                  1039.0  \n                \n                \n                  BIC                              \n                  1066.4  \n                  1064.5  \n                \n                \n                  Log.Lik.                         \n                  -516.288\n                  -512.513\n                \n                \n                  RMSE                             \n                  1.50    \n                  1.48    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nWhat does definitive mean here? Is it so simple as more significant coefficients? Marginally better performance measures?\n\ng1 = modelplot(mod_sum_parcel, coef_omit = 'Intercept') +  aes(color = ifelse(p.value < 0.05, \"Significant at 0.05\", \"Not significant at 0.05\")) +\n  scale_color_manual(values = c(\"grey\", \"blue\")) + ggtitle(\"Significance of Coefficient Values\", \"At Different Levels for Sum and Mean Scores Life Satisfaction \")\n\n\ng2 = modelplot(mod_sum_inter_parcel, coef_omit = 'Intercept') +  aes(color = ifelse(p.value < 0.05, \"Significant at 0.05\", \"Not significant at 0.05\")) +\n  scale_color_manual(values = c(\"grey\", \"blue\"))\n\nplot <- ggarrange(g1,g2, ncol=2, nrow=1);\n\n\n\n\nThis kind of brinkmanship is brittle. Any one of these kinds of choice can be justified but more often than not results from an suspect exploratory process. Steps down a “garden of forking paths” seeking some kind of story to justify an analysis or promote a conclusion. This post-hoc “seeking” is just bad science undermining the significance claims that accrue to reliable procedures. It warps the nature of testing procedure by corrupting the assumed consistency of repeatable trials. The guarantees of statistical significance attach to a conclusion just when the procedure is imagined replicible and repeated under identical conditions. By exploring the different representations and criteria of narrative adequacy we break those guarantees."
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#aggregate-driver-scores",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#aggregate-driver-scores",
    "title": "Measurement, Latent Factors and Theory Construction",
    "section": "Aggregate Driver Scores",
    "text": "Aggregate Driver Scores\n\ndf$se_acad_mean <- rowMeans(df[c('se_acad_p1', 'se_acad_p2', 'se_acad_p3')])\ndf$se_social_mean <- rowMeans(df[c('se_social_p1', 'se_social_p2', 'se_social_p3')])\ndf$sup_friends_mean <- rowMeans(df[c('sup_friends_p1', 'sup_friends_p2', 'sup_friends_p3')])\ndf$sup_parents_mean <- rowMeans(df[c('sup_parents_p1', 'sup_parents_p2', 'sup_parents_p3')])\n\n\nformula_parcel_mean = \"ls_mean ~ se_acad_mean + se_social_mean + sup_friends_mean + sup_parents_mean\"\n\nformula_parcel_sum = \"ls_sum ~ se_acad_mean + se_social_mean + sup_friends_mean + sup_parents_mean\"\n\nmod_sum_parcel = lm(formula_parcel_sum, df)\nmod_mean_parcel = lm(formula_parcel_mean, df)\n\nmodels_parcel = list(\"model_sum_score\" = mod_sum_parcel,\n     \"model_mean_score\"= mod_mean_parcel\n     )\n\nmodelsummary(models_parcel, stars=TRUE)\n\n\n \n\n  \n    \n    \n    tinytable_jxxnaq7a8a1a04s6zo8g\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                model_sum_score\n                model_mean_score\n              \n        \n        + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n        \n                \n                  (Intercept)     \n                  2.728** \n                  0.909** \n                \n                \n                                  \n                  (0.931) \n                  (0.310) \n                \n                \n                  se_acad_mean    \n                  0.307+  \n                  0.102+  \n                \n                \n                                  \n                  (0.158) \n                  (0.053) \n                \n                \n                  se_social_mean  \n                  1.269***\n                  0.423***\n                \n                \n                                  \n                  (0.175) \n                  (0.058) \n                \n                \n                  sup_friends_mean\n                  0.124   \n                  0.041   \n                \n                \n                                  \n                  (0.097) \n                  (0.032) \n                \n                \n                  sup_parents_mean\n                  0.726***\n                  0.242***\n                \n                \n                                  \n                  (0.099) \n                  (0.033) \n                \n                \n                  Num.Obs.        \n                  283     \n                  283     \n                \n                \n                  R2              \n                  0.489   \n                  0.489   \n                \n                \n                  R2 Adj.         \n                  0.482   \n                  0.482   \n                \n                \n                  AIC             \n                  1044.6  \n                  422.8   \n                \n                \n                  BIC             \n                  1066.4  \n                  444.6   \n                \n                \n                  Log.Lik.        \n                  -516.288\n                  -205.381\n                \n                \n                  RMSE            \n                  1.50    \n                  0.50    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\ng1 = modelplot(mod_sum_parcel, coef_omit = 'Intercept') +  aes(color = ifelse(p.value < 0.001, \"Significant at 0.001\", \"Not significant at 0.001\")) +\n  scale_color_manual(values = c(\"grey\", \"blue\")) + ggtitle(\"Significance of Coefficient Values\", \"At Different Levels\")\n\n\ng2 = modelplot(mod_mean_parcel, coef_omit = 'Intercept') +  aes(color = ifelse(p.value < 0.05, \"Significant at 0.05\", \"Not significant at 0.05\")) +\n  scale_color_manual(values = c(\"grey\", \"black\"))\n\nplot <- ggarrange(g1,g2, ncol=2, nrow=1);"
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#hierarchical-models",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#hierarchical-models",
    "title": "Measurement, Latent Factors and Theory Construction",
    "section": "Hierarchical Models",
    "text": "Hierarchical Models\nThe garden of forking paths presents itself within any set of covariates. How do we represent their effects? Which interactions are meaningful? How do we argue for one design over another? The questionable paths are multiplied when we begin to consider additional covariates and group effects.\nLet’s assess the question but this time we allow the model to account for differences in region.\n\nformula_hierarchy_mean = \"ls_mean ~ sup_parents_p1 + sup_parents_p2 + sup_parents_p3 + sup_friends_p1 + sup_friends_p2 + sup_friends_p3 + se_acad_p1 + se_acad_p2 + se_acad_p3 +\nse_social_p1 + se_social_p2 + se_social_p3  + (1 | region)\"\n\nformula_hierarchy_sum = \"ls_sum ~ sup_parents_p1 + sup_parents_p2 + sup_parents_p3 + sup_friends_p1 + sup_friends_p2 + sup_friends_p3 + se_acad_p1 + se_acad_p2 + se_acad_p3 +\nse_social_p1 + se_social_p2 + se_social_p3 + (1 | region)\"\n\nhierarchical_mean_fit <- lmer(formula_hierarchy_mean, data = df, REML = TRUE)\n\nboundary (singular) fit: see help('isSingular')\n\nhierarchical_sum_fit <- lmer(formula_hierarchy_sum, data = df, REML = TRUE)\n\nboundary (singular) fit: see help('isSingular')\n\ng1 = modelplot(hierarchical_mean_fit, re.form=NA) +  aes(color = ifelse(p.value < 0.001, \"Significant at 0.001\", \"Not significant at 0.001\")) +\n  scale_color_manual(values = c(\"grey\", \"blue\")) + ggtitle(\"Significance of Coefficient Values\", \"At Different Levels\")\n\ng2 = modelplot(hierarchical_sum_fit, re.form=NA) +  aes(color = ifelse(p.value < 0.05, \"Significant at 0.05\", \"Not significant 0.05\")) +\n  scale_color_manual(values = c(\"grey\", \"black\"))\n\n\nplot <- ggarrange(g1,g2, ncol=2, nrow=1);\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_segment()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\n\nmodelsummary(list(\"hierarchical_mean_fit\"= hierarchical_mean_fit,\n                  \"hierarchical_sum_fit\"= hierarchical_sum_fit), \n             stars = TRUE) |> \n style_tt(\n   i = 2:25,\n   j = 1:1,\n   background = \"#17C2AD\",\n   color = \"white\",\n   italic = TRUE)\n\n\n \n\n  \n    \n    \n    tinytable_6dqjw6ypw5guaj07q8f6\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                hierarchical_mean_fit\n                hierarchical_sum_fit\n              \n        \n        + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n        \n                \n                  (Intercept)          \n                  0.698*  \n                  2.094*  \n                \n                \n                                       \n                  (0.318) \n                  (0.954) \n                \n                \n                  sup_parents_p1       \n                  0.024   \n                  0.072   \n                \n                \n                                       \n                  (0.048) \n                  (0.143) \n                \n                \n                  sup_parents_p2       \n                  0.039   \n                  0.118   \n                \n                \n                                       \n                  (0.048) \n                  (0.144) \n                \n                \n                  sup_parents_p3       \n                  0.175***\n                  0.526***\n                \n                \n                                       \n                  (0.042) \n                  (0.126) \n                \n                \n                  sup_friends_p1       \n                  -0.091+ \n                  -0.272+ \n                \n                \n                                       \n                  (0.050) \n                  (0.150) \n                \n                \n                  sup_friends_p2       \n                  0.110*  \n                  0.331*  \n                \n                \n                                       \n                  (0.053) \n                  (0.160) \n                \n                \n                  sup_friends_p3       \n                  0.055   \n                  0.165   \n                \n                \n                                       \n                  (0.043) \n                  (0.130) \n                \n                \n                  se_acad_p1           \n                  -0.069  \n                  -0.208  \n                \n                \n                                       \n                  (0.064) \n                  (0.192) \n                \n                \n                  se_acad_p2           \n                  0.109   \n                  0.327   \n                \n                \n                                       \n                  (0.067) \n                  (0.202) \n                \n                \n                  se_acad_p3           \n                  0.051   \n                  0.153   \n                \n                \n                                       \n                  (0.058) \n                  (0.174) \n                \n                \n                  se_social_p1         \n                  0.118+  \n                  0.355+  \n                \n                \n                                       \n                  (0.067) \n                  (0.200) \n                \n                \n                  se_social_p2         \n                  0.170*  \n                  0.509*  \n                \n                \n                                       \n                  (0.073) \n                  (0.219) \n                \n                \n                  se_social_p3         \n                  0.148** \n                  0.443** \n                \n                \n                                       \n                  (0.054) \n                  (0.161) \n                \n                \n                  SD (Intercept region)\n                  0.000   \n                  0.000   \n                \n                \n                  SD (Observations)    \n                  0.498   \n                  1.493   \n                \n                \n                  Num.Obs.             \n                  283     \n                  283     \n                \n                \n                  R2 Marg.             \n                  0.506   \n                  0.506   \n                \n                \n                  AIC                  \n                  482.6   \n                  1075.9  \n                \n                \n                  BIC                  \n                  537.3   \n                  1130.6  \n                \n                \n                  RMSE                 \n                  0.49    \n                  1.46    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\nHierarchical Marginal Effects\n\ng = plot_predictions(hierarchical_mean_fit, condition = c(\"sup_parents_p3\", \"region\"), type = \"response\", re.form=NA) + ggtitle(\"Counterfactual Shift of Outcome: sup_parents_p3\", \"Holding all else Fixed\")\n\ng1 = plot_predictions(hierarchical_mean_fit, condition = c(\"sup_friends_p1\", \"region\"), type = \"response\", re.form=NA) + ggtitle(\"Counterfactual Shift of Outcome: sup_friends_p1\", \"Holding all else Fixed\")\n\ng2 = plot_predictions(hierarchical_mean_fit, condition = c(\"se_acad_p1\", \"region\"), \n                      type = \"response\", re.form=NA) + ggtitle(\"Counterfactual Shift of Outcome: se_acad_p1\", \"Holding all else Fixed\")\n\nplot <- ggarrange(g,g1,g2, ncol=1, nrow=3);"
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#confirmatory-factor-analysis",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#confirmatory-factor-analysis",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Confirmatory Factor Analysis",
    "text": "Confirmatory Factor Analysis\nWe will illustrate the details of confirmatory factor modelling using the lavaan framework. The focus is mostly on the mechanics of how these models are estimated using maximum likelihood techniques before illustrating the salient differences of Bayesian estimation.\nFirst recall that the idea of confirmatory factor analysis is that there are some latent constructs which determine our data generating process. In our survey we’ve already clustered our questions by themes so it makes sense to extend this idea to posit latent constructs mapped to each of these themes. In the jargon of structural equation models this is called the measurement model.\n\nmodel_measurement <- \"\n# Measurement model\nSUP_Parents =~ sup_parents_p1 + sup_parents_p2 + sup_parents_p3\nSUP_Friends =~ sup_friends_p1 + sup_friends_p2 + sup_friends_p3\nSE_Academic =~ se_acad_p1 + se_acad_p2 + se_acad_p3\nSE_Social =~ se_social_p1 + se_social_p2 + se_social_p3\nLS  =~ ls_p1 + ls_p2 + ls_p3\n\"\n\nmodel_measurement1 <- \"\n# Measurement model\nSUP_Parents =~ b1*sup_parents_p1 + b2*sup_parents_p2 + b3*sup_parents_p3\nSUP_Friends =~ a1*sup_friends_p1 + a2*sup_friends_p2 + a3*sup_friends_p3\nSE_Academic =~ c1*se_acad_p1 + c2*se_acad_p2 + c3*se_acad_p3\nSE_Social =~ se_social_p1 + se_social_p2 + se_social_p3\nLS  =~ ls_p1 + ls_p2 + ls_p3\n\na1 == a2 \na1 == a3\nb1 == b2\nb1 == b3\nc1 == c2\nc1 == c3\n\n\"\n\nfit_mod <- cfa(model_measurement, data = df)\nfit_mod_1<- cfa(model_measurement1, data = df)\n\nIn the above syntax we have specified two slightly different measurement models. In each case we allow that the questions of our survey are mapped to an appropriate latent factor e.g LS =~ ls_p1 + ls_p2 + ls_p3. The “=~” syntax denotes a “measured by” relationship in which the goal is to estimate how each of observed measurements load on the latent factor. In the first model we have allowed each of the factor loadings to be estimated freely, but in the second we have forced equal weights on the SUP_Friends construct. A benefit of this framework is that we do not have to resort to crude aggregations like sum-scores or mean-scores over the outcome variables we can allow that they vary freely and let the model estimate the multivariate relationships between the observed variables and these latent constructs.\nIf we plot the estimated parameters as before we’ll see some additional parameters reported.\n\ncfa_models = list(\"full_measurement_model\" = fit_mod, \n     \"measurement_model_reduced\" = fit_mod_1)\nmodelplot(cfa_models)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\nHere there are two distinct types of parameters: (i) the factor loadings accorded (=~) to the individual observed metrics and (ii) the covariances (~~) between the latent constructs. We can further report the extent of the model fit summaries.\n\nsummary(fit_mod, fit.measures = TRUE, standardized = TRUE) \n\nlavaan 0.6-18 ended normally after 49 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        40\n\n  Number of observations                           283\n\nModel Test User Model:\n                                                      \n  Test statistic                               223.992\n  Degrees of freedom                                80\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              2696.489\n  Degrees of freedom                               105\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.944\n  Tucker-Lewis Index (TLI)                       0.927\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4285.972\n  Loglikelihood unrestricted model (H1)      -4173.976\n                                                      \n  Akaike (AIC)                                8651.944\n  Bayesian (BIC)                              8797.761\n  Sample-size adjusted Bayesian (SABIC)       8670.921\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.080\n  90 Percent confidence interval - lower         0.067\n  90 Percent confidence interval - upper         0.092\n  P-value H_0: RMSEA <= 0.050                    0.000\n  P-value H_0: RMSEA >= 0.080                    0.500\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.056\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  SUP_Parents =~                                                        \n    sup_parents_p1    1.000                               0.935    0.873\n    sup_parents_p2    1.036    0.056   18.613    0.000    0.969    0.887\n    sup_parents_p3    0.996    0.059   16.754    0.000    0.932    0.816\n  SUP_Friends =~                                                        \n    sup_friends_p1    1.000                               1.021    0.906\n    sup_friends_p2    0.792    0.043   18.463    0.000    0.809    0.857\n    sup_friends_p3    0.891    0.050   17.741    0.000    0.910    0.831\n  SE_Academic =~                                                        \n    se_acad_p1        1.000                               0.695    0.878\n    se_acad_p2        0.809    0.050   16.290    0.000    0.562    0.820\n    se_acad_p3        0.955    0.058   16.500    0.000    0.664    0.829\n  SE_Social =~                                                          \n    se_social_p1      1.000                               0.638    0.843\n    se_social_p2      0.967    0.056   17.248    0.000    0.617    0.885\n    se_social_p3      0.928    0.067   13.880    0.000    0.592    0.741\n  LS =~                                                                 \n    ls_p1             1.000                               0.667    0.718\n    ls_p2             0.778    0.074   10.498    0.000    0.519    0.712\n    ls_p3             0.968    0.090   10.730    0.000    0.645    0.732\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  SUP_Parents ~~                                                        \n    SUP_Friends       0.132    0.064    2.073    0.038    0.138    0.138\n    SE_Academic       0.218    0.046    4.727    0.000    0.336    0.336\n    SE_Social         0.282    0.045    6.224    0.000    0.472    0.472\n    LS                0.405    0.057    7.132    0.000    0.650    0.650\n  SUP_Friends ~~                                                        \n    SE_Academic       0.071    0.047    1.493    0.136    0.100    0.100\n    SE_Social         0.196    0.046    4.281    0.000    0.301    0.301\n    LS                0.175    0.051    3.445    0.001    0.257    0.257\n  SE_Academic ~~                                                        \n    SE_Social         0.271    0.036    7.493    0.000    0.611    0.611\n    LS                0.238    0.039    6.065    0.000    0.514    0.514\n  SE_Social ~~                                                          \n    LS                0.321    0.042    7.659    0.000    0.755    0.755\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .sup_parents_p1    0.273    0.037    7.358    0.000    0.273    0.238\n   .sup_parents_p2    0.255    0.038    6.738    0.000    0.255    0.213\n   .sup_parents_p3    0.437    0.048    9.201    0.000    0.437    0.335\n   .sup_friends_p1    0.227    0.040    5.656    0.000    0.227    0.179\n   .sup_friends_p2    0.238    0.030    7.936    0.000    0.238    0.266\n   .sup_friends_p3    0.371    0.042    8.809    0.000    0.371    0.310\n   .se_acad_p1        0.144    0.022    6.593    0.000    0.144    0.229\n   .se_acad_p2        0.153    0.018    8.621    0.000    0.153    0.327\n   .se_acad_p3        0.200    0.024    8.372    0.000    0.200    0.313\n   .se_social_p1      0.166    0.020    8.134    0.000    0.166    0.290\n   .se_social_p2      0.106    0.016    6.542    0.000    0.106    0.217\n   .se_social_p3      0.288    0.028   10.132    0.000    0.288    0.451\n   .ls_p1             0.417    0.045    9.233    0.000    0.417    0.484\n   .ls_p2             0.261    0.028    9.321    0.000    0.261    0.492\n   .ls_p3             0.362    0.040    9.005    0.000    0.362    0.465\n    SUP_Parents       0.875    0.098    8.910    0.000    1.000    1.000\n    SUP_Friends       1.042    0.111    9.407    0.000    1.000    1.000\n    SE_Academic       0.483    0.054    8.880    0.000    1.000    1.000\n    SE_Social         0.407    0.048    8.403    0.000    1.000    1.000\n    LS                0.444    0.069    6.394    0.000    1.000    1.000\n\n\nNote how in addition to the individual parameter estimates the summaries highlight various measures of global model fit. These model fit statistics are important for evaluating alternative ways of parameterising our models. The number of parameters is a real concern in the maximum likelihood approaches to estimating these models. Too many parameters and we can easily over fit to the particular sample data. This stems in part from the limitations of the optimisation goal in the traditional CFA framework - we are intent to optimise model parameters to recover a compelling estimate based on the observed covariance matrix. Once we have more parameters than there are points in the covariance matrix the model is free to overfit considerably. This can then be checked as measure of local model fit and may highlight infelicities or suspicious convergence between the true data generating process and the learned model.\n\ng1 = plot_heatmap(cov(df[,  drivers]))\n\ng2 = plot_heatmap(data.frame(fitted(fit_mod)$cov)[drivers, drivers], title=\"Model Implied Covariances\", \"Fitted Values\")\n\nresids = cov(df[,  drivers]) - data.frame(fitted(fit_mod)$cov)[drivers, drivers]\n\ng3 = plot_heatmap(resids, title=\"Residuals\", \"Fitted Values versus Observe Sample Covariance\")\n\n\nplot <- ggarrange(g1,g2,g3, ncol=1, nrow=3);\n\n\n\n\n\nSummary Global Fit Measures\nWe can also compare models based on their global measures of model fit giving some indication of whether parameter specifications improve or reduce fidelity with the true data generating process.\n\nsummary_df = cbind(fitMeasures(fit_mod, c(\"chisq\", \"baseline.chisq\", \"cfi\", \"aic\", \"bic\", \"rmsea\",\"srmr\")),\n      fitMeasures(fit_mod_1, c(\"chisq\", \"baseline.chisq\", \"cfi\", \"aic\", \"bic\", \"rmsea\",\"srmr\")))\ncolnames(summary_df) = c('Full Model', 'Reduced Model')\n\nsummary_df |> kable() |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n \n  \n     \n    Full Model \n    Reduced Model \n  \n \n\n  \n    chisq \n    223.9922306 \n    256.0287010 \n  \n  \n    baseline.chisq \n    2696.4887420 \n    2696.4887420 \n  \n  \n    cfi \n    0.9444365 \n    0.9343896 \n  \n  \n    aic \n    8651.9435210 \n    8671.9799914 \n  \n  \n    bic \n    8797.7613969 \n    8795.9251859 \n  \n  \n    rmsea \n    0.0797501 \n    0.0835831 \n  \n  \n    srmr \n    0.0558656 \n    0.0625710 \n  \n\n\n\n\n\nThere a wealth of metrics associated with CFA model fit and it can be hard to see the forest for the trees.\n\n\nVisualising the Relationships\nOne of the better ways to visualise these models is to use the semPlot package. Here we can plot all the parameter estimates in one graph. Following convention the rectangular boxes represent observed measures. Oval or circular objects represent the latent constructs. The self-directed arrows on each node is the variance of that measure. The two-way arrows between nodes represents the covariance between those two nodes. The single headed arrows from the latent construct to the indicator variables denotes the factor loading of the variable on the construct. For instance SUP_F -> sp_f_3 is set at 0.89.\n\nsemPlot::semPaths(fit_mod, whatLabels = 'est', intercepts = FALSE, layout = \"spring\",)\n\n\n\n\nFor instance, in this plot you can see that for each latent construct one of the indicator variables has their factor loading set to 1. This is a mathematical requirement we’ll see below that is used to ensure identifiability of the parameters akin to setting a reference category in categorical regression. In the plot you can “read-off” the covariances between our constructs e.g. the covariance between LS and SUP_P is 0.36 the largest value amongst the set of covariances.\n\n\nComparing Models\nWe can use a variety of chi-squared tests to evaluate the goodness of fit for our models. If we pass in each model individually we perform a test comparing our model to the saturated model. The Chi-Squared test compares the model-implied variance-covariance matrix (expected) to the variance-covariance matrix computed from the actual data (observed). The null hypothesis for the Chi-Squared Goodness-of-Fit test is that the model fits the data perfectly, meaning that there is no significant difference between the observed and model-implied variance-covariance matrices. The goal is to see if the differences between these matrices are large enough that we can reject the null.\n\nlavTestLRT(fit_mod)\n\nChi-Squared Test Statistic (unscaled)\n\n          Df    AIC    BIC  Chisq Chisq diff Df diff           Pr(>Chisq)    \nSaturated  0                 0.00                                            \nModel     80 8651.9 8797.8 223.99     223.99      80 0.000000000000001443 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPassing in the one model we can reject the null hypothesis that the saturated model’s (perfect fit) and the candidate variance-covariance matrix are drawn from the same distribution. Comparing between our two model fits we also reject that these two models are drawn from the same distribution.\n\nlavTestLRT(fit_mod, fit_mod_1)\n\n\nChi-Squared Difference Test\n\n          Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff Pr(>Chisq)    \nfit_mod   80 8651.9 8797.8 223.99                                          \nfit_mod_1 86 8672.0 8795.9 256.03     32.036 0.12383       6 0.00001606 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe test also reports a number of other fit indices and the degrees of freedom available to each model. These are important because the Chi-Squared test is overly sensetive in large-sample data and the model adequacy is a multi-dimensional question."
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#structural-equation-models",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#structural-equation-models",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Structural Equation Models",
    "text": "Structural Equation Models\nSo far so good. We have a confirmatory factor measurement model. We’ve structured it so that we can make inferences about the correlations and covariances between 5 latent constructs of independent interest. We’ve calibrated the model fit statistics by ensuring the model can reasonably recover the observed variance-covariance structure. But what about our dependency relations between constructs? We can evaluate these too! Adding regressions to our model allows to express these relationships and then recover summary statistics of the same.\n\nmodel <- \"\n# Measurement model\nSUP_Parents =~ sup_parents_p1 + sup_parents_p2 + sup_parents_p3\nSUP_Friends =~ sup_friends_p1 + sup_friends_p2 + sup_friends_p3\nSE_Academic =~ se_acad_p1 + se_acad_p2 + se_acad_p3\nSE_Social =~ se_social_p1 + se_social_p2 + se_social_p3\nLS  =~ ls_p1 + ls_p2 + ls_p3\n\n# Structural model \n# Regressions\nSE_Academic ~ SUP_Parents + SUP_Friends\nSE_Social ~ SUP_Parents + SUP_Friends\nLS ~ SE_Academic + SE_Social + SUP_Parents + SUP_Friends\n\n# Residual covariances\nSE_Academic ~~ SE_Social\n\"\n\nfit_mod_sem <- sem(model, data = df)\n\nThis model structure adds a set of directional arrows between the latent constructs\n\nsemPlot::semPaths(fit_mod_sem, whatLabels = 'est', intercepts = FALSE)\n\n\n\n\nThe relative uncertainty in each estimates\n\nmodelplot(fit_mod_sem)\n\n\n\n\nCompare this structure against the previously simpler measurement model and we observe a puzzling phenomena. The models report identical measures of fit!\n\nsummary_df = cbind(fitMeasures(fit_mod, c(\"chisq\", \"baseline.chisq\", \"cfi\", \"aic\", \"bic\", \"rmsea\",\"srmr\")),\n      fitMeasures(fit_mod_sem, c(\"chisq\", \"baseline.chisq\", \"cfi\", \"aic\", \"bic\", \"rmsea\",\"srmr\")),\n      fitMeasures(fit_mod_1, c(\"chisq\", \"baseline.chisq\", \"cfi\", \"aic\", \"bic\", \"rmsea\",\"srmr\")))\ncolnames(summary_df) = c('Full Model', 'SEM Model', 'Reduced Model')\n\nsummary_df |> kable() |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n \n  \n     \n    Full Model \n    SEM Model \n    Reduced Model \n  \n \n\n  \n    chisq \n    223.9922306 \n    223.9922306 \n    256.0287010 \n  \n  \n    baseline.chisq \n    2696.4887420 \n    2696.4887420 \n    2696.4887420 \n  \n  \n    cfi \n    0.9444365 \n    0.9444365 \n    0.9343896 \n  \n  \n    aic \n    8651.9435210 \n    8651.9435210 \n    8671.9799914 \n  \n  \n    bic \n    8797.7613969 \n    8797.7613969 \n    8795.9251859 \n  \n  \n    rmsea \n    0.0797501 \n    0.0797501 \n    0.0835831 \n  \n  \n    srmr \n    0.0558656 \n    0.0558655 \n    0.0625710 \n  \n\n\n\n\n\nThe issue here is that the models have the same degrees of freedom which suggests in some sense we have already saturated our model fit and are unable to evaluate further parameter estimates.\n\nlavTestLRT(fit_mod_sem, fit_mod)\n\nWarning: lavaan->lavTestLRT():  \n   some models have the same degrees of freedom\n\n\n\nChi-Squared Difference Test\n\n            Df    AIC    BIC  Chisq      Chisq diff RMSEA Df diff Pr(>Chisq)\nfit_mod_sem 80 8651.9 8797.8 223.99                                         \nfit_mod     80 8651.9 8797.8 223.99 0.0000000043218     0       0           \n\nlavTestLRT(fit_mod_sem, fit_mod_1)\n\n\nChi-Squared Difference Test\n\n            Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff Pr(>Chisq)    \nfit_mod_sem 80 8651.9 8797.8 223.99                                          \nfit_mod_1   86 8672.0 8795.9 256.03     32.036 0.12383       6 0.00001606 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can see this similarly in the plotted residuals which are identical across the models despite meaningful structural differences.\n\nheat_df = data.frame(resid(fit_mod)$cov) \nheat_df = heat_df |> as.matrix() |> melt()\ncolnames(heat_df) <- c(\"x\", \"y\", \"value\")\n\ng1 = heat_df  |> mutate(value = round(value, 2)) |>  ggplot(aes(x = x, y = y, fill = value)) +\n  geom_tile() + geom_text(aes(label = value), color = \"black\", size = 4) +\n scale_fill_gradient2(\n    high = 'dodgerblue4',\n    mid = 'white',\n    low = 'firebrick2'\n  ) + theme(axis.text.x = element_text(angle=45)) + ggtitle(\"Residuals of the Sample Covariances and Model Implied Covariances\", \"A Visual Check of Structural-Model fit\")\n\n\nheat_df = data.frame(resid(fit_mod_sem)$cov) \nheat_df = heat_df |> as.matrix() |> melt()\ncolnames(heat_df) <- c(\"x\", \"y\", \"value\")\n\ng2 = heat_df  |> mutate(value = round(value, 2)) |>  ggplot(aes(x = x, y = y, fill = value)) +\n  geom_tile() + geom_text(aes(label = value), color = \"black\", size = 4) +\n scale_fill_gradient2(\n    high = 'dodgerblue4',\n    mid = 'white',\n    low = 'firebrick2'\n  ) + theme(axis.text.x = element_text(angle=45)) + ggtitle(\"Residuals of the Sample Covariances and Model Implied Covariances\", \"A Visual Check of Measurement-Model fit\")\n\n\nplot <- ggarrange(g1,g2, ncol=1, nrow=2);\n\n\n\n\nThis is a genuine limitation in the expressive power of SEM models when they are fit using maximum likelihood with finite degrees of freedom optimising for fidelity to the sample variance-covariance matrix. Model idenfification possibilities are constrained by the optimisation goal and the degrees of freedom available to the optimiser. Next we’ll show how the Bayesian approach to estimating these models frees us from this limitation."
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#now-soem-python",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#now-soem-python",
    "title": "Confirmatory Factor Analysis and Structural Equation Models",
    "section": "Now soem Python",
    "text": "Now soem Python\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pytensor import tensor as pt\nimport arviz as az\nimport networkx as nx\n\n\n\ndf_p = pd.read_csv('IIS.dat', sep='\\s+')\ndf_p.head()\n\n     PI    AD   IGC   FI   FC\n0  4.00  3.38  4.67  2.6  3.2\n1  2.57  3.00  3.50  2.4  2.8\n2  2.29  3.29  4.83  2.0  3.4\n3  2.43  3.63  4.33  3.6  3.8\n4  3.00  4.00  4.83  3.4  3.8\n\n\n\ncoords = {'obs': list(range(len(df_p))), \n          'indicators': ['PI', 'AD',    'IGC', 'FI', 'FC'],\n          'indicators_1': ['PI', 'AD',  'IGC'],\n          'indicators_2': ['FI', 'FC'],\n          'latent': ['Student', 'Faculty']\n          }\n\n\nobs_idx = list(range(len(df_p)))\nwith pm.Model(coords=coords) as model:\n  \n  Psi = pm.InverseGamma('Psi', 5, 10, dims='indicators')\n  lambdas_ = pm.Normal('lambdas_1', 1, 10, dims=('indicators_1'))\n  lambdas_1 = pm.Deterministic('lambdas1', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_1'))\n  lambdas_ = pm.Normal('lambdas_2', 1, 10, dims=('indicators_2'))\n  lambdas_2 = pm.Deterministic('lambdas2', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_2'))\n  tau = pm.Normal('tau', 3, 10, dims='indicators')\n  kappa = 0\n  sd_dist = pm.Exponential.dist(1.0, shape=2)\n  chol, _, _ = pm.LKJCholeskyCov('chol_cov', n=2, eta=2,\n    sd_dist=sd_dist, compute_corr=True)\n  ksi = pm.MvNormal('ksi', kappa, chol=chol, dims=('obs', 'latent'))\n\n  m1 = tau[0] + ksi[obs_idx, 0]*lambdas_1[0]\n  m2 = tau[1] + ksi[obs_idx, 0]*lambdas_1[1]\n  m3 = tau[2] + ksi[obs_idx, 0]*lambdas_1[2]\n  m4 = tau[3] + ksi[obs_idx, 1]*lambdas_2[0]\n  m5 = tau[4] + ksi[obs_idx, 1]*lambdas_2[1]\n  \n  mu = pm.Deterministic('mu', pm.math.stack([m1, m2, m3, m4, m5]).T)\n  _  = pm.Normal('likelihood', mu, Psi, observed=df_p.values)\n\n  idata = pm.sample(nuts_sampler='numpyro', target_accept=.95)\n\n\n\n\nPyMC Confirmatory Factor Model"
  },
  {
    "objectID": "scratch_work/testing_pymc_sem.html",
    "href": "scratch_work/testing_pymc_sem.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "import pymc as pm\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pytensor import tensor as pt\nimport arviz as az\n\n\n\n\n\ndf_p = pd.read_csv('IIS.dat', sep='\\s+')\n\ncoords = {'obs': list(range(len(df_p))), 'indicators': ['PI', 'AD', 'IGC', 'FI', 'FC']}\n\nN = len(df_p)\nM = 5\nwith pm.Model(coords=coords) as model:\n  \n  Psi = pm.InverseGamma('Psi', 5, 10, dims='indicators')\n  lambdas_ = pm.Normal('lambdas_', 1, 10, dims=('indicators'))\n  lambdas_ = pm.Deterministic('lambdas', pt.set_subtensor(lambdas_[0], 1))\n  tau = pm.Normal('tau', 3, 10, dims='indicators')\n  psi = pm.InverseGamma('psi_', 5, 10)\n  kappa = 0\n  ksi = pm.Normal('ksi', kappa, psi, dims='obs')\n  mus = []\n  for j in range(M):\n    m = tau[j] + ksi*lambdas_[j]\n    mus.append(m)\n\n  mu = pm.Deterministic('mu', pm.math.stack(mus).T)\n  _  = pm.Normal('likelihood', mu, Psi, observed=df_p.values)\n\n  idata = pm.sample()\n\n\n\npm.model_to_graphviz(model)\n\n\naz.summary(idata, var_names=['lambdas', 'tau', 'Psi', 'psi_', 'ksi'], coords= {'obs': [0, 7]})\n\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/diagnostics.py:592: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      lambdas[0]\n      1.000\n      0.000\n      1.000\n      1.000\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      lambdas[1]\n      0.815\n      0.052\n      0.719\n      0.910\n      0.002\n      0.002\n      515.0\n      1093.0\n      1.01\n    \n    \n      lambdas[2]\n      0.474\n      0.039\n      0.399\n      0.548\n      0.001\n      0.001\n      921.0\n      1357.0\n      1.00\n    \n    \n      lambdas[3]\n      1.105\n      0.075\n      0.963\n      1.245\n      0.003\n      0.002\n      693.0\n      1326.0\n      1.01\n    \n    \n      lambdas[4]\n      1.060\n      0.068\n      0.935\n      1.189\n      0.003\n      0.002\n      561.0\n      1206.0\n      1.01\n    \n    \n      tau[PI]\n      3.333\n      0.037\n      3.265\n      3.401\n      0.001\n      0.001\n      1380.0\n      2153.0\n      1.00\n    \n    \n      tau[AD]\n      3.898\n      0.026\n      3.852\n      3.950\n      0.001\n      0.001\n      1204.0\n      1869.0\n      1.01\n    \n    \n      tau[IGC]\n      4.596\n      0.020\n      4.560\n      4.635\n      0.000\n      0.000\n      1867.0\n      2455.0\n      1.00\n    \n    \n      tau[FI]\n      3.033\n      0.039\n      2.961\n      3.110\n      0.001\n      0.001\n      1331.0\n      2103.0\n      1.00\n    \n    \n      tau[FC]\n      3.712\n      0.035\n      3.647\n      3.780\n      0.001\n      0.001\n      1136.0\n      1954.0\n      1.00\n    \n    \n      Psi[PI]\n      0.598\n      0.023\n      0.556\n      0.641\n      0.000\n      0.000\n      4434.0\n      3298.0\n      1.00\n    \n    \n      Psi[AD]\n      0.362\n      0.017\n      0.331\n      0.396\n      0.000\n      0.000\n      2419.0\n      2866.0\n      1.00\n    \n    \n      Psi[IGC]\n      0.375\n      0.014\n      0.349\n      0.401\n      0.000\n      0.000\n      5071.0\n      2872.0\n      1.00\n    \n    \n      Psi[FI]\n      0.605\n      0.025\n      0.561\n      0.654\n      0.000\n      0.000\n      3199.0\n      3160.0\n      1.00\n    \n    \n      Psi[FC]\n      0.481\n      0.022\n      0.440\n      0.522\n      0.000\n      0.000\n      2389.0\n      2385.0\n      1.00\n    \n    \n      psi_\n      0.601\n      0.035\n      0.537\n      0.667\n      0.001\n      0.001\n      597.0\n      1241.0\n      1.00\n    \n    \n      ksi[0]\n      -0.236\n      0.225\n      -0.641\n      0.189\n      0.002\n      0.003\n      8287.0\n      3069.0\n      1.00\n    \n    \n      ksi[7]\n      0.848\n      0.225\n      0.423\n      1.277\n      0.003\n      0.002\n      5136.0\n      3133.0\n      1.00\n    \n  \n\n\n\n\n\ncoords = {'obs': list(range(len(df_p))), \n          'indicators': ['PI', 'AD',    'IGC', 'FI', 'FC'],\n          'indicators_1': ['PI', 'AD',  'IGC'],\n          'indicators_2': ['FI', 'FC'],\n          'latent': ['Student', 'Faculty']\n          }\n\nN = len(df_p)\nM = 5\nF = 2\nindicator_idx = [0, 1, 2, 3, 4]\nlatent_idx = [0, 0, 0, 1, 1]\nobs_idx = list(range(len(df_p)))\nwith pm.Model(coords=coords) as model:\n  \n  Psi = pm.InverseGamma('Psi', 5, 10, dims='indicators')\n  lambdas_ = pm.Normal('lambdas_1', 1, 10, dims=('indicators_1'))\n  lambdas_1 = pm.Deterministic('lambdas1', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_1'))\n  lambdas_ = pm.Normal('lambdas_2', 1, 10, dims=('indicators_2'))\n  lambdas_2 = pm.Deterministic('lambdas2', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_2'))\n  tau = pm.Normal('tau', 3, 10, dims='indicators')\n  kappa = 0\n  sd_dist = pm.Exponential.dist(1.0, shape=2)\n  chol, _, _ = pm.LKJCholeskyCov('chol_cov', n=2, eta=2,\n    sd_dist=sd_dist, compute_corr=True)\n  ksi = pm.MvNormal('ksi', kappa, chol=chol, dims=('obs', 'latent'))\n\n  m1 = tau[0] + ksi[obs_idx, 0]*lambdas_1[0]\n  m2 = tau[1] + ksi[obs_idx, 0]*lambdas_1[1]\n  m3 = tau[2] + ksi[obs_idx, 0]*lambdas_1[2]\n  m4 = tau[3] + ksi[obs_idx, 1]*lambdas_2[0]\n  m5 = tau[4] + ksi[obs_idx, 1]*lambdas_2[1]\n  \n  mu = pm.Deterministic('mu', pm.math.stack([m1, m2, m3, m4, m5]).T)\n  _  = pm.Normal('likelihood', mu, Psi, observed=df_p.values)\n\n  idata = pm.sample(nuts_sampler='numpyro', target_accept=.95)\n\n\n\npm.model_to_graphviz(model)\n\nCompiling...\nCompilation time = 0:00:01.713705\nSampling...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling time = 0:00:06.464597\nTransforming variables...\nTransformation time = 0:00:00.285241\n\n\n\n\n\n\naz.summary(idata, var_names=['lambdas1', 'lambdas2', 'tau', 'Psi', 'ksi', 'chol_cov_corr'], \n           coords= {'obs': [0, 7]})\n\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/diagnostics.py:592: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/diagnostics.py:592: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/diagnostics.py:592: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      lambdas1[PI]\n      1.000\n      0.000\n      1.000\n      1.000\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      lambdas1[AD]\n      0.903\n      0.062\n      0.790\n      1.022\n      0.003\n      0.002\n      364.0\n      746.0\n      1.00\n    \n    \n      lambdas1[IGC]\n      0.537\n      0.046\n      0.450\n      0.620\n      0.002\n      0.001\n      485.0\n      930.0\n      1.00\n    \n    \n      lambdas2[FI]\n      1.000\n      0.000\n      1.000\n      1.000\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      lambdas2[FC]\n      0.978\n      0.055\n      0.879\n      1.087\n      0.003\n      0.002\n      475.0\n      831.0\n      1.00\n    \n    \n      tau[PI]\n      3.332\n      0.037\n      3.259\n      3.397\n      0.002\n      0.001\n      556.0\n      1306.0\n      1.01\n    \n    \n      tau[AD]\n      3.896\n      0.027\n      3.846\n      3.947\n      0.001\n      0.001\n      395.0\n      824.0\n      1.01\n    \n    \n      tau[IGC]\n      4.595\n      0.020\n      4.557\n      4.633\n      0.001\n      0.001\n      610.0\n      1577.0\n      1.01\n    \n    \n      tau[FI]\n      3.032\n      0.040\n      2.955\n      3.107\n      0.002\n      0.001\n      469.0\n      1198.0\n      1.00\n    \n    \n      tau[FC]\n      3.711\n      0.035\n      3.643\n      3.774\n      0.002\n      0.001\n      392.0\n      983.0\n      1.00\n    \n    \n      Psi[PI]\n      0.610\n      0.025\n      0.565\n      0.656\n      0.001\n      0.000\n      1440.0\n      2685.0\n      1.00\n    \n    \n      Psi[AD]\n      0.317\n      0.020\n      0.282\n      0.354\n      0.001\n      0.001\n      718.0\n      1290.0\n      1.00\n    \n    \n      Psi[IGC]\n      0.355\n      0.013\n      0.332\n      0.382\n      0.000\n      0.000\n      2489.0\n      2569.0\n      1.00\n    \n    \n      Psi[FI]\n      0.568\n      0.026\n      0.523\n      0.620\n      0.001\n      0.001\n      1125.0\n      2174.0\n      1.00\n    \n    \n      Psi[FC]\n      0.422\n      0.026\n      0.371\n      0.467\n      0.001\n      0.001\n      670.0\n      1590.0\n      1.01\n    \n    \n      ksi[0, Student]\n      -0.222\n      0.221\n      -0.638\n      0.201\n      0.004\n      0.003\n      3391.0\n      2961.0\n      1.00\n    \n    \n      ksi[0, Faculty]\n      -0.361\n      0.274\n      -0.863\n      0.163\n      0.005\n      0.004\n      3708.0\n      2762.0\n      1.00\n    \n    \n      ksi[7, Student]\n      0.891\n      0.230\n      0.465\n      1.324\n      0.004\n      0.003\n      3365.0\n      2913.0\n      1.00\n    \n    \n      ksi[7, Faculty]\n      0.881\n      0.281\n      0.394\n      1.447\n      0.004\n      0.003\n      3960.0\n      2808.0\n      1.00\n    \n    \n      chol_cov_corr[0, 0]\n      1.000\n      0.000\n      1.000\n      1.000\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      chol_cov_corr[0, 1]\n      0.851\n      0.029\n      0.797\n      0.905\n      0.002\n      0.001\n      356.0\n      674.0\n      1.01\n    \n    \n      chol_cov_corr[1, 0]\n      0.851\n      0.029\n      0.797\n      0.905\n      0.002\n      0.001\n      356.0\n      674.0\n      1.01\n    \n    \n      chol_cov_corr[1, 1]\n      1.000\n      0.000\n      1.000\n      1.000\n      0.000\n      0.000\n      3955.0\n      3960.0\n      1.00\n    \n  \n\n\n\n\n\naz.plot_trace(idata, var_names=['lambdas1', 'lambdas2', 'tau', 'Psi', 'ksi', 'chol_cov_corr'])\n\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:487: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:487: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:487: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:699: RuntimeWarning: divide by zero encountered in divide\n  f = grid_counts / bin_width / len(x)\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:699: RuntimeWarning: invalid value encountered in divide\n  f = grid_counts / bin_width / len(x)\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:702: RuntimeWarning: divide by zero encountered in scalar divide\n  bw /= bin_width\n\n\nOverflowError: cannot convert float infinity to integer\n\n\n\n\n\n\ndf = pd.read_csv('../posts/post-with-code/CFA_AND_SEM/sem_data.csv')\ndrivers = ['se_acad_p1', 'se_acad_p2',\n       'se_acad_p3', 'se_social_p1', 'se_social_p2', 'se_social_p3',\n       'sup_friends_p1', 'sup_friends_p2', 'sup_friends_p3', 'sup_parents_p1',\n       'sup_parents_p2', 'sup_parents_p3', 'ls_p1', 'ls_p2', 'ls_p3']\ndrivers\n\n['se_acad_p1',\n 'se_acad_p2',\n 'se_acad_p3',\n 'se_social_p1',\n 'se_social_p2',\n 'se_social_p3',\n 'sup_friends_p1',\n 'sup_friends_p2',\n 'sup_friends_p3',\n 'sup_parents_p1',\n 'sup_parents_p2',\n 'sup_parents_p3',\n 'ls_p1',\n 'ls_p2',\n 'ls_p3']\n\n\n\ndef make_indirect_sem(priors): \n\n  coords = {'obs': list(range(len(df))), \n            'indicators': drivers,\n            'indicators_1': ['se_acad_p1','se_acad_p2','se_acad_p3'],\n            'indicators_2': ['se_social_p1','se_social_p2','se_social_p3'],\n            'indicators_3': ['sup_friends_p1','sup_friends_p2','sup_friends_p3'],\n            'indicators_4': [ 'sup_parents_p1','sup_parents_p2','sup_parents_p3'],\n            'indicators_5': ['ls_p1','ls_p2', 'ls_p3'],\n            'latent': ['SUP_F', 'SUP_P'], \n            'latent1': ['SUP_F', 'SUP_P'], \n            'latent_regression': ['SUP_F->SE_ACAD', 'SUP_P->SE_ACAD', 'SUP_F->SE_SOC', 'SUP_P->SE_SOC'],\n            'regression': ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P']\n            }\n\n  obs_idx = list(range(len(df)))\n  with pm.Model(coords=coords) as model:\n    \n    #Psi = pm.InverseGamma('Psi', 5, 10, dims='indicators')\n    Psi = pm.HalfNormal('Psi', 1)\n    lambdas_ = pm.Normal('lambdas_1',  priors['lambda'][0], priors['lambda'][1], dims=('indicators_1'))\n    lambdas_1 = pm.Deterministic('lambdas1', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_1'))\n    lambdas_ = pm.Normal('lambdas_2', priors['lambda'][0], priors['lambda'][1], dims=('indicators_2'))\n    lambdas_2 = pm.Deterministic('lambdas2', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_2'))\n    lambdas_ = pm.Normal('lambdas_3', priors['lambda'][0], priors['lambda'][1], dims=('indicators_3'))\n    lambdas_3 = pm.Deterministic('lambdas3', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_3'))\n    lambdas_ = pm.Normal('lambdas_4', priors['lambda'][0], priors['lambda'][1], dims=('indicators_4'))\n    lambdas_4 = pm.Deterministic('lambdas4', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_4'))\n    lambdas_ = pm.Normal('lambdas_5', priors['lambda'][0], priors['lambda'][1], dims=('indicators_5'))\n    lambdas_5 = pm.Deterministic('lambdas5', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_5'))\n    tau = pm.Normal('tau', 2, 5, dims='indicators')\n    kappa = 0\n    sd_dist = pm.Exponential.dist(1.0, shape=2)\n    chol, _, _ = pm.LKJCholeskyCov('chol_cov1', n=2, eta=priors['eta'],\n      sd_dist=sd_dist, compute_corr=True)\n    cov = pm.Deterministic(\"cov\", chol.dot(chol.T), dims=('latent', 'latent1'))\n    ksi = pm.MvNormal('ksi', kappa, chol=chol, dims=('obs', 'latent'))\n\n    beta_r = pm.Normal('beta_r', 0, 1, dims='latent_regression')\n    beta_r2 = pm.Normal('beta_r2', 0, 1, dims='regression')\n    resid_chol, _, _ = pm.LKJCholeskyCov('resid_chol', n=2, eta=priors['eta'],\n      sd_dist=sd_dist, compute_corr=True)\n    cov = pm.Deterministic(\"resid_cov\", chol.dot(chol.T))\n    sigmas_resid = pm.MvNormal('sigmas_resid', kappa, chol=resid_chol)\n\n    \n    # SE_ACAD ~ SUP_FRIENDS + SUP_PARENTS \n    regression_se_acad = pm.Normal('regr_se_acad', beta_r[0]*ksi[obs_idx, 0] + beta_r[1]*ksi[obs_idx, 1], sigmas_resid[0])\n    # SE_SOCIAL ~ SUP_FRIENDS + SUP_PARENTS \n    \n    regression_se_social = pm.Normal('regr_se_social', beta_r[2]*ksi[obs_idx, 0] + beta_r[3]*ksi[obs_idx, 1], sigmas_resid[1])\n\n\n    # LS ~ SE_ACAD + SE_SOCIAL + SUP_FRIEND + SUP_PARENTS\n    regression = pm.Normal('regr', beta_r2[0]*regression_se_acad + beta_r2[1]*regression_se_social +\n                                   beta_r2[2]*ksi[obs_idx, 0] + beta_r2[3]*ksi[obs_idx, 1], 1)\n\n    m0 = tau[0] + regression_se_acad*lambdas_1[0]\n    m1 = tau[1] + regression_se_acad*lambdas_1[1]\n    m2 = tau[2] + regression_se_acad*lambdas_1[2]\n    m3 = tau[3] + regression_se_social*lambdas_2[0]\n    m4 = tau[4] + regression_se_social*lambdas_2[1]\n    m5 = tau[5] + regression_se_social*lambdas_2[2]\n    m6 = tau[6] + ksi[obs_idx, 0]*lambdas_3[0]\n    m7 = tau[7] + ksi[obs_idx, 0]*lambdas_3[1]\n    m8 = tau[8] + ksi[obs_idx, 0]*lambdas_3[2]\n    m9 = tau[9] + ksi[obs_idx, 1]*lambdas_4[0]\n    m10 = tau[10] + ksi[obs_idx, 1]*lambdas_4[1]\n    m11 = tau[11] + ksi[obs_idx, 1]*lambdas_4[2]\n    m12 = tau[12] + regression*lambdas_5[0]\n    m13 = tau[13] + regression*lambdas_5[1]\n    m14 = tau[14] + regression*lambdas_5[2]\n    \n    mu = pm.Deterministic('mu', pm.math.stack([m0, m1, m2, m3, m4, m5, m6, m7,\n                                              m8, m9, m10, m11, m12, m13, m14]).T)\n    _  = pm.Normal('likelihood', mu, Psi, observed=df[drivers].values)\n\n    idata = pm.sample(10_000, chains=4, nuts_sampler='numpyro', target_accept=.99, tune=2000,\n                      idata_kwargs={\"log_likelihood\": True}, random_seed=110)\n    idata.extend(pm.sample_posterior_predictive(idata))\n\n    return model, idata\n\n\nmodel1, idata1 = make_indirect_sem(priors={'eta': 200, 'lambda': [1, 5]})\n#model2, idata2 = make_cfa(priors={'eta': 2, 'lambda': [1, 7]})\n#model3, idata3 = make_cfa(priors={'eta': 2, 'lambda': [1, 5]})\n#model4, idata4 = make_cfa(priors={'eta': 2, 'lambda': [1, 2]})\n\npm.model_to_graphviz(model1)\n\nCompiling...\nCompilation time = 0:00:11.448726\nSampling...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling time = 0:01:07.743995\nTransforming variables...\nTransformation time = 0:00:02.993369\nComputing Log Likelihood...\nLog Likelihood time = 0:00:01.953147\nSampling: [likelihood]\n\n\n\n\n\n\n\n    \n      \n      100.00% [40000/40000 00:04<00:00]\n    \n    \n\n\n\n\n\n\n\nfactor_loadings = pd.DataFrame(az.summary(idata1, var_names=['lambdas_1', 'lambdas_2', 'lambdas_3', 'lambdas_4', 'lambdas_5'])['mean']).reset_index()\nfactor_loadings['factor'] = factor_loadings['index'].str.split('[', expand=True)[0]\nfactor_loadings.columns =['factor_loading', 'factor_loading_weight', 'factor']\nfactor_loadings['factor_loading_weight_sq'] = factor_loadings['factor_loading_weight']**2\nfactor_loadings['sum_sq_loadings'] = factor_loadings.groupby('factor')['factor_loading_weight_sq'].transform(sum)\nfactor_loadings\n\n\n\n\n/var/folders/__/ng_3_9pn1f11ftyml_qr69vh0000gn/T/ipykernel_47600/1302158253.py:5: FutureWarning: The provided callable <built-in function sum> is currently using SeriesGroupBy.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n  factor_loadings['sum_sq_loadings'] = factor_loadings.groupby('factor')['factor_loading_weight_sq'].transform(sum)\n\n\n\n\n\n\n  \n    \n      \n      factor_loading\n      factor_loading_weight\n      factor\n      factor_loading_weight_sq\n      sum_sq_loadings\n    \n  \n  \n    \n      0\n      lambdas_1[se_acad_p1]\n      1.000\n      lambdas_1\n      1.000000\n      2.693765\n    \n    \n      1\n      lambdas_1[se_acad_p2]\n      0.822\n      lambdas_1\n      0.675684\n      2.693765\n    \n    \n      2\n      lambdas_1[se_acad_p3]\n      1.009\n      lambdas_1\n      1.018081\n      2.693765\n    \n    \n      3\n      lambdas_2[se_social_p1]\n      0.991\n      lambdas_2\n      0.982081\n      2.967734\n    \n    \n      4\n      lambdas_2[se_social_p2]\n      0.953\n      lambdas_2\n      0.908209\n      2.967734\n    \n    \n      5\n      lambdas_2[se_social_p3]\n      1.038\n      lambdas_2\n      1.077444\n      2.967734\n    \n    \n      6\n      lambdas_3[sup_friends_p1]\n      1.011\n      lambdas_3\n      1.022121\n      2.564621\n    \n    \n      7\n      lambdas_3[sup_friends_p2]\n      0.800\n      lambdas_3\n      0.640000\n      2.564621\n    \n    \n      8\n      lambdas_3[sup_friends_p3]\n      0.950\n      lambdas_3\n      0.902500\n      2.564621\n    \n    \n      9\n      lambdas_4[sup_parents_p1]\n      1.018\n      lambdas_4\n      1.036324\n      3.256529\n    \n    \n      10\n      lambdas_4[sup_parents_p2]\n      1.038\n      lambdas_4\n      1.077444\n      3.256529\n    \n    \n      11\n      lambdas_4[sup_parents_p3]\n      1.069\n      lambdas_4\n      1.142761\n      3.256529\n    \n    \n      12\n      lambdas_5[ls_p1]\n      0.978\n      lambdas_5\n      0.956484\n      1.760740\n    \n    \n      13\n      lambdas_5[ls_p2]\n      0.540\n      lambdas_5\n      0.291600\n      1.760740\n    \n    \n      14\n      lambdas_5[ls_p3]\n      0.716\n      lambdas_5\n      0.512656\n      1.760740\n    \n  \n\n\n\n\n\naz.plot_posterior(idata1, var_names=['beta_r2']);\n\n\n\n\n\nidata\n\nNameError: name 'idata' is not defined\n\n\n\nfig, axs = plt.subplots(5, 3, figsize=(20, 20))\naxs = axs.flatten()\nfor i in range(15):\n    temp = idata1['posterior_predictive'].sel({'likelihood_dim_3': i}).mean(dim=('chain', 'draw'))\n    axs[i].hist(df[drivers[i]], alpha=0.3, ec='black', bins=20, label='Observed Scores')\n    axs[i].hist(temp['likelihood'], color='purple', alpha=0.6, bins=20, label='Predicted Scores')\n    axs[i].set_title(f\"Posterior Predictive Checks {drivers[i]}\")\n    axs[i].legend()\n\n\n\n\n\nmodel_corr = pd.DataFrame(az.extract(idata1['posterior_predictive'])['likelihood'][:, :, 1]).corr()\nobs_corr = df[drivers].corr()\nmodel_corr.index = obs_corr.index\nmodel_corr.columns = obs_corr.columns\nresiduals = model_corr - obs_corr\nresiduals\n\n\n\n\n\n  \n    \n      \n      se_acad_p1\n      se_acad_p2\n      se_acad_p3\n      se_social_p1\n      se_social_p2\n      se_social_p3\n      sup_friends_p1\n      sup_friends_p2\n      sup_friends_p3\n      sup_parents_p1\n      sup_parents_p2\n      sup_parents_p3\n      ls_p1\n      ls_p2\n      ls_p3\n    \n  \n  \n    \n      se_acad_p1\n      0.000000\n      -0.175215\n      -0.100320\n      -0.256008\n      -0.069803\n      -0.043268\n      0.106058\n      0.143833\n      0.111176\n      0.060444\n      0.052299\n      -0.044005\n      0.069987\n      0.074403\n      -0.109597\n    \n    \n      se_acad_p2\n      -0.175215\n      0.000000\n      -0.073239\n      -0.165107\n      -0.046318\n      -0.043315\n      0.027555\n      0.051910\n      0.042732\n      0.023354\n      -0.002880\n      0.001407\n      -0.130904\n      -0.025723\n      -0.136290\n    \n    \n      se_acad_p3\n      -0.100320\n      -0.073239\n      0.000000\n      -0.160406\n      -0.090205\n      -0.007175\n      0.120886\n      0.106869\n      0.097160\n      0.049340\n      0.052668\n      -0.016623\n      -0.024014\n      0.058934\n      -0.144535\n    \n    \n      se_social_p1\n      -0.256008\n      -0.165107\n      -0.160406\n      0.000000\n      -0.156673\n      0.003863\n      0.109112\n      0.122272\n      0.066932\n      -0.095212\n      -0.049626\n      -0.057796\n      0.002552\n      -0.206562\n      -0.077841\n    \n    \n      se_social_p2\n      -0.069803\n      -0.046318\n      -0.090205\n      -0.156673\n      0.000000\n      -0.040756\n      -0.067308\n      -0.062484\n      -0.079128\n      -0.034654\n      0.000834\n      0.010193\n      -0.061100\n      -0.235220\n      -0.103068\n    \n    \n      se_social_p3\n      -0.043268\n      -0.043315\n      -0.007175\n      0.003863\n      -0.040756\n      0.000000\n      -0.157591\n      -0.037934\n      -0.016621\n      0.082550\n      0.116919\n      0.118168\n      0.038603\n      -0.191395\n      -0.049737\n    \n    \n      sup_friends_p1\n      0.106058\n      0.027555\n      0.120886\n      0.109112\n      -0.067308\n      -0.157591\n      0.000000\n      -0.016794\n      0.031400\n      -0.015375\n      -0.029872\n      0.174683\n      0.102836\n      -0.126019\n      0.114363\n    \n    \n      sup_friends_p2\n      0.143833\n      0.051910\n      0.106869\n      0.122272\n      -0.062484\n      -0.037934\n      -0.016794\n      0.000000\n      0.010417\n      -0.019700\n      -0.025366\n      0.090220\n      0.080904\n      -0.216324\n      0.107350\n    \n    \n      sup_friends_p3\n      0.111176\n      0.042732\n      0.097160\n      0.066932\n      -0.079128\n      -0.016621\n      0.031400\n      0.010417\n      0.000000\n      -0.007207\n      -0.020034\n      0.120934\n      0.067308\n      -0.169508\n      0.007993\n    \n    \n      sup_parents_p1\n      0.060444\n      0.023354\n      0.049340\n      -0.095212\n      -0.034654\n      0.082550\n      -0.015375\n      -0.019700\n      -0.007207\n      0.000000\n      -0.016150\n      0.099228\n      0.149332\n      -0.036599\n      -0.157902\n    \n    \n      sup_parents_p2\n      0.052299\n      -0.002880\n      0.052668\n      -0.049626\n      0.000834\n      0.116919\n      -0.029872\n      -0.025366\n      -0.020034\n      -0.016150\n      0.000000\n      0.075903\n      0.144902\n      0.032625\n      -0.163102\n    \n    \n      sup_parents_p3\n      -0.044005\n      0.001407\n      -0.016623\n      -0.057796\n      0.010193\n      0.118168\n      0.174683\n      0.090220\n      0.120934\n      0.099228\n      0.075903\n      0.000000\n      0.084342\n      0.026240\n      -0.181748\n    \n    \n      ls_p1\n      0.069987\n      -0.130904\n      -0.024014\n      0.002552\n      -0.061100\n      0.038603\n      0.102836\n      0.080904\n      0.067308\n      0.149332\n      0.144902\n      0.084342\n      0.000000\n      0.082554\n      0.118299\n    \n    \n      ls_p2\n      0.074403\n      -0.025723\n      0.058934\n      -0.206562\n      -0.235220\n      -0.191395\n      -0.126019\n      -0.216324\n      -0.169508\n      -0.036599\n      0.032625\n      0.026240\n      0.082554\n      0.000000\n      0.086234\n    \n    \n      ls_p3\n      -0.109597\n      -0.136290\n      -0.144535\n      -0.077841\n      -0.103068\n      -0.049737\n      0.114363\n      0.107350\n      0.007993\n      -0.157902\n      -0.163102\n      -0.181748\n      0.118299\n      0.086234\n      0.000000\n    \n  \n\n\n\n\n\na = idata1['posterior']['regr_se_acad'].mean(dim=('chain', 'draw')).values\nb = idata1['posterior']['regr_se_social'].mean(dim=('chain', 'draw')).values\n\npd.DataFrame([a, b]).T.corr()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n    \n  \n  \n    \n      0\n      1.000000\n      0.579266\n    \n    \n      1\n      0.579266\n      1.000000\n    \n  \n\n\n\n\n\nsummary_df =az.summary(idata1, var_names=['beta_r', 'beta_r2'])\nsummary_df\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      beta_r[SUP_F->SE_ACAD]\n      0.050\n      0.043\n      -0.030\n      0.133\n      0.000\n      0.000\n      36652.0\n      31661.0\n      1.0\n    \n    \n      beta_r[SUP_P->SE_ACAD]\n      0.266\n      0.048\n      0.176\n      0.357\n      0.000\n      0.000\n      23226.0\n      26848.0\n      1.0\n    \n    \n      beta_r[SUP_F->SE_SOC]\n      0.163\n      0.038\n      0.091\n      0.233\n      0.000\n      0.000\n      28982.0\n      28173.0\n      1.0\n    \n    \n      beta_r[SUP_P->SE_SOC]\n      0.301\n      0.044\n      0.219\n      0.385\n      0.000\n      0.000\n      15760.0\n      24474.0\n      1.0\n    \n    \n      beta_r2[SE_ACAD]\n      0.196\n      0.122\n      -0.037\n      0.419\n      0.001\n      0.001\n      31610.0\n      32109.0\n      1.0\n    \n    \n      beta_r2[SE_SOCIAL]\n      0.550\n      0.152\n      0.260\n      0.831\n      0.001\n      0.001\n      22994.0\n      26669.0\n      1.0\n    \n    \n      beta_r2[SUP_F]\n      0.025\n      0.070\n      -0.109\n      0.155\n      0.000\n      0.000\n      38047.0\n      31664.0\n      1.0\n    \n    \n      beta_r2[SUP_P]\n      0.309\n      0.085\n      0.145\n      0.465\n      0.000\n      0.000\n      31945.0\n      29107.0\n      1.0\n    \n  \n\n\n\n\n\n\ndef calculate_effects(summary_df, var='SUP_P'):\n    #Indirect Paths\n    ## VAR -> SE_SOC ->LS\n    indirect_parent_soc = summary_df.loc[f'beta_r[{var}->SE_SOC]']['mean']*summary_df.loc['beta_r2[SE_SOCIAL]']['mean']\n\n    ## VAR -> SE_SOC ->LS\n    indirect_parent_acad = summary_df.loc[f'beta_r[{var}->SE_ACAD]']['mean']*summary_df.loc['beta_r2[SE_ACAD]']['mean']\n\n    ## Total Indirect Effects\n    total_indirect = indirect_parent_soc + indirect_parent_acad\n\n    ## Total Effects\n    total_effect = total_indirect + summary_df.loc[f'beta_r2[{var}]']['mean']\n\n    return pd.DataFrame([[indirect_parent_soc, indirect_parent_acad, total_indirect, total_effect]], \n                columns=[f'{var} -> SE_SOC ->LS', f'{var} -> SE_ACAD ->LS', f'Total Indirect Effects {var}', f'Total Effects {var}']\n                )\n\ncalculate_effects(summary_df, 'SUP_P')\n\n\n\n\n\n  \n    \n      \n      SUP_P -> SE_SOC ->LS\n      SUP_P -> SE_ACAD ->LS\n      Total Indirect Effects SUP_P\n      Total Effects SUP_P\n    \n  \n  \n    \n      0\n      0.16555\n      0.052136\n      0.217686\n      0.526686\n    \n  \n\n\n\n\n\ncalculate_effects(summary_df, 'SUP_F')\n\n\n\n\n\n  \n    \n      \n      SUP_F -> SE_SOC ->LS\n      SUP_F -> SE_ACAD ->LS\n      Total Indirect Effects SUP_F\n      Total Effects SUP_F\n    \n  \n  \n    \n      0\n      0.08965\n      0.0098\n      0.09945\n      0.12445\n    \n  \n\n\n\n\n\naz.plot_trace(idata1, var_names=['lambdas1', 'lambdas2', 'lambdas3', 'lambdas4', 'lambdas5',\n                                 'tau', 'Psi', 'ksi', 'beta_r2']);\n\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:487: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:487: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:487: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:487: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:487: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 8))\naz.plot_forest(idata1, var_names=['lambdas1', 'lambdas2', 'lambdas3', 'lambdas4', 'lambdas5', 'beta_r2'], combined=True, ax=ax);\n\n\n\n\n\nidata1\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:               (chain: 4, draw: 10000, indicators_1: 3,\n                           indicators_2: 3, indicators_3: 3, indicators_4: 3,\n                           indicators_5: 3, indicators: 15, obs: 283,\n                           latent: 2, latent_regression: 4, regression: 4,\n                           regr_se_acad_dim_0: 283, regr_se_social_dim_0: 283,\n                           regr_dim_0: 283, chol_cov1_dim_0: 3,\n                           chol_cov1_corr_dim_0: 2, chol_cov1_corr_dim_1: 2,\n                           chol_cov1_stds_dim_0: 2, latent1: 2, mu_dim_0: 283,\n                           mu_dim_1: 15)\nCoordinates: (12/22)\n  * chain                 (chain) int64 0 1 2 3\n  * draw                  (draw) int64 0 1 2 3 4 5 ... 9995 9996 9997 9998 9999\n  * indicators_1          (indicators_1) <U10 'se_acad_p1' ... 'se_acad_p3'\n  * indicators_2          (indicators_2) <U12 'se_social_p1' ... 'se_social_p3'\n  * indicators_3          (indicators_3) <U14 'sup_friends_p1' ... 'sup_frien...\n  * indicators_4          (indicators_4) <U14 'sup_parents_p1' ... 'sup_paren...\n    ...                    ...\n  * chol_cov1_corr_dim_0  (chol_cov1_corr_dim_0) int64 0 1\n  * chol_cov1_corr_dim_1  (chol_cov1_corr_dim_1) int64 0 1\n  * chol_cov1_stds_dim_0  (chol_cov1_stds_dim_0) int64 0 1\n  * latent1               (latent1) <U5 'SUP_F' 'SUP_P'\n  * mu_dim_0              (mu_dim_0) int64 0 1 2 3 4 5 ... 278 279 280 281 282\n  * mu_dim_1              (mu_dim_1) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14\nData variables: (12/23)\n    lambdas_1             (chain, draw, indicators_1) float64 -0.1326 ... 0.7856\n    lambdas_2             (chain, draw, indicators_2) float64 0.4584 ... 0.7451\n    lambdas_3             (chain, draw, indicators_3) float64 1.191 ... 0.8761\n    lambdas_4             (chain, draw, indicators_4) float64 0.2207 ... 0.955\n    lambdas_5             (chain, draw, indicators_5) float64 2.385 ... 0.617\n    tau                   (chain, draw, indicators) float64 5.093 ... 5.169\n    ...                    ...\n    lambdas4              (chain, draw, indicators_4) float64 1.0 ... 0.955\n    lambdas5              (chain, draw, indicators_5) float64 1.0 ... 0.617\n    chol_cov1_corr        (chain, draw, chol_cov1_corr_dim_0, chol_cov1_corr_dim_1) float64 ...\n    chol_cov1_stds        (chain, draw, chol_cov1_stds_dim_0) float64 1.105 ....\n    cov                   (chain, draw, latent, latent1) float64 1.222 ... 1.0\n    mu                    (chain, draw, mu_dim_0, mu_dim_1) float64 5.332 ......\nAttributes:\n    created_at:     2024-08-20T20:15:31.157627\n    arviz_version:  0.17.0xarray.DatasetDimensions:chain: 4draw: 10000indicators_1: 3indicators_2: 3indicators_3: 3indicators_4: 3indicators_5: 3indicators: 15obs: 283latent: 2latent_regression: 4regression: 4regr_se_acad_dim_0: 283regr_se_social_dim_0: 283regr_dim_0: 283chol_cov1_dim_0: 3chol_cov1_corr_dim_0: 2chol_cov1_corr_dim_1: 2chol_cov1_stds_dim_0: 2latent1: 2mu_dim_0: 283mu_dim_1: 15Coordinates: (22)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 9996 9997 9998 9999array([   0,    1,    2, ..., 9997, 9998, 9999])indicators_1(indicators_1)<U10'se_acad_p1' ... 'se_acad_p3'array(['se_acad_p1', 'se_acad_p2', 'se_acad_p3'], dtype='<U10')indicators_2(indicators_2)<U12'se_social_p1' ... 'se_social_p3'array(['se_social_p1', 'se_social_p2', 'se_social_p3'], dtype='<U12')indicators_3(indicators_3)<U14'sup_friends_p1' ... 'sup_friend...array(['sup_friends_p1', 'sup_friends_p2', 'sup_friends_p3'], dtype='<U14')indicators_4(indicators_4)<U14'sup_parents_p1' ... 'sup_parent...array(['sup_parents_p1', 'sup_parents_p2', 'sup_parents_p3'], dtype='<U14')indicators_5(indicators_5)<U5'ls_p1' 'ls_p2' 'ls_p3'array(['ls_p1', 'ls_p2', 'ls_p3'], dtype='<U5')indicators(indicators)<U14'se_acad_p1' ... 'ls_p3'array(['se_acad_p1', 'se_acad_p2', 'se_acad_p3', 'se_social_p1',\n       'se_social_p2', 'se_social_p3', 'sup_friends_p1', 'sup_friends_p2',\n       'sup_friends_p3', 'sup_parents_p1', 'sup_parents_p2', 'sup_parents_p3',\n       'ls_p1', 'ls_p2', 'ls_p3'], dtype='<U14')obs(obs)int640 1 2 3 4 5 ... 278 279 280 281 282array([  0,   1,   2, ..., 280, 281, 282])latent(latent)<U5'SUP_F' 'SUP_P'array(['SUP_F', 'SUP_P'], dtype='<U5')latent_regression(latent_regression)<U14'SUP_F->SE_ACAD' ... 'SUP_P->SE_...array(['SUP_F->SE_ACAD', 'SUP_P->SE_ACAD', 'SUP_F->SE_SOC', 'SUP_P->SE_SOC'],\n      dtype='<U14')regression(regression)<U9'SE_ACAD' 'SE_SOCIAL' ... 'SUP_P'array(['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P'], dtype='<U9')regr_se_acad_dim_0(regr_se_acad_dim_0)int640 1 2 3 4 5 ... 278 279 280 281 282array([  0,   1,   2, ..., 280, 281, 282])regr_se_social_dim_0(regr_se_social_dim_0)int640 1 2 3 4 5 ... 278 279 280 281 282array([  0,   1,   2, ..., 280, 281, 282])regr_dim_0(regr_dim_0)int640 1 2 3 4 5 ... 278 279 280 281 282array([  0,   1,   2, ..., 280, 281, 282])chol_cov1_dim_0(chol_cov1_dim_0)int640 1 2array([0, 1, 2])chol_cov1_corr_dim_0(chol_cov1_corr_dim_0)int640 1array([0, 1])chol_cov1_corr_dim_1(chol_cov1_corr_dim_1)int640 1array([0, 1])chol_cov1_stds_dim_0(chol_cov1_stds_dim_0)int640 1array([0, 1])latent1(latent1)<U5'SUP_F' 'SUP_P'array(['SUP_F', 'SUP_P'], dtype='<U5')mu_dim_0(mu_dim_0)int640 1 2 3 4 5 ... 278 279 280 281 282array([  0,   1,   2, ..., 280, 281, 282])mu_dim_1(mu_dim_1)int640 1 2 3 4 5 6 7 8 9 10 11 12 13 14array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])Data variables: (23)lambdas_1(chain, draw, indicators_1)float64-0.1326 0.7008 ... 0.6641 0.7856array([[[-0.13263639,  0.7008339 ,  0.86823216],\n        [ 2.37366942,  0.75348259,  0.82639786],\n        [ 0.25029946,  0.69769814,  0.90689214],\n        ...,\n        [ 1.51333927,  0.69158627,  0.84254988],\n        [ 0.77611216,  0.75714448,  0.81697963],\n        [ 0.43621952,  0.7395401 ,  0.82342112]],\n\n       [[ 0.97854627,  0.64555917,  0.80494756],\n        [ 0.21628951,  0.67116008,  0.86211266],\n        [-0.52818357,  0.77421815,  0.87909445],\n        ...,\n        [ 0.48155106,  0.6863708 ,  0.90775711],\n        [ 1.62852943,  0.69795972,  0.79195226],\n        [ 0.713433  ,  0.70858836,  0.9041954 ]],\n\n       [[ 0.6920907 ,  0.72723715,  0.87570877],\n        [ 0.57015811,  0.70537702,  0.83540543],\n        [ 1.41350501,  0.64896095,  0.83041478],\n        ...,\n        [-1.49910534,  0.67216661,  0.81932402],\n        [ 3.12923275,  0.72311182,  0.83942162],\n        [-0.93751717,  0.68582651,  0.81838779]],\n\n       [[ 1.55562098,  0.67890506,  0.81375481],\n        [ 2.21961859,  0.74124238,  0.77391992],\n        [ 0.29635944,  0.59466504,  0.78550315],\n        ...,\n        [ 0.55040816,  0.63828075,  0.76505226],\n        [ 1.9096522 ,  0.69841577,  0.88076336],\n        [-0.26000092,  0.66409603,  0.78556541]]])lambdas_2(chain, draw, indicators_2)float640.4584 0.7841 ... 0.719 0.7451array([[[ 0.45840947,  0.78409947,  0.79556843],\n        [ 1.2814501 ,  0.76213871,  0.79932917],\n        [-0.26780764,  0.81321987,  0.75634971],\n        ...,\n        [-1.14100118,  0.75398585,  0.7961498 ],\n        [ 0.8363699 ,  0.74824545,  0.6888498 ],\n        [ 0.82605192,  0.7395168 ,  0.68695646]],\n\n       [[ 1.16469465,  0.79810651,  0.72852553],\n        [ 3.57817586,  0.77453962,  0.75629498],\n        [ 1.85409686,  0.73573672,  0.72234245],\n        ...,\n        [ 2.29546278,  0.83187722,  0.78820498],\n        [ 2.58186499,  0.88876711,  0.79846387],\n        [ 0.64022855,  0.82846398,  0.76940996]],\n\n       [[ 0.40185352,  0.79288095,  0.8342257 ],\n        [ 1.93969132,  0.82448106,  0.72086825],\n        [ 0.30992145,  0.72033186,  0.80041915],\n        ...,\n        [ 1.28703894,  0.78499373,  0.67240037],\n        [ 1.97428935,  0.74599395,  0.76189312],\n        [ 0.71822684,  0.78834293,  0.63490628]],\n\n       [[ 2.13000244,  0.80632405,  0.78827939],\n        [ 2.47413025,  0.85917706,  0.77948591],\n        [-0.86423979,  0.74620022,  0.75769738],\n        ...,\n        [ 1.38144506,  0.72521435,  0.7303024 ],\n        [ 0.75470713,  0.85095254,  0.77531198],\n        [ 0.94719492,  0.71902137,  0.74510897]]])lambdas_3(chain, draw, indicators_3)float641.191 0.7569 ... 0.7921 0.8761array([[[ 1.19142533,  0.75688836,  0.90343597],\n        [ 0.57850022,  0.7176848 ,  0.80317205],\n        [ 2.26649555,  0.77074817,  0.90416063],\n        ...,\n        [ 0.28042182,  0.76961555,  0.84664995],\n        [ 1.19704607,  0.77699081,  0.88783454],\n        [ 1.34368915,  0.77683274,  0.90680877]],\n\n       [[ 1.01959192,  0.817206  ,  0.99247439],\n        [ 0.70133909,  0.84394323,  0.92734256],\n        [ 0.65456964,  0.78755724,  0.97686568],\n        ...,\n        [ 1.50189935,  0.83936568,  0.96084727],\n        [ 1.68193613,  0.80897896,  0.92995568],\n        [ 0.32939294,  0.8074814 ,  0.94634231]],\n\n       [[ 1.61718243,  0.76261184,  0.88149179],\n        [ 2.15621244,  0.8352147 ,  0.92855291],\n        [-0.29229385,  0.89449305,  0.98190939],\n        ...,\n        [-2.17208748,  0.80064177,  0.87890319],\n        [ 3.85265418,  0.75622485,  0.96201308],\n        [-1.18316459,  0.850205  ,  0.8388567 ]],\n\n       [[ 1.16582898,  0.8194599 ,  0.8739116 ],\n        [ 2.20635756,  0.82935282,  0.89140703],\n        [-0.48308458,  0.80361535,  0.96603315],\n        ...,\n        [ 0.36820571,  0.79980394,  0.86747523],\n        [ 1.13043887,  0.80830953,  0.92906992],\n        [ 1.8423773 ,  0.79205013,  0.87609535]]])lambdas_4(chain, draw, indicators_4)float640.2207 1.072 0.9641 ... 1.004 0.955array([[[ 0.2206767 ,  1.07190016,  0.96412386],\n        [ 0.83763267,  1.05523423,  1.0022816 ],\n        [ 0.52283043,  0.98058084,  0.96050913],\n        ...,\n        [ 1.06707289,  1.08526762,  1.10584266],\n        [-0.34659672,  1.07550691,  0.99010885],\n        [-0.48978428,  1.06293623,  0.99030376]],\n\n       [[ 1.67330805,  1.06185427,  0.92786736],\n        [ 0.78861769,  1.05089232,  0.98680901],\n        [ 1.10389243,  1.00854041,  0.93600326],\n        ...,\n        [ 0.69788618,  1.07640799,  1.02946004],\n        [ 1.01553337,  0.96575982,  0.92383987],\n        [ 0.81199129,  1.1216565 ,  0.95777207]],\n\n       [[ 1.1081748 ,  0.97467798,  0.98013496],\n        [ 0.46845525,  1.02924839,  0.96121384],\n        [ 1.28024111,  0.97225375,  0.88289559],\n        ...,\n        [ 1.90579878,  1.04469427,  0.93928757],\n        [-0.18857811,  1.0896662 ,  1.01707551],\n        [ 1.59447194,  1.09991707,  1.03662255]],\n\n       [[ 1.69995414,  1.06545274,  0.9710698 ],\n        [ 2.17325291,  0.99120572,  1.05770903],\n        [ 1.01807776,  1.13534209,  1.01922106],\n        ...,\n        [ 2.2273087 ,  1.03607955,  1.05134002],\n        [-0.57230219,  1.00028223,  0.93901865],\n        [ 2.06103291,  1.00433297,  0.95497702]]])lambdas_5(chain, draw, indicators_5)float642.385 0.5469 ... 0.5687 0.617array([[[ 2.38460513,  0.54689596,  0.62188738],\n        [ 0.1274943 ,  0.52124762,  0.54527328],\n        [ 1.14882291,  0.52012897,  0.56494137],\n        ...,\n        [ 1.68879723,  0.45317851,  0.54110369],\n        [ 2.10954133,  0.50611793,  0.53875726],\n        [ 1.86477303,  0.50992452,  0.54987472]],\n\n       [[ 1.55812515,  0.44015813,  0.69480796],\n        [ 1.14019053,  0.5483047 ,  0.72594974],\n        [ 0.20806795,  0.5312595 ,  0.72954133],\n        ...,\n        [ 1.17556293,  0.63751484,  0.64707653],\n        [ 2.02105561,  0.47901252,  0.63663288],\n        [ 0.17271385,  0.62174358,  0.62262435]],\n\n       [[-0.68597076,  0.52522707,  0.71107392],\n        [ 2.2058827 ,  0.50356019,  0.68211666],\n        [-0.09821401,  0.6121456 ,  0.63232234],\n        ...,\n        [ 1.66882701,  0.60273101,  0.61727572],\n        [ 0.38822406,  0.58208316,  0.71524028],\n        [ 0.59228318,  0.55922986,  0.60313161]],\n\n       [[ 1.67258615,  0.4817728 ,  0.675911  ],\n        [ 1.48231493,  0.55202498,  0.69200841],\n        [ 1.88990354,  0.52530851,  0.57782457],\n        ...,\n        [ 1.1972816 ,  0.51706953,  0.76448678],\n        [ 1.52056118,  0.51687005,  0.57557131],\n        [ 0.55031139,  0.56873512,  0.61701066]]])tau(chain, draw, indicators)float645.093 5.306 5.141 ... 5.768 5.169array([[[5.09329827, 5.30614388, 5.14113899, ..., 5.36377125,\n         5.87428143, 5.33200535],\n        [5.07399862, 5.27340478, 5.13349499, ..., 5.34739046,\n         5.80661632, 5.25902603],\n        [5.07151082, 5.28066695, 5.09877439, ..., 5.34474447,\n         5.87497353, 5.3148829 ],\n        ...,\n        [5.08234189, 5.31261501, 5.23045869, ..., 5.08540152,\n         5.74498823, 5.1722319 ],\n        [5.15594438, 5.32550645, 5.13088607, ..., 5.12751125,\n         5.68991416, 5.18642425],\n        [5.14140564, 5.33406519, 5.15767398, ..., 5.15052613,\n         5.70801493, 5.19819014]],\n\n       [[5.20215696, 5.36475419, 5.24680865, ..., 5.22779295,\n         5.80393576, 5.28076643],\n        [5.17704402, 5.36483547, 5.22300428, ..., 5.18361005,\n         5.80957359, 5.20988416],\n        [5.18433483, 5.3491839 , 5.20343636, ..., 5.19190616,\n         5.79159164, 5.22175939],\n...\n        [5.07705537, 5.29461632, 5.14188296, ..., 5.23178745,\n         5.76713572, 5.21729239],\n        [5.10299849, 5.33118178, 5.19498712, ..., 5.20390179,\n         5.75368601, 5.22308288],\n        [5.09565131, 5.31819537, 5.18599277, ..., 5.18626848,\n         5.77896417, 5.22095074]],\n\n       [[5.16207228, 5.29972341, 5.16286799, ..., 5.19363861,\n         5.73591577, 5.18896538],\n        [5.11938686, 5.33024434, 5.18827496, ..., 5.11595624,\n         5.74188324, 5.20620646],\n        [5.1278932 , 5.33571949, 5.18636825, ..., 5.19350988,\n         5.81679342, 5.25358547],\n        ...,\n        [5.09153184, 5.2672608 , 5.16263   , ..., 5.16805932,\n         5.76876115, 5.16549421],\n        [5.02153822, 5.26202597, 5.11721101, ..., 5.09571696,\n         5.70929424, 5.19824848],\n        [5.05053304, 5.32215886, 5.18224342, ..., 5.14336777,\n         5.76785248, 5.16921207]]])ksi(chain, draw, obs, latent)float64-0.1421 0.3197 ... -0.001034 0.3712array([[[[-1.42057866e-01,  3.19695272e-01],\n         [-1.63729525e+00, -7.59854976e-01],\n         [-1.39001974e+00,  9.22516845e-01],\n         ...,\n         [-1.33412542e+00, -6.47684085e-01],\n         [ 5.52968874e-01,  3.14993519e-01],\n         [ 1.94655981e-01,  8.23978136e-01]],\n\n        [[ 1.09594981e+00,  9.33481333e-01],\n         [-9.80463909e-01, -5.64094695e-01],\n         [-2.34594038e+00,  8.51433764e-01],\n         ...,\n         [-1.86079170e+00,  7.68176193e-01],\n         [ 1.55086244e-01,  3.87797852e-01],\n         [ 1.25494235e-01,  2.15726282e-01]],\n\n        [[ 1.18420065e+00,  6.50851895e-01],\n         [-1.25765694e+00, -3.89264656e-02],\n         [-1.60958088e+00,  7.65290579e-01],\n         ...,\n...\n         ...,\n         [-1.95676752e+00,  2.86316671e-01],\n         [-2.46120166e-01,  5.09916396e-01],\n         [ 3.19117381e-01,  4.81847292e-01]],\n\n        [[ 3.20940966e-01,  3.24209679e-01],\n         [-1.60461220e+00, -7.47315453e-01],\n         [-2.06357359e+00,  9.91732007e-01],\n         ...,\n         [-1.20992926e+00, -1.35331174e-01],\n         [ 1.26032329e-01, -1.44084528e-01],\n         [-2.82672991e-01,  8.23129032e-01]],\n\n        [[ 1.20223062e+00,  8.27872117e-01],\n         [-9.89146597e-01, -9.32454161e-01],\n         [-2.01876543e+00,  6.24850023e-01],\n         ...,\n         [-1.84805881e+00,  2.23505089e-01],\n         [-1.55062141e-01,  5.46750325e-01],\n         [-1.03422864e-03,  3.71203811e-01]]]])beta_r(chain, draw, latent_regression)float640.04925 0.2206 ... -0.007368 0.3772array([[[ 0.04925231,  0.22055989,  0.2099594 ,  0.2959968 ],\n        [ 0.10572755,  0.18563103,  0.11578686,  0.29400979],\n        [ 0.09368737,  0.12317357,  0.14590854,  0.30345888],\n        ...,\n        [ 0.08939871,  0.34295989,  0.30563601,  0.26435384],\n        [ 0.11277086,  0.35492946,  0.12054101,  0.34413182],\n        [ 0.11882881,  0.35145366,  0.12738201,  0.3424642 ]],\n\n       [[ 0.07089394,  0.17803651,  0.22003407,  0.30384034],\n        [ 0.11885974,  0.26444603,  0.18940868,  0.32018758],\n        [ 0.06606527,  0.29812412,  0.03410882,  0.32651327],\n        ...,\n        [ 0.10439417,  0.27567538,  0.15008196,  0.30082785],\n        [ 0.02855471,  0.18366436,  0.09371784,  0.28461362],\n        [ 0.06374667,  0.26727059,  0.14923389,  0.34968938]],\n\n       [[ 0.19161132,  0.07358122,  0.06219027,  0.39706032],\n        [-0.04349205,  0.39359224,  0.1177639 ,  0.3378558 ],\n        [ 0.10772223,  0.09908365,  0.23598655,  0.30334968],\n        ...,\n        [ 0.02700202,  0.15704707,  0.25914912,  0.29780432],\n        [ 0.0383732 ,  0.37293833,  0.08340355,  0.36032082],\n        [ 0.00309559,  0.20240023,  0.25083556,  0.34118653]],\n\n       [[ 0.05559247,  0.19452873,  0.17508855,  0.21265062],\n        [ 0.0877364 ,  0.26447198,  0.23609225,  0.18787286],\n        [-0.01336804,  0.43952101,  0.11485531,  0.39816367],\n        ...,\n        [-0.04957235,  0.20229116, -0.05469908,  0.35572111],\n        [ 0.11522668,  0.32123287,  0.30341494,  0.28803413],\n        [ 0.02805412,  0.28622377, -0.0073676 ,  0.37722083]]])beta_r2(chain, draw, regression)float64-0.03085 0.5836 ... 0.01175 0.1588array([[[-0.03084677,  0.58363717, -0.00473256,  0.22360069],\n        [ 0.23377726,  0.63275236, -0.01583651,  0.26314023],\n        [-0.07430238,  0.57402481, -0.00158823,  0.22699151],\n        ...,\n        [ 0.37758146,  0.27336967,  0.00443163,  0.33709705],\n        [ 0.18416675,  0.4593499 , -0.00649138,  0.16242024],\n        [ 0.19705919,  0.44043597, -0.00085926,  0.14073763]],\n\n       [[-0.01478698,  0.44353392,  0.01391653,  0.26957838],\n        [ 0.21065665,  0.59039483,  0.10612355,  0.2717773 ],\n        [ 0.14148248,  0.44933107, -0.01963217,  0.1348271 ],\n        ...,\n        [ 0.05633249,  0.51051449, -0.02003404,  0.27909638],\n        [ 0.00762459,  0.72389948, -0.03280551,  0.27419233],\n        [ 0.16947297,  0.55940246, -0.00974756,  0.26633937]],\n\n       [[ 0.09635922,  0.40586884,  0.03316916,  0.44223022],\n        [ 0.10684555,  0.77937689, -0.01466083,  0.08264678],\n        [ 0.10990606,  0.4479734 , -0.0169406 ,  0.43467062],\n        ...,\n        [ 0.10645066,  0.56369841, -0.09524847,  0.24556307],\n        [ 0.02678715,  0.28814363,  0.08650192,  0.31249237],\n        [ 0.17988458,  0.55070507, -0.08325841,  0.25496777]],\n\n       [[ 0.02309923,  0.57247834,  0.02470903,  0.39487266],\n        [-0.01078161,  0.52602815, -0.0278622 ,  0.35370865],\n        [ 0.11874464,  0.47657107,  0.07705297,  0.34279867],\n        ...,\n        [ 0.23971203,  0.44239894, -0.09263048,  0.12658021],\n        [-0.03605893,  0.43906543,  0.06785102,  0.28393823],\n        [ 0.27937979,  0.45905761,  0.01175385,  0.15880752]]])regr_se_acad(chain, draw, regr_se_acad_dim_0)float640.2391 -0.2778 ... -0.3221 1.077array([[[ 2.39133340e-01, -2.77797155e-01,  2.52199409e-01, ...,\n          2.98600338e-01, -4.36875672e-01,  9.96315795e-01],\n        [-5.67943499e-01, -1.04054466e+00, -3.56204317e-01, ...,\n          1.16137271e-02, -1.54716128e-02,  8.56829587e-01],\n        [-6.68662390e-02, -4.66230384e-01, -2.36124905e-02, ...,\n          2.34406327e-01, -3.14674708e-01,  1.08610220e+00],\n        ...,\n        [ 3.24366353e-02, -9.87768102e-01, -2.34849108e-01, ...,\n         -4.05074036e-01, -1.87922833e-01,  4.26806961e-01],\n        [ 1.17261151e-01, -8.85877371e-01, -3.98712555e-01, ...,\n          3.86239698e-02, -2.74045233e-01,  4.94068364e-01],\n        [ 9.47790112e-02, -8.87870134e-01, -4.63331698e-01, ...,\n          1.58901346e-01, -2.94919185e-01,  4.41513945e-01]],\n\n       [[ 6.88752699e-02, -4.80013413e-01, -5.42134013e-01, ...,\n          3.40832120e-01, -3.74618889e-01,  7.64535756e-01],\n        [-5.04309943e-01, -6.09023342e-01, -2.17832439e-01, ...,\n         -1.20958358e-01, -8.81991352e-02,  6.02586202e-01],\n        [-6.11331163e-01, -3.42820213e-01, -1.61743427e-01, ...,\n         -3.77448379e-01, -3.52084187e-01,  5.55235505e-01],\n...\n        [-5.91269684e-02, -5.56530343e-01,  6.48349505e-02, ...,\n         -3.67424377e-01, -2.91674261e-01,  8.08132950e-01],\n        [-1.63786055e-01, -7.77717427e-01, -3.30313999e-01, ...,\n          5.98286905e-01,  2.66790243e-02,  1.05096701e+00],\n        [-3.03328788e-01, -4.91758797e-01,  6.50867373e-02, ...,\n         -7.36370570e-01, -3.13677720e-01,  7.75351244e-01]],\n\n       [[-3.40763027e-01, -1.09158203e+00, -2.30770405e-01, ...,\n          5.91262098e-02, -2.88338095e-01,  1.24578659e+00],\n        [-4.19146313e-01, -1.33055140e+00, -5.01672085e-01, ...,\n         -2.66107498e-01, -3.07116443e-01,  8.28873401e-01],\n        [-2.55050719e-01, -1.89706658e-01,  1.73433248e-04, ...,\n         -4.17228972e-01,  1.85796373e-01,  5.39008676e-01],\n        ...,\n        [-2.58811525e-02, -9.16518168e-01, -7.90981052e-02, ...,\n          7.07296965e-01, -3.95810771e-01,  1.19836173e+00],\n        [-2.44000862e-01, -2.70717159e-01, -5.67315759e-01, ...,\n         -5.59462674e-01,  1.55029989e-01,  6.71329871e-01],\n        [-3.96042280e-01, -1.30459032e+00,  2.26713351e-02, ...,\n          1.01215535e+00, -3.22062936e-01,  1.07715410e+00]]])regr_se_social(chain, draw, regr_se_social_dim_0)float64-0.1629 -0.09252 ... 0.1143 -0.1082array([[[-0.16286717, -0.09251969, -0.24492843, ...,  0.81225534,\n          0.0519921 , -0.13820825],\n        [ 0.46762274, -0.21434818, -0.08740612, ...,  0.13566872,\n         -0.05549826,  0.4365518 ],\n        [ 0.42013805, -0.13378694, -0.56249595, ...,  0.59943404,\n         -0.1241816 ,  0.08261941],\n        ...,\n        [ 0.51031909,  0.39815046, -0.29255183, ...,  0.4387024 ,\n         -0.69546518,  0.16071917],\n        [ 0.12532541, -0.11545866, -0.45776806, ...,  0.71617968,\n          0.30789927,  0.36497096],\n        [ 0.2172866 , -0.1895829 , -0.42103653, ...,  0.75158076,\n          0.29258923,  0.36981464]],\n\n       [[ 1.0950189 ,  0.03858976, -0.53921284, ...,  0.38237018,\n          0.53590354, -0.0779816 ],\n        [ 0.66317558, -0.34145399, -0.05693122, ..., -0.20939876,\n          0.58309388,  0.1384567 ],\n        [ 0.14558056, -0.17747538,  0.18695547, ...,  0.29390285,\n          0.08646252,  0.10790666],\n...\n        [ 0.83661369, -0.35032137, -0.27690349, ...,  0.22871072,\n          0.65851599,  0.30060843],\n        [-0.24741037, -0.00167453, -0.3717294 , ...,  0.46261821,\n          0.16874442,  0.08228225],\n        [ 0.90632029, -0.26138847, -0.48343925, ...,  0.37114465,\n          0.16824719,  0.34428388]],\n\n       [[ 0.49514179, -0.40503084, -0.7684361 , ...,  0.6779293 ,\n          0.12131264, -0.36030378],\n        [ 0.62314505, -0.24329736, -0.69331673, ...,  0.63178037,\n          0.40360617, -0.14287279],\n        [-0.08492894, -0.17985748, -0.04310168, ...,  0.310627  ,\n         -0.01817032,  0.35689518],\n        ...,\n        [ 0.02760717, -0.15675281, -0.09852986, ...,  0.30665005,\n          0.25972502,  0.0231826 ],\n        [ 0.28528212, -0.4896648 , -0.34645666, ...,  0.5269568 ,\n         -0.15272452,  0.36042717],\n        [ 0.21248171, -0.16629265,  0.05580386, ...,  0.13567601,\n          0.11431418, -0.10819873]]])regr(chain, draw, regr_dim_0)float640.5973 -0.7072 ... -0.1954 1.164array([[[ 0.59731867, -0.7072157 ,  0.05094034, ...,  0.25563859,\n         -1.18006802,  0.80680474],\n        [ 0.76983047, -1.10553332, -0.08207854, ...,  0.20469514,\n          0.0260348 ,  0.31741112],\n        [ 0.34706697, -0.86292452,  0.45645375, ..., -0.00354639,\n         -1.85847353,  0.9421915 ],\n        ...,\n        [-0.12840484, -0.74832513,  0.40483996, ...,  0.71379973,\n         -1.03499088,  1.7129523 ],\n        [-0.13705263, -1.25427246,  0.32271166, ...,  0.20417648,\n         -1.62650861,  0.81197408],\n        [-0.15188955, -1.42292249,  0.3833948 , ...,  0.15505703,\n         -1.55852924,  0.68270001]],\n\n       [[ 1.20759547, -1.48542263,  0.48688743, ...,  0.62517698,\n         -0.75255555,  0.56252209],\n        [ 0.55319799, -1.57171324,  0.54210426, ...,  0.27657498,\n         -0.25250008,  0.39040956],\n        [ 0.18078376, -1.38505689,  0.49736301, ...,  0.11900292,\n         -0.92419093,  0.84491476],\n...\n        [ 0.45580569, -1.29134287, -0.22808486, ...,  0.01602278,\n         -0.73096956,  1.15733247],\n        [ 0.65426143, -0.60635453,  0.30695939, ...,  0.30309662,\n         -0.72919816,  0.83155466],\n        [ 0.7539259 , -0.92144605,  0.27456709, ...,  0.15566674,\n         -0.94981304,  1.27442442]],\n\n       [[ 0.91487324, -0.98363593, -0.32973237, ...,  0.46072206,\n         -0.51243792,  0.87215454],\n        [ 1.24902647, -2.09505602, -0.26395969, ..., -0.04678352,\n         -0.14524134,  0.91845682],\n        [ 0.55354524, -0.24690908,  0.70644087, ...,  0.38429825,\n         -0.89838781,  1.03465527],\n        ...,\n        [ 0.28213787, -1.25666558,  0.14969955, ...,  0.31916722,\n         -0.36434537,  1.28706172],\n        [ 0.32378109, -0.25433007,  0.72057169, ...,  0.56627043,\n         -1.53238152,  1.05391109],\n        [ 0.32426857, -0.95872086,  0.43507743, ...,  0.07158179,\n         -0.19540808,  1.16427325]]])Psi(chain, draw, indicators)float640.3843 0.4389 ... 0.5022 0.7065array([[[0.38427963, 0.43893621, 0.47750227, ..., 0.61223278,\n         0.52511883, 0.73201923],\n        [0.37830185, 0.39783607, 0.45717325, ..., 0.58371673,\n         0.53085649, 0.70148407],\n        [0.38454069, 0.42177372, 0.45696627, ..., 0.50482133,\n         0.56023155, 0.68634635],\n        ...,\n        [0.43767705, 0.40140879, 0.48707191, ..., 0.45230959,\n         0.61545859, 0.75531733],\n        [0.40859978, 0.41803348, 0.46376556, ..., 0.47259048,\n         0.62806942, 0.70224137],\n        [0.39135526, 0.41688268, 0.45852213, ..., 0.46359065,\n         0.6236609 , 0.6981334 ]],\n\n       [[0.38227823, 0.49048018, 0.50673737, ..., 0.51716749,\n         0.58585346, 0.69206256],\n        [0.3819676 , 0.44681264, 0.45281408, ..., 0.55538025,\n         0.56834183, 0.65741805],\n        [0.39045473, 0.44503557, 0.50289947, ..., 0.53237347,\n         0.55347782, 0.63168067],\n...\n        [0.38853644, 0.42684312, 0.46521323, ..., 0.59087807,\n         0.52791665, 0.64230932],\n        [0.34613734, 0.42263743, 0.48381595, ..., 0.51940118,\n         0.51925701, 0.70510896],\n        [0.42662582, 0.39811792, 0.47834621, ..., 0.57413199,\n         0.54224219, 0.63391562]],\n\n       [[0.39107018, 0.37448445, 0.47794182, ..., 0.56773355,\n         0.53002837, 0.66904062],\n        [0.36711648, 0.36591897, 0.46449995, ..., 0.60779192,\n         0.54160276, 0.65046918],\n        [0.39164456, 0.45313514, 0.48351535, ..., 0.50041495,\n         0.54652522, 0.66919873],\n        ...,\n        [0.29848471, 0.45995637, 0.48675316, ..., 0.49804845,\n         0.53953285, 0.62888418],\n        [0.35137319, 0.44882955, 0.50567132, ..., 0.46201911,\n         0.61011201, 0.67138842],\n        [0.379999  , 0.43845011, 0.49345016, ..., 0.45548035,\n         0.50218163, 0.70646512]]])chol_cov1(chain, draw, chol_cov1_dim_0)float641.105 0.1501 ... 0.1728 0.9851array([[[1.10548322, 0.15006615, 0.87114293],\n        [1.02186277, 0.14725024, 0.93217787],\n        [1.07719629, 0.11095585, 0.96329982],\n        ...,\n        [1.0455881 , 0.11479428, 0.88607208],\n        [1.03998455, 0.14292966, 0.90882305],\n        [1.05975403, 0.1489005 , 0.91203167]],\n\n       [[1.02060411, 0.09488747, 0.92261541],\n        [0.99409161, 0.02452056, 0.9240079 ],\n        [0.97209611, 0.12448274, 0.9048105 ],\n        ...,\n        [1.01370072, 0.09578055, 0.90653514],\n        [0.97738606, 0.22806092, 1.03296995],\n        [0.99878332, 0.05012368, 0.87053352]],\n\n       [[1.02453808, 0.21539403, 0.86202198],\n        [0.98711601, 0.09479156, 1.00766262],\n        [0.93970627, 0.16290666, 1.02228883],\n        ...,\n        [1.00878963, 0.14966442, 0.91726925],\n        [1.03846034, 0.07078063, 0.91857836],\n        [0.98824482, 0.13278408, 0.87063656]],\n\n       [[1.03560124, 0.21375753, 0.9908815 ],\n        [1.06312843, 0.20844752, 0.94475018],\n        [0.96358369, 0.06306969, 0.90334152],\n        ...,\n        [1.00745981, 0.08363887, 1.01187068],\n        [1.0311157 , 0.11802959, 0.92681447],\n        [1.03966306, 0.17283349, 0.98513968]]])lambdas1(chain, draw, indicators_1)float641.0 0.7008 0.8682 ... 0.6641 0.7856array([[[1.        , 0.7008339 , 0.86823216],\n        [1.        , 0.75348259, 0.82639786],\n        [1.        , 0.69769814, 0.90689214],\n        ...,\n        [1.        , 0.69158627, 0.84254988],\n        [1.        , 0.75714448, 0.81697963],\n        [1.        , 0.7395401 , 0.82342112]],\n\n       [[1.        , 0.64555917, 0.80494756],\n        [1.        , 0.67116008, 0.86211266],\n        [1.        , 0.77421815, 0.87909445],\n        ...,\n        [1.        , 0.6863708 , 0.90775711],\n        [1.        , 0.69795972, 0.79195226],\n        [1.        , 0.70858836, 0.9041954 ]],\n\n       [[1.        , 0.72723715, 0.87570877],\n        [1.        , 0.70537702, 0.83540543],\n        [1.        , 0.64896095, 0.83041478],\n        ...,\n        [1.        , 0.67216661, 0.81932402],\n        [1.        , 0.72311182, 0.83942162],\n        [1.        , 0.68582651, 0.81838779]],\n\n       [[1.        , 0.67890506, 0.81375481],\n        [1.        , 0.74124238, 0.77391992],\n        [1.        , 0.59466504, 0.78550315],\n        ...,\n        [1.        , 0.63828075, 0.76505226],\n        [1.        , 0.69841577, 0.88076336],\n        [1.        , 0.66409603, 0.78556541]]])lambdas2(chain, draw, indicators_2)float641.0 0.7841 0.7956 ... 0.719 0.7451array([[[1.        , 0.78409947, 0.79556843],\n        [1.        , 0.76213871, 0.79932917],\n        [1.        , 0.81321987, 0.75634971],\n        ...,\n        [1.        , 0.75398585, 0.7961498 ],\n        [1.        , 0.74824545, 0.6888498 ],\n        [1.        , 0.7395168 , 0.68695646]],\n\n       [[1.        , 0.79810651, 0.72852553],\n        [1.        , 0.77453962, 0.75629498],\n        [1.        , 0.73573672, 0.72234245],\n        ...,\n        [1.        , 0.83187722, 0.78820498],\n        [1.        , 0.88876711, 0.79846387],\n        [1.        , 0.82846398, 0.76940996]],\n\n       [[1.        , 0.79288095, 0.8342257 ],\n        [1.        , 0.82448106, 0.72086825],\n        [1.        , 0.72033186, 0.80041915],\n        ...,\n        [1.        , 0.78499373, 0.67240037],\n        [1.        , 0.74599395, 0.76189312],\n        [1.        , 0.78834293, 0.63490628]],\n\n       [[1.        , 0.80632405, 0.78827939],\n        [1.        , 0.85917706, 0.77948591],\n        [1.        , 0.74620022, 0.75769738],\n        ...,\n        [1.        , 0.72521435, 0.7303024 ],\n        [1.        , 0.85095254, 0.77531198],\n        [1.        , 0.71902137, 0.74510897]]])lambdas3(chain, draw, indicators_3)float641.0 0.7569 0.9034 ... 0.7921 0.8761array([[[1.        , 0.75688836, 0.90343597],\n        [1.        , 0.7176848 , 0.80317205],\n        [1.        , 0.77074817, 0.90416063],\n        ...,\n        [1.        , 0.76961555, 0.84664995],\n        [1.        , 0.77699081, 0.88783454],\n        [1.        , 0.77683274, 0.90680877]],\n\n       [[1.        , 0.817206  , 0.99247439],\n        [1.        , 0.84394323, 0.92734256],\n        [1.        , 0.78755724, 0.97686568],\n        ...,\n        [1.        , 0.83936568, 0.96084727],\n        [1.        , 0.80897896, 0.92995568],\n        [1.        , 0.8074814 , 0.94634231]],\n\n       [[1.        , 0.76261184, 0.88149179],\n        [1.        , 0.8352147 , 0.92855291],\n        [1.        , 0.89449305, 0.98190939],\n        ...,\n        [1.        , 0.80064177, 0.87890319],\n        [1.        , 0.75622485, 0.96201308],\n        [1.        , 0.850205  , 0.8388567 ]],\n\n       [[1.        , 0.8194599 , 0.8739116 ],\n        [1.        , 0.82935282, 0.89140703],\n        [1.        , 0.80361535, 0.96603315],\n        ...,\n        [1.        , 0.79980394, 0.86747523],\n        [1.        , 0.80830953, 0.92906992],\n        [1.        , 0.79205013, 0.87609535]]])lambdas4(chain, draw, indicators_4)float641.0 1.072 0.9641 ... 1.004 0.955array([[[1.        , 1.07190016, 0.96412386],\n        [1.        , 1.05523423, 1.0022816 ],\n        [1.        , 0.98058084, 0.96050913],\n        ...,\n        [1.        , 1.08526762, 1.10584266],\n        [1.        , 1.07550691, 0.99010885],\n        [1.        , 1.06293623, 0.99030376]],\n\n       [[1.        , 1.06185427, 0.92786736],\n        [1.        , 1.05089232, 0.98680901],\n        [1.        , 1.00854041, 0.93600326],\n        ...,\n        [1.        , 1.07640799, 1.02946004],\n        [1.        , 0.96575982, 0.92383987],\n        [1.        , 1.1216565 , 0.95777207]],\n\n       [[1.        , 0.97467798, 0.98013496],\n        [1.        , 1.02924839, 0.96121384],\n        [1.        , 0.97225375, 0.88289559],\n        ...,\n        [1.        , 1.04469427, 0.93928757],\n        [1.        , 1.0896662 , 1.01707551],\n        [1.        , 1.09991707, 1.03662255]],\n\n       [[1.        , 1.06545274, 0.9710698 ],\n        [1.        , 0.99120572, 1.05770903],\n        [1.        , 1.13534209, 1.01922106],\n        ...,\n        [1.        , 1.03607955, 1.05134002],\n        [1.        , 1.00028223, 0.93901865],\n        [1.        , 1.00433297, 0.95497702]]])lambdas5(chain, draw, indicators_5)float641.0 0.5469 0.6219 ... 0.5687 0.617array([[[1.        , 0.54689596, 0.62188738],\n        [1.        , 0.52124762, 0.54527328],\n        [1.        , 0.52012897, 0.56494137],\n        ...,\n        [1.        , 0.45317851, 0.54110369],\n        [1.        , 0.50611793, 0.53875726],\n        [1.        , 0.50992452, 0.54987472]],\n\n       [[1.        , 0.44015813, 0.69480796],\n        [1.        , 0.5483047 , 0.72594974],\n        [1.        , 0.5312595 , 0.72954133],\n        ...,\n        [1.        , 0.63751484, 0.64707653],\n        [1.        , 0.47901252, 0.63663288],\n        [1.        , 0.62174358, 0.62262435]],\n\n       [[1.        , 0.52522707, 0.71107392],\n        [1.        , 0.50356019, 0.68211666],\n        [1.        , 0.6121456 , 0.63232234],\n        ...,\n        [1.        , 0.60273101, 0.61727572],\n        [1.        , 0.58208316, 0.71524028],\n        [1.        , 0.55922986, 0.60313161]],\n\n       [[1.        , 0.4817728 , 0.675911  ],\n        [1.        , 0.55202498, 0.69200841],\n        [1.        , 0.52530851, 0.57782457],\n        ...,\n        [1.        , 0.51706953, 0.76448678],\n        [1.        , 0.51687005, 0.57557131],\n        [1.        , 0.56873512, 0.61701066]]])chol_cov1_corr(chain, draw, chol_cov1_corr_dim_0, chol_cov1_corr_dim_1)float641.0 0.1698 0.1698 ... 0.1728 1.0array([[[[1.        , 0.16976309],\n         [0.16976309, 1.        ]],\n\n        [[1.        , 0.15602901],\n         [0.15602901, 1.        ]],\n\n        [[1.        , 0.11442654],\n         [0.11442654, 1.        ]],\n\n        ...,\n\n        [[1.        , 0.12848038],\n         [0.12848038, 1.        ]],\n\n        [[1.        , 0.1553594 ],\n         [0.1553594 , 1.        ]],\n\n        [[1.        , 0.16112913],\n         [0.16112913, 1.        ]]],\n\n...\n\n       [[[1.        , 0.21087368],\n         [0.21087368, 1.        ]],\n\n        [[1.        , 0.21545573],\n         [0.21545573, 1.        ]],\n\n        [[1.        , 0.06964866],\n         [0.06964866, 1.        ]],\n\n        ...,\n\n        [[1.        , 0.08237674],\n         [0.08237674, 1.        ]],\n\n        [[1.        , 0.12632947],\n         [0.12632947, 1.        ]],\n\n        [[1.        , 0.17280138],\n         [0.17280138, 1.        ]]]])chol_cov1_stds(chain, draw, chol_cov1_stds_dim_0)float641.105 0.884 1.022 ... 1.04 1.0array([[[1.10548322, 0.8839739 ],\n        [1.02186277, 0.9437363 ],\n        [1.07719629, 0.96966888],\n        ...,\n        [1.0455881 , 0.89347717],\n        [1.03998455, 0.9199936 ],\n        [1.05975403, 0.92410667]],\n\n       [[1.02060411, 0.92748198],\n        [0.99409161, 0.9243332 ],\n        [0.97209611, 0.91333345],\n        ...,\n        [1.01370072, 0.91158098],\n        [0.97738606, 1.05784626],\n        [0.99878332, 0.87197534]],\n\n       [[1.02453808, 0.88852489],\n        [0.98711601, 1.01211135],\n        [0.93970627, 1.03518744],\n        ...,\n        [1.00878963, 0.9293989 ],\n        [1.03846034, 0.92130131],\n        [0.98824482, 0.88070406]],\n\n       [[1.03560124, 1.0136757 ],\n        [1.06312843, 0.96747262],\n        [0.96358369, 0.90554055],\n        ...,\n        [1.00745981, 1.0153215 ],\n        [1.0311157 , 0.93429976],\n        [1.03966306, 1.00018579]]])cov(chain, draw, latent, latent1)float641.222 0.1659 0.1659 ... 0.1797 1.0array([[[[1.22209315, 0.16589561],\n         [0.16589561, 0.78140986]],\n\n        [[1.04420352, 0.15046954],\n         [0.15046954, 0.89063821]],\n\n        [[1.16035184, 0.11952123],\n         [0.11952123, 0.94025775]],\n\n        ...,\n\n        [[1.09325447, 0.12002754],\n         [0.12002754, 0.79830145]],\n\n        [[1.08156786, 0.14864464],\n         [0.14864464, 0.84638823]],\n\n        [[1.12307861, 0.15779791],\n         [0.15779791, 0.85397313]]],\n\n...\n\n       [[[1.07246992, 0.22136756],\n         [0.22136756, 1.02753843]],\n\n        [[1.13024207, 0.22160648],\n         [0.22160648, 0.93600327]],\n\n        [[0.92849352, 0.06077292],\n         [0.06077292, 0.82000368]],\n\n        ...,\n\n        [[1.01497527, 0.0842628 ],\n         [0.0842628 , 1.03087774]],\n\n        [[1.06319959, 0.12170216],\n         [0.12170216, 0.87291605]],\n\n        [[1.08089928, 0.17968859],\n         [0.17968859, 1.00037161]]]])mu(chain, draw, mu_dim_0, mu_dim_1)float645.332 5.474 5.349 ... 6.43 5.888array([[[[5.33243161, 5.47373663, 5.34876225, ..., 5.96108991,\n          6.2009526 , 5.70347029],\n         [4.81550112, 5.11145421, 4.89994657, ..., 4.65655555,\n          5.48750803, 4.89219684],\n         [5.34549768, 5.48289377, 5.36010663, ..., 5.41471158,\n          5.9021405 , 5.36368451],\n         ...,\n         [5.39189861, 5.51541311, 5.40039341, ..., 5.61940984,\n          6.01408915, 5.49098377],\n         [4.6564226 , 4.9999666 , 4.76182949, ..., 4.18370323,\n          5.22890701, 4.59813595],\n         [6.08961407, 6.00439576, 6.0061724 , ..., 6.17057599,\n          6.31551968, 5.83374704]],\n\n        [[4.50605513, 4.84546924, 4.6641477 , ..., 6.11722093,\n          6.20788863, 5.67879402],\n         [4.03345396, 4.48937249, 4.27359111, ..., 4.24185714,\n          5.23035971, 4.65620824],\n         [4.71779431, 5.00501103, 4.83912851, ..., 5.26531193,\n          5.76383308, 5.2142708 ],\n...\n         [4.46207555, 4.87128842, 4.62445678, ..., 5.6619874 ,\n          6.00198247, 5.5241775 ],\n         [5.17656821, 5.37030136, 5.25375574, ..., 3.56333545,\n          4.91725213, 4.31625365],\n         [5.69286809, 5.73089334, 5.70849376, ..., 6.14962805,\n          6.25402933, 5.80484947]],\n\n        [[4.65449076, 5.05914876, 4.8711263 , ..., 5.46763634,\n          5.9522754 , 5.36928923],\n         [3.74594272, 4.45578561, 4.15740239, ..., 4.18464691,\n          5.22259426, 4.57767108],\n         [5.07320437, 5.33721481, 5.20005323, ..., 5.5784452 ,\n          6.01529629, 5.43765948],\n         ...,\n         [6.06268839, 5.99432721, 5.97735764, ..., 5.21494956,\n          5.80856356, 5.2133788 ],\n         [4.7284701 , 5.10827815, 4.92924192, ..., 4.94795969,\n          5.65671704, 5.0486432 ],\n         [6.12768714, 6.03749263, 6.02841842, ..., 6.30764102,\n          6.43001557, 5.88758107]]]])Indexes: (22)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       9990, 9991, 9992, 9993, 9994, 9995, 9996, 9997, 9998, 9999],\n      dtype='int64', name='draw', length=10000))indicators_1PandasIndexPandasIndex(Index(['se_acad_p1', 'se_acad_p2', 'se_acad_p3'], dtype='object', name='indicators_1'))indicators_2PandasIndexPandasIndex(Index(['se_social_p1', 'se_social_p2', 'se_social_p3'], dtype='object', name='indicators_2'))indicators_3PandasIndexPandasIndex(Index(['sup_friends_p1', 'sup_friends_p2', 'sup_friends_p3'], dtype='object', name='indicators_3'))indicators_4PandasIndexPandasIndex(Index(['sup_parents_p1', 'sup_parents_p2', 'sup_parents_p3'], dtype='object', name='indicators_4'))indicators_5PandasIndexPandasIndex(Index(['ls_p1', 'ls_p2', 'ls_p3'], dtype='object', name='indicators_5'))indicatorsPandasIndexPandasIndex(Index(['se_acad_p1', 'se_acad_p2', 'se_acad_p3', 'se_social_p1',\n       'se_social_p2', 'se_social_p3', 'sup_friends_p1', 'sup_friends_p2',\n       'sup_friends_p3', 'sup_parents_p1', 'sup_parents_p2', 'sup_parents_p3',\n       'ls_p1', 'ls_p2', 'ls_p3'],\n      dtype='object', name='indicators'))obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       273, 274, 275, 276, 277, 278, 279, 280, 281, 282],\n      dtype='int64', name='obs', length=283))latentPandasIndexPandasIndex(Index(['SUP_F', 'SUP_P'], dtype='object', name='latent'))latent_regressionPandasIndexPandasIndex(Index(['SUP_F->SE_ACAD', 'SUP_P->SE_ACAD', 'SUP_F->SE_SOC', 'SUP_P->SE_SOC'], dtype='object', name='latent_regression'))regressionPandasIndexPandasIndex(Index(['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P'], dtype='object', name='regression'))regr_se_acad_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       273, 274, 275, 276, 277, 278, 279, 280, 281, 282],\n      dtype='int64', name='regr_se_acad_dim_0', length=283))regr_se_social_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       273, 274, 275, 276, 277, 278, 279, 280, 281, 282],\n      dtype='int64', name='regr_se_social_dim_0', length=283))regr_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       273, 274, 275, 276, 277, 278, 279, 280, 281, 282],\n      dtype='int64', name='regr_dim_0', length=283))chol_cov1_dim_0PandasIndexPandasIndex(Index([0, 1, 2], dtype='int64', name='chol_cov1_dim_0'))chol_cov1_corr_dim_0PandasIndexPandasIndex(Index([0, 1], dtype='int64', name='chol_cov1_corr_dim_0'))chol_cov1_corr_dim_1PandasIndexPandasIndex(Index([0, 1], dtype='int64', name='chol_cov1_corr_dim_1'))chol_cov1_stds_dim_0PandasIndexPandasIndex(Index([0, 1], dtype='int64', name='chol_cov1_stds_dim_0'))latent1PandasIndexPandasIndex(Index(['SUP_F', 'SUP_P'], dtype='object', name='latent1'))mu_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       273, 274, 275, 276, 277, 278, 279, 280, 281, 282],\n      dtype='int64', name='mu_dim_0', length=283))mu_dim_1PandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], dtype='int64', name='mu_dim_1'))Attributes: (2)created_at :2024-08-20T20:15:31.157627arviz_version :0.17.0\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:           (chain: 4, draw: 10000, likelihood_dim_2: 283,\n                       likelihood_dim_3: 15)\nCoordinates:\n  * chain             (chain) int64 0 1 2 3\n  * draw              (draw) int64 0 1 2 3 4 5 ... 9994 9995 9996 9997 9998 9999\n  * likelihood_dim_2  (likelihood_dim_2) int64 0 1 2 3 4 ... 278 279 280 281 282\n  * likelihood_dim_3  (likelihood_dim_3) int64 0 1 2 3 4 5 ... 9 10 11 12 13 14\nData variables:\n    likelihood        (chain, draw, likelihood_dim_2, likelihood_dim_3) float64 ...\nAttributes:\n    created_at:                 2024-08-20T20:15:35.425409\n    arviz_version:              0.17.0\n    inference_library:          pymc\n    inference_library_version:  5.10.3xarray.DatasetDimensions:chain: 4draw: 10000likelihood_dim_2: 283likelihood_dim_3: 15Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 9996 9997 9998 9999array([   0,    1,    2, ..., 9997, 9998, 9999])likelihood_dim_2(likelihood_dim_2)int640 1 2 3 4 5 ... 278 279 280 281 282array([  0,   1,   2, ..., 280, 281, 282])likelihood_dim_3(likelihood_dim_3)int640 1 2 3 4 5 6 7 8 9 10 11 12 13 14array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])Data variables: (1)likelihood(chain, draw, likelihood_dim_2, likelihood_dim_3)float645.851 5.46 5.759 ... 7.23 4.588array([[[[5.85070663, 5.46035588, 5.7589315 , ..., 6.63639614,\n          5.27835382, 4.61107418],\n         [4.99999483, 5.36540836, 5.01526598, ..., 4.35902364,\n          5.8112928 , 3.96054044],\n         [5.63220902, 5.04794817, 5.87328232, ..., 5.70877735,\n          6.87370873, 5.15320078],\n         ...,\n         [5.71019462, 5.22347911, 5.948029  , ..., 5.73183859,\n          5.92483764, 7.11376151],\n         [4.67925791, 6.30734351, 4.17105192, ..., 3.72617891,\n          4.70777009, 5.60080204],\n         [5.86442147, 5.34165209, 5.93709333, ..., 6.0273743 ,\n          6.32948876, 7.26182453]],\n\n        [[4.25975194, 4.72201294, 5.06309965, ..., 5.71207883,\n          6.02934541, 4.56586237],\n         [3.91897269, 4.43078104, 4.05787359, ..., 3.93407779,\n          6.04426927, 4.76931078],\n         [4.54494189, 5.04171487, 4.5708707 , ..., 6.62551026,\n          4.94051444, 5.62471998],\n...\n         [4.1180587 , 5.12244197, 4.44273671, ..., 5.70722813,\n          5.0174093 , 5.00937699],\n         [5.00361209, 5.20558751, 4.61931201, ..., 3.2952441 ,\n          4.24153372, 4.60586937],\n         [5.46256112, 5.6245736 , 6.12139409, ..., 7.00458459,\n          7.52136017, 6.29393866]],\n\n        [[4.55835897, 5.44928315, 4.72938171, ..., 5.23880526,\n          6.12319534, 4.39815495],\n         [3.70299948, 4.31897636, 3.87667135, ..., 4.50068886,\n          5.05058052, 3.72989017],\n         [5.07978672, 4.83377275, 4.95997553, ..., 5.48559804,\n          5.80016676, 6.44838777],\n         ...,\n         [5.72247176, 6.14331955, 5.90200037, ..., 5.44543284,\n          6.22117312, 5.26210037],\n         [4.36208699, 4.9500386 , 4.77355957, ..., 4.39684382,\n          5.80688929, 6.03377103],\n         [5.97029952, 5.88113199, 6.10833301, ..., 5.831811  ,\n          7.22991946, 4.58777143]]]])Indexes: (4)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       9990, 9991, 9992, 9993, 9994, 9995, 9996, 9997, 9998, 9999],\n      dtype='int64', name='draw', length=10000))likelihood_dim_2PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       273, 274, 275, 276, 277, 278, 279, 280, 281, 282],\n      dtype='int64', name='likelihood_dim_2', length=283))likelihood_dim_3PandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], dtype='int64', name='likelihood_dim_3'))Attributes: (4)created_at :2024-08-20T20:15:35.425409arviz_version :0.17.0inference_library :pymcinference_library_version :5.10.3\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:           (chain: 4, draw: 10000, likelihood_dim_0: 283,\n                       likelihood_dim_1: 15)\nCoordinates:\n  * chain             (chain) int64 0 1 2 3\n  * draw              (draw) int64 0 1 2 3 4 5 ... 9994 9995 9996 9997 9998 9999\n  * likelihood_dim_0  (likelihood_dim_0) int64 0 1 2 3 4 ... 278 279 280 281 282\n  * likelihood_dim_1  (likelihood_dim_1) int64 0 1 2 3 4 5 ... 9 10 11 12 13 14\nData variables:\n    likelihood        (chain, draw, likelihood_dim_0, likelihood_dim_1) float64 ...\nAttributes:\n    created_at:     2024-08-20T20:15:31.167325\n    arviz_version:  0.17.0xarray.DatasetDimensions:chain: 4draw: 10000likelihood_dim_0: 283likelihood_dim_1: 15Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 9996 9997 9998 9999array([   0,    1,    2, ..., 9997, 9998, 9999])likelihood_dim_0(likelihood_dim_0)int640 1 2 3 4 5 ... 278 279 280 281 282array([  0,   1,   2, ..., 280, 281, 282])likelihood_dim_1(likelihood_dim_1)int640 1 2 3 4 5 6 7 8 9 10 11 12 13 14array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])Data variables: (1)likelihood(chain, draw, likelihood_dim_0, likelihood_dim_1)float64-0.7274 -0.1203 ... -0.2399 -0.5904array([[[[-7.27428553e-01, -1.20304986e-01, -1.75951258e+00, ...,\n          -9.53973332e-01, -8.21413971e-01, -6.45620240e-01],\n         [-1.64257117e-01, -1.86505036e+00, -2.99088805e-01, ...,\n          -5.67656182e-01, -7.05749378e-01, -7.50516898e-01],\n         [-4.85974370e+00, -1.22587152e+00, -1.81324068e-01, ...,\n          -1.55396375e+00, -5.68039217e-01, -2.34220323e+00],\n         ...,\n         [-7.30252911e-04, -1.76113600e+00, -2.01508989e-01, ...,\n          -4.31274796e-01, -4.01268641e-01, -6.61177718e-01],\n         [-3.62244818e-01, -2.57692641e-01, -8.95990300e-01, ...,\n          -7.84894198e-01, -6.22285026e-01, -1.11134109e+00],\n         [ 2.78477880e-02, -1.51809671e-01, -2.45261475e-01, ...,\n          -4.63631881e-01, -3.36517750e-01, -6.13534343e-01]],\n\n        [[-3.77525608e-01, -1.66211609e+00, -2.00703845e-01, ...,\n          -1.28232241e+00, -8.07101092e-01, -5.96863264e-01],\n         [-9.58028106e-01, -1.28251716e-01, -5.05869927e-01, ...,\n          -3.92878606e-01, -3.79826807e-01, -5.89175198e-01],\n         [-1.10174764e+00, -4.08727458e+00, -7.20527463e-01, ...,\n          -2.05448413e+00, -4.09177047e-01, -2.06256657e+00],\n...\n         [-2.62033339e+00, -1.79008126e-01, -1.73602495e+00, ...,\n          -1.46840791e-01, -5.10114522e-01, -6.03915706e-01],\n         [ 7.10184992e-04, -1.07284673e+00, -2.49452828e-01, ...,\n          -1.71799478e-01, -1.17921436e+00, -1.66797713e+00],\n         [-6.93075175e-01, -1.57387381e-01, -2.67544732e-01, ...,\n          -2.25837964e-01, -5.06093432e-01, -5.23868174e-01]],\n\n        [[-9.35544053e-02, -7.76994456e-01, -4.95436037e-01, ...,\n          -1.76007083e-01, -1.49183995e+00, -5.88573446e-01],\n         [-2.31087566e+00, -1.69659421e-01, -7.45166703e-01, ...,\n          -1.85816944e-01, -3.28382397e-01, -5.77500852e-01],\n         [-2.94841044e+00, -1.78259157e+00, -2.49081697e-01, ...,\n          -1.50593226e+00, -7.56601506e-01, -2.64207809e+00],\n         ...,\n         [-2.04169788e+00, -4.35607883e+00, -6.80524652e-01, ...,\n          -6.24307816e-01, -2.36945040e-01, -5.72800641e-01],\n         [-2.06645760e-01, -4.28293810e-01, -5.47912157e-01, ...,\n          -4.08918585e+00, -2.30341402e-01, -6.52652858e-01],\n         [ 4.78512656e-02, -1.79027435e-01, -2.90755583e-01, ...,\n          -1.34126705e-01, -2.39855844e-01, -5.90420042e-01]]]])Indexes: (4)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       9990, 9991, 9992, 9993, 9994, 9995, 9996, 9997, 9998, 9999],\n      dtype='int64', name='draw', length=10000))likelihood_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       273, 274, 275, 276, 277, 278, 279, 280, 281, 282],\n      dtype='int64', name='likelihood_dim_0', length=283))likelihood_dim_1PandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], dtype='int64', name='likelihood_dim_1'))Attributes: (2)created_at :2024-08-20T20:15:31.167325arviz_version :0.17.0\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:          (chain: 4, draw: 10000)\nCoordinates:\n  * chain            (chain) int64 0 1 2 3\n  * draw             (draw) int64 0 1 2 3 4 5 ... 9994 9995 9996 9997 9998 9999\nData variables:\n    acceptance_rate  (chain, draw) float64 0.9966 0.9828 ... 0.9881 0.9779\n    step_size        (chain, draw) float64 0.06094 0.06094 ... 0.05858 0.05858\n    diverging        (chain, draw) bool False False False ... False False False\n    energy           (chain, draw) float64 5.795e+03 5.831e+03 ... 5.761e+03\n    n_steps          (chain, draw) int64 63 63 63 63 63 63 ... 63 63 63 63 63 63\n    tree_depth       (chain, draw) int64 6 6 6 6 6 6 6 6 6 ... 6 6 6 6 6 6 6 6 6\n    lp               (chain, draw) float64 5.068e+03 5.078e+03 ... 5.03e+03\nAttributes:\n    created_at:     2024-08-20T20:15:31.166065\n    arviz_version:  0.17.0xarray.DatasetDimensions:chain: 4draw: 10000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 9996 9997 9998 9999array([   0,    1,    2, ..., 9997, 9998, 9999])Data variables: (7)acceptance_rate(chain, draw)float640.9966 0.9828 ... 0.9881 0.9779array([[0.99655009, 0.98283854, 0.99385371, ..., 0.96527513, 0.97676805,\n        0.98950263],\n       [0.99740477, 0.99662212, 0.98717303, ..., 0.99475838, 0.99964662,\n        0.98880662],\n       [0.99999998, 0.9999551 , 0.98915186, ..., 0.99522171, 0.99504278,\n        0.9999999 ],\n       [0.99279847, 0.96502445, 0.99909414, ..., 0.99339823, 0.98805703,\n        0.97794803]])step_size(chain, draw)float640.06094 0.06094 ... 0.05858 0.05858array([[0.06093791, 0.06093791, 0.06093791, ..., 0.06093791, 0.06093791,\n        0.06093791],\n       [0.05129879, 0.05129879, 0.05129879, ..., 0.05129879, 0.05129879,\n        0.05129879],\n       [0.05382202, 0.05382202, 0.05382202, ..., 0.05382202, 0.05382202,\n        0.05382202],\n       [0.05858427, 0.05858427, 0.05858427, ..., 0.05858427, 0.05858427,\n        0.05858427]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])energy(chain, draw)float645.795e+03 5.831e+03 ... 5.761e+03array([[5794.52908116, 5830.97584521, 5799.96461931, ..., 5714.5532589 ,\n        5769.52145705, 5821.61841561],\n       [5798.34991256, 5810.51349605, 5831.05621859, ..., 5773.47845966,\n        5710.98318926, 5722.67200128],\n       [5850.87030658, 5799.31810444, 5765.26640305, ..., 5760.70952164,\n        5738.67227314, 5697.11083123],\n       [5727.59759419, 5693.67077448, 5701.88640134, ..., 5712.56224689,\n        5734.00185708, 5760.74297287]])n_steps(chain, draw)int6463 63 63 63 63 ... 63 63 63 63 63array([[63, 63, 63, ..., 63, 63, 63],\n       [63, 63, 63, ..., 63, 63, 63],\n       [63, 63, 63, ..., 63, 63, 63],\n       [63, 63, 63, ..., 63, 63, 63]])tree_depth(chain, draw)int646 6 6 6 6 6 6 6 ... 6 6 6 6 6 6 6 6array([[6, 6, 6, ..., 6, 6, 6],\n       [6, 6, 6, ..., 6, 6, 6],\n       [6, 6, 6, ..., 6, 6, 6],\n       [6, 6, 6, ..., 6, 6, 6]])lp(chain, draw)float645.068e+03 5.078e+03 ... 5.03e+03array([[5068.07686714, 5077.76803335, 5050.53004311, ..., 5020.95919183,\n        5054.79008061, 5061.71360701],\n       [5062.64431757, 5056.64738088, 5099.10995519, ..., 5028.36138164,\n        4992.19656415, 4986.96724123],\n       [5078.82061035, 5037.46114278, 5043.97060126, ..., 5038.10634215,\n        5004.13933756, 5001.10183279],\n       [4974.79335334, 5006.6139136 , 5003.93256654, ..., 4987.35310337,\n        4999.5623336 , 5030.31928439]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       9990, 9991, 9992, 9993, 9994, 9995, 9996, 9997, 9998, 9999],\n      dtype='int64', name='draw', length=10000))Attributes: (2)created_at :2024-08-20T20:15:31.166065arviz_version :0.17.0\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:           (likelihood_dim_0: 283, likelihood_dim_1: 15)\nCoordinates:\n  * likelihood_dim_0  (likelihood_dim_0) int64 0 1 2 3 4 ... 278 279 280 281 282\n  * likelihood_dim_1  (likelihood_dim_1) int64 0 1 2 3 4 5 ... 9 10 11 12 13 14\nData variables:\n    likelihood        (likelihood_dim_0, likelihood_dim_1) float64 4.857 ... ...\nAttributes:\n    created_at:                 2024-08-20T20:15:31.167698\n    arviz_version:              0.17.0\n    inference_library:          numpyro\n    inference_library_version:  0.13.2\n    sampling_time:              61.346018xarray.DatasetDimensions:likelihood_dim_0: 283likelihood_dim_1: 15Coordinates: (2)likelihood_dim_0(likelihood_dim_0)int640 1 2 3 4 5 ... 278 279 280 281 282array([  0,   1,   2, ..., 280, 281, 282])likelihood_dim_1(likelihood_dim_1)int640 1 2 3 4 5 6 7 8 9 10 11 12 13 14array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])Data variables: (1)likelihood(likelihood_dim_0, likelihood_dim_1)float644.857 5.571 4.5 ... 6.333 6.5 5.75array([[4.85714286, 5.57142857, 4.5       , ..., 5.33333333, 6.75      ,\n        5.5       ],\n       [4.57142857, 4.28571429, 4.66666667, ..., 4.33333333, 5.        ,\n        4.5       ],\n       [4.14285714, 6.14285714, 5.33333333, ..., 6.33333333, 5.5       ,\n        4.        ],\n       ...,\n       [5.28571429, 4.71428571, 5.5       , ..., 5.66666667, 5.75      ,\n        5.25      ],\n       [5.        , 4.75      , 5.33333333, ..., 3.66666667, 5.66666667,\n        5.33333333],\n       [6.14285714, 5.85714286, 5.83333333, ..., 6.33333333, 6.5       ,\n        5.75      ]])Indexes: (2)likelihood_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       273, 274, 275, 276, 277, 278, 279, 280, 281, 282],\n      dtype='int64', name='likelihood_dim_0', length=283))likelihood_dim_1PandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], dtype='int64', name='likelihood_dim_1'))Attributes: (5)created_at :2024-08-20T20:15:31.167698arviz_version :0.17.0inference_library :numpyroinference_library_version :0.13.2sampling_time :61.346018\n                      \n                  \n            \n            \n              \n            \n            \n\n\n\nfig, ax = plt.subplots(figsize=(10, 30))\naz.plot_forest(idata1, var_names=['ksi'], combined=True, ax=ax, coords={'latent': ['SUP_P']});\n\n\n\n\n\ncorrelation_df = pd.DataFrame(az.extract(idata1['posterior'])['chol_cov_corr'].mean(axis=2))\n#correlation_df.index = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']\n#correlation_df.columns = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']\ncorrelation_df\n\n\n\n\n\n  \n    \n      \n      0\n      1\n    \n  \n  \n    \n      0\n      1.000000\n      0.181016\n    \n    \n      1\n      0.181016\n      1.000000\n    \n  \n\n\n\n\n\naz.summary(idata1, var_names=['cov'])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      cov[SUP_F, SUP_F]\n      0.673\n      0.454\n      0.022\n      1.169\n      0.157\n      0.115\n      12.0\n      23.0\n      1.79\n    \n    \n      cov[SUP_F, SUP_P]\n      0.086\n      0.067\n      -0.059\n      0.206\n      0.010\n      0.007\n      49.0\n      23.0\n      1.15\n    \n    \n      cov[SUP_P, SUP_F]\n      0.086\n      0.067\n      -0.059\n      0.206\n      0.010\n      0.007\n      49.0\n      23.0\n      1.15\n    \n    \n      cov[SUP_P, SUP_P]\n      0.558\n      0.234\n      0.266\n      0.966\n      0.077\n      0.056\n      13.0\n      111.0\n      1.67\n    \n  \n\n\n\n\n\naz.plot_pair(idata1, var_names='cov');\n\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/plots/pairplot.py:232: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n  gridsize = int(dataset.dims[\"draw\"] ** 0.35)"
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#confirmatory-factor-models-with-pymc",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#confirmatory-factor-models-with-pymc",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Confirmatory Factor Models with PyMC",
    "text": "Confirmatory Factor Models with PyMC\nSo far we’ve seen CFA and SEM models specified using a kind of formula syntax. This useful but here we switch to the distributional perspective which is more directly relevant to the Bayesian phrasing of the CFA model. We’ll unpack these components of the distributional view with a simpler data set initially for illustrative purposes and then we’ll switch back to the Life Satisfaction question.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pytensor import tensor as pt\nimport arviz as az\nimport networkx as nx\nnp.random.seed(150)\n\n\n\ndf_p = pd.read_csv('IIS.dat', sep='\\s+')\ndf_p.head()\n\n     PI    AD   IGC   FI   FC\n0  4.00  3.38  4.67  2.6  3.2\n1  2.57  3.00  3.50  2.4  2.8\n2  2.29  3.29  4.83  2.0  3.4\n3  2.43  3.63  4.33  3.6  3.8\n4  3.00  4.00  4.83  3.4  3.8\n\n\nWe have here a dataset drawn from Mislevy and Levy. which records measures of students collegiate experiences along multiple dimensions. The data here represents the averages of PI peer interaction, AD academic development IGC goal commitment, FI faculty interaction and FC faculty concern items for 500 hundred students. We will fit a 2 factor Bayesian CFA model to this data set - one factor for he faculty items and the other factor for self-reporting items. First consider the code and then we’ll step through it in more detail.\n\ncoords = {'obs': list(range(len(df_p))), \n          'indicators': ['PI', 'AD',    'IGC', 'FI', 'FC'],\n          'indicators_1': ['PI', 'AD',  'IGC'],\n          'indicators_2': ['FI', 'FC'],\n          'latent': ['Student', 'Faculty']\n          }\n\n\nobs_idx = list(range(len(df_p)))\nwith pm.Model(coords=coords) as model:\n  \n  Psi = pm.InverseGamma('Psi', 5, 10, dims='indicators')\n  lambdas_ = pm.Normal('lambdas_1', 1, 10, dims=('indicators_1'))\n  # Force a fixed scale on the factor loadings for factor 1\n  lambdas_1 = pm.Deterministic('lambdas1', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_1'))\n  lambdas_ = pm.Normal('lambdas_2', 1, 10, dims=('indicators_2'))\n  # Force a fixed scale on the factor loadings for factor 2\n  lambdas_2 = pm.Deterministic('lambdas2', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_2'))\n  tau = pm.Normal('tau', 3, 10, dims='indicators')\n  # Specify covariance structure between latent factors\n  kappa = 0\n  sd_dist = pm.Exponential.dist(1.0, shape=2)\n  chol, _, _ = pm.LKJCholeskyCov('chol_cov', n=2, eta=2,\n    sd_dist=sd_dist, compute_corr=True)\n  ksi = pm.MvNormal('ksi', kappa, chol=chol, dims=('obs', 'latent'))\n\n  # Construct Observation matrix\n  m1 = tau[0] + ksi[obs_idx, 0]*lambdas_1[0]\n  m2 = tau[1] + ksi[obs_idx, 0]*lambdas_1[1]\n  m3 = tau[2] + ksi[obs_idx, 0]*lambdas_1[2]\n  m4 = tau[3] + ksi[obs_idx, 1]*lambdas_2[0]\n  m5 = tau[4] + ksi[obs_idx, 1]*lambdas_2[1]\n  \n  mu = pm.Deterministic('mu', pm.math.stack([m1, m2, m3, m4, m5]).T)\n  _  = pm.Normal('likelihood', mu, Psi, observed=df_p.values)\n\n  idata = pm.sample(nuts_sampler='numpyro', target_accept=.95, \n                    idata_kwargs={\"log_likelihood\": True})\n  idata.extend(pm.sample_posterior_predictive(idata))\n  \nsummary_df = az.summary(idata, var_names=['lambdas1', 'lambdas2', 'tau', 'Psi', 'ksi', 'chol_cov_corr'], coords= {'obs': [0, 7]})\n\nThe model specification here has a Normal likelihood term which takes as observed data the matrix of student responses. However we construct the input for the likelihood term in a way that we posit two latent factor variables ksi[obs_idx, 0] and ksi[obs_idx, 1] respectively. These factors are modified by the factor loading terms in e.g. lambdas_1 specification to create a regression model m1 which predicts the indicator variable PI. In this way we allow that the latent factors are related to the observed data matrix and the manner in which calibrating against observed data modifies our priors for the parameterisations of the lambdas, ksi terms reflects what the model learns about the interactions between the latent constructs. Crucially we force a covariance structure on ksi terms to reflect the correlations and covariance between these constructs of interest and the factor loading terms have a required scale fixed by forcing the first indicator loading to 1 for each factor.\nIn a Bayesian approach to the estimation of these models we are not limited by the degrees of freedom available in the optimisation routine. Instead we’re aiming to estimate the posterior distribution using MCMC on draws from well specified priors on each the quantities listed above. In a picture the structure is like this:\n\n\n\nPyMC Confirmatory Factor Model\n\n\nWe are conditioning on the observed data and seek to estimate sound parameterisations of the ksi, lambda and tau that go into building our mu structure. The marginal distribution of the observables is derived by integrating over the distribution of the latent variables in this fashion so that:\n\\[ p(x_{i}.....x_{n} | ksi, \\Psi, \\tau, \\Lambda) \\sim Normal(\\tau + \\Lambda\\cdot ksi, \\Psi) \\] For more details on the precise derivation of this likelihood term we recommend the discussion in the book. It is this same likelihood specification that is used in maximum likelihood fits of the same and the Normal assumption has proved viable and computationally tractable. The model yields summary statistics for each of these parameters.\n\npy$summary_df |> kable() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n \n  \n     \n    mean \n    sd \n    hdi_3% \n    hdi_97% \n    mcse_mean \n    mcse_sd \n    ess_bulk \n    ess_tail \n    r_hat \n  \n \n\n  \n    lambdas1[PI] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    lambdas1[AD] \n    0.906 \n    0.062 \n    0.792 \n    1.026 \n    0.004 \n    0.003 \n    297 \n    639 \n    1.00 \n  \n  \n    lambdas1[IGC] \n    0.539 \n    0.045 \n    0.455 \n    0.621 \n    0.002 \n    0.002 \n    420 \n    728 \n    1.00 \n  \n  \n    lambdas2[FI] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    lambdas2[FC] \n    0.980 \n    0.056 \n    0.876 \n    1.085 \n    0.003 \n    0.002 \n    492 \n    973 \n    1.00 \n  \n  \n    tau[PI] \n    3.335 \n    0.037 \n    3.262 \n    3.401 \n    0.002 \n    0.001 \n    520 \n    1071 \n    1.01 \n  \n  \n    tau[AD] \n    3.899 \n    0.026 \n    3.850 \n    3.949 \n    0.001 \n    0.001 \n    314 \n    681 \n    1.01 \n  \n  \n    tau[IGC] \n    4.597 \n    0.020 \n    4.560 \n    4.636 \n    0.001 \n    0.001 \n    535 \n    1153 \n    1.00 \n  \n  \n    tau[FI] \n    3.035 \n    0.040 \n    2.963 \n    3.114 \n    0.002 \n    0.001 \n    384 \n    929 \n    1.01 \n  \n  \n    tau[FC] \n    3.714 \n    0.035 \n    3.646 \n    3.780 \n    0.002 \n    0.001 \n    354 \n    900 \n    1.01 \n  \n  \n    Psi[PI] \n    0.611 \n    0.024 \n    0.568 \n    0.657 \n    0.001 \n    0.000 \n    1621 \n    2465 \n    1.00 \n  \n  \n    Psi[AD] \n    0.315 \n    0.019 \n    0.277 \n    0.351 \n    0.001 \n    0.001 \n    637 \n    999 \n    1.01 \n  \n  \n    Psi[IGC] \n    0.354 \n    0.013 \n    0.329 \n    0.378 \n    0.000 \n    0.000 \n    2540 \n    2862 \n    1.00 \n  \n  \n    Psi[FI] \n    0.570 \n    0.026 \n    0.522 \n    0.617 \n    0.001 \n    0.001 \n    1132 \n    2043 \n    1.00 \n  \n  \n    Psi[FC] \n    0.422 \n    0.025 \n    0.374 \n    0.467 \n    0.001 \n    0.001 \n    874 \n    1767 \n    1.00 \n  \n  \n    ksi[0, Student] \n    -0.232 \n    0.221 \n    -0.656 \n    0.163 \n    0.004 \n    0.003 \n    3739 \n    2853 \n    1.00 \n  \n  \n    ksi[0, Faculty] \n    -0.373 \n    0.277 \n    -0.887 \n    0.133 \n    0.004 \n    0.003 \n    3864 \n    2980 \n    1.00 \n  \n  \n    ksi[7, Student] \n    0.884 \n    0.228 \n    0.446 \n    1.301 \n    0.004 \n    0.003 \n    3298 \n    2670 \n    1.00 \n  \n  \n    ksi[7, Faculty] \n    0.870 \n    0.273 \n    0.335 \n    1.370 \n    0.005 \n    0.003 \n    3539 \n    3054 \n    1.00 \n  \n  \n    chol_cov_corr[0, 0] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    chol_cov_corr[0, 1] \n    0.850 \n    0.028 \n    0.798 \n    0.903 \n    0.001 \n    0.001 \n    497 \n    737 \n    1.01 \n  \n  \n    chol_cov_corr[1, 0] \n    0.850 \n    0.028 \n    0.798 \n    0.903 \n    0.001 \n    0.001 \n    497 \n    737 \n    1.01 \n  \n  \n    chol_cov_corr[1, 1] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    3626 \n    3792 \n    1.00 \n  \n\n\n\n\n\nWe can additionally check the diagnostic trace plots to ensure that the model has sampled well even across the relatively complex structure.\n\naz.plot_trace(idata, var_names=['lambdas1', 'lambdas2', 'tau', 'Psi', 'ksi']);\n\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:487: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:487: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n\n\n\n\n\nThese diagnostic plots and parameter estimates reflect well the same figures reported by Levy and Mislevy, so we won’t dwell too much further on this model only to say that the basic structure here is expanded upon as we apply more factor structures and SEM like structures to our original Life-Satisfaction dataset.\n\nFactoring for Life Satisfaction\nNow we want fit the life-satisfaction measurement model using PyMC. This has fundamentally a similar structure to the simple CFA model we’ve seen above. However we now have 5 factors with 3 factor loadings to be estimated for three indicators per factor. We will fit this model and then show how to pull out some of model fit statistics that are relevant for the assessment of the model.\n\n\ndf = pd.read_csv('sem_data.csv')\ndrivers = ['se_acad_p1', 'se_acad_p2',\n       'se_acad_p3', 'se_social_p1', 'se_social_p2', 'se_social_p3',\n       'sup_friends_p1', 'sup_friends_p2', 'sup_friends_p3', 'sup_parents_p1',\n       'sup_parents_p2', 'sup_parents_p3', 'ls_p1', 'ls_p2', 'ls_p3']\n       \n\n\ncoords = {'obs': list(range(len(df))), \n          'indicators': drivers,\n          'indicators_1': ['se_acad_p1','se_acad_p2','se_acad_p3'],\n          'indicators_2': ['se_social_p1','se_social_p2','se_social_p3'],\n          'indicators_3': ['sup_friends_p1','sup_friends_p2','sup_friends_p3'],\n          'indicators_4': [ 'sup_parents_p1','sup_parents_p2','sup_parents_p3'],\n          'indicators_5': ['ls_p1','ls_p2', 'ls_p3'],\n          'latent': ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS'],\n          'latent1': ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']\n          }\n\nobs_idx = list(range(len(df)))\nwith pm.Model(coords=coords) as model:\n  \n  Psi = pm.InverseGamma('Psi', 5, 10, dims='indicators')\n  lambdas_ = pm.Normal('lambdas_1', 1, 10, dims=('indicators_1'))\n  lambdas_1 = pm.Deterministic('lambdas1', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_1'))\n  lambdas_ = pm.Normal('lambdas_2', 1, 10, dims=('indicators_2'))\n  lambdas_2 = pm.Deterministic('lambdas2', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_2'))\n  lambdas_ = pm.Normal('lambdas_3', 1, 10, dims=('indicators_3'))\n  lambdas_3 = pm.Deterministic('lambdas3', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_3'))\n  lambdas_ = pm.Normal('lambdas_4', 1, 10, dims=('indicators_4'))\n  lambdas_4 = pm.Deterministic('lambdas4', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_4'))\n  lambdas_ = pm.Normal('lambdas_5', 1, 10, dims=('indicators_5'))\n  lambdas_5 = pm.Deterministic('lambdas5', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_5'))\n  tau = pm.Normal('tau', 3, 10, dims='indicators')\n  kappa = 0\n  sd_dist = pm.Exponential.dist(1.0, shape=5)\n  chol, _, _ = pm.LKJCholeskyCov('chol_cov', n=5, eta=2,\n    sd_dist=sd_dist, compute_corr=True)\n  cov = pm.Deterministic(\"cov\", chol.dot(chol.T), dims=('latent', 'latent1'))\n  ksi = pm.MvNormal('ksi', kappa, chol=chol, dims=('obs', 'latent'))\n\n  m0 = tau[0] + ksi[obs_idx, 0]*lambdas_1[0]\n  m1 = tau[1] + ksi[obs_idx, 0]*lambdas_1[1]\n  m2 = tau[2] + ksi[obs_idx, 0]*lambdas_1[2]\n  m3 = tau[3] + ksi[obs_idx, 1]*lambdas_2[0]\n  m4 = tau[4] + ksi[obs_idx, 1]*lambdas_2[1]\n  m5 = tau[5] + ksi[obs_idx, 1]*lambdas_2[2]\n  m6 = tau[6] + ksi[obs_idx, 2]*lambdas_3[0]\n  m7 = tau[7] + ksi[obs_idx, 2]*lambdas_3[1]\n  m8 = tau[8] + ksi[obs_idx, 2]*lambdas_3[2]\n  m9 = tau[9] + ksi[obs_idx, 3]*lambdas_4[0]\n  m10 = tau[10] + ksi[obs_idx, 3]*lambdas_4[1]\n  m11 = tau[11] + ksi[obs_idx, 3]*lambdas_4[2]\n  m12 = tau[12] + ksi[obs_idx, 4]*lambdas_5[0]\n  m13 = tau[13] + ksi[obs_idx, 4]*lambdas_5[1]\n  m14 = tau[14] + ksi[obs_idx, 4]*lambdas_5[2]\n  \n  mu = pm.Deterministic('mu', pm.math.stack([m0, m1, m2, m3, m4, m5, m6, m7,\n                                             m8, m9, m10, m11, m12, m13, m14]).T)\n  _  = pm.Normal('likelihood', mu, Psi, observed=df[drivers].values)\n\n  idata = pm.sample(nuts_sampler='numpyro', target_accept=.95, tune=1000,\n                    idata_kwargs={\"log_likelihood\": True}, random_seed=100)\n  idata.extend(pm.sample_posterior_predictive(idata))\n  \nsummary_df1 = az.summary(idata, var_names=['lambdas1', 'lambdas2', 'lambdas3', 'lambdas4', 'lambdas5', 'tau', 'Psi'])\ncov_df = pd.DataFrame(az.extract(idata['posterior'])['cov'].mean(axis=2))\ncov_df.index = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']\ncov_df.columns = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']\n\ncorrelation_df = pd.DataFrame(az.extract(idata['posterior'])['chol_cov_corr'].mean(axis=2))\ncorrelation_df.index = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']\ncorrelation_df.columns = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']\n\nfactor_loadings = pd.DataFrame(az.summary(idata, var_names=['lambdas1', 'lambdas2', 'lambdas3', 'lambdas4', 'lambdas5'])['mean']).reset_index()\nfactor_loadings['factor'] = factor_loadings['index'].str.split('[', expand=True)[0]\nfactor_loadings.columns =['factor_loading', 'factor_loading_weight', 'factor']\nfactor_loadings['factor_loading_weight_sq'] = factor_loadings['factor_loading_weight']**2\nfactor_loadings['sum_sq_loadings'] = factor_loadings.groupby('factor')['factor_loading_weight_sq'].transform(sum)\n\nHere we’ve calculated a bunch of summary statistics which we will highlight below, but first observe the analogous model structure here to the simple CFA model above.\n Next we’ll plot the posterior summary statistics for each of the key parameters.\n\npy$summary_df1 |> kable() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n \n  \n     \n    mean \n    sd \n    hdi_3% \n    hdi_97% \n    mcse_mean \n    mcse_sd \n    ess_bulk \n    ess_tail \n    r_hat \n  \n \n\n  \n    lambdas1[se_acad_p1] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    lambdas1[se_acad_p2] \n    0.817 \n    0.052 \n    0.720 \n    0.915 \n    0.001 \n    0.001 \n    1193 \n    2008 \n    1.00 \n  \n  \n    lambdas1[se_acad_p3] \n    0.967 \n    0.060 \n    0.854 \n    1.076 \n    0.002 \n    0.001 \n    1286 \n    2037 \n    1.00 \n  \n  \n    lambdas2[se_social_p1] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    lambdas2[se_social_p2] \n    0.965 \n    0.058 \n    0.856 \n    1.071 \n    0.002 \n    0.002 \n    757 \n    1634 \n    1.00 \n  \n  \n    lambdas2[se_social_p3] \n    0.941 \n    0.072 \n    0.805 \n    1.076 \n    0.002 \n    0.002 \n    878 \n    1580 \n    1.00 \n  \n  \n    lambdas3[sup_friends_p1] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    lambdas3[sup_friends_p2] \n    0.802 \n    0.044 \n    0.720 \n    0.887 \n    0.001 \n    0.001 \n    1045 \n    1701 \n    1.00 \n  \n  \n    lambdas3[sup_friends_p3] \n    0.905 \n    0.053 \n    0.805 \n    1.006 \n    0.002 \n    0.001 \n    1235 \n    2150 \n    1.00 \n  \n  \n    lambdas4[sup_parents_p1] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    lambdas4[sup_parents_p2] \n    1.040 \n    0.059 \n    0.931 \n    1.150 \n    0.002 \n    0.002 \n    758 \n    1383 \n    1.00 \n  \n  \n    lambdas4[sup_parents_p3] \n    1.010 \n    0.064 \n    0.898 \n    1.137 \n    0.002 \n    0.001 \n    1051 \n    1840 \n    1.00 \n  \n  \n    lambdas5[ls_p1] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    lambdas5[ls_p2] \n    0.791 \n    0.085 \n    0.627 \n    0.944 \n    0.004 \n    0.003 \n    541 \n    1074 \n    1.00 \n  \n  \n    lambdas5[ls_p3] \n    0.990 \n    0.103 \n    0.806 \n    1.187 \n    0.004 \n    0.003 \n    543 \n    878 \n    1.00 \n  \n  \n    tau[se_acad_p1] \n    5.153 \n    0.044 \n    5.069 \n    5.234 \n    0.002 \n    0.001 \n    488 \n    1151 \n    1.01 \n  \n  \n    tau[se_acad_p2] \n    5.345 \n    0.039 \n    5.271 \n    5.414 \n    0.002 \n    0.001 \n    528 \n    1033 \n    1.01 \n  \n  \n    tau[se_acad_p3] \n    5.209 \n    0.045 \n    5.127 \n    5.297 \n    0.002 \n    0.001 \n    526 \n    1290 \n    1.01 \n  \n  \n    tau[se_social_p1] \n    5.286 \n    0.042 \n    5.208 \n    5.366 \n    0.002 \n    0.002 \n    380 \n    743 \n    1.01 \n  \n  \n    tau[se_social_p2] \n    5.473 \n    0.039 \n    5.397 \n    5.544 \n    0.002 \n    0.001 \n    363 \n    742 \n    1.01 \n  \n  \n    tau[se_social_p3] \n    5.437 \n    0.045 \n    5.351 \n    5.522 \n    0.002 \n    0.001 \n    492 \n    982 \n    1.00 \n  \n  \n    tau[sup_friends_p1] \n    5.782 \n    0.068 \n    5.651 \n    5.904 \n    0.004 \n    0.003 \n    333 \n    763 \n    1.01 \n  \n  \n    tau[sup_friends_p2] \n    6.007 \n    0.057 \n    5.909 \n    6.125 \n    0.003 \n    0.002 \n    397 \n    872 \n    1.00 \n  \n  \n    tau[sup_friends_p3] \n    5.987 \n    0.066 \n    5.864 \n    6.115 \n    0.003 \n    0.002 \n    385 \n    890 \n    1.01 \n  \n  \n    tau[sup_parents_p1] \n    5.973 \n    0.061 \n    5.858 \n    6.085 \n    0.003 \n    0.002 \n    427 \n    1059 \n    1.00 \n  \n  \n    tau[sup_parents_p2] \n    5.925 \n    0.062 \n    5.807 \n    6.040 \n    0.003 \n    0.002 \n    394 \n    924 \n    1.01 \n  \n  \n    tau[sup_parents_p3] \n    5.716 \n    0.066 \n    5.596 \n    5.840 \n    0.003 \n    0.002 \n    470 \n    1294 \n    1.00 \n  \n  \n    tau[ls_p1] \n    5.188 \n    0.053 \n    5.092 \n    5.289 \n    0.002 \n    0.001 \n    654 \n    1378 \n    1.00 \n  \n  \n    tau[ls_p2] \n    5.775 \n    0.041 \n    5.693 \n    5.849 \n    0.002 \n    0.001 \n    716 \n    1596 \n    1.00 \n  \n  \n    tau[ls_p3] \n    5.219 \n    0.051 \n    5.121 \n    5.314 \n    0.002 \n    0.001 \n    666 \n    1609 \n    1.00 \n  \n  \n    Psi[se_acad_p1] \n    0.412 \n    0.028 \n    0.359 \n    0.465 \n    0.001 \n    0.001 \n    1278 \n    1740 \n    1.00 \n  \n  \n    Psi[se_acad_p2] \n    0.413 \n    0.024 \n    0.367 \n    0.456 \n    0.001 \n    0.000 \n    2170 \n    2268 \n    1.00 \n  \n  \n    Psi[se_acad_p3] \n    0.468 \n    0.027 \n    0.418 \n    0.519 \n    0.001 \n    0.000 \n    1844 \n    2408 \n    1.00 \n  \n  \n    Psi[se_social_p1] \n    0.431 \n    0.026 \n    0.381 \n    0.477 \n    0.001 \n    0.000 \n    1382 \n    2219 \n    1.00 \n  \n  \n    Psi[se_social_p2] \n    0.361 \n    0.025 \n    0.314 \n    0.405 \n    0.001 \n    0.000 \n    1486 \n    2135 \n    1.00 \n  \n  \n    Psi[se_social_p3] \n    0.553 \n    0.029 \n    0.500 \n    0.606 \n    0.001 \n    0.000 \n    2594 \n    2803 \n    1.00 \n  \n  \n    Psi[sup_friends_p1] \n    0.517 \n    0.040 \n    0.439 \n    0.587 \n    0.001 \n    0.001 \n    866 \n    1739 \n    1.00 \n  \n  \n    Psi[sup_friends_p2] \n    0.508 \n    0.031 \n    0.454 \n    0.568 \n    0.001 \n    0.001 \n    1420 \n    1985 \n    1.00 \n  \n  \n    Psi[sup_friends_p3] \n    0.625 \n    0.036 \n    0.562 \n    0.694 \n    0.001 \n    0.001 \n    2090 \n    2329 \n    1.00 \n  \n  \n    Psi[sup_parents_p1] \n    0.550 \n    0.035 \n    0.485 \n    0.615 \n    0.001 \n    0.001 \n    1530 \n    2075 \n    1.00 \n  \n  \n    Psi[sup_parents_p2] \n    0.536 \n    0.038 \n    0.465 \n    0.605 \n    0.001 \n    0.001 \n    1192 \n    2078 \n    1.01 \n  \n  \n    Psi[sup_parents_p3] \n    0.675 \n    0.038 \n    0.602 \n    0.745 \n    0.001 \n    0.001 \n    2089 \n    2371 \n    1.00 \n  \n  \n    Psi[ls_p1] \n    0.671 \n    0.038 \n    0.603 \n    0.744 \n    0.001 \n    0.001 \n    1045 \n    2387 \n    1.00 \n  \n  \n    Psi[ls_p2] \n    0.534 \n    0.030 \n    0.477 \n    0.591 \n    0.001 \n    0.001 \n    1409 \n    2472 \n    1.00 \n  \n  \n    Psi[ls_p3] \n    0.622 \n    0.035 \n    0.554 \n    0.688 \n    0.001 \n    0.001 \n    1597 \n    2393 \n    1.00 \n  \n\n\n\n\n\nUsing a forest plots it’s easier to see how the uncertainty attaches to some of these estimates.\n\n\nfig, ax = plt.subplots(figsize=(15, 8))\naz.plot_forest(idata, var_names=['lambdas1', 'lambdas2', 'lambdas3', 'lambdas4', 'lambdas5'], combined=True, ax=ax);\nax.set_title(\"Factor Loadings for each of the Five Factors\");\n\n\n\n\nWe can also pull out the covariances between our latent constructs. Highlighting the key covariances relations between support and self-efficacy measures with the life-satisfaction outcome.\n\npy$cov_df |> kable(caption= \"Covariances Amongst Latent Factors\",digits=2) |> kable_styling() %>% kable_classic(full_width = F, html_font = \"Cambria\") |> row_spec(5, color = \"red\")\n\n\n\nCovariances Amongst Latent Factors\n \n  \n     \n    SE_ACAD \n    SE_SOCIAL \n    SUP_F \n    SUP_P \n    LS \n  \n \n\n  \n    SE_ACAD \n    0.47 \n    0.26 \n    0.06 \n    0.20 \n    0.22 \n  \n  \n    SE_SOCIAL \n    0.26 \n    0.39 \n    0.19 \n    0.26 \n    0.30 \n  \n  \n    SUP_F \n    0.06 \n    0.19 \n    1.03 \n    0.12 \n    0.16 \n  \n  \n    SUP_P \n    0.20 \n    0.26 \n    0.12 \n    0.86 \n    0.38 \n  \n  \n    LS \n    0.22 \n    0.30 \n    0.16 \n    0.38 \n    0.42 \n  \n\n\n\n\n\nOne benefit of fitting a mutlivariate normal relationship between these factors is that we can pull and translate the covariance relation in terms of the correlation patterns implied.\n\npy$correlation_df |> kable( caption= \"Correlations Amongst Latent Factors\", digits=2) |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\") |> row_spec(5, color = \"red\")\n\n\n\nCorrelations Amongst Latent Factors\n \n  \n     \n    SE_ACAD \n    SE_SOCIAL \n    SUP_F \n    SUP_P \n    LS \n  \n \n\n  \n    SE_ACAD \n    1.00 \n    0.60 \n    0.09 \n    0.32 \n    0.50 \n  \n  \n    SE_SOCIAL \n    0.60 \n    1.00 \n    0.29 \n    0.45 \n    0.75 \n  \n  \n    SUP_F \n    0.09 \n    0.29 \n    1.00 \n    0.12 \n    0.25 \n  \n  \n    SUP_P \n    0.32 \n    0.45 \n    0.12 \n    1.00 \n    0.64 \n  \n  \n    LS \n    0.50 \n    0.75 \n    0.25 \n    0.64 \n    1.00 \n  \n\n\n\n\n\nAnother lens on the factors is the relative factor strength which highlights where the majority of substantive variation exists in our dataset.\n\npy$factor_loadings[ c('factor', 'factor_loading', 'factor_loading_weight', 'factor_loading_weight_sq', 'sum_sq_loadings')] |> kable( caption= \"Factor Loadings and Sum of Squared loadings\", digits=2) |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\") |> column_spec(5, bold=TRUE, color = ifelse(py$factor_loadings$sum_sq_loadings > 2.8, \"red\", \"black\"))\n\n\n\nFactor Loadings and Sum of Squared loadings\n \n  \n    factor \n    factor_loading \n    factor_loading_weight \n    factor_loading_weight_sq \n    sum_sq_loadings \n  \n \n\n  \n    lambdas1 \n    lambdas1[se_acad_p1] \n    1.00 \n    1.00 \n    2.60 \n  \n  \n    lambdas1 \n    lambdas1[se_acad_p2] \n    0.82 \n    0.67 \n    2.60 \n  \n  \n    lambdas1 \n    lambdas1[se_acad_p3] \n    0.97 \n    0.94 \n    2.60 \n  \n  \n    lambdas2 \n    lambdas2[se_social_p1] \n    1.00 \n    1.00 \n    2.82 \n  \n  \n    lambdas2 \n    lambdas2[se_social_p2] \n    0.96 \n    0.93 \n    2.82 \n  \n  \n    lambdas2 \n    lambdas2[se_social_p3] \n    0.94 \n    0.89 \n    2.82 \n  \n  \n    lambdas3 \n    lambdas3[sup_friends_p1] \n    1.00 \n    1.00 \n    2.46 \n  \n  \n    lambdas3 \n    lambdas3[sup_friends_p2] \n    0.80 \n    0.64 \n    2.46 \n  \n  \n    lambdas3 \n    lambdas3[sup_friends_p3] \n    0.90 \n    0.82 \n    2.46 \n  \n  \n    lambdas4 \n    lambdas4[sup_parents_p1] \n    1.00 \n    1.00 \n    3.10 \n  \n  \n    lambdas4 \n    lambdas4[sup_parents_p2] \n    1.04 \n    1.08 \n    3.10 \n  \n  \n    lambdas4 \n    lambdas4[sup_parents_p3] \n    1.01 \n    1.02 \n    3.10 \n  \n  \n    lambdas5 \n    lambdas5[ls_p1] \n    1.00 \n    1.00 \n    2.61 \n  \n  \n    lambdas5 \n    lambdas5[ls_p2] \n    0.79 \n    0.63 \n    2.61 \n  \n  \n    lambdas5 \n    lambdas5[ls_p3] \n    0.99 \n    0.98 \n    2.61 \n  \n\n\n\n\n\nSo we can evaluate some of the similar relationships at the level of summary statistics, but more powerfully we’ve built a generative model of the process yielding our observed data and as such we can evalute the fidelity of the predictive output of our model using posterior predictive checks.\n\ndef make_ppc(idata, samples=100):\n  fig, axs = plt.subplots(5, 3, figsize=(20, 20))\n  axs = axs.flatten()\n  for i in range(15):\n    for j in range(samples):\n      temp = az.extract(idata['posterior_predictive'].sel({'likelihood_dim_3': i}))['likelihood'].values[:, j]\n      temp = pd.DataFrame(temp, columns=['likelihood'])\n      if j == 0:\n        axs[i].hist(df[drivers[i]], alpha=0.3, ec='black', bins=20, label='Observed Scores')\n        axs[i].hist(temp['likelihood'], color='purple', alpha=0.1, bins=20, label='Predicted Scores')\n      else: \n        axs[i].hist(df[drivers[i]], alpha=0.3, ec='black', bins=20)\n        axs[i].hist(temp['likelihood'], color='purple', alpha=0.1, bins=20)\n      axs[i].set_title(f\"Posterior Predictive Checks {drivers[i]}\")\n      axs[i].legend();\n  plt.show()\n  \nmake_ppc(idata)\n\n\n\n\nIn this way we can see how plausibly our model can capture the patterns in the observed data at the level of individual predictions. But additionally we can assess the posterior predictive fit at the level of summary statistics by defining the residuals on the model-fit covariance matrix based on draws from the posterior distribution.\n\n\ndef get_posterior_resids(idata, samples=100, metric='cov'):\n  resids = []\n  for i in range(100):\n    if metric == 'cov':\n      model_cov = pd.DataFrame(az.extract(idata['posterior_predictive'])['likelihood'][:, :, i]).cov()\n      obs_cov = df[drivers].cov()\n    else: \n      model_cov = pd.DataFrame(az.extract(idata['posterior_predictive'])['likelihood'][:, :, i]).corr()\n      obs_cov = df[drivers].corr()\n    model_cov.index = obs_cov.index\n    model_cov.columns = obs_cov.columns\n    residuals = model_cov - obs_cov\n    resids.append(residuals.values.flatten())\n  \n  residuals_posterior = pd.DataFrame(pd.DataFrame(resids).mean().values.reshape(15, 15))\n  residuals_posterior.index = obs_cov.index\n  residuals_posterior.columns = obs_cov.index\n  return residuals_posterior\n\nresiduals_posterior_cov = get_posterior_resids(idata, 500)\nresiduals_posterior_corr = get_posterior_resids(idata, 500, metric='corr')\n\n  \n\nThen we can plot the familiar heat-maps from above highlighting the comparable accuracy of the Bayesian model fit.\n\nplot_heatmap(py$residuals_posterior_cov, \"Residuals of the Sample Covariances and Model Implied Covariances\", \"A Visual Check of Bayesian Measurement Model fit\")\n\n\n\nplot_heatmap(py$residuals_posterior_corr, \"Residuals of the Sample Correlations and Model Implied Correlations\", \"A Visual Check of Bayesian Measurement Model fit\")\n\n\n\n\nThis process highlights one major difference in how the Bayesian CFA model is free to fit to the observed data. The parameterisation of the model is calibrated through the likelihood mechanism but the posterior samples provide important information not recoverable by optimizing fits for measurement and structural parameters. In the Bayesian setting we have access to the latent factor scores for each of the individual responses, giving us a means of assessing outliers on the latent scales. For instance\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 10))\naxs = axs.flatten()\nax = axs[0]\nax1 = axs[1]\naz.plot_forest(idata, var_names=['ksi'], combined=True, ax=ax, coords={'latent': ['SUP_P']});\naz.plot_forest(idata, var_names=['ksi'], combined=True, ax=ax1, colors=\"slateblue\", coords={'latent': ['LS']});\nax.set_yticklabels([]);\nax.set_xlabel(\"SUP_P\");\nax1.set_yticklabels([]);\nax1.set_xlabel(\"LS\");\nax.axvline(-2, color='red');\nax1.axvline(-2, color='red');\nax.set_title(\"Individual Parental Support Metric \\n On Latent Factor SUP_P\");\nax1.set_title(\"Individual Life Satisfaction Metric \\n On Latent Factor LS\");\nplt.show();\n\n\n\n\nSo while we are fitting a confirmatory factor model the edges between exploratory and confirmatory blur in this setting. Just because we have we a hypothesized factor structure we wish to evaluate does not mean the implications of such structures are crystal clear. Here we need to explore that the individually reported scores render plausible judgments about the relative position of each survey respondent on these latent scales. These are the implications of our model fit that would help us confirm the plausibility of the hypothesized factors, but additionally they offer new potential options for exploring out data and categorizing our observations. This is a common experience in contemporary Bayesian model workflow which recommends iterative model refinement and evaluation."
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#measurment-and-measurement-constructs",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#measurment-and-measurement-constructs",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Measurment and Measurement Constructs",
    "text": "Measurment and Measurement Constructs\nScience is easier when there is a recipe. When there is some procedure to adopt or routine to ape, you can out-source the responsibility for methodological justification. One egregious pattern in this vein tries to mask implausible nonsense with the mathematical gloss of “statistical significance”. Seen from 1000 feet, this is misguided but not surprising. Lip-service is paid to the idea of scientific method and we absolve ourselves of the requirement for principled justification and substantive argument.\nEvidence should be marshaled in service to argument, and it’s an absurd pretense to claim that data speaks for itself in this argument. Good and compelling argumentation is at the heart of any sound inference. It is a necessary obligation if you expect anyone to make any decision on the strength of evidence you provide. Procedures and routine tests offer only a facsimile of sound argument and p-values are a poor substitute for logical consequence and substantive derivation.\nData is found, gathered or maybe even carefully curated. In all cases there is need for a defensive posture, an argument that the data is fit-for-purpose. Nowhere is this more clear than in psychometrics where the data is often derived from a strategically constructed survey aimed at a particular target phenomena. Some intuited, but not yet measured, concept that arguably plays a role in human action, motivation or sentiment. The relative “fuzziness” of the subject matter in psychometrics has had a catalyzing effect on the methodological rigour sought in the science. Survey designs are agonised over for correct tone and rhythm of sentence structure. Measurement scales are doubly checked for reliability and correctness. Analysis steps are justified and tested under a wealth of modelling routines. Model architectures are defined and refined to better express the hypothesized structures in the data-generating process.\nWe will examine a smattering of choices available in the analysis of psychometric survey data. First stepping through these analysis routines using lmer, lavaan before switching to PyMC to demonstrate how to fit Confirmatory Factor Analysis (CFA) models and Structural Equation Models (SEM) in a Bayesian fashion using a probabilistic programming language.Throughout we’ll draw on Levy and Mislevy’s Bayesian Psychometric Modeling to highlight how good model design seeks structures which justify claim(s) of conditional independence thereby supporting the argument to compelling inference.\n\nThe Data\nThe data is borrowed from work by Boris Mayer and Andrew Ellis found here. They demonstrate CFA and SEM modelling with Lavaan. We’ll load up their data and begin some exploratory work.\n\ndf = read.csv('sem_data.csv')\ndf$ls_sum <- df$ls_p1 + df$ls_p2 + df$ls_p3\ndf$ls_mean <- rowMeans(df[ c('ls_p1', 'ls_p2', 'ls_p3')])\n\nhead(df) |> kable() |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n \n  \n    ID \n    region \n    gender \n    age \n    se_acad_p1 \n    se_acad_p2 \n    se_acad_p3 \n    se_social_p1 \n    se_social_p2 \n    se_social_p3 \n    sup_friends_p1 \n    sup_friends_p2 \n    sup_friends_p3 \n    sup_parents_p1 \n    sup_parents_p2 \n    sup_parents_p3 \n    ls_p1 \n    ls_p2 \n    ls_p3 \n    ls_sum \n    ls_mean \n  \n \n\n  \n    1 \n    west \n    female \n    13 \n    4.857143 \n    5.571429 \n    4.500000 \n    5.80 \n    5.500000 \n    5.40 \n    6.5 \n    6.5 \n    7.0 \n    7.0 \n    7.0 \n    6.0 \n    5.333333 \n    6.75 \n    5.50 \n    17.58333 \n    5.861111 \n  \n  \n    2 \n    west \n    male \n    14 \n    4.571429 \n    4.285714 \n    4.666667 \n    5.00 \n    5.500000 \n    4.80 \n    4.5 \n    4.5 \n    5.5 \n    5.0 \n    6.0 \n    4.5 \n    4.333333 \n    5.00 \n    4.50 \n    13.83333 \n    4.611111 \n  \n  \n    10 \n    west \n    female \n    14 \n    4.142857 \n    6.142857 \n    5.333333 \n    5.20 \n    4.666667 \n    6.00 \n    4.0 \n    4.5 \n    3.5 \n    7.0 \n    7.0 \n    6.5 \n    6.333333 \n    5.50 \n    4.00 \n    15.83333 \n    5.277778 \n  \n  \n    11 \n    west \n    female \n    14 \n    5.000000 \n    5.428571 \n    4.833333 \n    6.40 \n    5.833333 \n    6.40 \n    7.0 \n    7.0 \n    7.0 \n    7.0 \n    7.0 \n    7.0 \n    4.333333 \n    6.50 \n    6.25 \n    17.08333 \n    5.694444 \n  \n  \n    12 \n    west \n    female \n    14 \n    5.166667 \n    5.600000 \n    4.800000 \n    5.25 \n    5.400000 \n    5.25 \n    7.0 \n    7.0 \n    7.0 \n    6.5 \n    6.5 \n    7.0 \n    5.666667 \n    6.00 \n    5.75 \n    17.41667 \n    5.805556 \n  \n  \n    14 \n    west \n    male \n    14 \n    4.857143 \n    4.857143 \n    4.166667 \n    5.20 \n    5.000000 \n    4.20 \n    5.5 \n    6.5 \n    7.0 \n    6.5 \n    6.5 \n    6.5 \n    5.000000 \n    5.50 \n    5.50 \n    16.00000 \n    5.333333 \n  \n\n\n\n\n\nWe have survey responses from ~300 individuals who have answered questions regarding their upbringing, self-efficacy and reported life-satisfaction. The hypothetical dependency structure in this life-satisfaction data set posits a moderated relationship between scores related to life-satisfaction, parental and family support and self-efficacy. It is not a trivial task to be able to design a survey that can elicit answers plausibly mapped to each of these “factors” or themes, never mind finding a model of their relationship that can inform us as to the relative of impact of each on life-satisfaction outcomes.\n We will try to show some of the subtle aspects of these relationships as we go. The high level summary statistics show the variation across these measures. We can thematically cluster them because the source survey deliberately targeted each of the underlying themes in effort to pin down the relations between these hard to measure constructs. We are assuming that our measurement scales are well designed and that the mapping of themes is appropriate.\n\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\ndatasummary_skim(df)|> \n style_tt(\n   i = 15:17,\n   j = 1:1,\n   background = \"#20AACC\",\n   color = \"white\",\n   italic = TRUE) |> \n style_tt(\n   i = 18:19,\n   j = 1:1,\n   background = \"#2888A0\",\n   color = \"white\",\n   italic = TRUE) |> \n style_tt(\n   i = 2:14,\n   j = 1:1,\n   background = \"#17C2AD\",\n   color = \"white\",\n   italic = TRUE)\n\n\n \n\n  \n    \n    \n    tinytable_ocdtt0ymi9t18mtrz4el\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                Unique\n                Missing Pct.\n                Mean\n                SD\n                Min\n                Median\n                Max\n                Histogram\n              \n        \n        \n        \n                \n                  ID\n                  283\n                  0\n                  187.9\n                  106.3\n                  1.0\n                  201.0\n                  367.0\n                  \n                \n                \n                  age\n                  5\n                  0\n                  14.7\n                  0.8\n                  13.0\n                  15.0\n                  17.0\n                  \n                \n                \n                  se_acad_p1\n                  32\n                  0\n                  5.2\n                  0.8\n                  3.1\n                  5.1\n                  7.0\n                  \n                \n                \n                  se_acad_p2\n                  36\n                  0\n                  5.3\n                  0.7\n                  3.1\n                  5.4\n                  7.0\n                  \n                \n                \n                  se_acad_p3\n                  29\n                  0\n                  5.2\n                  0.8\n                  2.8\n                  5.2\n                  7.0\n                  \n                \n                \n                  se_social_p1\n                  24\n                  0\n                  5.3\n                  0.8\n                  1.8\n                  5.4\n                  7.0\n                  \n                \n                \n                  se_social_p2\n                  27\n                  0\n                  5.5\n                  0.7\n                  2.7\n                  5.5\n                  7.0\n                  \n                \n                \n                  se_social_p3\n                  31\n                  0\n                  5.4\n                  0.8\n                  3.0\n                  5.5\n                  7.0\n                  \n                \n                \n                  sup_friends_p1\n                  13\n                  0\n                  5.8\n                  1.1\n                  1.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_friends_p2\n                  10\n                  0\n                  6.0\n                  0.9\n                  2.5\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_friends_p3\n                  13\n                  0\n                  6.0\n                  1.1\n                  1.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_parents_p1\n                  11\n                  0\n                  6.0\n                  1.1\n                  1.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_parents_p2\n                  11\n                  0\n                  5.9\n                  1.1\n                  2.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_parents_p3\n                  13\n                  0\n                  5.7\n                  1.1\n                  1.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  ls_p1\n                  15\n                  0\n                  5.2\n                  0.9\n                  2.0\n                  5.3\n                  7.0\n                  \n                \n                \n                  ls_p2\n                  21\n                  0\n                  5.8\n                  0.7\n                  2.5\n                  5.8\n                  7.0\n                  \n                \n                \n                  ls_p3\n                  22\n                  0\n                  5.2\n                  0.9\n                  2.0\n                  5.2\n                  7.0\n                  \n                \n                \n                  ls_sum\n                  98\n                  0\n                  16.2\n                  2.1\n                  8.7\n                  16.4\n                  20.8\n                  \n                \n                \n                  ls_mean\n                  97\n                  0\n                  5.4\n                  0.7\n                  2.9\n                  5.5\n                  6.9\n                  \n                \n                \n                   \n                    \n                  N\n                  %\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  region\n                  east\n                  142\n                  50.2\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  \n                  west\n                  141\n                  49.8\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  gender\n                  female\n                  132\n                  46.6\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  \n                  male\n                  151\n                  53.4\n                  \n                  \n                  \n                  \n                  \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nNote how we’ve distinguished among the metrics for the “outcome” metrics and the “driver” metrics. Such a distinction may seem trivial, but it is only possible because we bring substantive knowledge to bear on the problem in the design of our survey and the postulation of the theoretical construct. Our data is a multivariate outcome vector. There are a large range of possible interacting effects and the patterns of realization for our life-satisfaction scores. The true “drivers” of satisfaction may be quite different than the hypothesised structure. It is this open question that we’re aiming to uncover in the analysis of our survey data.\n\n\nSample Covariances and Correlations\nLet’s plot the relations amongst our various indicator scores.\n\ndrivers = c('se_acad_p1', 'se_acad_p2', 'se_acad_p3', 'se_social_p1', 'se_social_p2', 'se_social_p3', 'sup_friends_p1','sup_friends_p2', 'sup_friends_p3', 'sup_parents_p1' , 'sup_parents_p2' , 'sup_parents_p3', 'ls_p1', 'ls_p2', 'ls_p3')\n\n\n\nplot_heatmap <- function(df, title=\"Sample Covariances\", subtitle=\"Observed Measures\") {\n  heat_df = df |> as.matrix() |> melt()\n  colnames(heat_df) <- c(\"x\", \"y\", \"value\")\n  g <- heat_df |> mutate(value = round(value, 2)) |>  ggplot(aes(x = x, y = y, fill = value)) +\n    geom_tile() + geom_text(aes(label = value), color = \"black\", size = 4) +\n   scale_fill_gradient2(\n      high = 'dodgerblue4',\n      mid = 'white',\n      low = 'firebrick2'\n    ) + theme(axis.text.x = element_text(angle=45)) + ggtitle(title, subtitle)\n  \n  g\n}\n\ng1 = plot_heatmap(cov(df[,  drivers]))\n\ng2 = plot_heatmap(cor(df[,  drivers]), \"Sample Correlations\")\n\nplot <- ggarrange(g1,g2, ncol=1, nrow=2);\n\n\n\n\nNote the relatively strong correlations between measures of parental support and the life-satisfaction outcome ls_p3. Similarly, the social self-efficacy scores seem well correlated with the secondary life satisfaction indicator ls_p2. These observed correlations merit some further investigation. We can also plot the pairs of scatter plots to “dig deeper”. What kind of correlation holds between these scores? Are any driven by extreme outliers? This is what we’re looking for in a pair plot. Should we cull the outliers? Leave them in? We leave them in.\n\ndf <- df |> \n  mutate(id = row_number()) \n\n# Prepare data to be plotted on the x axis\nx_vars <- pivot_longer(data = df,\n             cols = se_acad_p1:ls_p3,\n             names_to = \"variable_x\",\n             values_to = \"x\")\n\n# Prepare data to be plotted on the y axis  \ny_vars <- pivot_longer(data = df,\n                       cols = se_acad_p1:ls_p3,\n                       names_to = \"variable_y\",\n                       values_to = \"y\") \n\n# Join data for x and y axes and make plot\nfull_join(x_vars, y_vars, \n          by = c(\"id\"),\n          relationship = \"many-to-many\") |>\n  ggplot() + \n  aes(x = x, y = y) +\n  geom_point() + geom_smooth(method='lm') +\n  facet_grid(c(\"variable_x\", \"variable_y\"))  + ggtitle(\"Pair Plot of Indicator Metrics\", \n                                                       \"Comparing Against Life Satisfaction Scores\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe scatter plots among the highly correlated variables in the heatmap do seem to exhibit some kind of linear relationship with aspects of the life-satisfaction scores. We now turn to modelling these relationships to tease out some kind of inferential summary of those relationships."
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#structural-equation-modelling-in-pymc",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#structural-equation-modelling-in-pymc",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Structural Equation Modelling in PyMC",
    "text": "Structural Equation Modelling in PyMC\nIn the context of structural equation modeling our hypothetical factor structure and dependency relations are even more specific than the simple “measurement model” we’ve just seen. A structural equation model incorporates a measurement model and adds regression components between the latent and manifest variables as is required to evaluate candidate hypotheses regarding the structure of the data generating process. The range of possibilities we can encode here is vast, but candidate model architectures are constrained by our theory of the case and the demands of explanatory coherence.\n\nSEM and Indirect Effects\nWith the life-satisfaction data-set we’ve been been looking at we want to evaluate the indirect and direct effects of parental and peer support on life-satisfaction outcomes. To do so we need to add a dependency structure to our data generating process and fit the regressions which would inform us as to relative importance of these types of support as mediated through our self-efficacy scores.\n\ndef make_indirect_sem(priors): \n\n  coords = {'obs': list(range(len(df))), \n            'indicators': drivers,\n            'indicators_1': ['se_acad_p1','se_acad_p2','se_acad_p3'],\n            'indicators_2': ['se_social_p1','se_social_p2','se_social_p3'],\n            'indicators_3': ['sup_friends_p1','sup_friends_p2','sup_friends_p3'],\n            'indicators_4': [ 'sup_parents_p1','sup_parents_p2','sup_parents_p3'],\n            'indicators_5': ['ls_p1','ls_p2', 'ls_p3'],\n            'latent': ['SUP_F', 'SUP_P'], \n            'latent1': ['SUP_F', 'SUP_P'], \n            'latent_regression': ['SUP_F->SE_ACAD', 'SUP_P->SE_ACAD', 'SUP_F->SE_SOC', 'SUP_P->SE_SOC'],\n            'regression': ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P']\n            }\n\n  obs_idx = list(range(len(df)))\n  with pm.Model(coords=coords) as model:\n    \n    Psi = pm.InverseGamma('Psi', 5, 10, dims='indicators')\n    lambdas_ = pm.Normal('lambdas_1',  priors['lambda'][0], priors['lambda'][1], dims=('indicators_1'))\n    lambdas_1 = pm.Deterministic('lambdas1', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_1'))\n    lambdas_ = pm.Normal('lambdas_2', priors['lambda'][0], priors['lambda'][1], dims=('indicators_2'))\n    lambdas_2 = pm.Deterministic('lambdas2', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_2'))\n    lambdas_ = pm.Normal('lambdas_3', priors['lambda'][0], priors['lambda'][1], dims=('indicators_3'))\n    lambdas_3 = pm.Deterministic('lambdas3', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_3'))\n    lambdas_ = pm.Normal('lambdas_4', priors['lambda'][0], priors['lambda'][1], dims=('indicators_4'))\n    lambdas_4 = pm.Deterministic('lambdas4', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_4'))\n    lambdas_ = pm.Normal('lambdas_5', priors['lambda'][0], priors['lambda'][1], dims=('indicators_5'))\n    lambdas_5 = pm.Deterministic('lambdas5', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_5'))\n    tau = pm.Normal('tau', 3, 10, dims='indicators')\n    kappa = 0\n    sd_dist = pm.Exponential.dist(1.0, shape=2)\n    chol, _, _ = pm.LKJCholeskyCov('chol_cov', n=2, eta=priors['eta'],\n      sd_dist=sd_dist, compute_corr=True)\n    cov = pm.Deterministic(\"cov\", chol.dot(chol.T), dims=('latent', 'latent1'))\n    ksi = pm.MvNormal('ksi', kappa, chol=chol, dims=('obs', 'latent'))\n\n    # Regression Components\n    beta_r = pm.Normal('beta_r', 0, 0.5, dims='latent_regression')\n    beta_r2 = pm.Normal('beta_r2', 0, 1, dims='regression')\n    resid_chol, _, _ = pm.LKJCholeskyCov('resid_chol', n=2, eta=priors['eta'],\n      sd_dist=sd_dist, compute_corr=True)\n    _ = pm.Deterministic(\"resid_cov\", chol.dot(chol.T))\n    sigmas_resid = pm.MvNormal('sigmas_resid', kappa, chol=resid_chol)\n\n    # SE_ACAD ~ SUP_FRIENDS + SUP_PARENTS \n    regression_se_acad = pm.Normal('regr_se_acad', beta_r[0]*ksi[obs_idx, 0] + beta_r[1]*ksi[obs_idx, 1], sigmas_resid[0])\n    # SE_SOCIAL ~ SUP_FRIENDS + SUP_PARENTS \n    \n    regression_se_social = pm.Normal('regr_se_social', beta_r[2]*ksi[obs_idx, 0] + beta_r[3]*ksi[obs_idx, 1], sigmas_resid[1])\n\n    # LS ~ SE_ACAD + SE_SOCIAL + SUP_FRIEND + SUP_PARENTS\n    regression = pm.Normal('regr', beta_r2[0]*regression_se_acad + beta_r2[1]*regression_se_social +\n                                   beta_r2[2]*ksi[obs_idx, 0] + beta_r2[3]*ksi[obs_idx, 1], 1)\n\n    m0 = tau[0] + regression_se_acad*lambdas_1[0]\n    m1 = tau[1] + regression_se_acad*lambdas_1[1]\n    m2 = tau[2] + regression_se_acad*lambdas_1[2]\n    m3 = tau[3] + regression_se_social*lambdas_2[0]\n    m4 = tau[4] + regression_se_social*lambdas_2[1]\n    m5 = tau[5] + regression_se_social*lambdas_2[2]\n    m6 = tau[6] + ksi[obs_idx, 0]*lambdas_3[0]\n    m7 = tau[7] + ksi[obs_idx, 0]*lambdas_3[1]\n    m8 = tau[8] + ksi[obs_idx, 0]*lambdas_3[2]\n    m9 = tau[9] + ksi[obs_idx, 1]*lambdas_4[0]\n    m10 = tau[10] + ksi[obs_idx, 1]*lambdas_4[1]\n    m11 = tau[11] + ksi[obs_idx, 1]*lambdas_4[2]\n    m12 = tau[12] + regression*lambdas_5[0]\n    m13 = tau[13] + regression*lambdas_5[1]\n    m14 = tau[14] + regression*lambdas_5[2]\n    \n    mu = pm.Deterministic('mu', pm.math.stack([m0, m1, m2, m3, m4, m5, m6, m7,\n                                              m8, m9, m10, m11, m12, m13, m14]).T)\n    _  = pm.Normal('likelihood', mu, Psi, observed=df[drivers].values)\n\n    idata = pm.sample(10_000, chains=4, nuts_sampler='numpyro', target_accept=.99, tune=2000,\n                      idata_kwargs={\"log_likelihood\": True}, random_seed=110)\n    idata.extend(pm.sample_posterior_predictive(idata))\n\n    return model, idata\n\n\nmodel2, idata2 = make_indirect_sem(priors={'eta': 2, 'lambda': [1, 1]})\n\nThere is quite a bit of extra structure now in our model. This structure articulates the path-dependence of the self-efficacy constructs on the support constructs. We’ve specified these as separate regressions with coefficient values for each of the support constructs, but additionally we need to ensure there is a correlation between realized self-efficacy predictions so we’ve wrapped the regression equations in two normal distributions with correlated sigma or residual terms. In this manner we preserve the multivariate relationships and allow the model to place weight on this prior if there is a strong co-determination effect. It’s perhaps easier to see in a picture than think through the code directly:\n The main point to emphasize here is the expressive nature of the modelling paradigm. Our ability to construct and estimate different chains of dependence allows us to test evaluate the direct and indirect effects due to the different “drivers” of life-satisfaction.\nAs before we can still recover posterior predictive check to evaluate granular level perfromance against the observed data points.\n\nmake_ppc(idata2)\n\n\n\n\nBut additionally we can pull out the regression summary coefficients and report on the direct and indirect effects of our constructs\n\n\nsummary_df = az.summary(idata2, var_names=['beta_r', 'beta_r2'])\n\ndef calculate_effects(summary_df, var='SUP_P'):\n    #Indirect Paths\n    ## VAR -> SE_SOC ->LS\n    indirect_parent_soc = summary_df.loc[f'beta_r[{var}->SE_SOC]']['mean']*summary_df.loc['beta_r2[SE_SOCIAL]']['mean']\n\n    ## VAR -> SE_SOC ->LS\n    indirect_parent_acad = summary_df.loc[f'beta_r[{var}->SE_ACAD]']['mean']*summary_df.loc['beta_r2[SE_ACAD]']['mean']\n\n    ## Total Indirect Effects\n    total_indirect = indirect_parent_soc + indirect_parent_acad\n\n    ## Total Effects\n    total_effect = total_indirect + summary_df.loc[f'beta_r2[{var}]']['mean']\n\n    return pd.DataFrame([[indirect_parent_soc, indirect_parent_acad, total_indirect, total_effect]], \n                columns=[f'{var} -> SE_SOC ->LS', f'{var} -> SE_ACAD ->LS', f'Total Indirect Effects {var}', f'Total Effects {var}']\n                )\n\nindirect_p = calculate_effects(summary_df, 'SUP_P')\nindirect_f = calculate_effects(summary_df, 'SUP_F')\n\nresiduals_posterior_cov = get_posterior_resids(idata2, 500)\nresiduals_posterior_corr = get_posterior_resids(idata2, 500, 'corr')\n\nThe regression effects seem well sampled and show the strength of relationships between social self-efficacy scores and reported life-satisfaction\n\npy$summary_df |> kable( caption= \"Regression Coefficients Amongst Latent Factors\", digits=2) |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\nRegression Coefficients Amongst Latent Factors\n \n  \n     \n    mean \n    sd \n    hdi_3% \n    hdi_97% \n    mcse_mean \n    mcse_sd \n    ess_bulk \n    ess_tail \n    r_hat \n  \n \n\n  \n    beta_r[SUP_F->SE_ACAD] \n    0.05 \n    0.04 \n    -0.04 \n    0.13 \n    0 \n    0 \n    35872 \n    29560 \n    1 \n  \n  \n    beta_r[SUP_P->SE_ACAD] \n    0.26 \n    0.05 \n    0.17 \n    0.36 \n    0 \n    0 \n    22718 \n    26763 \n    1 \n  \n  \n    beta_r[SUP_F->SE_SOC] \n    0.16 \n    0.04 \n    0.08 \n    0.22 \n    0 \n    0 \n    27895 \n    29498 \n    1 \n  \n  \n    beta_r[SUP_P->SE_SOC] \n    0.31 \n    0.04 \n    0.23 \n    0.40 \n    0 \n    0 \n    13525 \n    23161 \n    1 \n  \n  \n    beta_r2[SE_ACAD] \n    0.17 \n    0.12 \n    -0.06 \n    0.38 \n    0 \n    0 \n    28858 \n    29141 \n    1 \n  \n  \n    beta_r2[SE_SOCIAL] \n    0.58 \n    0.15 \n    0.30 \n    0.85 \n    0 \n    0 \n    23839 \n    28409 \n    1 \n  \n  \n    beta_r2[SUP_F] \n    0.02 \n    0.07 \n    -0.12 \n    0.15 \n    0 \n    0 \n    32759 \n    30795 \n    1 \n  \n  \n    beta_r2[SUP_P] \n    0.27 \n    0.09 \n    0.11 \n    0.44 \n    0 \n    0 \n    25497 \n    30201 \n    1 \n  \n\n\n\n\n\nAdditionally we can pull out estimates of the indirect and total effects of parental and peer support on the outcome of life-satisfaction using the multiplicative path-tracing rules here.\n\npy$indirect_p |> kable( caption= \"Total and Indirect Effects: Parental Support\", digits=2) |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\nTotal and Indirect Effects: Parental Support\n \n  \n    SUP_P -> SE_SOC ->LS \n    SUP_P -> SE_ACAD ->LS \n    Total Indirect Effects SUP_P \n    Total Effects SUP_P \n  \n \n\n  \n    0.18 \n    0.04 \n    0.22 \n    0.5 \n  \n\n\n\n\npy$indirect_f |> kable( caption= \"Total and Indirect Effects: Friend Support\", digits=4) |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\nTotal and Indirect Effects: Friend Support\n \n  \n    SUP_F -> SE_SOC ->LS \n    SUP_F -> SE_ACAD ->LS \n    Total Indirect Effects SUP_F \n    Total Effects SUP_F \n  \n \n\n  \n    0.0896 \n    0.0076 \n    0.0972 \n    0.1152 \n  \n\n\n\n\n\nNote that we’ve defined these effects as point estimates on the mean realizations of the regression coefficients. But we could have defined these quantities in the model context and sampled the posterior distribution too. However, the model was already complex enough that we kept it simpler for the clarity of exposition.\nFinally we can plot the residual variance-covariance for the Bayesian estimate of Structural equation model we hypothesized.\n\nplot_heatmap(py$residuals_posterior_cov, \"Residuals of the Sample Covariances and Model Implied Covariances\", \"A Visual Check of Bayesian Structural Model fit\")\n\n\n\n\nThe overall impression seems quite reasonable, but notably distinct from the fit of the simpler measurement model. In this way the flexibility of the Bayesian estimation strategy is not as constrained as the MLE variance-covariance estimation strategy and we can learn of meaningful interactions in a richer more theoretically compelling model while still calibrating against the observed data. This is borne out global model comparisons too.\n\ncompare_df = az.compare({'Measurement Model': idata, 'Structural Model': idata2});\naz.plot_compare(compare_df);\nplt.show()\n\n\n\n\nThe structural model is marginally worse by measures of global fit to the data, but this summary skips over the variety of questions we can answer with this new structure. The broad fidelity to observed data seen in the posterior predictive checks and the variance covariance residuals suggests that we ought to be willing to take this hit to global performance if we can answer questions left unanswered by the measurement model."
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#exploratory-and-confirmatory-modes",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#exploratory-and-confirmatory-modes",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Exploratory and Confirmatory Modes",
    "text": "Exploratory and Confirmatory Modes\nOne of the things that psychometrics has pioneered well is the distinction between an exploratory and confirmatory models. This distinction, when made explicit, partially guards against the abuse of inferential integrity we see in more common work-flows. But additionally, models are often opaque - you may (as above) have improved some measure of model fit, changed the parameter weighting accorded to an observed feature, but what does that mean? Exploration of model architectures, design choices and feature creation is just how we come to understand the meaning of a model specification. Even in the simple case of regression we’ve seen how adding an interaction term changes the interpretability of a model. How then are we to stand behind uncertainty estimates accorded to parameter weights when we barely intuit the implications of a model design?\n\nMarginal Effects\nThe answer is to not to rely on intuition, but push forward and test the tangible implications of a fitted model. A model is hypothesis which should be applied to stringent test. We should subject the logical consequences of the design to appropriate scrutiny. We understand the implications and relative plausibility of any model in terms of the predicted outcomes more easily than we understand the subtle interaction effects expressed parameter movements. As such we should adopt this view in our evaluation of a model fit too.\nConsider how we do this using Vincent Arel-Bundock’s wonderful marginaleffects package passing a grid of new values through to our fitted model.These implications are a test of model plausibility too.\n\npred <- predictions(mod_sum_parcel, newdata = datagrid(sup_parents_mean = 1:10, se_social_mean = 1:10 ))\npred1 <- predictions(mod_sum_inter_parcel, newdata = datagrid(sup_parents_mean = 1:10, se_social_mean = 1:10))\n\npred1  |> tail(10) |> kableExtra::kable() |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n \n  \n     \n    rowid \n    estimate \n    std.error \n    statistic \n    p.value \n    s.value \n    conf.low \n    conf.high \n    se_acad_mean \n    sup_friends_mean \n    sup_parents_mean \n    se_social_mean \n    ls_sum \n  \n \n\n  \n    91 \n    91 \n    19.27615 \n    2.2881204 \n    8.424449 \n    0 \n    54.61499 \n    14.79152 \n    23.76079 \n    5.237686 \n    5.929329 \n    10 \n    1 \n    17.58333 \n  \n  \n    92 \n    92 \n    19.21698 \n    1.7840261 \n    10.771691 \n    0 \n    87.46457 \n    15.72035 \n    22.71361 \n    5.237686 \n    5.929329 \n    10 \n    2 \n    17.58333 \n  \n  \n    93 \n    93 \n    19.15780 \n    1.2888397 \n    14.864380 \n    0 \n    163.60757 \n    16.63172 \n    21.68388 \n    5.237686 \n    5.929329 \n    10 \n    3 \n    17.58333 \n  \n  \n    94 \n    94 \n    19.09863 \n    0.8188846 \n    23.322730 \n    0 \n    397.24885 \n    17.49364 \n    20.70361 \n    5.237686 \n    5.929329 \n    10 \n    4 \n    17.58333 \n  \n  \n    95 \n    95 \n    19.03945 \n    0.4595015 \n    41.435010 \n    0 \n    Inf \n    18.13884 \n    19.94006 \n    5.237686 \n    5.929329 \n    10 \n    5 \n    17.58333 \n  \n  \n    96 \n    96 \n    18.98027 \n    0.5318049 \n    35.690291 \n    0 \n    924.33454 \n    17.93795 \n    20.02259 \n    5.237686 \n    5.929329 \n    10 \n    6 \n    17.58333 \n  \n  \n    97 \n    97 \n    18.92110 \n    0.9410614 \n    20.106123 \n    0 \n    296.26806 \n    17.07665 \n    20.76554 \n    5.237686 \n    5.929329 \n    10 \n    7 \n    17.58333 \n  \n  \n    98 \n    98 \n    18.86192 \n    1.4210849 \n    13.272902 \n    0 \n    131.14398 \n    16.07665 \n    21.64720 \n    5.237686 \n    5.929329 \n    10 \n    8 \n    17.58333 \n  \n  \n    99 \n    99 \n    18.80274 \n    1.9194980 \n    9.795657 \n    0 \n    72.84938 \n    15.04060 \n    22.56489 \n    5.237686 \n    5.929329 \n    10 \n    9 \n    17.58333 \n  \n  \n    100 \n    100 \n    18.74357 \n    2.4249884 \n    7.729343 \n    0 \n    46.39459 \n    13.99068 \n    23.49646 \n    5.237686 \n    5.929329 \n    10 \n    10 \n    17.58333 \n  \n\n\n\n\n\nWe will see here how the impact of changes in se_social_mean is much reduced when sup_parent_mean is held fixed at high values (9, 10) when our model allows for an interaction effect.\n\npred |> head(10) |> kableExtra::kable() |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n \n  \n    rowid \n    estimate \n    std.error \n    statistic \n    p.value \n    s.value \n    conf.low \n    conf.high \n    se_acad_mean \n    sup_friends_mean \n    sup_parents_mean \n    se_social_mean \n    ls_sum \n  \n \n\n  \n    1 \n    7.065559 \n    0.7860786 \n    8.988362 \n    0 \n    61.78928 \n    5.524873 \n    8.606245 \n    5.237686 \n    5.929329 \n    1 \n    1 \n    17.58333 \n  \n  \n    2 \n    8.334837 \n    0.6546017 \n    12.732685 \n    0 \n    120.95075 \n    7.051842 \n    9.617833 \n    5.237686 \n    5.929329 \n    1 \n    2 \n    17.58333 \n  \n  \n    3 \n    9.604115 \n    0.5480173 \n    17.525206 \n    0 \n    226.01129 \n    8.530021 \n    10.678209 \n    5.237686 \n    5.929329 \n    1 \n    3 \n    17.58333 \n  \n  \n    4 \n    10.873394 \n    0.4830922 \n    22.507905 \n    0 \n    370.25976 \n    9.926550 \n    11.820237 \n    5.237686 \n    5.929329 \n    1 \n    4 \n    17.58333 \n  \n  \n    5 \n    12.142672 \n    0.4771468 \n    25.448505 \n    0 \n    472.16118 \n    11.207481 \n    13.077862 \n    5.237686 \n    5.929329 \n    1 \n    5 \n    17.58333 \n  \n  \n    6 \n    13.411950 \n    0.5321613 \n    25.202790 \n    0 \n    463.16951 \n    12.368933 \n    14.454967 \n    5.237686 \n    5.929329 \n    1 \n    6 \n    17.58333 \n  \n  \n    7 \n    14.681228 \n    0.6324223 \n    23.214278 \n    0 \n    393.60150 \n    13.441703 \n    15.920753 \n    5.237686 \n    5.929329 \n    1 \n    7 \n    17.58333 \n  \n  \n    8 \n    15.950506 \n    0.7602342 \n    20.981043 \n    0 \n    322.26019 \n    14.460474 \n    17.440538 \n    5.237686 \n    5.929329 \n    1 \n    8 \n    17.58333 \n  \n  \n    9 \n    17.219784 \n    0.9039855 \n    19.048739 \n    0 \n    266.32548 \n    15.448005 \n    18.991563 \n    5.237686 \n    5.929329 \n    1 \n    9 \n    17.58333 \n  \n  \n    10 \n    18.489062 \n    1.0571941 \n    17.488806 \n    0 \n    225.08895 \n    16.417000 \n    20.561124 \n    5.237686 \n    5.929329 \n    1 \n    10 \n    17.58333 \n  \n\n\n\n\n\nThis modifying effect is not as evident in the simpler model.\n\nRegression Marginal Effects\nWe can see this more clearly with a plot of the marginal effects.\n\ng = plot_predictions(mod_sum_parcel, condition = c(\"se_social_mean\", \"sup_parents_mean\"), type = \"response\") + ggtitle(\"Counterfactual Shift of Outcome: se_social_mean\", \"Holding all else Fixed: Simple Model\")\n\ng1 = plot_predictions(mod_sum_inter_parcel, condition = c(\"se_social_mean\", \"sup_parents_mean\"), type = \"response\") + ggtitle(\"Counterfactual Shift of Outcome: se_social_mean\", \"Holding all else Fixed Interaction Model\")\n\nplot <- ggarrange(g,g1, ncol=1, nrow=2);\n\n\n\n\nHere we’ve pulled out some of the implications in terms of the outcome predictions and see how the interaction effect modifies the additive effect downwards at the upper end of the se_social_mean scale."
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#models-with-controls",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#models-with-controls",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Models with Controls",
    "text": "Models with Controls\nThe garden of forking paths presents itself within any set of covariates. How do we represent their effects? Which interactions are meaningful? How do we argue for one model design over another? The questionable paths are multiplied when we begin to consider additional covariates and group effects. But also additional covariates help structure our expectations too. Yes, you can cut and chop your way to through the garden to find some spurious correlation but more plausibly you can bring in structurally important variables which helpfully moderate the outcomes based on our understanding of the data generating process.\nLet’s assess the question but this time we allow the model to account for differences in region.\n\nformula_no_grp_sum = \"ls_sum ~ sup_parents_p1 + sup_parents_p2 + sup_parents_p3 + sup_friends_p1 + sup_friends_p2 + sup_friends_p3 + se_acad_p1 + se_acad_p2 + se_acad_p3 +\nse_social_p1 + se_social_p2 + se_social_p3\"\n\nformula_grp_sum = \"ls_sum ~ sup_parents_p1 + sup_parents_p2 + sup_parents_p3 + sup_friends_p1 + sup_friends_p2 + sup_friends_p3 + se_acad_p1 + se_acad_p2 + se_acad_p3 +\nse_social_p1 + se_social_p2 + se_social_p3 + factor(region)\"\n\nno_grp_sum_fit <- lm(formula_no_grp_sum , data = df)\ngrp_sum_fit <- lm(formula_grp_sum, data = df)\n\ng1 = modelplot(no_grp_sum_fit, re.form=NA) +  aes(color = ifelse(p.value < 0.05, \"Significant at 0.05\", \"Not significant at 0.001\")) +\n  scale_color_manual(values = c(\"grey\", \"blue\"), guide='none') + ggtitle(\"Significance of Coefficient Values\", \"No Group Effects Model\")\n\ng2 = modelplot(grp_sum_fit, re.form=NA) +  aes(color = ifelse(p.value < 0.05, \"Significant at 0.05\", \"Not significant 0.05\")) +\n  scale_color_manual(values = c(\"grey\", \"blue\")) + ggtitle(\"Significance of Coefficient Values\", \"Group Effects Model\")\n\n\n\nplot <- ggarrange(g1,g2, ncol=2, nrow=1);\n\n\n\n\nWe can see here how the additional factor variable is reported to be significant conditional on a model specification. However the intercept is no longer well identified.\n\nmodelsummary(list(\"No Group Effects Fit\"= no_grp_sum_fit,\n                  \"Group Effects Fit\"= grp_sum_fit), \n             stars = TRUE) |> \n style_tt(\n   i = 2:25,\n   j = 1:1,\n   background = \"#17C2AD\",\n   color = \"white\",\n   italic = TRUE)\n\n\n \n\n  \n    \n    \n    tinytable_6dqjw6ypw5guaj07q8f6\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                No Group Effects Fit\n                Group Effects Fit\n              \n        \n        + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n        \n                \n                  (Intercept)       \n                  2.094*  \n                  1.979+  \n                \n                \n                                    \n                  (0.954) \n                  (1.041) \n                \n                \n                  sup_parents_p1    \n                  0.072   \n                  0.066   \n                \n                \n                                    \n                  (0.143) \n                  (0.145) \n                \n                \n                  sup_parents_p2    \n                  0.118   \n                  0.122   \n                \n                \n                                    \n                  (0.144) \n                  (0.145) \n                \n                \n                  sup_parents_p3    \n                  0.526***\n                  0.529***\n                \n                \n                                    \n                  (0.126) \n                  (0.126) \n                \n                \n                  sup_friends_p1    \n                  -0.272+ \n                  -0.272+ \n                \n                \n                                    \n                  (0.150) \n                  (0.150) \n                \n                \n                  sup_friends_p2    \n                  0.331*  \n                  0.332*  \n                \n                \n                                    \n                  (0.160) \n                  (0.160) \n                \n                \n                  sup_friends_p3    \n                  0.165   \n                  0.166   \n                \n                \n                                    \n                  (0.130) \n                  (0.131) \n                \n                \n                  se_acad_p1        \n                  -0.208  \n                  -0.212  \n                \n                \n                                    \n                  (0.192) \n                  (0.193) \n                \n                \n                  se_acad_p2        \n                  0.327   \n                  0.328   \n                \n                \n                                    \n                  (0.202) \n                  (0.202) \n                \n                \n                  se_acad_p3        \n                  0.153   \n                  0.169   \n                \n                \n                                    \n                  (0.174) \n                  (0.184) \n                \n                \n                  se_social_p1      \n                  0.355+  \n                  0.345+  \n                \n                \n                                    \n                  (0.200) \n                  (0.204) \n                \n                \n                  se_social_p2      \n                  0.509*  \n                  0.517*  \n                \n                \n                                    \n                  (0.219) \n                  (0.221) \n                \n                \n                  se_social_p3      \n                  0.443** \n                  0.446** \n                \n                \n                                    \n                  (0.161) \n                  (0.161) \n                \n                \n                  factor(region)west\n                          \n                  0.056   \n                \n                \n                                    \n                          \n                  (0.202) \n                \n                \n                  Num.Obs.          \n                  283     \n                  283     \n                \n                \n                  R2                \n                  0.517   \n                  0.517   \n                \n                \n                  R2 Adj.           \n                  0.496   \n                  0.494   \n                \n                \n                  AIC               \n                  1044.7  \n                  1046.6  \n                \n                \n                  BIC               \n                  1095.7  \n                  1101.3  \n                \n                \n                  Log.Lik.          \n                  -508.341\n                  -508.300\n                \n                \n                  RMSE              \n                  1.46    \n                  1.46    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nAgain the cleanest way to interpret the implications of these specifications is derive the conditional marginal effects and assess these for plausibility.\n\nGroup Marginal Effects\nWe see here the different levels expected for different regional responses for changes in each of these input variables.\n\ng = plot_predictions(grp_sum_fit, condition = c(\"sup_parents_p3\", \"region\"), type = \"response\") + ggtitle(\"Counterfactual Shift of Outcome: sup_parents_p3\", \"Holding all else Fixed\")\n\ng1 = plot_predictions(grp_sum_fit, condition = c(\"sup_friends_p1\", \"region\"), type = \"response\") + ggtitle(\"Counterfactual Shift of Outcome: sup_friends_p1\", \"Holding all else Fixed\")\n\ng2 = plot_predictions(grp_sum_fit, condition = c(\"se_acad_p1\", \"region\"), \n                      type = \"response\") + ggtitle(\"Counterfactual Shift of Outcome: se_acad_p1\", \"Holding all else Fixed\")\n\nplot <- ggarrange(g,g1,g2, ncol=1, nrow=3);\n\n\n\n\nIn this case the differences are slight, but the point here is just to consider how aspects of the data generating process can be modified by broadly demographic features systematically. In an ideal circumstance a probability-sample of survey respondents should mitigate the need for additionally controls variables in so far as a probability sample should ensure exchangeability"
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#model-design-and-conditional-exchangeability",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#model-design-and-conditional-exchangeability",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Model Design and Conditional Exchangeability",
    "text": "Model Design and Conditional Exchangeability\nModelling is nearly always about contrasts and questions of meaningful differences. What we seek to do when we build a model is to find a structure that enables “fair” or “justified” inferences about these contrasts. This is maybe most palpably brought home when we consider cases of causal inference. Here we want to define a meaningful causal estimand - some contrast between treatment group and control group where we can be confident that the two groups are suitably “representative” so that the observed differences represents the effect of the treatment. We are saying that conditional on our model design we consider the potential outcomes to be exchangable. Or another way of putting it is that we have controlled for all aspects of systematic differences between the control and treatment group and this warrants the causal interpretation of the contrast between treatment and control.\nBut as we’ve seen above the cleanest way to interpret almost any regression model is to understand the model design in terms of the marginal effects on the outcome scale. These are just contrasts. All statistical models are, in some sense, focused on finding a structure that licenses an inference about some contrast of interest between the levels of some observed variable and the implied outcomes. In this way we want to include as much structure in our model that would induce the status of conditional exchangeability between the units of study across such group contrasts. This notion of conditional exchangeability is inherently an epistemic notion. We believe that our model is apt to induce the status of conditional exchangeability such that the people in our survey have no systematic difference which biases the measured differences. The implied differences in some marginal effect (while holding all else fixed) is a “fair” representations the same counterfactual adjustment in the population or sub-population defined by the covariate profile \\(X\\). The model implied difference is a proxy for the result of possible future interventions and as such merits our attention in policy design.\nIn particular the view here is that we ought to be deliberate in how we structure our models to license the plausibility of the exchangeability claim. By De Finetti’s theorem a distribution of exchangeable sequence of variables be expressed as mixture of conditional independent variables.\n\\[ p(x_{1}....x_{m}) = \\dfrac{p(X | \\theta)p(\\theta)}{p_{i}(X)} \\]\nSo if we specify the conditional distribution correctly, we recover the conditions that warrant inference with a well designed model.The mixture distribution is just the vector of parameters \\(\\boldsymbol{\\theta}\\) upon which we condition our model. Mislevy and Levy highlight this important observation has implications for model development by quoting Judea Pearl:\n\n[C]onditional independence is not a grace of nature for which we must wait passively, but rather a psychological necessity which we satisfy by organising our knowledge in a specific way. An important tool in such an organisation is the identification of intermediate variables that induce conditional independence among observables; if such variables are not in our vocabulary, we create them. In medical diagnosis, for instance, when some symptoms directly influence one another, the medical profession invents a name for that interaction (e.g. “syndrome”, “complication”, “pathological state”) and treats it as a new auxilary variable that induces conditional independence.” - Pearl quoted in Bayesian Psychometric Modeling p61\n\nThe organisation of our data into meaningful categories or clusters is common in machine learning, but similarly in psychometrics we see the prevalence of factor models which are more explicitly causal models where we posit some unifying theme to the cluster of measures. In each tradition we are seeking conditional independence by allowing the model to account for these dependencies in how we structure the model. These choices represent a fruitful aspect of walking down through the garden of forking paths. There is then a tension between the exploratory seeking for some pre-determined story and an experimental seeking of clarity. With this in mind we now turn to types of modeling architecture that open up new expressive possibilities and offer a way to think about the relationships between observed data and latent factors that drive the observed outcomes. This greater expressive power offers different routes to inducing patterns of conditional exchangeability and different risks too."
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#sem-and-indirect-effects",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#sem-and-indirect-effects",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "SEM and Indirect Effects",
    "text": "SEM and Indirect Effects\n\ndef make_indirect_sem(priors): \n\n  coords = {'obs': list(range(len(df))), \n            'indicators': drivers,\n            'indicators_1': ['se_acad_p1','se_acad_p2','se_acad_p3'],\n            'indicators_2': ['se_social_p1','se_social_p2','se_social_p3'],\n            'indicators_3': ['sup_friends_p1','sup_friends_p2','sup_friends_p3'],\n            'indicators_4': [ 'sup_parents_p1','sup_parents_p2','sup_parents_p3'],\n            'indicators_5': ['ls_p1','ls_p2', 'ls_p3'],\n            'latent': ['SUP_F', 'SUP_P'], \n            'latent1': ['SUP_F', 'SUP_P'], \n            'latent_regression': ['SUP_F->SE_ACAD', 'SUP_P->SE_ACAD', 'SUP_F->SE_SOC', 'SUP_P->SE_SOC'],\n            'regression': ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P']\n            }\n\n  obs_idx = list(range(len(df)))\n  with pm.Model(coords=coords) as model:\n    \n    Psi = pm.InverseGamma('Psi', 5, 10, dims='indicators')\n    lambdas_ = pm.Normal('lambdas_1',  priors['lambda'][0], priors['lambda'][1], dims=('indicators_1'))\n    lambdas_1 = pm.Deterministic('lambdas1', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_1'))\n    lambdas_ = pm.Normal('lambdas_2', priors['lambda'][0], priors['lambda'][1], dims=('indicators_2'))\n    lambdas_2 = pm.Deterministic('lambdas2', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_2'))\n    lambdas_ = pm.Normal('lambdas_3', priors['lambda'][0], priors['lambda'][1], dims=('indicators_3'))\n    lambdas_3 = pm.Deterministic('lambdas3', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_3'))\n    lambdas_ = pm.Normal('lambdas_4', priors['lambda'][0], priors['lambda'][1], dims=('indicators_4'))\n    lambdas_4 = pm.Deterministic('lambdas4', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_4'))\n    lambdas_ = pm.Normal('lambdas_5', priors['lambda'][0], priors['lambda'][1], dims=('indicators_5'))\n    lambdas_5 = pm.Deterministic('lambdas5', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_5'))\n    tau = pm.Normal('tau', 3, 10, dims='indicators')\n    kappa = 0\n    sd_dist = pm.Exponential.dist(1.0, shape=2)\n    chol, _, _ = pm.LKJCholeskyCov('chol_cov', n=2, eta=priors['eta'],\n      sd_dist=sd_dist, compute_corr=True)\n    cov = pm.Deterministic(\"cov\", chol.dot(chol.T), dims=('latent', 'latent1'))\n    ksi = pm.MvNormal('ksi', kappa, chol=chol, dims=('obs', 'latent'))\n\n    beta_r = pm.Normal('beta_r', 0, 0.5, dims='latent_regression')\n    beta_r2 = pm.Normal('beta_r2', 0, 1, dims='regression')\n\n    # SE_ACAD ~ SUP_FRIENDS + SUP_PARENTS \n    regression_se_acad = pm.Normal('regr_se_acad', beta_r[0]*ksi[obs_idx, 0] + beta_r[1]*ksi[obs_idx, 1], 1)\n    # SE_SOCIAL ~ SUP_FRIENDS + SUP_PARENTS \n    \n    regression_se_social = pm.Normal('regr_se_social', beta_r[2]*ksi[obs_idx, 0] + beta_r[3]*ksi[obs_idx, 1], 1)\n\n    # LS ~ SE_ACAD + SE_SOCIAL + SUP_FRIEND + SUP_PARENTS\n    regression = pm.Normal('regr', beta_r2[0]*regression_se_acad + beta_r2[1]*regression_se_social +\n                                   beta_r2[2]*ksi[obs_idx, 0] + beta_r2[3]*ksi[obs_idx, 1], 1)\n\n    m0 = tau[0] + regression_se_acad*lambdas_1[0]\n    m1 = tau[1] + regression_se_acad*lambdas_1[1]\n    m2 = tau[2] + regression_se_acad*lambdas_1[2]\n    m3 = tau[3] + regression_se_social*lambdas_2[0]\n    m4 = tau[4] + regression_se_social*lambdas_2[1]\n    m5 = tau[5] + regression_se_social*lambdas_2[2]\n    m6 = tau[6] + ksi[obs_idx, 0]*lambdas_3[0]\n    m7 = tau[7] + ksi[obs_idx, 0]*lambdas_3[1]\n    m8 = tau[8] + ksi[obs_idx, 0]*lambdas_3[2]\n    m9 = tau[9] + ksi[obs_idx, 1]*lambdas_4[0]\n    m10 = tau[10] + ksi[obs_idx, 1]*lambdas_4[1]\n    m11 = tau[11] + ksi[obs_idx, 1]*lambdas_4[2]\n    m12 = tau[12] + regression*lambdas_5[0]\n    m13 = tau[13] + regression*lambdas_5[1]\n    m14 = tau[14] + regression*lambdas_5[2]\n    \n    mu = pm.Deterministic('mu', pm.math.stack([m0, m1, m2, m3, m4, m5, m6, m7,\n                                              m8, m9, m10, m11, m12, m13, m14]).T)\n    _  = pm.Normal('likelihood', mu, Psi, observed=df[drivers].values)\n\n    idata = pm.sample(10_000, chains=4, nuts_sampler='numpyro', target_accept=.99, tune=2000,\n                      idata_kwargs={\"log_likelihood\": True}, random_seed=110)\n    idata.extend(pm.sample_posterior_predictive(idata))\n\n    return model, idata\n\n\nmodel2, idata2 = make_indirect_sem(priors={'eta': 2, 'lambda': [1, 1]})\n\n\n\n\nIndirect Effects and Total Effects Model\n\n\n\nmake_ppc(idata2)\n\n\n\n\n\n\nsummary_df = az.summary(idata2, var_names=['beta_r', 'beta_r2'])\n\ndef calculate_effects(summary_df, var='SUP_P'):\n    #Indirect Paths\n    ## VAR -> SE_SOC ->LS\n    indirect_parent_soc = summary_df.loc[f'beta_r[{var}->SE_SOC]']['mean']*summary_df.loc['beta_r2[SE_SOCIAL]']['mean']\n\n    ## VAR -> SE_SOC ->LS\n    indirect_parent_acad = summary_df.loc[f'beta_r[{var}->SE_ACAD]']['mean']*summary_df.loc['beta_r2[SE_ACAD]']['mean']\n\n    ## Total Indirect Effects\n    total_indirect = indirect_parent_soc + indirect_parent_acad\n\n    ## Total Effects\n    total_effect = total_indirect + summary_df.loc[f'beta_r2[{var}]']['mean']\n\n    return pd.DataFrame([[indirect_parent_soc, indirect_parent_acad, total_indirect, total_effect]], \n                columns=[f'{var} -> SE_SOC ->LS', f'{var} -> SE_ACAD ->LS', f'Total Indirect Effects {var}', f'Total Effects {var}']\n                )\n\nindirect_p = calculate_effects(summary_df, 'SUP_P')\nindirect_f = calculate_effects(summary_df, 'SUP_F')\n\nresiduals_posterior_cov = get_posterior_resids(idata2, 500)\nresiduals_posterior_corr = get_posterior_resids(idata2, 500, 'corr')\n\n\npy$summary_df |> kable( caption= \"Regression Coefficients Amongst Latent Factors\", digits=2) |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\nRegression Coefficients Amongst Latent Factors\n \n  \n     \n    mean \n    sd \n    hdi_3% \n    hdi_97% \n    mcse_mean \n    mcse_sd \n    ess_bulk \n    ess_tail \n    r_hat \n  \n \n\n  \n    beta_r[SUP_F->SE_ACAD] \n    0.04 \n    0.06 \n    -0.07 \n    0.16 \n    0 \n    0 \n    40471 \n    32150 \n    1 \n  \n  \n    beta_r[SUP_P->SE_ACAD] \n    0.25 \n    0.07 \n    0.12 \n    0.38 \n    0 \n    0 \n    32312 \n    27922 \n    1 \n  \n  \n    beta_r[SUP_F->SE_SOC] \n    0.16 \n    0.06 \n    0.04 \n    0.28 \n    0 \n    0 \n    40841 \n    31961 \n    1 \n  \n  \n    beta_r[SUP_P->SE_SOC] \n    0.32 \n    0.07 \n    0.19 \n    0.46 \n    0 \n    0 \n    31175 \n    29363 \n    1 \n  \n  \n    beta_r2[SE_ACAD] \n    0.14 \n    0.10 \n    -0.05 \n    0.33 \n    0 \n    0 \n    28629 \n    30816 \n    1 \n  \n  \n    beta_r2[SE_SOCIAL] \n    0.47 \n    0.11 \n    0.26 \n    0.69 \n    0 \n    0 \n    24873 \n    29698 \n    1 \n  \n  \n    beta_r2[SUP_F] \n    0.03 \n    0.07 \n    -0.10 \n    0.16 \n    0 \n    0 \n    33317 \n    30559 \n    1 \n  \n  \n    beta_r2[SUP_P] \n    0.28 \n    0.08 \n    0.12 \n    0.44 \n    0 \n    0 \n    25342 \n    28644 \n    1 \n  \n\n\n\n\n\n\npy$indirect_p |> kable( caption= \"Total and Indirect Effects: Parental Support\", digits=2) |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\nTotal and Indirect Effects: Parental Support\n \n  \n    SUP_P -> SE_SOC ->LS \n    SUP_P -> SE_ACAD ->LS \n    Total Indirect Effects SUP_P \n    Total Effects SUP_P \n  \n \n\n  \n    0.15 \n    0.04 \n    0.19 \n    0.47 \n  \n\n\n\n\npy$indirect_f |> kable( caption= \"Total and Indirect Effects: Friend Support\", digits=4) |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\nTotal and Indirect Effects: Friend Support\n \n  \n    SUP_F -> SE_SOC ->LS \n    SUP_F -> SE_ACAD ->LS \n    Total Indirect Effects SUP_F \n    Total Effects SUP_F \n  \n \n\n  \n    0.0743 \n    0.0061 \n    0.0803 \n    0.1083 \n  \n\n\n\n\n\n\nplot_heatmap(py$residuals_posterior_cov, \"Residuals of the Sample Covariances and Model Implied Covariances\", \"A Visual Check of Bayesian Structural Model fit\")"
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#conclusion",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#conclusion",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Conclusion",
    "text": "Conclusion\nAt the outset of this analysis we stated the hypothetical structure of interest. This suggests a pure confirmatory project, but in practice much of the feasibility of this analysis path is dependent on viability of the various model architectures with respect to the data. It’s easy to tell the story of increasingly sophisticated models, more powerful, more insightful inferential claims. Yet, this narrative is compelling only in so far as there is a justifiable sequence of steps through the ascension model complexity. The increase in complexity needs to be off-set by a rationale at each step."
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#measurement-and-measurement-constructs",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#measurement-and-measurement-constructs",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Measurement and Measurement Constructs",
    "text": "Measurement and Measurement Constructs\nScience is easier when there is a recipe. When there is some procedure to adopt, routine to ape or track to follow, you can out-source the responsibility for methodological justification. One egregious pattern in this vein tries to mask implausible nonsense with the mathematical gloss of “statistical significance”. Seen from 1000 feet, this is misguided but not surprising. Lip-service is paid to the idea of scientific method and we absolve ourselves of the requirement for principled justification, substantive argument with a facsimile of evidence.\nEvidence should be marshaled in service to argument. But it’s an absurd pretense to claim that data speaks for itself in this argument. Good and compelling argumentation is at the heart of any sound inference. It is a necessary obligation if you expect anyone to make any decision on the strength of evidence you provide. Procedures and routine tests offer only a facsimile of sound argument and p-values are a poor substitute for narrative cogency, logical consequence and substantive derivation.\nData is found, gathered or (maybe even) carefully curated. In all cases there is need for a defensive posture and an argument that the data is fit-for-purpose. Nowhere is this more clear than in psychometrics where the data is often derived from a strategically constructed survey aimed at a particular target phenomena. Some intuited, but not yet measured, concept that arguably plays a role in human action, motivation or sentiment. The relative “fuzziness” of the subject matter in psychometrics has had a catalyzing effect on the methodological rigour sought in the science. Survey designs are agonized over for correct tone and rhythm of sentence structure. Measurement scales are doubly checked for reliability and correctness. The literature is consulted and questions are refined. Analysis steps are justified and tested under a wealth of modelling routines. Model architectures are defined and refined to better express the hypothesized structures in the data-generating process. We will see how such due diligence leads to powerful and expressive models that grant us tractability on thorny questions of human affect.\n\nStructure of Presentation\nWe aim to follow a an escalating series of model complexity highlighting the variety of choices that go into modeling psychometric survey data, ultimately concluding by demonstrating how to fit appropriately complex models in PyMC.\n\nExploratory Data Visualization\n\nPulling out views to try and make the multivariate survey response seem a bit more concrete.\n\nExploratory Regression Models\n\nHighlighting the nature of the representation that is used when we fit this data into a simple regression context. We also discuss how a model is a tool for exploring the data as much as it is a statement about the nature of the relations in our data-set.\n\nInterlude on Model Design and Conditional Independence\n\nA brief digression on what we’re aiming to do when fit a model to this kind of data. In particular how we need richer structures to articulate the models that can actually answer questions of interest.\n\nExpressive Structure: Building the Model we Need\n\nConfirmatory Factor Models in lavaan\nStructural Equation Models in lavaan\nBayesian Confirmatory Factor Models in PyMC\nBayesian Structural Equation Models in PyMC\n\n\nThroughout we’ll draw on Levy and Mislevy’s Bayesian Psychometric Modeling to highlight how good model design seeks structures which justify claim(s) of conditional independence; thereby supporting the argument to compelling inference."
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#conclusion-build-the-model-with-lasers",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#conclusion-build-the-model-with-lasers",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Conclusion: Build the Model with Lasers!",
    "text": "Conclusion: Build the Model with Lasers!\n\n“Model experiments offer less inferential power to learn about the world. But the possibility of model experiments to surprise, that is, to produce results that are unexpected against the background of existing knowledge and understanding, remains […] important … The surprising results of model experiments lead not to the discovery of new phenomena in the real world, but the recognition of new things in the world of the model and thence to the development of new categories of things and new concepts and ideas …” - Mary Morgan in The World in the Model\n\nAt the outset of this analysis we stated the hypothetical structure of interest. This suggests a pure confirmatory project, but in practice much of the feasibility of this analysis is path dependent and constrained by the viability of the various model architectures with respect to the data. It’s easy to tell the story of increasingly sophisticated models, more powerful, more insightful inferential claims. Yet, this narrative is compelling only in so far as there is a justifiable sequence of steps through the ascension of model complexity. The increase in complexity is off-set by a rationale at each step. Our final model encompasses aspects of our initial modeling in a expansionary escalation that preserves the insight in the earliest editions but improves them with our burgeoning understanding of the computational and theoretical dimensions of our problem.\nTo recap we have worked through a series of simple regression models seeking to tease out the weighted importance of various factors on the outcomes of life-satisfaction. These models highlighted fairly consistently the relative importance of metrics related to parental support and social self-efficacy. We also saw how there was a significant modification effect for the interaction term. These are all suggestive that we’ve picked out a meaningful set of metrics and which ought to support inferences about the phenomena of interest. But how do we parcel those metrics in such a way to support clean and compelling statements about the influence of each on reported life-satisfaction? Here we sought to leverage CFA and SEM models to clarify the structure of this influence and the particular pathways by which parental support drives these outcomes. Our MLE estimation strategy breaks when trying to estimate mediation effects in a complex SEM structure.\nThese latter models were then re-articulated as a species of Bayesian hierarchical models which aim to induce conditional independence amongst the thematically similar metrics by grouping each under a common factor. Which is to say, we assume that the joint distribution of outcomes is exchangeable and we can fit a conditional mixture to represent that joint distribution. We then showed how these conditional structures can be further adapted to answer questions about direct and indirect causation that bear precisely on the hypothetical structure we declared to be our target at the outset. Yet it’s not enough to fit a model and claim to be done. You have to proffer a defense that the model estimated is both plausible and recovers the observed data well. The very process of iterative Bayesian model development encodes these checks and balances. Iterative model refinement is no guide to which questions should be asked of a model, yet questions occur as we explore the posterior distribution. Inspiration strikes when you have samples to hand and can test the implications of some idle thought. Perhaps some parameterisation yields poor model fit and the re-parameterisation (a shift in some focal parameter’s prior) recovers better performance and prompts a new theoretical lens on your problem. While not a panacea to all forms of statistical misstep, this workflow is a positive, rich and rewarding methodological practice. The iterative model workflow is a natural seeking of the “right” conditionalisation to inform our decisions. Our theory is rich, and maybe even a tad risque for conservative tastes, but it articulates the latent unobservable constructs we believe to operate in reality. In the end we make an inference to the best explanation by an argument that covers theoretical nuance, computational burden and points of practical compromise. But we are making an argument - Yes, we’ve walked down the garden of forking garden paths, but our choices were deliberate and we suffer no regret."
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#the-data",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#the-data",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "The Data",
    "text": "The Data\nOur data is borrowed from work by Boris Mayer and Andrew Ellis found here. They demonstrate CFA and SEM modelling with lavaan. We’ll load up their data.\n\ndf = read.csv('sem_data.csv')\ndf$ls_sum <- df$ls_p1 + df$ls_p2 + df$ls_p3\ndf$ls_mean <- rowMeans(df[ c('ls_p1', 'ls_p2', 'ls_p3')])\n\nhead(df) |> kable() |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n \n  \n    ID \n    region \n    gender \n    age \n    se_acad_p1 \n    se_acad_p2 \n    se_acad_p3 \n    se_social_p1 \n    se_social_p2 \n    se_social_p3 \n    sup_friends_p1 \n    sup_friends_p2 \n    sup_friends_p3 \n    sup_parents_p1 \n    sup_parents_p2 \n    sup_parents_p3 \n    ls_p1 \n    ls_p2 \n    ls_p3 \n    ls_sum \n    ls_mean \n  \n \n\n  \n    1 \n    west \n    female \n    13 \n    4.857143 \n    5.571429 \n    4.500000 \n    5.80 \n    5.500000 \n    5.40 \n    6.5 \n    6.5 \n    7.0 \n    7.0 \n    7.0 \n    6.0 \n    5.333333 \n    6.75 \n    5.50 \n    17.58333 \n    5.861111 \n  \n  \n    2 \n    west \n    male \n    14 \n    4.571429 \n    4.285714 \n    4.666667 \n    5.00 \n    5.500000 \n    4.80 \n    4.5 \n    4.5 \n    5.5 \n    5.0 \n    6.0 \n    4.5 \n    4.333333 \n    5.00 \n    4.50 \n    13.83333 \n    4.611111 \n  \n  \n    10 \n    west \n    female \n    14 \n    4.142857 \n    6.142857 \n    5.333333 \n    5.20 \n    4.666667 \n    6.00 \n    4.0 \n    4.5 \n    3.5 \n    7.0 \n    7.0 \n    6.5 \n    6.333333 \n    5.50 \n    4.00 \n    15.83333 \n    5.277778 \n  \n  \n    11 \n    west \n    female \n    14 \n    5.000000 \n    5.428571 \n    4.833333 \n    6.40 \n    5.833333 \n    6.40 \n    7.0 \n    7.0 \n    7.0 \n    7.0 \n    7.0 \n    7.0 \n    4.333333 \n    6.50 \n    6.25 \n    17.08333 \n    5.694444 \n  \n  \n    12 \n    west \n    female \n    14 \n    5.166667 \n    5.600000 \n    4.800000 \n    5.25 \n    5.400000 \n    5.25 \n    7.0 \n    7.0 \n    7.0 \n    6.5 \n    6.5 \n    7.0 \n    5.666667 \n    6.00 \n    5.75 \n    17.41667 \n    5.805556 \n  \n  \n    14 \n    west \n    male \n    14 \n    4.857143 \n    4.857143 \n    4.166667 \n    5.20 \n    5.000000 \n    4.20 \n    5.5 \n    6.5 \n    7.0 \n    6.5 \n    6.5 \n    6.5 \n    5.000000 \n    5.50 \n    5.50 \n    16.00000 \n    5.333333 \n  \n\n\n\n\n\nWe have survey responses from ~300 individuals who have answered questions regarding their upbringing, self-efficacy and reported life-satisfaction. The hypothetical dependency structure in this life-satisfaction data-set posits a moderated relationship between scores related to life-satisfaction, parental and family support and self-efficacy. It is not a trivial task to be able to design a survey that can elicit answers plausibly mapped to each of these “factors” or themes, never mind finding a model of their relationship that can inform us as to the relative of impact of each on life-satisfaction outcomes.\n We will try to show some of the subtle aspects of these relationships as we go.\nWe can thematically cluster our survey questions along these lines because the source survey deliberately targeted each of the underlying themes in effort to pin down the relations between these hard to measure constructs. We are assuming that our measurement scales are well designed and that the mapping of themes is appropriate. Here we can see how the high level summary statistics show the variation across these measures.\n\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\ndatasummary_skim(df)|> \n style_tt(\n   i = 15:17,\n   j = 1:1,\n   background = \"#20AACC\",\n   color = \"white\",\n   italic = TRUE) |> \n style_tt(\n   i = 18:19,\n   j = 1:1,\n   background = \"#2888A0\",\n   color = \"white\",\n   italic = TRUE) |> \n style_tt(\n   i = 2:14,\n   j = 1:1,\n   background = \"#17C2AD\",\n   color = \"white\",\n   italic = TRUE)\n\n\n \n\n  \n    \n    \n    tinytable_ocdtt0ymi9t18mtrz4el\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                Unique\n                Missing Pct.\n                Mean\n                SD\n                Min\n                Median\n                Max\n                Histogram\n              \n        \n        \n        \n                \n                  ID\n                  283\n                  0\n                  187.9\n                  106.3\n                  1.0\n                  201.0\n                  367.0\n                  \n                \n                \n                  age\n                  5\n                  0\n                  14.7\n                  0.8\n                  13.0\n                  15.0\n                  17.0\n                  \n                \n                \n                  se_acad_p1\n                  32\n                  0\n                  5.2\n                  0.8\n                  3.1\n                  5.1\n                  7.0\n                  \n                \n                \n                  se_acad_p2\n                  36\n                  0\n                  5.3\n                  0.7\n                  3.1\n                  5.4\n                  7.0\n                  \n                \n                \n                  se_acad_p3\n                  29\n                  0\n                  5.2\n                  0.8\n                  2.8\n                  5.2\n                  7.0\n                  \n                \n                \n                  se_social_p1\n                  24\n                  0\n                  5.3\n                  0.8\n                  1.8\n                  5.4\n                  7.0\n                  \n                \n                \n                  se_social_p2\n                  27\n                  0\n                  5.5\n                  0.7\n                  2.7\n                  5.5\n                  7.0\n                  \n                \n                \n                  se_social_p3\n                  31\n                  0\n                  5.4\n                  0.8\n                  3.0\n                  5.5\n                  7.0\n                  \n                \n                \n                  sup_friends_p1\n                  13\n                  0\n                  5.8\n                  1.1\n                  1.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_friends_p2\n                  10\n                  0\n                  6.0\n                  0.9\n                  2.5\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_friends_p3\n                  13\n                  0\n                  6.0\n                  1.1\n                  1.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_parents_p1\n                  11\n                  0\n                  6.0\n                  1.1\n                  1.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_parents_p2\n                  11\n                  0\n                  5.9\n                  1.1\n                  2.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  sup_parents_p3\n                  13\n                  0\n                  5.7\n                  1.1\n                  1.0\n                  6.0\n                  7.0\n                  \n                \n                \n                  ls_p1\n                  15\n                  0\n                  5.2\n                  0.9\n                  2.0\n                  5.3\n                  7.0\n                  \n                \n                \n                  ls_p2\n                  21\n                  0\n                  5.8\n                  0.7\n                  2.5\n                  5.8\n                  7.0\n                  \n                \n                \n                  ls_p3\n                  22\n                  0\n                  5.2\n                  0.9\n                  2.0\n                  5.2\n                  7.0\n                  \n                \n                \n                  ls_sum\n                  98\n                  0\n                  16.2\n                  2.1\n                  8.7\n                  16.4\n                  20.8\n                  \n                \n                \n                  ls_mean\n                  97\n                  0\n                  5.4\n                  0.7\n                  2.9\n                  5.5\n                  6.9\n                  \n                \n                \n                   \n                    \n                  N\n                  %\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  region\n                  east\n                  142\n                  50.2\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  \n                  west\n                  141\n                  49.8\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  gender\n                  female\n                  132\n                  46.6\n                  \n                  \n                  \n                  \n                  \n                \n                \n                  \n                  male\n                  151\n                  53.4\n                  \n                  \n                  \n                  \n                  \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nNote how we’ve distinguished among the metrics for the “outcome” metrics and the “driver” metrics. Such a distinction may seem trivial, but it is only possible because we bring substantive knowledge to bear on the problem in the design of our survey and the postulation of the theoretical construct. Practically our data is just a multivariate outcome vector. It’s theory that offers a structural understanding of how these metrics reflect a dependence. There are a large range of possible interacting effects and the patterns of realization for our life-satisfaction scores. The true “drivers” of satisfaction may be quite different than the hypothesised structure. It is this open question that we’re aiming to uncover in the analysis of our survey data. Does our data support the view that parental support is essential to life-satisfaction? To what degree? How do we tease out this implications precisely?\n\nSample Covariances and Correlations\nTo start, let’s plot the relations amongst our various indicator scores.\n\ndrivers = c('se_acad_p1', 'se_acad_p2', 'se_acad_p3', 'se_social_p1', 'se_social_p2', 'se_social_p3', 'sup_friends_p1','sup_friends_p2', 'sup_friends_p3', 'sup_parents_p1' , 'sup_parents_p2' , 'sup_parents_p3', 'ls_p1', 'ls_p2', 'ls_p3')\n\n\n\nplot_heatmap <- function(df, title=\"Sample Covariances\", subtitle=\"Observed Measures\") {\n  heat_df = df |> as.matrix() |> melt()\n  colnames(heat_df) <- c(\"x\", \"y\", \"value\")\n  g <- heat_df |> mutate(value = round(value, 2)) |>  ggplot(aes(x = x, y = y, fill = value)) +\n    geom_tile() + geom_text(aes(label = value), color = \"black\", size = 4) +\n   scale_fill_gradient2(\n      high = 'dodgerblue4',\n      mid = 'white',\n      low = 'firebrick2'\n    ) + theme(axis.text.x = element_text(angle=45)) + ggtitle(title, subtitle)\n  \n  g\n}\n\ng1 = plot_heatmap(cov(df[,  drivers]))\n\ng2 = plot_heatmap(cor(df[,  drivers]), \"Sample Correlations\")\n\nplot <- ggarrange(g1,g2, ncol=1, nrow=2);\n\n\n\n\nNote the relatively strong correlations between measures of parental support and the life-satisfaction outcome ls_p3. Similarly, the social self-efficacy scores seem well correlated with the secondary life satisfaction indicator ls_p2. These observed correlations merit some further investigation. We can also plot the pairs of scatter plots to “dig deeper”. What kind of correlation holds between these scores? Are any driven by extreme outliers? This is what we’re looking for in a pair plot. Should we cull the outliers? Leave them in? We leave them in.\n\n\nPair Plots of Multivariate Outcome\n\ndf <- df |> \n  mutate(id = row_number()) \n\n# Prepare data to be plotted on the x axis\nx_vars <- pivot_longer(data = df,\n             cols = se_acad_p1:ls_p3,\n             names_to = \"variable_x\",\n             values_to = \"x\")\n\n# Prepare data to be plotted on the y axis  \ny_vars <- pivot_longer(data = df,\n                       cols = se_acad_p1:ls_p3,\n                       names_to = \"variable_y\",\n                       values_to = \"y\") \n\n# Join data for x and y axes and make plot\nfull_join(x_vars, y_vars, \n          by = c(\"id\"),\n          relationship = \"many-to-many\") |>\n  ggplot() + \n  aes(x = x, y = y) +\n  geom_point() + geom_smooth(method='lm') +\n  facet_grid(c(\"variable_x\", \"variable_y\"))  + ggtitle(\"Pair Plot of Indicator Metrics\", \n                                                       \"Comparing Against Life Satisfaction Scores\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe scatter plots among the highly correlated variables in the heat-map do seem to exhibit some kind of linear relationship with aspects of the life-satisfaction scores. We now turn to modelling these relationships to tease out some kind of inferential summary of those relationships."
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#exploratory-regression-models",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#exploratory-regression-models",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Exploratory Regression Models",
    "text": "Exploratory Regression Models\nTo model these effects we make use of the aggregated sum and mean scores to express the relationships between these indicator metrics and life-satisfaction. We fit a variety of models each one escalating in the number of indicator metrics we incorporate into our model of the life-satisfaction outcome. This side-steps the multivariate nature of hypothesised constructs and crudely amalgamates the indicator metrics. This may be more or less justified depending on how similar in theme the three outcome questions ls_p1, ls_p2, ls_p3 are in nature. We’ve dodged the question of thematic unity as all our metrics “load equally” on the outcome variable here and we let our regression model sort out the weighting.\n\nformula_sum_1st = \" ls_sum ~ se_acad_p1  + se_social_p1 +  sup_friends_p1  + sup_parents_p1\"\nformula_mean_1st = \" ls_mean ~ se_acad_p1  + se_social_p1 +  sup_friends_p1  + sup_parents_p1\"\n\nformula_sum_12 = \" ls_sum ~ se_acad_p1  + se_acad_p2 +  se_social_p1 + se_social_p2 + \nsup_friends_p1 + sup_friends_p2  + sup_parents_p1 + sup_parents_p2\"\nformula_mean_12 = \" ls_mean ~ se_acad_p1  + se_acad_p2 +  se_social_p1 + se_social_p2 + \nsup_friends_p1 + sup_friends_p2  + sup_parents_p1 + sup_parents_p2\"\n\n\nformula_sum = \" ls_sum ~ se_acad_p1 + se_acad_p2 + se_acad_p3 + se_social_p1 +  se_social_p2 + se_social_p3 +  sup_friends_p1 + sup_friends_p2 + sup_friends_p3 + sup_parents_p1 + sup_parents_p2 + sup_parents_p3\"\nformula_mean = \" ls_mean ~ se_acad_p1 + se_acad_p2 + se_acad_p3 + se_social_p1 +  se_social_p2 + se_social_p3 +  sup_friends_p1 + sup_friends_p2 + sup_friends_p3 + sup_parents_p1 + sup_parents_p2 + sup_parents_p3\"\nmod_sum = lm(formula_sum, df)\nmod_sum_1st = lm(formula_sum_1st, df)\nmod_sum_12 = lm(formula_sum_12, df)\nmod_mean = lm(formula_mean, df)\nmod_mean_1st = lm(formula_mean_1st, df)\nmod_mean_12 = lm(formula_mean_12, df)\n\nnorm <- function(x) {\n    (x - mean(x)) / sd(x)\n}\n\ndf_norm <- as.data.frame(lapply(df[c(5:19)], norm))\n\ndf_norm$ls_sum <- df$ls_sum\ndf_norm$ls_mean <- df$ls_mean\n\nmod_sum_norm = lm(formula_sum, df_norm)\nmod_mean_norm = lm(formula_mean, df_norm)\n\nmodels = list(\n    \"Outcome: sum_score\" = list(\"model_sum_1st_factors\" = mod_sum_1st,\n     \"model_sum_1st_2nd_factors\" = mod_sum_12,\n     \"model_sum_score\" = mod_sum,\n     \"model_sum_score_norm\" = mod_sum_norm\n     ),\n    \"Outcome: mean_score\" = list(\n      \"model_mean_1st_factors\" = mod_mean_1st,\n     \"model_mean_1st_2nd_factors\" = mod_mean_12,\n     \"model_mean_score\"= mod_mean, \n     \"model_mean_score_norm\" = mod_mean_norm\n    )\n    )\n\nThe classical presentation of regression models reports the coefficient weights accorded to each of the input variables. We present these models to highlight that the manner in which we represent our theoretical constructs has ramifications for the interpretation of the data generating process. In particular, note how different degrees of significance are accorded to the different variables depending on which alternate variables are included.\n\n#| code-fold: true\n#| code-summary: \"Show the code\"\n\nmodelsummary(models, stars=TRUE, shape =\"cbind\") |> \n style_tt(\n   i = 2:25,\n   j = 1:1,\n   background = \"#17C2AD\",\n   color = \"white\",\n   italic = TRUE)\n\n\n \n\n  \n    \n    \n    tinytable_pclqteqllnuzsr3iakja\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n\n \nOutcome: sum_score\nOutcome: mean_score\n\n        \n              \n                 \n                model_sum_1st_factors\n                model_sum_1st_2nd_factors\n                model_sum_score\n                model_sum_score_norm\n                model_mean_1st_factors\n                model_mean_1st_2nd_factors\n                model_mean_score\n                model_mean_score_norm\n              \n        \n        + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n        \n                \n                  (Intercept)   \n                  5.118***\n                  2.644** \n                  2.094*  \n                  16.192***\n                  1.706***\n                  0.881** \n                  0.698*  \n                  5.397***\n                \n                \n                                \n                  (0.907) \n                  (0.985) \n                  (0.954) \n                  (0.089)  \n                  (0.302) \n                  (0.328) \n                  (0.318) \n                  (0.030) \n                \n                \n                  se_acad_p1    \n                  0.242   \n                  -0.034  \n                  -0.208  \n                  -0.165   \n                  0.081   \n                  -0.011  \n                  -0.069  \n                  -0.055  \n                \n                \n                                \n                  (0.147) \n                  (0.180) \n                  (0.192) \n                  (0.153)  \n                  (0.049) \n                  (0.060) \n                  (0.064) \n                  (0.051) \n                \n                \n                  se_social_p1  \n                  1.088***\n                  0.501*  \n                  0.355+  \n                  0.269+   \n                  0.363***\n                  0.167*  \n                  0.118+  \n                  0.090+  \n                \n                \n                                \n                  (0.162) \n                  (0.204) \n                  (0.200) \n                  (0.152)  \n                  (0.054) \n                  (0.068) \n                  (0.067) \n                  (0.051) \n                \n                \n                  sup_friends_p1\n                  0.125   \n                  -0.224+ \n                  -0.272+ \n                  -0.307+  \n                  0.042   \n                  -0.075+ \n                  -0.091+ \n                  -0.102+ \n                \n                \n                                \n                  (0.088) \n                  (0.133) \n                  (0.150) \n                  (0.169)  \n                  (0.029) \n                  (0.044) \n                  (0.050) \n                  (0.056) \n                \n                \n                  sup_parents_p1\n                  0.561***\n                  0.238+  \n                  0.072   \n                  0.077    \n                  0.187***\n                  0.079+  \n                  0.024   \n                  0.026   \n                \n                \n                                \n                  (0.100) \n                  (0.141) \n                  (0.143) \n                  (0.154)  \n                  (0.033) \n                  (0.047) \n                  (0.048) \n                  (0.051) \n                \n                \n                  se_acad_p2    \n                          \n                  0.448*  \n                  0.327   \n                  0.224    \n                          \n                  0.149*  \n                  0.109   \n                  0.075   \n                \n                \n                                \n                          \n                  (0.197) \n                  (0.202) \n                  (0.139)  \n                          \n                  (0.066) \n                  (0.067) \n                  (0.046) \n                \n                \n                  se_social_p2  \n                          \n                  0.756***\n                  0.509*  \n                  0.356*   \n                          \n                  0.252***\n                  0.170*  \n                  0.119*  \n                \n                \n                                \n                          \n                  (0.213) \n                  (0.219) \n                  (0.153)  \n                          \n                  (0.071) \n                  (0.073) \n                  (0.051) \n                \n                \n                  sup_friends_p2\n                          \n                  0.369*  \n                  0.331*  \n                  0.313*   \n                          \n                  0.123*  \n                  0.110*  \n                  0.104*  \n                \n                \n                                \n                          \n                  (0.157) \n                  (0.160) \n                  (0.151)  \n                          \n                  (0.052) \n                  (0.053) \n                  (0.050) \n                \n                \n                  sup_parents_p2\n                          \n                  0.370** \n                  0.118   \n                  0.129    \n                          \n                  0.123** \n                  0.039   \n                  0.043   \n                \n                \n                                \n                          \n                  (0.138) \n                  (0.144) \n                  (0.157)  \n                          \n                  (0.046) \n                  (0.048) \n                  (0.052) \n                \n                \n                  se_acad_p3    \n                          \n                          \n                  0.153   \n                  0.123    \n                          \n                          \n                  0.051   \n                  0.041   \n                \n                \n                                \n                          \n                          \n                  (0.174) \n                  (0.140)  \n                          \n                          \n                  (0.058) \n                  (0.047) \n                \n                \n                  se_social_p3  \n                          \n                          \n                  0.443** \n                  0.354**  \n                          \n                          \n                  0.148** \n                  0.118** \n                \n                \n                                \n                          \n                          \n                  (0.161) \n                  (0.129)  \n                          \n                          \n                  (0.054) \n                  (0.043) \n                \n                \n                  sup_friends_p3\n                          \n                          \n                  0.165   \n                  0.181    \n                          \n                          \n                  0.055   \n                  0.060   \n                \n                \n                                \n                          \n                          \n                  (0.130) \n                  (0.143)  \n                          \n                          \n                  (0.043) \n                  (0.048) \n                \n                \n                  sup_parents_p3\n                          \n                          \n                  0.526***\n                  0.602*** \n                          \n                          \n                  0.175***\n                  0.201***\n                \n                \n                                \n                          \n                          \n                  (0.126) \n                  (0.144)  \n                          \n                          \n                  (0.042) \n                  (0.048) \n                \n                \n                  Num.Obs.      \n                  283     \n                  283     \n                  283     \n                  283      \n                  283     \n                  283     \n                  283     \n                  283     \n                \n                \n                  R2            \n                  0.399   \n                  0.467   \n                  0.517   \n                  0.517    \n                  0.399   \n                  0.467   \n                  0.517   \n                  0.517   \n                \n                \n                  R2 Adj.       \n                  0.390   \n                  0.451   \n                  0.496   \n                  0.496    \n                  0.390   \n                  0.451   \n                  0.496   \n                  0.496   \n                \n                \n                  AIC           \n                  1090.9  \n                  1064.7  \n                  1044.7  \n                  1044.7   \n                  469.1   \n                  442.9   \n                  422.9   \n                  422.9   \n                \n                \n                  BIC           \n                  1112.8  \n                  1101.2  \n                  1095.7  \n                  1095.7   \n                  491.0   \n                  479.4   \n                  473.9   \n                  473.9   \n                \n                \n                  Log.Lik.      \n                  -539.455\n                  -522.373\n                  -508.341\n                  -508.341 \n                  -228.548\n                  -211.466\n                  -197.434\n                  -197.434\n                \n                \n                  RMSE          \n                  1.63    \n                  1.53    \n                  1.46    \n                  1.46     \n                  0.54    \n                  0.51    \n                  0.49    \n                  0.49    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nWe can similarly plot the coefficient values and their uncertainty highlighting how the representation or scaling of the variables impact the scale of the coefficient weights and therefore the surety of any subsequent claims.\n\nmodels_sum = list(\n     \"model_sum_1st_factors\" = mod_sum_1st,\n     \"model_sum_1st_2nd_factors\" = mod_sum_12,\n     \"model_sum_score\" = mod_sum,\n     \"model_sum_score_norm\" = mod_sum_norm\n    )\n\nmodels_mean = list(\n   \"model_mean_1st_factors\" = mod_mean_1st,\n   \"model_mean_1st_2nd_factors\" = mod_mean_12,\n   \"model_mean_score\"= mod_mean,\n   \"model_mean_score_norm\" = mod_mean_norm\n)\n\ng1 = modelplot(models_sum, coef_omit = 'Intercept') + geom_vline(xintercept = 0, linetype=\"dotted\", \n                color = \"black\") + ggtitle(\"Comparing Sum Model Parameter Estimates\", \"Across Covariates\")\n\n\ng2 = modelplot(models_mean, coef_omit = 'Intercept') + geom_vline(xintercept = 0, linetype=\"dotted\", \n                color = \"black\") + ggtitle(\"Comparing Mean Model Parameter Estimates\", \"Across Covariates\")\n\nplot <- ggarrange(g1,g2, ncol=2, nrow=1);\n\n\n\n\n\nSignificant Coefficients?\nAn alternative lens on these figures highlights the statistical significance of the coefficients. But again, these criteria are much abused. Significance at what level? Conditional on which representation? Which composite outcome score? It should matter if thematically similar relationships are highlighted as we vary the input variables. This is a kind of check on the hypothesised dependency structure - on what cluster of variables does our model place the most weight?\n\ng1 = modelplot(mod_mean, coef_omit = 'Intercept') +  aes(color = ifelse(p.value < 0.001, \"Significant at 0.001\", \"Not significant at 0.001\")) +\n  scale_color_manual(values = c(\"grey\", \"blue\")) + ggtitle(\"Significance of Coefficient Values\", \"Mean Model at Different Levels\")\n\n\ng2 = modelplot(mod_mean, coef_omit = 'Intercept') +  aes(color = ifelse(p.value < 0.05, \"Significant at 0.05\", \"Not significant at 0.05\")) +\n  scale_color_manual(values = c(\"grey\", \"black\"))\n\nplot <- ggarrange(g1,g2, ncol=2, nrow=1);\n\n\n\n\n\n\nAggregate Driver Scores\nPerhaps we play with the feature representation and increase the proportion of significant indicators. Can we now tell a more definitive story about how parental support and social self-efficact are determinants of self-reported life-satisfaction scores? Let’s focus here on the sum score representation and add interaction effects.\n\ndf$se_acad_mean <- rowMeans(df[c('se_acad_p1', 'se_acad_p2', 'se_acad_p3')])\ndf$se_social_mean <- rowMeans(df[c('se_social_p1', 'se_social_p2', 'se_social_p3')])\ndf$sup_friends_mean <- rowMeans(df[c('sup_friends_p1', 'sup_friends_p2', 'sup_friends_p3')])\ndf$sup_parents_mean <- rowMeans(df[c('sup_parents_p1', 'sup_parents_p2', 'sup_parents_p3')])\n\n\nformula_parcel_sum = \"ls_sum ~ se_acad_mean + se_social_mean +\nsup_friends_mean + sup_parents_mean \" #sup_parents_mean*se_social_mean\"\n\nformula_parcel_sum_inter = \"ls_sum ~ se_acad_mean + se_social_mean + \nsup_friends_mean + sup_parents_mean + sup_parents_mean*se_social_mean\"\n\nmod_sum_parcel = lm(formula_parcel_sum, df)\nmod_sum_inter_parcel = lm(formula_parcel_sum_inter, df)\n\nmodels_parcel = list(\"model_sum_score\" = mod_sum_parcel,\n     \"model_sum_inter_score\"= mod_sum_inter_parcel\n     )\n\nmodelsummary(models_parcel, stars=TRUE)\n\n\n \n\n  \n    \n    \n    tinytable_jxxnaq7a8a1a04s6zo8g\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                model_sum_score\n                model_sum_inter_score\n              \n        \n        + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n        \n                \n                  (Intercept)                      \n                  2.728** \n                  -6.103+ \n                \n                \n                                                   \n                  (0.931) \n                  (3.356) \n                \n                \n                  se_acad_mean                     \n                  0.307+  \n                  0.370*  \n                \n                \n                                                   \n                  (0.158) \n                  (0.158) \n                \n                \n                  se_social_mean                   \n                  1.269***\n                  2.859***\n                \n                \n                                                   \n                  (0.175) \n                  (0.606) \n                \n                \n                  sup_friends_mean                 \n                  0.124   \n                  0.183+  \n                \n                \n                                                   \n                  (0.097) \n                  (0.099) \n                \n                \n                  sup_parents_mean                 \n                  0.726***\n                  2.242***\n                \n                \n                                                   \n                  (0.099) \n                  (0.562) \n                \n                \n                  se_social_mean × sup_parents_mean\n                          \n                  -0.292**\n                \n                \n                                                   \n                          \n                  (0.107) \n                \n                \n                  Num.Obs.                         \n                  283     \n                  283     \n                \n                \n                  R2                               \n                  0.489   \n                  0.503   \n                \n                \n                  R2 Adj.                          \n                  0.482   \n                  0.494   \n                \n                \n                  AIC                              \n                  1044.6  \n                  1039.0  \n                \n                \n                  BIC                              \n                  1066.4  \n                  1064.5  \n                \n                \n                  Log.Lik.                         \n                  -516.288\n                  -512.513\n                \n                \n                  RMSE                             \n                  1.50    \n                  1.48    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nWhat does definitive mean here? Is it so simple as more significant coefficients? Marginally better performance measures?\n\ng1 = modelplot(mod_sum_parcel, coef_omit = 'Intercept') +  aes(color = ifelse(p.value < 0.05, \"Significant at 0.05\", \"Not significant at 0.05\")) +\n  scale_color_manual(values = c(\"grey\", \"blue\"), guide=FALSE) + ggtitle(\"Significance of Coefficient Values\", \"At Different Levels for Sum and Mean Scores Life Satisfaction \")\n\n\ng2 = modelplot(mod_sum_inter_parcel, coef_omit = 'Intercept') +  aes(color = ifelse(p.value < 0.05, \"Significant at 0.05\", \"Not significant at 0.05\")) +\n  scale_color_manual(values = c(\"grey\", \"blue\"))\n\nplot <- ggarrange(g1,g2, ncol=2, nrow=1);\n\nWarning: The `guide` argument in `scale_*()` cannot be `FALSE`. This was deprecated in\nggplot2 3.3.4.\nℹ Please use \"none\" instead.\nℹ The deprecated feature was likely used in the modelsummary package.\n  Please report the issue at\n  <https://github.com/vincentarelbundock/modelsummary/issues/>.\n\n\n\n\n\nThis kind of brinkmanship is brittle. Any one of these kinds of choice can be justified but more often than not results from an suspect exploratory process. Steps down a “garden of forking paths” seeking some kind of story to justify an analysis or promote a conclusion. This post-hoc “seeking” is just bad science undermining the significance claims that accrue to reliable procedures. It warps the nature of testing procedure by corrupting the assumed consistency of repeatable trials. The guarantees of statistical significance attach to a conclusion just when the procedure is imagined replicible and repeated under identical conditions. By exploring the different representations and criteria of narrative adequacy we break those guarantees. The false precision of appeals to null hypothesis statistical hides this breadth of this discovery process while leveraging the rhetoric of theoretical guarantees that apply only in an alternative possible world.\n\n\nExploratory and Confirmatory Modes\nOne of the things that psychometrics has pioneered well is the distinction between an exploratory and confirmatory models. This distinction, when made explicit, partially guards against the abuse of inferential integrity we see in more common work-flows. We may fit a model to our data, we may tweak or adjust it in any way we care to - this is exploratory work, the point is that the model design is validated on unseen data. But additionally, models are often opaque - you may (as above) have improved some measure of model fit, changed the parameter weighting accorded to an observed feature, but what does that mean? Exploration of model architectures, design choices and feature creation is just how we come to understand the meaning of a model specification. Even in the simple case of regression we’ve seen how adding an interaction term changes the interpretability of a model. How then are we to stand behind uncertainty estimates accorded to parameter weights when we barely intuit the implications of a model design?\n\n\nMarginal Effects\nThe answer is to not to rely on intuition, but push forward and test the tangible implications of a fitted model. A model is hypothesis which should be applied to stringent test. We should subject the logical consequences of the design to appropriate scrutiny. We understand the implications and relative plausibility of any model in terms of the predicted outcomes more easily than we understand the subtle interaction effects expressed parameter movements. As such we should adopt this view in our evaluation of a model fit too.\nConsider how we do this using Vincent Arel-Bundock’s wonderful marginaleffects package passing a grid of new values through to our fitted model.These implications are a test of model plausibility too.\n\npred <- predictions(mod_sum_parcel, newdata = datagrid(sup_parents_mean = 1:10, se_social_mean = 1:10 ))\npred1 <- predictions(mod_sum_inter_parcel, newdata = datagrid(sup_parents_mean = 1:10, se_social_mean = 1:10))\n\npred1  |> tail(10) |> kableExtra::kable() |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n \n  \n     \n    rowid \n    estimate \n    std.error \n    statistic \n    p.value \n    s.value \n    conf.low \n    conf.high \n    se_acad_mean \n    sup_friends_mean \n    sup_parents_mean \n    se_social_mean \n    ls_sum \n  \n \n\n  \n    91 \n    91 \n    19.27615 \n    2.2881204 \n    8.424449 \n    0 \n    54.61499 \n    14.79152 \n    23.76079 \n    5.237686 \n    5.929329 \n    10 \n    1 \n    17.58333 \n  \n  \n    92 \n    92 \n    19.21698 \n    1.7840261 \n    10.771691 \n    0 \n    87.46457 \n    15.72035 \n    22.71361 \n    5.237686 \n    5.929329 \n    10 \n    2 \n    17.58333 \n  \n  \n    93 \n    93 \n    19.15780 \n    1.2888397 \n    14.864380 \n    0 \n    163.60757 \n    16.63172 \n    21.68388 \n    5.237686 \n    5.929329 \n    10 \n    3 \n    17.58333 \n  \n  \n    94 \n    94 \n    19.09863 \n    0.8188846 \n    23.322730 \n    0 \n    397.24885 \n    17.49364 \n    20.70361 \n    5.237686 \n    5.929329 \n    10 \n    4 \n    17.58333 \n  \n  \n    95 \n    95 \n    19.03945 \n    0.4595015 \n    41.435010 \n    0 \n    Inf \n    18.13884 \n    19.94006 \n    5.237686 \n    5.929329 \n    10 \n    5 \n    17.58333 \n  \n  \n    96 \n    96 \n    18.98027 \n    0.5318049 \n    35.690291 \n    0 \n    924.33454 \n    17.93795 \n    20.02259 \n    5.237686 \n    5.929329 \n    10 \n    6 \n    17.58333 \n  \n  \n    97 \n    97 \n    18.92110 \n    0.9410614 \n    20.106123 \n    0 \n    296.26806 \n    17.07665 \n    20.76554 \n    5.237686 \n    5.929329 \n    10 \n    7 \n    17.58333 \n  \n  \n    98 \n    98 \n    18.86192 \n    1.4210849 \n    13.272902 \n    0 \n    131.14398 \n    16.07665 \n    21.64720 \n    5.237686 \n    5.929329 \n    10 \n    8 \n    17.58333 \n  \n  \n    99 \n    99 \n    18.80274 \n    1.9194980 \n    9.795657 \n    0 \n    72.84938 \n    15.04060 \n    22.56489 \n    5.237686 \n    5.929329 \n    10 \n    9 \n    17.58333 \n  \n  \n    100 \n    100 \n    18.74357 \n    2.4249884 \n    7.729343 \n    0 \n    46.39459 \n    13.99068 \n    23.49646 \n    5.237686 \n    5.929329 \n    10 \n    10 \n    17.58333 \n  \n\n\n\n\n\nWe will see here how the impact of changes in se_social_mean is much reduced when sup_parent_mean is held fixed at high values (9, 10) when our model allows for an interaction effect.\n\npred |> head(10) |> kableExtra::kable() |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n \n  \n    rowid \n    estimate \n    std.error \n    statistic \n    p.value \n    s.value \n    conf.low \n    conf.high \n    se_acad_mean \n    sup_friends_mean \n    sup_parents_mean \n    se_social_mean \n    ls_sum \n  \n \n\n  \n    1 \n    7.065559 \n    0.7860786 \n    8.988362 \n    0 \n    61.78928 \n    5.524873 \n    8.606245 \n    5.237686 \n    5.929329 \n    1 \n    1 \n    17.58333 \n  \n  \n    2 \n    8.334837 \n    0.6546017 \n    12.732685 \n    0 \n    120.95075 \n    7.051842 \n    9.617833 \n    5.237686 \n    5.929329 \n    1 \n    2 \n    17.58333 \n  \n  \n    3 \n    9.604115 \n    0.5480173 \n    17.525206 \n    0 \n    226.01129 \n    8.530021 \n    10.678209 \n    5.237686 \n    5.929329 \n    1 \n    3 \n    17.58333 \n  \n  \n    4 \n    10.873394 \n    0.4830922 \n    22.507905 \n    0 \n    370.25976 \n    9.926550 \n    11.820237 \n    5.237686 \n    5.929329 \n    1 \n    4 \n    17.58333 \n  \n  \n    5 \n    12.142672 \n    0.4771468 \n    25.448505 \n    0 \n    472.16118 \n    11.207481 \n    13.077862 \n    5.237686 \n    5.929329 \n    1 \n    5 \n    17.58333 \n  \n  \n    6 \n    13.411950 \n    0.5321613 \n    25.202790 \n    0 \n    463.16951 \n    12.368933 \n    14.454967 \n    5.237686 \n    5.929329 \n    1 \n    6 \n    17.58333 \n  \n  \n    7 \n    14.681228 \n    0.6324223 \n    23.214278 \n    0 \n    393.60150 \n    13.441703 \n    15.920753 \n    5.237686 \n    5.929329 \n    1 \n    7 \n    17.58333 \n  \n  \n    8 \n    15.950506 \n    0.7602342 \n    20.981043 \n    0 \n    322.26019 \n    14.460474 \n    17.440538 \n    5.237686 \n    5.929329 \n    1 \n    8 \n    17.58333 \n  \n  \n    9 \n    17.219784 \n    0.9039855 \n    19.048739 \n    0 \n    266.32548 \n    15.448005 \n    18.991563 \n    5.237686 \n    5.929329 \n    1 \n    9 \n    17.58333 \n  \n  \n    10 \n    18.489062 \n    1.0571941 \n    17.488806 \n    0 \n    225.08895 \n    16.417000 \n    20.561124 \n    5.237686 \n    5.929329 \n    1 \n    10 \n    17.58333 \n  \n\n\n\n\n\nThis modifying effect is not as evident in the simpler model.\n\nRegression Marginal Effects\nWe can see this more clearly with a plot of the marginal effects.\n\ng = plot_predictions(mod_sum_parcel, condition = c(\"se_social_mean\", \"sup_parents_mean\"), type = \"response\") + ggtitle(\"Counterfactual Shift of Outcome: se_social_mean\", \"Holding all else Fixed: Simple Model\")\n\ng1 = plot_predictions(mod_sum_inter_parcel, condition = c(\"se_social_mean\", \"sup_parents_mean\"), type = \"response\") + ggtitle(\"Counterfactual Shift of Outcome: se_social_mean\", \"Holding all else Fixed Interaction Model\")\n\nplot <- ggarrange(g,g1, ncol=1, nrow=2);\n\n\n\n\nHere we’ve pulled out some of the implications in terms of the outcome predictions and see how the interaction effect modifies the additive effect downwards at the upper end of the se_social_mean scale. That is to say the y-axis recounts the aggregate ls_sum outcome variable and the x-axis varies values for se_social_mean while holding values the other covariates fixed at the mean level. The differences we see in the two plots are due to the extra effect of the modification the interaction term has on our prediction scale. The significance and practical impact of this interaction term should make us think harder about the manner in which social self-efficacy and parental support are related.\n\n\n\nModels with Controls\nThe garden of forking paths presents itself within any set of covariates. How do we represent their effects? Which interactions are meaningful? How do we argue for one model design over another? The questionable paths are multiplied when we begin to consider additional covariates and group effects. But also additional covariates help structure our expectations too. Yes, you can cut and chop your way to through the garden to find some spurious correlation but more plausibly you can bring in structurally important variables which helpfully moderate the outcomes based on our understanding of the data generating process.\nLet’s assess the question by allowing the model to account for differences in region. You might imagine other demographic features which contribute to varying life-satisfaction scores and it would be a further model refinement if we could collect the associated measures to augment our analysis. For now we focus on region.\n\nformula_no_grp_sum = \"ls_sum ~ sup_parents_p1 + sup_parents_p2 + sup_parents_p3 + sup_friends_p1 + sup_friends_p2 + sup_friends_p3 + se_acad_p1 + se_acad_p2 + se_acad_p3 +\nse_social_p1 + se_social_p2 + se_social_p3\"\n\nformula_grp_sum = \"ls_sum ~ sup_parents_p1 + sup_parents_p2 + sup_parents_p3 + sup_friends_p1 + sup_friends_p2 + sup_friends_p3 + se_acad_p1 + se_acad_p2 + se_acad_p3 +\nse_social_p1 + se_social_p2 + se_social_p3 + factor(region)\"\n\nno_grp_sum_fit <- lm(formula_no_grp_sum , data = df)\ngrp_sum_fit <- lm(formula_grp_sum, data = df)\n\ng1 = modelplot(no_grp_sum_fit, re.form=NA) +  aes(color = ifelse(p.value < 0.05, \"Significant at 0.05\", \"Not significant at 0.001\")) +\n  scale_color_manual(values = c(\"grey\", \"blue\"), guide='none') + ggtitle(\"Significance of Coefficient Values\", \"No Group Effects Model\")\n\ng2 = modelplot(grp_sum_fit, re.form=NA) +  aes(color = ifelse(p.value < 0.05, \"Significant at 0.05\", \"Not significant 0.05\")) +\n  scale_color_manual(values = c(\"grey\", \"blue\")) + ggtitle(\"Significance of Coefficient Values\", \"Group Effects Model\")\n\n\n\nplot <- ggarrange(g1,g2, ncol=2, nrow=1);\n\n\n\n\nWe can see here how the additional factor variable is reported to be significant conditional on a model specification. However the intercept is no longer well identified.\n\nmodelsummary(list(\"No Group Effects Fit\"= no_grp_sum_fit,\n                  \"Group Effects Fit\"= grp_sum_fit), \n             stars = TRUE) |> \n style_tt(\n   i = 2:25,\n   j = 1:1,\n   background = \"#17C2AD\",\n   color = \"white\",\n   italic = TRUE)\n\n\n \n\n  \n    \n    \n    tinytable_6dqjw6ypw5guaj07q8f6\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                No Group Effects Fit\n                Group Effects Fit\n              \n        \n        + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n        \n                \n                  (Intercept)       \n                  2.094*  \n                  1.979+  \n                \n                \n                                    \n                  (0.954) \n                  (1.041) \n                \n                \n                  sup_parents_p1    \n                  0.072   \n                  0.066   \n                \n                \n                                    \n                  (0.143) \n                  (0.145) \n                \n                \n                  sup_parents_p2    \n                  0.118   \n                  0.122   \n                \n                \n                                    \n                  (0.144) \n                  (0.145) \n                \n                \n                  sup_parents_p3    \n                  0.526***\n                  0.529***\n                \n                \n                                    \n                  (0.126) \n                  (0.126) \n                \n                \n                  sup_friends_p1    \n                  -0.272+ \n                  -0.272+ \n                \n                \n                                    \n                  (0.150) \n                  (0.150) \n                \n                \n                  sup_friends_p2    \n                  0.331*  \n                  0.332*  \n                \n                \n                                    \n                  (0.160) \n                  (0.160) \n                \n                \n                  sup_friends_p3    \n                  0.165   \n                  0.166   \n                \n                \n                                    \n                  (0.130) \n                  (0.131) \n                \n                \n                  se_acad_p1        \n                  -0.208  \n                  -0.212  \n                \n                \n                                    \n                  (0.192) \n                  (0.193) \n                \n                \n                  se_acad_p2        \n                  0.327   \n                  0.328   \n                \n                \n                                    \n                  (0.202) \n                  (0.202) \n                \n                \n                  se_acad_p3        \n                  0.153   \n                  0.169   \n                \n                \n                                    \n                  (0.174) \n                  (0.184) \n                \n                \n                  se_social_p1      \n                  0.355+  \n                  0.345+  \n                \n                \n                                    \n                  (0.200) \n                  (0.204) \n                \n                \n                  se_social_p2      \n                  0.509*  \n                  0.517*  \n                \n                \n                                    \n                  (0.219) \n                  (0.221) \n                \n                \n                  se_social_p3      \n                  0.443** \n                  0.446** \n                \n                \n                                    \n                  (0.161) \n                  (0.161) \n                \n                \n                  factor(region)west\n                          \n                  0.056   \n                \n                \n                                    \n                          \n                  (0.202) \n                \n                \n                  Num.Obs.          \n                  283     \n                  283     \n                \n                \n                  R2                \n                  0.517   \n                  0.517   \n                \n                \n                  R2 Adj.           \n                  0.496   \n                  0.494   \n                \n                \n                  AIC               \n                  1044.7  \n                  1046.6  \n                \n                \n                  BIC               \n                  1095.7  \n                  1101.3  \n                \n                \n                  Log.Lik.          \n                  -508.341\n                  -508.300\n                \n                \n                  RMSE              \n                  1.46    \n                  1.46    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nAgain the cleanest way to interpret the implications of these specifications is derive the conditional marginal effects and assess these for plausibility.\n\nGroup Marginal Effects\nWe see here the different levels expected for different regional responses for changes in each of these input variables.\n\ng = plot_predictions(grp_sum_fit, condition = c(\"sup_parents_p3\", \"region\"), type = \"response\") + ggtitle(\"Counterfactual Shift of Outcome: sup_parents_p3\", \"Holding all else Fixed\")\n\ng1 = plot_predictions(grp_sum_fit, condition = c(\"sup_friends_p1\", \"region\"), type = \"response\") + ggtitle(\"Counterfactual Shift of Outcome: sup_friends_p1\", \"Holding all else Fixed\")\n\ng2 = plot_predictions(grp_sum_fit, condition = c(\"se_acad_p1\", \"region\"), \n                      type = \"response\") + ggtitle(\"Counterfactual Shift of Outcome: se_acad_p1\", \"Holding all else Fixed\")\n\nplot <- ggarrange(g,g1,g2, ncol=1, nrow=3);\n\n\n\n\nIn this case the differences are slight, but the point here is just to consider how aspects of the data generating process can be modified by broadly demographic features systematically. In an ideal circumstance a probability-sample of survey respondents should mitigate the need for additionally controls variables in so far as a probability sample should ensure exchangeability"
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#interlude-model-design-and-conditional-exchangeability",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#interlude-model-design-and-conditional-exchangeability",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Interlude: Model Design and Conditional Exchangeability",
    "text": "Interlude: Model Design and Conditional Exchangeability\nModelling is nearly always about contrasts and questions of meaningful differences. What we seek to do when we build a model is to find a structure that enables “fair” or “justified” inferences about these contrasts. This is maybe most palpably brought home when we consider cases of causal inference. Here we want to define a meaningful causal estimand - some contrast between treatment group and control group where we can be confident that the two groups are suitably “representative” so that the observed differences represents solely the effect of the treatment. We are saying that conditional on our model design we consider the potential outcomes to be exchangable. Or another way of putting it is that we have controlled for all aspects of systematic differences between the control and treatment group and this warrants the causal interpretation of the contrast between treatment and control.\nBut as we’ve seen above the cleanest way to interpret almost any regression model is to understand the model design in terms of the marginal effects on the outcome scale. These are just contrasts. All statistical models are, in some sense, focused on finding a structure that licenses an inference about some contrast of interest between the levels of some observed variable and the implied outcomes. In this way we want to include as much structure in our model that would induce the status of conditional exchangeability between the units of study across such group contrasts. This notion of conditional exchangeability is inherently an epistemic notion. We believe that our model is apt to induce the status of conditional exchangeability such that the people in our survey have no systematic difference which biases the measured differences. The implied differences in some marginal effect (while holding all else fixed) is a “fair” representation of the same counterfactual adjustment in the population or sub-population defined by the covariate profile \\(X\\). The model implied difference is a proxy for the result of possible future interventions and as such merits our attention in policy design.\nOne point of caution here is that we ought not to be too quick when interpreting model coefficients as causal effects. If we have not designed our model to isolate the causal or generalisable effect, while making appropriarate adjustments for risks of confounding, we will end up conflating biased estimates with true effects. In particular the view here is that we ought to be deliberate in how we structure our models to license the plausibility of the exchangeability claim with respect to a relationship of interest. By De Finetti’s theorem a distribution of exchangeable sequence of variables be expressed as mixture of conditional independent variables.\n\\[ p(x_{1}....x_{m}) = \\dfrac{p(X | \\theta)p(\\theta)}{p_{i}(X)} \\]\nSo if we specify the conditional distribution correctly, we recover the conditions that warrant inference with a well designed model.The mixture distribution is just the vector of parameters \\(\\boldsymbol{\\theta}\\) upon which we condition our model. Mislevy and Levy highlight this important observation has implications for model development by quoting Judea Pearl:\n\n[C]onditional independence is not a grace of nature for which we must wait passively, but rather a psychological necessity which we satisfy by organising our knowledge in a specific way. An important tool in such an organisation is the identification of intermediate variables that induce conditional independence among observables; if such variables are not in our vocabulary, we create them. In medical diagnosis, for instance, when some symptoms directly influence one another, the medical profession invents a name for that interaction (e.g. “syndrome”, “complication”, “pathological state”) and treats it as a new auxiliary variable that induces conditional independence.” - Pearl quoted in Bayesian Psychometric Modeling p61\n\nThe organisation of our data into meaningful categories or clusters is common in machine learning, but similarly in psychometrics we see the prevalence of factor models which are more explicitly structural models where we posit some unifying theme to the cluster of measures. In each tradition we are seeking conditional independence by allowing the model to account for these dependencies in how we structure the model. Pearl famously goes on to work-out conditions in the data-generating processes that lead to confounded causal inference stemming from inadequate conditionalisation strategies. Assessing different independence structures for our model represent a fruitful aspect of walking down through the garden of forking paths. There is then a difference between the exploratory seeking for some pre-determined story and an experimental seeking of clarity. With this in mind we now turn to types of modeling architecture that open up new expressive possibilities and offer a way to think about the relationships between observed data and latent factors that drive the observed outcomes. This greater expressive power offers different routes to inducing patterns of conditional exchangeability and different risks too."
  },
  {
    "objectID": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#expressive-structure-building-the-model-we-need",
    "href": "posts/post-with-code/CFA_AND_SEM/CFA_AND_SEM.html#expressive-structure-building-the-model-we-need",
    "title": "Measurement, Latent Factors and the Garden of Forking Paths",
    "section": "Expressive Structure: Building the Model we Need",
    "text": "Expressive Structure: Building the Model we Need\nIn some sense the modeling we’ve seen so far is simply inadequate to express our theory of the case. The hypothetical structure with which we started abstracted over the measures in our data. The theory talks of LIFE SATISFACTION and PARENTAL SUPPORT as grand holistic constructs, yet in our regression model(s) we have loosely coupled our measurements and the intended target via indirect but hopefully indicative measures. We can do better if we’re more explicit about how these manifest metrics ladder up to appropriate latent constructs. These latent constructs are theoretical posits we use to structure the nature of the inference and express the thematic unity between certain of our indicator variables. The metaphysically conservative theoretician may balk at such models, but it is practically akin to the usage of cluster variables in machine learning.\n\nConfirmatory Factor Analysis\nWe will illustrate the details of confirmatory factor modelling using the lavaan framework. The focus is mostly on the mechanics of how these models are estimated using maximum likelihood techniques before illustrating the salient differences of Bayesian estimation.\nFirst recall that the idea of confirmatory factor analysis is that there are some latent constructs which determine our data generating process. In our survey we’ve already clustered our questions by themes so it makes sense to extend this idea to posit latent constructs mapped to each of these themes. In the jargon of structural equation models this is called the measurement model i.e. a model which aims to “simply” predict/retrodict the measured outcomes before we try anything more sophisticated.\n\nmodel_measurement <- \"\n# Measurement model\nSUP_Parents =~ sup_parents_p1 + sup_parents_p2 + sup_parents_p3\nSUP_Friends =~ sup_friends_p1 + sup_friends_p2 + sup_friends_p3\nSE_Academic =~ se_acad_p1 + se_acad_p2 + se_acad_p3\nSE_Social =~ se_social_p1 + se_social_p2 + se_social_p3\nLS  =~ ls_p1 + ls_p2 + ls_p3\n\"\n\nmodel_measurement1 <- \"\n# Measurement model\nSUP_Parents =~ b1*sup_parents_p1 + b2*sup_parents_p2 + b3*sup_parents_p3\nSUP_Friends =~ a1*sup_friends_p1 + a2*sup_friends_p2 + a3*sup_friends_p3\nSE_Academic =~ c1*se_acad_p1 + c2*se_acad_p2 + c3*se_acad_p3\nSE_Social =~ se_social_p1 + se_social_p2 + se_social_p3\nLS  =~ ls_p1 + ls_p2 + ls_p3\n\na1 == a2 \na1 == a3\nb1 == b2\nb1 == b3\nc1 == c2\nc1 == c3\n\n\"\n\nfit_mod <- cfa(model_measurement, data = df)\nfit_mod_1<- cfa(model_measurement1, data = df)\n\nIn the above syntax we have specified two slightly different measurement models. In each case we allow that the questions of our survey are mapped to an appropriate latent factor e.g LS =~ ls_p1 + ls_p2 + ls_p3. The “=~” syntax denotes a “measured by” relationship in which the goal is to estimate how each of observed measurements load on the latent factor. In the first model we have allowed each of the factor loadings to be estimated freely, but in the second we have forced equal weights on the SUP_Friends construct. A benefit of this framework is that we do not have to resort to crude aggregations like sum-scores or mean-scores over the outcome variables we can allow that they vary freely and let the model estimate the multivariate relationships between the observed variables and these latent constructs.\nIf we plot the estimated parameters as before we’ll see some additional parameters reported.\n\ncfa_models = list(\"full_measurement_model\" = fit_mod, \n     \"measurement_model_reduced\" = fit_mod_1)\nmodelplot(cfa_models)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\nHere there are two distinct types of parameters: (i) the factor loadings accorded (=~) to the individual observed metrics and (ii) the covariances (~~) between the latent constructs. We can further report the extent of the model fit summaries.\n\nsummary(fit_mod, fit.measures = TRUE, standardized = TRUE) \n\nlavaan 0.6-18 ended normally after 49 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        40\n\n  Number of observations                           283\n\nModel Test User Model:\n                                                      \n  Test statistic                               223.992\n  Degrees of freedom                                80\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              2696.489\n  Degrees of freedom                               105\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.944\n  Tucker-Lewis Index (TLI)                       0.927\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4285.972\n  Loglikelihood unrestricted model (H1)      -4173.976\n                                                      \n  Akaike (AIC)                                8651.944\n  Bayesian (BIC)                              8797.761\n  Sample-size adjusted Bayesian (SABIC)       8670.921\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.080\n  90 Percent confidence interval - lower         0.067\n  90 Percent confidence interval - upper         0.092\n  P-value H_0: RMSEA <= 0.050                    0.000\n  P-value H_0: RMSEA >= 0.080                    0.500\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.056\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  SUP_Parents =~                                                        \n    sup_parents_p1    1.000                               0.935    0.873\n    sup_parents_p2    1.036    0.056   18.613    0.000    0.969    0.887\n    sup_parents_p3    0.996    0.059   16.754    0.000    0.932    0.816\n  SUP_Friends =~                                                        \n    sup_friends_p1    1.000                               1.021    0.906\n    sup_friends_p2    0.792    0.043   18.463    0.000    0.809    0.857\n    sup_friends_p3    0.891    0.050   17.741    0.000    0.910    0.831\n  SE_Academic =~                                                        \n    se_acad_p1        1.000                               0.695    0.878\n    se_acad_p2        0.809    0.050   16.290    0.000    0.562    0.820\n    se_acad_p3        0.955    0.058   16.500    0.000    0.664    0.829\n  SE_Social =~                                                          \n    se_social_p1      1.000                               0.638    0.843\n    se_social_p2      0.967    0.056   17.248    0.000    0.617    0.885\n    se_social_p3      0.928    0.067   13.880    0.000    0.592    0.741\n  LS =~                                                                 \n    ls_p1             1.000                               0.667    0.718\n    ls_p2             0.778    0.074   10.498    0.000    0.519    0.712\n    ls_p3             0.968    0.090   10.730    0.000    0.645    0.732\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  SUP_Parents ~~                                                        \n    SUP_Friends       0.132    0.064    2.073    0.038    0.138    0.138\n    SE_Academic       0.218    0.046    4.727    0.000    0.336    0.336\n    SE_Social         0.282    0.045    6.224    0.000    0.472    0.472\n    LS                0.405    0.057    7.132    0.000    0.650    0.650\n  SUP_Friends ~~                                                        \n    SE_Academic       0.071    0.047    1.493    0.136    0.100    0.100\n    SE_Social         0.196    0.046    4.281    0.000    0.301    0.301\n    LS                0.175    0.051    3.445    0.001    0.257    0.257\n  SE_Academic ~~                                                        \n    SE_Social         0.271    0.036    7.493    0.000    0.611    0.611\n    LS                0.238    0.039    6.065    0.000    0.514    0.514\n  SE_Social ~~                                                          \n    LS                0.321    0.042    7.659    0.000    0.755    0.755\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .sup_parents_p1    0.273    0.037    7.358    0.000    0.273    0.238\n   .sup_parents_p2    0.255    0.038    6.738    0.000    0.255    0.213\n   .sup_parents_p3    0.437    0.048    9.201    0.000    0.437    0.335\n   .sup_friends_p1    0.227    0.040    5.656    0.000    0.227    0.179\n   .sup_friends_p2    0.238    0.030    7.936    0.000    0.238    0.266\n   .sup_friends_p3    0.371    0.042    8.809    0.000    0.371    0.310\n   .se_acad_p1        0.144    0.022    6.593    0.000    0.144    0.229\n   .se_acad_p2        0.153    0.018    8.621    0.000    0.153    0.327\n   .se_acad_p3        0.200    0.024    8.372    0.000    0.200    0.313\n   .se_social_p1      0.166    0.020    8.134    0.000    0.166    0.290\n   .se_social_p2      0.106    0.016    6.542    0.000    0.106    0.217\n   .se_social_p3      0.288    0.028   10.132    0.000    0.288    0.451\n   .ls_p1             0.417    0.045    9.233    0.000    0.417    0.484\n   .ls_p2             0.261    0.028    9.321    0.000    0.261    0.492\n   .ls_p3             0.362    0.040    9.005    0.000    0.362    0.465\n    SUP_Parents       0.875    0.098    8.910    0.000    1.000    1.000\n    SUP_Friends       1.042    0.111    9.407    0.000    1.000    1.000\n    SE_Academic       0.483    0.054    8.880    0.000    1.000    1.000\n    SE_Social         0.407    0.048    8.403    0.000    1.000    1.000\n    LS                0.444    0.069    6.394    0.000    1.000    1.000\n\n\nNote how in addition to the individual parameter estimates the summaries highlight various measures of global model fit. These model fit statistics are important for evaluating alternative ways of parameterising our models. The number of parameters is a real concern in the maximum likelihood approaches to estimating these models. Too many parameters and we can easily over fit to the particular sample data. This stems in part from the limitations of the optimization goal in the traditional CFA framework - we are intent to optimize model parameters to recover a compelling estimate based on the observed covariance matrix. Once we have more parameters than there are points in the covariance matrix the model is free to overfit considerably. This can then be checked as measure of local model fit and may highlight infelicities or suspicious convergence between the true data generating process and the learned model.\n\ng1 = plot_heatmap(cov(df[,  drivers]))\n\ng2 = plot_heatmap(data.frame(fitted(fit_mod)$cov)[drivers, drivers], title=\"Model Implied Covariances\", \"Fitted Values\")\n\nresids = cov(df[,  drivers]) - data.frame(fitted(fit_mod)$cov)[drivers, drivers]\n\ng3 = plot_heatmap(resids, title=\"Residuals\", \"Fitted Values versus Observe Sample Covariance\")\n\n\nplot <- ggarrange(g1,g2,g3, ncol=1, nrow=3);\n\n\n\n\n\nSummary Global Fit Measures\nWe can also compare models based on their global measures of model fit giving some indication of whether parameter specifications improve or reduce fidelity with the true data generating process.\n\nsummary_df = cbind(fitMeasures(fit_mod, c(\"chisq\", \"baseline.chisq\", \"cfi\", \"aic\", \"bic\", \"rmsea\",\"srmr\")),\n      fitMeasures(fit_mod_1, c(\"chisq\", \"baseline.chisq\", \"cfi\", \"aic\", \"bic\", \"rmsea\",\"srmr\")))\ncolnames(summary_df) = c('Full Measurement Model', 'Reduced Measurement Model')\n\nsummary_df |> kable() |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n \n  \n     \n    Full Measurement Model \n    Reduced Measurement Model \n  \n \n\n  \n    chisq \n    223.9922306 \n    256.0287010 \n  \n  \n    baseline.chisq \n    2696.4887420 \n    2696.4887420 \n  \n  \n    cfi \n    0.9444365 \n    0.9343896 \n  \n  \n    aic \n    8651.9435210 \n    8671.9799914 \n  \n  \n    bic \n    8797.7613969 \n    8795.9251859 \n  \n  \n    rmsea \n    0.0797501 \n    0.0835831 \n  \n  \n    srmr \n    0.0558656 \n    0.0625710 \n  \n\n\n\n\n\nThere a wealth of metrics associated with CFA model fit and it can be hard to see the forest for the trees.\n\n\nVisualizing the Relationships\nOne of the better ways to visualize these models is to use the semPlot package. Here we can plot all the parameter estimates in one graph. Following convention the rectangular boxes represent observed measures. Oval or circular objects represent the latent constructs. The self-directed arrows on each node is the variance of that measure. The two-way arrows between nodes represents the covariance between those two nodes. The single headed arrows from the latent construct to the indicator variables denotes the factor loading of the variable on the construct. For instance SUP_F -> sp_f_3 is set at 0.89.\n\nsemPlot::semPaths(fit_mod, whatLabels = 'est', intercepts = FALSE)\n\n\n\n\nFor instance, in this plot you can see that for each latent construct one of the indicator variables has their factor loading set to 1. This is a mathematical requirement we’ll see below that is used to ensure identifiability of the parameters akin to setting a reference category in categorical regression. In the plot you can “read-off” the covariances between our constructs e.g. the covariance between LS and SUP_P is 0.36 the largest value amongst the set of covariances.\n\n\nComparing Models\nWe can use a variety of chi-squared tests to evaluate the goodness of fit for our models. If we pass in each model individually we perform a test comparing our model to the saturated model. The Chi-Squared test compares the model-implied variance-covariance matrix (expected) to the variance-covariance matrix computed from the actual data (observed). The null hypothesis for the Chi-Squared Goodness-of-Fit test is that the model fits the data perfectly, meaning that there is no significant difference between the observed and model-implied variance-covariance matrices. The goal is to see if the differences between these matrices are large enough that we can reject the null.\n\nlavTestLRT(fit_mod)\n\nChi-Squared Test Statistic (unscaled)\n\n          Df    AIC    BIC  Chisq Chisq diff Df diff           Pr(>Chisq)    \nSaturated  0                 0.00                                            \nModel     80 8651.9 8797.8 223.99     223.99      80 0.000000000000001443 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPassing in the one model we can reject the null hypothesis that the saturated model’s (perfect fit) and the candidate variance-covariance matrix are drawn from the same distribution. Comparing between our two model fits we also reject that these two models are drawn from the same distribution.\n\nlavTestLRT(fit_mod, fit_mod_1)\n\n\nChi-Squared Difference Test\n\n          Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff Pr(>Chisq)    \nfit_mod   80 8651.9 8797.8 223.99                                          \nfit_mod_1 86 8672.0 8795.9 256.03     32.036 0.12383       6 0.00001606 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe test also reports a number of other fit indices and the degrees of freedom available to each model. These are important because the Chi-Squared test is overly sensitive in large-sample data and the model adequacy is a multi-dimensional question.\n\n\n\nStructural Equation Models\nSo far so good. We have a confirmatory factor measurement model. We’ve structured it so that we can make inferences about the correlations and covariances between 5 latent constructs of independent interest. We’ve calibrated the model fit statistics by ensuring the model can reasonably recover the observed variance-covariance structure. But what about our dependency relations between constructs? We can evaluate these too! Adding regressions to our model allows to express these relationships and then recover summary statistics of the same.\n\nmodel <- \"\n# Measurement model\nSUP_Parents =~ sup_parents_p1 + sup_parents_p2 + sup_parents_p3\nSUP_Friends =~ sup_friends_p1 + sup_friends_p2 + sup_friends_p3\nSE_Academic =~ se_acad_p1 + se_acad_p2 + se_acad_p3\nSE_Social =~ se_social_p1 + se_social_p2 + se_social_p3\nLS  =~ ls_p1 + ls_p2 + ls_p3\n\n# Structural model \n# Regressions\nSE_Academic ~ SUP_Parents + SUP_Friends\nSE_Social ~ SUP_Parents + SUP_Friends\nLS ~ SE_Academic + SE_Social + SUP_Parents + SUP_Friends\n\n# Residual covariances\nSE_Academic ~~ SE_Social\n\"\n\nfit_mod_sem <- sem(model, data = df)\n\nThis model structure adds a set of directional arrows between the latent constructs\n\nsemPlot::semPaths(fit_mod_sem, whatLabels = 'est', intercepts = FALSE)\n\n\n\n\nThe relative uncertainty in each estimates\n\nmodelplot(fit_mod_sem)\n\n\n\n\nCompare this structure against the previous and simpler measurement model and we observe a puzzling phenomena. The models report identical measures of fit! Despite the extra structure in the SEM model the optimization routine has returned identical parameter specifications.\n\nsummary_df = cbind(fitMeasures(fit_mod, c(\"chisq\", \"baseline.chisq\", \"cfi\", \"aic\", \"bic\", \"rmsea\",\"srmr\")),\n      fitMeasures(fit_mod_sem, c(\"chisq\", \"baseline.chisq\", \"cfi\", \"aic\", \"bic\", \"rmsea\",\"srmr\")),\n      fitMeasures(fit_mod_1, c(\"chisq\", \"baseline.chisq\", \"cfi\", \"aic\", \"bic\", \"rmsea\",\"srmr\")))\ncolnames(summary_df) = c('Measurement Model', 'SEM Model', 'Reduced Measurement Model')\n\nsummary_df |> kable() |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n \n  \n     \n    Measurement Model \n    SEM Model \n    Reduced Measurement Model \n  \n \n\n  \n    chisq \n    223.9922306 \n    223.9922306 \n    256.0287010 \n  \n  \n    baseline.chisq \n    2696.4887420 \n    2696.4887420 \n    2696.4887420 \n  \n  \n    cfi \n    0.9444365 \n    0.9444365 \n    0.9343896 \n  \n  \n    aic \n    8651.9435210 \n    8651.9435210 \n    8671.9799914 \n  \n  \n    bic \n    8797.7613969 \n    8797.7613969 \n    8795.9251859 \n  \n  \n    rmsea \n    0.0797501 \n    0.0797501 \n    0.0835831 \n  \n  \n    srmr \n    0.0558656 \n    0.0558655 \n    0.0625710 \n  \n\n\n\n\n\nThe issue here is that the models have the same degrees of freedom which suggests in some sense we have already saturated our model fit and are unable to evaluate further parameter estimates.\n\nlavTestLRT(fit_mod_sem, fit_mod)\n\nWarning: lavaan->lavTestLRT():  \n   some models have the same degrees of freedom\n\n\n\nChi-Squared Difference Test\n\n            Df    AIC    BIC  Chisq      Chisq diff RMSEA Df diff Pr(>Chisq)\nfit_mod_sem 80 8651.9 8797.8 223.99                                         \nfit_mod     80 8651.9 8797.8 223.99 0.0000000043218     0       0           \n\nlavTestLRT(fit_mod_sem, fit_mod_1)\n\n\nChi-Squared Difference Test\n\n            Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff Pr(>Chisq)    \nfit_mod_sem 80 8651.9 8797.8 223.99                                          \nfit_mod_1   86 8672.0 8795.9 256.03     32.036 0.12383       6 0.00001606 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can see this similarly in the plotted residuals which are identical across the models despite meaningful structural differences.\n\nheat_df = data.frame(resid(fit_mod)$cov) \nheat_df = heat_df |> as.matrix() |> melt()\ncolnames(heat_df) <- c(\"x\", \"y\", \"value\")\n\ng1 = heat_df  |> mutate(value = round(value, 2)) |>  ggplot(aes(x = x, y = y, fill = value)) +\n  geom_tile() + geom_text(aes(label = value), color = \"black\", size = 4) +\n scale_fill_gradient2(\n    high = 'dodgerblue4',\n    mid = 'white',\n    low = 'firebrick2'\n  ) + theme(axis.text.x = element_text(angle=45)) + ggtitle(\"Residuals of the Sample Covariances and Model Implied Covariances\", \"A Visual Check of Structural-Model fit\")\n\n\nheat_df = data.frame(resid(fit_mod_sem)$cov) \nheat_df = heat_df |> as.matrix() |> melt()\ncolnames(heat_df) <- c(\"x\", \"y\", \"value\")\n\ng2 = heat_df  |> mutate(value = round(value, 2)) |>  ggplot(aes(x = x, y = y, fill = value)) +\n  geom_tile() + geom_text(aes(label = value), color = \"black\", size = 4) +\n scale_fill_gradient2(\n    high = 'dodgerblue4',\n    mid = 'white',\n    low = 'firebrick2'\n  ) + theme(axis.text.x = element_text(angle=45)) + ggtitle(\"Residuals of the Sample Covariances and Model Implied Covariances\", \"A Visual Check of Measurement-Model fit\")\n\n\nplot <- ggarrange(g1,g2, ncol=1, nrow=2);\n\n\n\n\nThis is a genuine limitation in the expressive power of SEM models when they are fit using maximum likelihood while optimizing for fidelity to the sample variance-covariance matrix. Model identification possibilities are constrained by the optimisation goal and the degrees of freedom available to the optimiser. This constrains the model structures we can evaluate. Next we’ll show how the Bayesian approach to estimating these models frees us from this limitation.\n\n\nConfirmatory Factor Models with PyMC\nSo far we’ve seen CFA and SEM models specified using a kind of formula syntax. This useful but here we switch to the distributional perspective which is more directly relevant to the Bayesian phrasing of the CFA model. We’ll unpack these components of the distributional view with a simpler data set initially for illustrative purposes and then we’ll switch back to the Life Satisfaction question.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pytensor import tensor as pt\nimport arviz as az\nimport networkx as nx\nnp.random.seed(150)\n\n\n\ndf_p = pd.read_csv('IIS.dat', sep='\\s+')\ndf_p.head()\n\n     PI    AD   IGC   FI   FC\n0  4.00  3.38  4.67  2.6  3.2\n1  2.57  3.00  3.50  2.4  2.8\n2  2.29  3.29  4.83  2.0  3.4\n3  2.43  3.63  4.33  3.6  3.8\n4  3.00  4.00  4.83  3.4  3.8\n\n\nWe have here a data-set drawn from Mislevy and Levy. which records measures of students collegiate experiences along multiple dimensions. The data here represents the averages of PI peer interaction, AD academic development IGC goal commitment, FI faculty interaction and FC faculty concern items for 500 hundred students. We will fit a 2 factor Bayesian CFA model to this data set - one factor for he faculty items and the other factor for self-reporting items. First consider the code and then we’ll step through it in more detail.\n\ncoords = {'obs': list(range(len(df_p))), \n          'indicators': ['PI', 'AD',    'IGC', 'FI', 'FC'],\n          'indicators_1': ['PI', 'AD',  'IGC'],\n          'indicators_2': ['FI', 'FC'],\n          'latent': ['Student', 'Faculty']\n          }\n\n\nobs_idx = list(range(len(df_p)))\nwith pm.Model(coords=coords) as model:\n  \n  Psi = pm.InverseGamma('Psi', 5, 10, dims='indicators')\n  lambdas_ = pm.Normal('lambdas_1', 1, 10, dims=('indicators_1'))\n  # Force a fixed scale on the factor loadings for factor 1\n  lambdas_1 = pm.Deterministic('lambdas1', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_1'))\n  lambdas_ = pm.Normal('lambdas_2', 1, 10, dims=('indicators_2'))\n  # Force a fixed scale on the factor loadings for factor 2\n  lambdas_2 = pm.Deterministic('lambdas2', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_2'))\n  tau = pm.Normal('tau', 3, 10, dims='indicators')\n  # Specify covariance structure between latent factors\n  kappa = 0\n  sd_dist = pm.Exponential.dist(1.0, shape=2)\n  chol, _, _ = pm.LKJCholeskyCov('chol_cov', n=2, eta=2,\n    sd_dist=sd_dist, compute_corr=True)\n  ksi = pm.MvNormal('ksi', kappa, chol=chol, dims=('obs', 'latent'))\n\n  # Construct Observation matrix\n  m1 = tau[0] + ksi[obs_idx, 0]*lambdas_1[0]\n  m2 = tau[1] + ksi[obs_idx, 0]*lambdas_1[1]\n  m3 = tau[2] + ksi[obs_idx, 0]*lambdas_1[2]\n  m4 = tau[3] + ksi[obs_idx, 1]*lambdas_2[0]\n  m5 = tau[4] + ksi[obs_idx, 1]*lambdas_2[1]\n  \n  mu = pm.Deterministic('mu', pm.math.stack([m1, m2, m3, m4, m5]).T)\n  _  = pm.Normal('likelihood', mu, Psi, observed=df_p.values)\n\n  idata = pm.sample(nuts_sampler='numpyro', target_accept=.95, \n                    idata_kwargs={\"log_likelihood\": True})\n  idata.extend(pm.sample_posterior_predictive(idata))\n  \nsummary_df = az.summary(idata, var_names=['lambdas1', 'lambdas2', 'tau', 'Psi', 'ksi', 'chol_cov_corr'], coords= {'obs': [0, 7]})\n\nThe model specification here has a Normal likelihood term which takes as observed data the matrix of student responses. However we construct the input for the likelihood term in a way that we posit two latent factor variables ksi[obs_idx, 0] and ksi[obs_idx, 1] respectively. These factors are modified by the factor loading terms in e.g. lambdas_1 specification to create a regression model m1 which predicts the indicator variable PI. The lambda terms are drawn from a shared multivariate normal distribution with a weakly informative prior and fed forward into our pseudo outcome matrix mu. In this way we allow that the latent factors are related to the observed data matrix. The process of Bayesian updating then calibrates their values against observed data and modifies our priors for the parameterisations of the lambdas, ksi terms. This reflects what the model learns about the interactions between the latent constructs. Crucially we force a covariance structure on ksi terms to reflect the correlations and covariance between these constructs of interest, and we specify the factor loading terms to have a required scale by forcing the first indicator loading to 1 for each factor.\nIn a Bayesian approach to the estimation of these models we are not limited by the degrees of freedom available in the optimisation routine. Instead we’re aiming to estimate the posterior distribution using MCMC on draws from well specified priors on each the quantities listed above. In a picture the structure is like this:\n\n\n\nPyMC Confirmatory Factor Model\n\n\nWe are conditioning on the observed data and seek to estimate sound parameterisations of the ksi, lambda and tau that go into building our mu structure. The marginal distribution of the observables is derived by integrating over the distribution of the latent variables in this fashion so that:\n\\[ p(x_{i}.....x_{n} | ksi, \\Psi, \\tau, \\Lambda) \\sim Normal(\\tau + \\Lambda\\cdot ksi, \\Psi) \\] For more details on the precise derivation of this likelihood term we recommend the discussion in the book. It is this same likelihood specification that is used in maximum likelihood fits of the same and the Normal assumption has proved viable and computationally tractable. The model yields summary statistics for each of these parameters.\n\npy$summary_df |> kable() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n \n  \n     \n    mean \n    sd \n    hdi_3% \n    hdi_97% \n    mcse_mean \n    mcse_sd \n    ess_bulk \n    ess_tail \n    r_hat \n  \n \n\n  \n    lambdas1[PI] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    lambdas1[AD] \n    0.904 \n    0.062 \n    0.788 \n    1.022 \n    0.004 \n    0.003 \n    279 \n    594 \n    1.02 \n  \n  \n    lambdas1[IGC] \n    0.538 \n    0.046 \n    0.457 \n    0.630 \n    0.002 \n    0.002 \n    405 \n    948 \n    1.01 \n  \n  \n    lambdas2[FI] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    lambdas2[FC] \n    0.981 \n    0.057 \n    0.878 \n    1.087 \n    0.002 \n    0.002 \n    606 \n    1087 \n    1.00 \n  \n  \n    tau[PI] \n    3.333 \n    0.036 \n    3.269 \n    3.405 \n    0.001 \n    0.001 \n    712 \n    1505 \n    1.01 \n  \n  \n    tau[AD] \n    3.897 \n    0.026 \n    3.848 \n    3.946 \n    0.001 \n    0.001 \n    471 \n    1120 \n    1.01 \n  \n  \n    tau[IGC] \n    4.596 \n    0.020 \n    4.558 \n    4.636 \n    0.001 \n    0.001 \n    708 \n    1302 \n    1.01 \n  \n  \n    tau[FI] \n    3.033 \n    0.039 \n    2.951 \n    3.098 \n    0.002 \n    0.001 \n    607 \n    1286 \n    1.01 \n  \n  \n    tau[FC] \n    3.712 \n    0.034 \n    3.650 \n    3.780 \n    0.002 \n    0.001 \n    488 \n    1058 \n    1.01 \n  \n  \n    Psi[PI] \n    0.611 \n    0.024 \n    0.564 \n    0.652 \n    0.001 \n    0.000 \n    1558 \n    2805 \n    1.00 \n  \n  \n    Psi[AD] \n    0.316 \n    0.020 \n    0.279 \n    0.352 \n    0.001 \n    0.001 \n    654 \n    1670 \n    1.01 \n  \n  \n    Psi[IGC] \n    0.355 \n    0.014 \n    0.329 \n    0.379 \n    0.000 \n    0.000 \n    2974 \n    2704 \n    1.00 \n  \n  \n    Psi[FI] \n    0.569 \n    0.025 \n    0.519 \n    0.615 \n    0.001 \n    0.000 \n    1447 \n    2467 \n    1.00 \n  \n  \n    Psi[FC] \n    0.420 \n    0.026 \n    0.372 \n    0.468 \n    0.001 \n    0.001 \n    720 \n    1261 \n    1.00 \n  \n  \n    ksi[0, Student] \n    -0.230 \n    0.223 \n    -0.666 \n    0.180 \n    0.004 \n    0.003 \n    3840 \n    3014 \n    1.00 \n  \n  \n    ksi[0, Faculty] \n    -0.370 \n    0.275 \n    -0.890 \n    0.156 \n    0.004 \n    0.003 \n    4335 \n    3051 \n    1.00 \n  \n  \n    ksi[7, Student] \n    0.894 \n    0.228 \n    0.454 \n    1.304 \n    0.004 \n    0.003 \n    2847 \n    2782 \n    1.00 \n  \n  \n    ksi[7, Faculty] \n    0.876 \n    0.275 \n    0.387 \n    1.433 \n    0.004 \n    0.003 \n    3791 \n    2812 \n    1.00 \n  \n  \n    chol_cov_corr[0, 0] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    chol_cov_corr[0, 1] \n    0.849 \n    0.028 \n    0.794 \n    0.899 \n    0.001 \n    0.001 \n    415 \n    615 \n    1.01 \n  \n  \n    chol_cov_corr[1, 0] \n    0.849 \n    0.028 \n    0.794 \n    0.899 \n    0.001 \n    0.001 \n    415 \n    615 \n    1.01 \n  \n  \n    chol_cov_corr[1, 1] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    3785 \n    3584 \n    1.00 \n  \n\n\n\n\n\nWe can additionally check the diagnostic trace plots to ensure that the model has sampled well even across the relatively complex structure.\n\naz.plot_trace(idata, var_names=['lambdas1', 'lambdas2', 'tau', 'Psi', 'ksi']);\n\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:487: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n/Users/nathanielforde/mambaforge/envs/pymc_causal/lib/python3.11/site-packages/arviz/stats/density_utils.py:487: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n\n\n\n\n\nThese diagnostic plots and parameter estimates reflect well the same figures reported by Levy and Mislevy, so we won’t dwell too much further on this model only to say that the basic structure here is expanded upon as we apply more factor structures and SEM like structures to our original Life-Satisfaction data-set.\n\nFactoring for Life Satisfaction\nNow we want fit the life-satisfaction measurement model using PyMC. This has fundamentally a similar structure to the simple CFA model we’ve seen above. However we now have 5 factors with 3 factor loadings to be estimated for three indicators per factor. We will fit this model and then show how to pull out some of model fit statistics that are relevant for the assessment of the model.\n\n\ndf = pd.read_csv('sem_data.csv')\ndrivers = ['se_acad_p1', 'se_acad_p2',\n       'se_acad_p3', 'se_social_p1', 'se_social_p2', 'se_social_p3',\n       'sup_friends_p1', 'sup_friends_p2', 'sup_friends_p3', 'sup_parents_p1',\n       'sup_parents_p2', 'sup_parents_p3', 'ls_p1', 'ls_p2', 'ls_p3']\n       \n\n\ncoords = {'obs': list(range(len(df))), \n          'indicators': drivers,\n          'indicators_1': ['se_acad_p1','se_acad_p2','se_acad_p3'],\n          'indicators_2': ['se_social_p1','se_social_p2','se_social_p3'],\n          'indicators_3': ['sup_friends_p1','sup_friends_p2','sup_friends_p3'],\n          'indicators_4': [ 'sup_parents_p1','sup_parents_p2','sup_parents_p3'],\n          'indicators_5': ['ls_p1','ls_p2', 'ls_p3'],\n          'latent': ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS'],\n          'latent1': ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']\n          }\n\nobs_idx = list(range(len(df)))\nwith pm.Model(coords=coords) as model:\n  \n  Psi = pm.InverseGamma('Psi', 5, 10, dims='indicators')\n  lambdas_ = pm.Normal('lambdas_1', 1, 10, dims=('indicators_1'))\n  lambdas_1 = pm.Deterministic('lambdas1', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_1'))\n  lambdas_ = pm.Normal('lambdas_2', 1, 10, dims=('indicators_2'))\n  lambdas_2 = pm.Deterministic('lambdas2', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_2'))\n  lambdas_ = pm.Normal('lambdas_3', 1, 10, dims=('indicators_3'))\n  lambdas_3 = pm.Deterministic('lambdas3', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_3'))\n  lambdas_ = pm.Normal('lambdas_4', 1, 10, dims=('indicators_4'))\n  lambdas_4 = pm.Deterministic('lambdas4', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_4'))\n  lambdas_ = pm.Normal('lambdas_5', 1, 10, dims=('indicators_5'))\n  lambdas_5 = pm.Deterministic('lambdas5', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_5'))\n  tau = pm.Normal('tau', 3, 10, dims='indicators')\n  kappa = 0\n  sd_dist = pm.Exponential.dist(1.0, shape=5)\n  chol, _, _ = pm.LKJCholeskyCov('chol_cov', n=5, eta=2,\n    sd_dist=sd_dist, compute_corr=True)\n  cov = pm.Deterministic(\"cov\", chol.dot(chol.T), dims=('latent', 'latent1'))\n  ksi = pm.MvNormal('ksi', kappa, chol=chol, dims=('obs', 'latent'))\n\n  m0 = tau[0] + ksi[obs_idx, 0]*lambdas_1[0]\n  m1 = tau[1] + ksi[obs_idx, 0]*lambdas_1[1]\n  m2 = tau[2] + ksi[obs_idx, 0]*lambdas_1[2]\n  m3 = tau[3] + ksi[obs_idx, 1]*lambdas_2[0]\n  m4 = tau[4] + ksi[obs_idx, 1]*lambdas_2[1]\n  m5 = tau[5] + ksi[obs_idx, 1]*lambdas_2[2]\n  m6 = tau[6] + ksi[obs_idx, 2]*lambdas_3[0]\n  m7 = tau[7] + ksi[obs_idx, 2]*lambdas_3[1]\n  m8 = tau[8] + ksi[obs_idx, 2]*lambdas_3[2]\n  m9 = tau[9] + ksi[obs_idx, 3]*lambdas_4[0]\n  m10 = tau[10] + ksi[obs_idx, 3]*lambdas_4[1]\n  m11 = tau[11] + ksi[obs_idx, 3]*lambdas_4[2]\n  m12 = tau[12] + ksi[obs_idx, 4]*lambdas_5[0]\n  m13 = tau[13] + ksi[obs_idx, 4]*lambdas_5[1]\n  m14 = tau[14] + ksi[obs_idx, 4]*lambdas_5[2]\n  \n  mu = pm.Deterministic('mu', pm.math.stack([m0, m1, m2, m3, m4, m5, m6, m7,\n                                             m8, m9, m10, m11, m12, m13, m14]).T)\n  _  = pm.Normal('likelihood', mu, Psi, observed=df[drivers].values)\n\n  idata = pm.sample(nuts_sampler='numpyro', target_accept=.95, tune=1000,\n                    idata_kwargs={\"log_likelihood\": True}, random_seed=100)\n  idata.extend(pm.sample_posterior_predictive(idata))\n  \nsummary_df1 = az.summary(idata, var_names=['lambdas1', 'lambdas2', 'lambdas3', 'lambdas4', 'lambdas5', 'tau', 'Psi'])\ncov_df = pd.DataFrame(az.extract(idata['posterior'])['cov'].mean(axis=2))\ncov_df.index = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']\ncov_df.columns = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']\n\ncorrelation_df = pd.DataFrame(az.extract(idata['posterior'])['chol_cov_corr'].mean(axis=2))\ncorrelation_df.index = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']\ncorrelation_df.columns = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']\n\nfactor_loadings = pd.DataFrame(az.summary(idata, var_names=['lambdas1', 'lambdas2', 'lambdas3', 'lambdas4', 'lambdas5'])['mean']).reset_index()\nfactor_loadings['factor'] = factor_loadings['index'].str.split('[', expand=True)[0]\nfactor_loadings.columns =['factor_loading', 'factor_loading_weight', 'factor']\nfactor_loadings['factor_loading_weight_sq'] = factor_loadings['factor_loading_weight']**2\nfactor_loadings['sum_sq_loadings'] = factor_loadings.groupby('factor')['factor_loading_weight_sq'].transform(sum)\n\nHere we’ve calculated a bunch of summary statistics which we will highlight below, but first observe the analogous model structure here to the simple CFA model above.\n Next we’ll plot the posterior summary statistics for each of the key parameters.\n\npy$summary_df1 |> kable() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\n \n  \n     \n    mean \n    sd \n    hdi_3% \n    hdi_97% \n    mcse_mean \n    mcse_sd \n    ess_bulk \n    ess_tail \n    r_hat \n  \n \n\n  \n    lambdas1[se_acad_p1] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    lambdas1[se_acad_p2] \n    0.817 \n    0.052 \n    0.720 \n    0.915 \n    0.001 \n    0.001 \n    1193 \n    2008 \n    1.00 \n  \n  \n    lambdas1[se_acad_p3] \n    0.967 \n    0.060 \n    0.854 \n    1.076 \n    0.002 \n    0.001 \n    1286 \n    2037 \n    1.00 \n  \n  \n    lambdas2[se_social_p1] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    lambdas2[se_social_p2] \n    0.965 \n    0.058 \n    0.856 \n    1.071 \n    0.002 \n    0.002 \n    757 \n    1634 \n    1.00 \n  \n  \n    lambdas2[se_social_p3] \n    0.941 \n    0.072 \n    0.805 \n    1.076 \n    0.002 \n    0.002 \n    878 \n    1580 \n    1.00 \n  \n  \n    lambdas3[sup_friends_p1] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    lambdas3[sup_friends_p2] \n    0.802 \n    0.044 \n    0.720 \n    0.887 \n    0.001 \n    0.001 \n    1045 \n    1701 \n    1.00 \n  \n  \n    lambdas3[sup_friends_p3] \n    0.905 \n    0.053 \n    0.805 \n    1.006 \n    0.002 \n    0.001 \n    1235 \n    2150 \n    1.00 \n  \n  \n    lambdas4[sup_parents_p1] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    lambdas4[sup_parents_p2] \n    1.040 \n    0.059 \n    0.931 \n    1.150 \n    0.002 \n    0.002 \n    758 \n    1383 \n    1.00 \n  \n  \n    lambdas4[sup_parents_p3] \n    1.010 \n    0.064 \n    0.898 \n    1.137 \n    0.002 \n    0.001 \n    1051 \n    1840 \n    1.00 \n  \n  \n    lambdas5[ls_p1] \n    1.000 \n    0.000 \n    1.000 \n    1.000 \n    0.000 \n    0.000 \n    4000 \n    4000 \n    NaN \n  \n  \n    lambdas5[ls_p2] \n    0.791 \n    0.085 \n    0.627 \n    0.944 \n    0.004 \n    0.003 \n    541 \n    1074 \n    1.00 \n  \n  \n    lambdas5[ls_p3] \n    0.990 \n    0.103 \n    0.806 \n    1.187 \n    0.004 \n    0.003 \n    543 \n    878 \n    1.00 \n  \n  \n    tau[se_acad_p1] \n    5.153 \n    0.044 \n    5.069 \n    5.234 \n    0.002 \n    0.001 \n    488 \n    1151 \n    1.01 \n  \n  \n    tau[se_acad_p2] \n    5.345 \n    0.039 \n    5.271 \n    5.414 \n    0.002 \n    0.001 \n    528 \n    1033 \n    1.01 \n  \n  \n    tau[se_acad_p3] \n    5.209 \n    0.045 \n    5.127 \n    5.297 \n    0.002 \n    0.001 \n    526 \n    1290 \n    1.01 \n  \n  \n    tau[se_social_p1] \n    5.286 \n    0.042 \n    5.208 \n    5.366 \n    0.002 \n    0.002 \n    380 \n    743 \n    1.01 \n  \n  \n    tau[se_social_p2] \n    5.473 \n    0.039 \n    5.397 \n    5.544 \n    0.002 \n    0.001 \n    363 \n    742 \n    1.01 \n  \n  \n    tau[se_social_p3] \n    5.437 \n    0.045 \n    5.351 \n    5.522 \n    0.002 \n    0.001 \n    492 \n    982 \n    1.00 \n  \n  \n    tau[sup_friends_p1] \n    5.782 \n    0.068 \n    5.651 \n    5.904 \n    0.004 \n    0.003 \n    333 \n    763 \n    1.01 \n  \n  \n    tau[sup_friends_p2] \n    6.007 \n    0.057 \n    5.909 \n    6.125 \n    0.003 \n    0.002 \n    397 \n    872 \n    1.00 \n  \n  \n    tau[sup_friends_p3] \n    5.987 \n    0.066 \n    5.864 \n    6.115 \n    0.003 \n    0.002 \n    385 \n    890 \n    1.01 \n  \n  \n    tau[sup_parents_p1] \n    5.973 \n    0.061 \n    5.858 \n    6.085 \n    0.003 \n    0.002 \n    427 \n    1059 \n    1.00 \n  \n  \n    tau[sup_parents_p2] \n    5.925 \n    0.062 \n    5.807 \n    6.040 \n    0.003 \n    0.002 \n    394 \n    924 \n    1.01 \n  \n  \n    tau[sup_parents_p3] \n    5.716 \n    0.066 \n    5.596 \n    5.840 \n    0.003 \n    0.002 \n    470 \n    1294 \n    1.00 \n  \n  \n    tau[ls_p1] \n    5.188 \n    0.053 \n    5.092 \n    5.289 \n    0.002 \n    0.001 \n    654 \n    1378 \n    1.00 \n  \n  \n    tau[ls_p2] \n    5.775 \n    0.041 \n    5.693 \n    5.849 \n    0.002 \n    0.001 \n    716 \n    1596 \n    1.00 \n  \n  \n    tau[ls_p3] \n    5.219 \n    0.051 \n    5.121 \n    5.314 \n    0.002 \n    0.001 \n    666 \n    1609 \n    1.00 \n  \n  \n    Psi[se_acad_p1] \n    0.412 \n    0.028 \n    0.359 \n    0.465 \n    0.001 \n    0.001 \n    1278 \n    1740 \n    1.00 \n  \n  \n    Psi[se_acad_p2] \n    0.413 \n    0.024 \n    0.367 \n    0.456 \n    0.001 \n    0.000 \n    2170 \n    2268 \n    1.00 \n  \n  \n    Psi[se_acad_p3] \n    0.468 \n    0.027 \n    0.418 \n    0.519 \n    0.001 \n    0.000 \n    1844 \n    2408 \n    1.00 \n  \n  \n    Psi[se_social_p1] \n    0.431 \n    0.026 \n    0.381 \n    0.477 \n    0.001 \n    0.000 \n    1382 \n    2219 \n    1.00 \n  \n  \n    Psi[se_social_p2] \n    0.361 \n    0.025 \n    0.314 \n    0.405 \n    0.001 \n    0.000 \n    1486 \n    2135 \n    1.00 \n  \n  \n    Psi[se_social_p3] \n    0.553 \n    0.029 \n    0.500 \n    0.606 \n    0.001 \n    0.000 \n    2594 \n    2803 \n    1.00 \n  \n  \n    Psi[sup_friends_p1] \n    0.517 \n    0.040 \n    0.439 \n    0.587 \n    0.001 \n    0.001 \n    866 \n    1739 \n    1.00 \n  \n  \n    Psi[sup_friends_p2] \n    0.508 \n    0.031 \n    0.454 \n    0.568 \n    0.001 \n    0.001 \n    1420 \n    1985 \n    1.00 \n  \n  \n    Psi[sup_friends_p3] \n    0.625 \n    0.036 \n    0.562 \n    0.694 \n    0.001 \n    0.001 \n    2090 \n    2329 \n    1.00 \n  \n  \n    Psi[sup_parents_p1] \n    0.550 \n    0.035 \n    0.485 \n    0.615 \n    0.001 \n    0.001 \n    1530 \n    2075 \n    1.00 \n  \n  \n    Psi[sup_parents_p2] \n    0.536 \n    0.038 \n    0.465 \n    0.605 \n    0.001 \n    0.001 \n    1192 \n    2078 \n    1.01 \n  \n  \n    Psi[sup_parents_p3] \n    0.675 \n    0.038 \n    0.602 \n    0.745 \n    0.001 \n    0.001 \n    2089 \n    2371 \n    1.00 \n  \n  \n    Psi[ls_p1] \n    0.671 \n    0.038 \n    0.603 \n    0.744 \n    0.001 \n    0.001 \n    1045 \n    2387 \n    1.00 \n  \n  \n    Psi[ls_p2] \n    0.534 \n    0.030 \n    0.477 \n    0.591 \n    0.001 \n    0.001 \n    1409 \n    2472 \n    1.00 \n  \n  \n    Psi[ls_p3] \n    0.622 \n    0.035 \n    0.554 \n    0.688 \n    0.001 \n    0.001 \n    1597 \n    2393 \n    1.00 \n  \n\n\n\n\n\nUsing a forest plots it’s easier to see how the uncertainty attaches to some of these estimates.\n\n\nfig, ax = plt.subplots(figsize=(15, 8))\naz.plot_forest(idata, var_names=['lambdas1', 'lambdas2', 'lambdas3', 'lambdas4', 'lambdas5'], combined=True, ax=ax);\nax.set_title(\"Factor Loadings for each of the Five Factors\");\n\n\n\n\nWe can also pull out the covariances between our latent constructs. Highlighting the key covariances relations between support and self-efficacy measures with the life-satisfaction outcome.\n\npy$cov_df |> kable(caption= \"Covariances Amongst Latent Factors\",digits=2) |> kable_styling() %>% kable_classic(full_width = F, html_font = \"Cambria\") |> row_spec(5, color = \"red\")\n\n\n\nCovariances Amongst Latent Factors\n \n  \n     \n    SE_ACAD \n    SE_SOCIAL \n    SUP_F \n    SUP_P \n    LS \n  \n \n\n  \n    SE_ACAD \n    0.47 \n    0.26 \n    0.06 \n    0.20 \n    0.22 \n  \n  \n    SE_SOCIAL \n    0.26 \n    0.39 \n    0.19 \n    0.26 \n    0.30 \n  \n  \n    SUP_F \n    0.06 \n    0.19 \n    1.03 \n    0.12 \n    0.16 \n  \n  \n    SUP_P \n    0.20 \n    0.26 \n    0.12 \n    0.86 \n    0.38 \n  \n  \n    LS \n    0.22 \n    0.30 \n    0.16 \n    0.38 \n    0.42 \n  \n\n\n\n\n\nOne benefit of fitting a mutlivariate normal relationship between these factors is that we can pull and translate the covariance relation in terms of the correlation patterns implied.\n\npy$correlation_df |> kable( caption= \"Correlations Amongst Latent Factors\", digits=2) |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\") |> row_spec(5, color = \"red\")\n\n\n\nCorrelations Amongst Latent Factors\n \n  \n     \n    SE_ACAD \n    SE_SOCIAL \n    SUP_F \n    SUP_P \n    LS \n  \n \n\n  \n    SE_ACAD \n    1.00 \n    0.60 \n    0.09 \n    0.32 \n    0.50 \n  \n  \n    SE_SOCIAL \n    0.60 \n    1.00 \n    0.29 \n    0.45 \n    0.75 \n  \n  \n    SUP_F \n    0.09 \n    0.29 \n    1.00 \n    0.12 \n    0.25 \n  \n  \n    SUP_P \n    0.32 \n    0.45 \n    0.12 \n    1.00 \n    0.64 \n  \n  \n    LS \n    0.50 \n    0.75 \n    0.25 \n    0.64 \n    1.00 \n  \n\n\n\n\n\nAnother lens on the factors is the relative factor strength which highlights where the majority of substantive variation exists in our data-set.\n\npy$factor_loadings[ c('factor', 'factor_loading', 'factor_loading_weight', 'factor_loading_weight_sq', 'sum_sq_loadings')] |> kable( caption= \"Factor Loadings and Sum of Squared loadings\", digits=2) |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\") |> column_spec(5, bold=TRUE, color = ifelse(py$factor_loadings$sum_sq_loadings > 2.8, \"red\", \"black\"))\n\n\n\nFactor Loadings and Sum of Squared loadings\n \n  \n    factor \n    factor_loading \n    factor_loading_weight \n    factor_loading_weight_sq \n    sum_sq_loadings \n  \n \n\n  \n    lambdas1 \n    lambdas1[se_acad_p1] \n    1.00 \n    1.00 \n    2.60 \n  \n  \n    lambdas1 \n    lambdas1[se_acad_p2] \n    0.82 \n    0.67 \n    2.60 \n  \n  \n    lambdas1 \n    lambdas1[se_acad_p3] \n    0.97 \n    0.94 \n    2.60 \n  \n  \n    lambdas2 \n    lambdas2[se_social_p1] \n    1.00 \n    1.00 \n    2.82 \n  \n  \n    lambdas2 \n    lambdas2[se_social_p2] \n    0.96 \n    0.93 \n    2.82 \n  \n  \n    lambdas2 \n    lambdas2[se_social_p3] \n    0.94 \n    0.89 \n    2.82 \n  \n  \n    lambdas3 \n    lambdas3[sup_friends_p1] \n    1.00 \n    1.00 \n    2.46 \n  \n  \n    lambdas3 \n    lambdas3[sup_friends_p2] \n    0.80 \n    0.64 \n    2.46 \n  \n  \n    lambdas3 \n    lambdas3[sup_friends_p3] \n    0.90 \n    0.82 \n    2.46 \n  \n  \n    lambdas4 \n    lambdas4[sup_parents_p1] \n    1.00 \n    1.00 \n    3.10 \n  \n  \n    lambdas4 \n    lambdas4[sup_parents_p2] \n    1.04 \n    1.08 \n    3.10 \n  \n  \n    lambdas4 \n    lambdas4[sup_parents_p3] \n    1.01 \n    1.02 \n    3.10 \n  \n  \n    lambdas5 \n    lambdas5[ls_p1] \n    1.00 \n    1.00 \n    2.61 \n  \n  \n    lambdas5 \n    lambdas5[ls_p2] \n    0.79 \n    0.63 \n    2.61 \n  \n  \n    lambdas5 \n    lambdas5[ls_p3] \n    0.99 \n    0.98 \n    2.61 \n  \n\n\n\n\n\nSo we can evaluate some of the similar relationships at the level of summary statistics, but more powerfully we’ve built a generative model of the process yielding our observed data and as such we can evaluate the fidelity of the predictive output of our model using posterior predictive checks.\n\ndef make_ppc(idata, samples=100):\n  fig, axs = plt.subplots(5, 3, figsize=(20, 20))\n  axs = axs.flatten()\n  for i in range(15):\n    for j in range(samples):\n      temp = az.extract(idata['posterior_predictive'].sel({'likelihood_dim_3': i}))['likelihood'].values[:, j]\n      temp = pd.DataFrame(temp, columns=['likelihood'])\n      if j == 0:\n        axs[i].hist(df[drivers[i]], alpha=0.3, ec='black', bins=20, label='Observed Scores')\n        axs[i].hist(temp['likelihood'], color='purple', alpha=0.1, bins=20, label='Predicted Scores')\n      else: \n        axs[i].hist(df[drivers[i]], alpha=0.3, ec='black', bins=20)\n        axs[i].hist(temp['likelihood'], color='purple', alpha=0.1, bins=20)\n      axs[i].set_title(f\"Posterior Predictive Checks {drivers[i]}\")\n      axs[i].legend();\n  plt.show()\n  \nmake_ppc(idata, samples=100)\n\n\n\n\nIn this way we can see how plausibly our model can capture the patterns in the observed data at the level of individual predictions. But additionally we can assess the posterior predictive fit at the level of summary statistics by defining the residuals on the model-fit covariance matrix based on draws from the posterior distribution.\n\n\ndef get_posterior_resids(idata, samples=100, metric='cov'):\n  resids = []\n  for i in range(100):\n    if metric == 'cov':\n      model_cov = pd.DataFrame(az.extract(idata['posterior_predictive'])['likelihood'][:, :, i]).cov()\n      obs_cov = df[drivers].cov()\n    else: \n      model_cov = pd.DataFrame(az.extract(idata['posterior_predictive'])['likelihood'][:, :, i]).corr()\n      obs_cov = df[drivers].corr()\n    model_cov.index = obs_cov.index\n    model_cov.columns = obs_cov.columns\n    residuals = model_cov - obs_cov\n    resids.append(residuals.values.flatten())\n  \n  residuals_posterior = pd.DataFrame(pd.DataFrame(resids).mean().values.reshape(15, 15))\n  residuals_posterior.index = obs_cov.index\n  residuals_posterior.columns = obs_cov.index\n  return residuals_posterior\n\nresiduals_posterior_cov = get_posterior_resids(idata, 2500)\nresiduals_posterior_corr = get_posterior_resids(idata, 2500, metric='corr')\n\n  \n\nThen we can plot the familiar heat-maps from above highlighting the comparable accuracy of the Bayesian model fit.\n\nplot_heatmap(py$residuals_posterior_cov, \"Residuals of the Sample Covariances and Model Implied Covariances\", \"A Visual Check of Bayesian Measurement Model fit\")\n\n\n\nplot_heatmap(py$residuals_posterior_corr, \"Residuals of the Sample Correlations and Model Implied Correlations\", \"A Visual Check of Bayesian Measurement Model fit\")\n\n\n\n\nThis process highlights one major difference in how the Bayesian CFA model is free to fit to the observed data. The parameterisation of the model is calibrated through the likelihood mechanism but the posterior samples provide important information not recoverable by optimizing fits for measurement and structural parameters in the MLE setting. In the Bayesian setting we have access to the latent factor scores for each of the individual responses, giving us a means of assessing outliers on the latent scales. For instance\n\nfig, axs = plt.subplots(1, 3, figsize=(10, 10))\naxs = axs.flatten()\nax = axs[0]\nax1 = axs[1]\nax2 = axs[2]\naz.plot_forest(idata, var_names=['ksi'], combined=True, ax=ax, coords={'latent': ['SUP_P']});\naz.plot_forest(idata, var_names=['ksi'], combined=True, ax=ax1, colors=\"forestgreen\", coords={'latent': ['SE_SOCIAL']});\naz.plot_forest(idata, var_names=['ksi'], combined=True, ax=ax2, colors=\"slateblue\", coords={'latent': ['LS']});\nax.set_yticklabels([]);\nax.set_xlabel(\"SUP_P\");\nax1.set_yticklabels([]);\nax1.set_xlabel(\"SE_SOCIAL\");\nax2.set_yticklabels([]);\nax2.set_xlabel(\"LS\");\nax.axvline(-2, color='red');\nax1.axvline(-2, color='red');\nax2.axvline(-2, color='red');\nax.set_title(\"Individual Parental Support Metric \\n On Latent Factor SUP_P\");\nax1.set_title(\"Individual Social Self Efficacy \\n On Latent Factor SE_SOCIAL\");\nax2.set_title(\"Individual Life Satisfaction Metric \\n On Latent Factor LS\");\nplt.show();\n\n\n\n\nSo while we are fitting a confirmatory factor model the edges between exploratory and confirmatory blur in this setting. Just because we have we a hypothesized factor structure we wish to evaluate does not mean the implications of such structures are crystal clear. Here we need to explore that the individually reported scores render plausible judgments about the relative position of each survey respondent on these latent scales. These are the implications of our model fit that would help us confirm the plausibility of the hypothesized factors, but additionally they offer new potential options for exploring our data and categorizing our observations. This is a common experience in contemporary Bayesian model workflow which recommends iterative model refinement and evaluation.\n\n“A key part of Bayesian workflow is adapting the model to the data at hand and the questions of interest. The model does not exist in isolation and is not specified from the outside; it emerges from engagement with the application and the available data” - Gelman et al in Bayesian Workflow\n\nThis is the lens that matters. Yes, iterative refinement and model checking can lead to models which are over-fit to the sample data. This is always a risk, but explicit modeling workflow highlights prior designs and exposes this sequence of model choices to stringent tests and evaluative consideration. The choices that the determine trajectory of our refinements should represent a process of learning and discovery that uncovers more questions of interest as we expand the capacity of our models.\n\n\n\nStructural Equation Modelling in PyMC\nIn the context of structural equation modeling our hypothetical factor structure and dependency relations are even more specific than the simple “measurement model” we’ve just seen. A structural equation model incorporates a measurement model (a sub-component) and adds regression components between the latent and manifest variables as is required to evaluate candidate hypotheses regarding the structure of the data generating process. The range of possibilities we can encode here is vast, but candidate model architectures are constrained by our theory of the case and the demands of explanatory coherence.\n\nSEM and Indirect Effects\nWith the life-satisfaction data-set we’ve been been looking at we want to evaluate the indirect and direct effects of parental and peer support on life-satisfaction outcomes. To do so we need to add a dependency structure to our data generating process and fit the regressions which would inform us as to relative importance of these types of support as mediated through our self-efficacy scores.\n\ndef make_indirect_sem(priors): \n\n  coords = {'obs': list(range(len(df))), \n            'indicators': drivers,\n            'indicators_1': ['se_acad_p1','se_acad_p2','se_acad_p3'],\n            'indicators_2': ['se_social_p1','se_social_p2','se_social_p3'],\n            'indicators_3': ['sup_friends_p1','sup_friends_p2','sup_friends_p3'],\n            'indicators_4': [ 'sup_parents_p1','sup_parents_p2','sup_parents_p3'],\n            'indicators_5': ['ls_p1','ls_p2', 'ls_p3'],\n            'latent': ['SUP_F', 'SUP_P'], \n            'latent1': ['SUP_F', 'SUP_P'], \n            'latent_regression': ['SUP_F->SE_ACAD', 'SUP_P->SE_ACAD', 'SUP_F->SE_SOC', 'SUP_P->SE_SOC'],\n            'regression': ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P']\n            }\n\n  obs_idx = list(range(len(df)))\n  with pm.Model(coords=coords) as model:\n    \n    Psi = pm.InverseGamma('Psi', 5, 10, dims='indicators')\n    lambdas_ = pm.Normal('lambdas_1',  priors['lambda'][0], priors['lambda'][1], dims=('indicators_1'))\n    lambdas_1 = pm.Deterministic('lambdas1', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_1'))\n    lambdas_ = pm.Normal('lambdas_2', priors['lambda'][0], priors['lambda'][1], dims=('indicators_2'))\n    lambdas_2 = pm.Deterministic('lambdas2', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_2'))\n    lambdas_ = pm.Normal('lambdas_3', priors['lambda'][0], priors['lambda'][1], dims=('indicators_3'))\n    lambdas_3 = pm.Deterministic('lambdas3', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_3'))\n    lambdas_ = pm.Normal('lambdas_4', priors['lambda'][0], priors['lambda'][1], dims=('indicators_4'))\n    lambdas_4 = pm.Deterministic('lambdas4', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_4'))\n    lambdas_ = pm.Normal('lambdas_5', priors['lambda'][0], priors['lambda'][1], dims=('indicators_5'))\n    lambdas_5 = pm.Deterministic('lambdas5', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_5'))\n    tau = pm.Normal('tau', 3, 10, dims='indicators')\n    kappa = 0\n    sd_dist = pm.Exponential.dist(1.0, shape=2)\n    chol, _, _ = pm.LKJCholeskyCov('chol_cov', n=2, eta=priors['eta'],\n      sd_dist=sd_dist, compute_corr=True)\n    cov = pm.Deterministic(\"cov\", chol.dot(chol.T), dims=('latent', 'latent1'))\n    ksi = pm.MvNormal('ksi', kappa, chol=chol, dims=('obs', 'latent'))\n\n    # Regression Components\n    beta_r = pm.Normal('beta_r', 0, 0.5, dims='latent_regression')\n    beta_r2 = pm.Normal('beta_r2', 0, 1, dims='regression')\n    resid_chol, _, _ = pm.LKJCholeskyCov('resid_chol', n=2, eta=priors['eta'],\n      sd_dist=sd_dist, compute_corr=True)\n    _ = pm.Deterministic(\"resid_cov\", chol.dot(chol.T))\n    sigmas_resid = pm.MvNormal('sigmas_resid', kappa, chol=resid_chol)\n\n    # SE_ACAD ~ SUP_FRIENDS + SUP_PARENTS \n    regression_se_acad = pm.Normal('regr_se_acad', beta_r[0]*ksi[obs_idx, 0] + beta_r[1]*ksi[obs_idx, 1], sigmas_resid[0])\n    # SE_SOCIAL ~ SUP_FRIENDS + SUP_PARENTS \n    \n    regression_se_social = pm.Normal('regr_se_social', beta_r[2]*ksi[obs_idx, 0] + beta_r[3]*ksi[obs_idx, 1], sigmas_resid[1])\n\n    # LS ~ SE_ACAD + SE_SOCIAL + SUP_FRIEND + SUP_PARENTS\n    regression = pm.Normal('regr', beta_r2[0]*regression_se_acad + beta_r2[1]*regression_se_social +\n                                   beta_r2[2]*ksi[obs_idx, 0] + beta_r2[3]*ksi[obs_idx, 1], 1)\n\n    m0 = tau[0] + regression_se_acad*lambdas_1[0]\n    m1 = tau[1] + regression_se_acad*lambdas_1[1]\n    m2 = tau[2] + regression_se_acad*lambdas_1[2]\n    m3 = tau[3] + regression_se_social*lambdas_2[0]\n    m4 = tau[4] + regression_se_social*lambdas_2[1]\n    m5 = tau[5] + regression_se_social*lambdas_2[2]\n    m6 = tau[6] + ksi[obs_idx, 0]*lambdas_3[0]\n    m7 = tau[7] + ksi[obs_idx, 0]*lambdas_3[1]\n    m8 = tau[8] + ksi[obs_idx, 0]*lambdas_3[2]\n    m9 = tau[9] + ksi[obs_idx, 1]*lambdas_4[0]\n    m10 = tau[10] + ksi[obs_idx, 1]*lambdas_4[1]\n    m11 = tau[11] + ksi[obs_idx, 1]*lambdas_4[2]\n    m12 = tau[12] + regression*lambdas_5[0]\n    m13 = tau[13] + regression*lambdas_5[1]\n    m14 = tau[14] + regression*lambdas_5[2]\n    \n    mu = pm.Deterministic('mu', pm.math.stack([m0, m1, m2, m3, m4, m5, m6, m7,\n                                              m8, m9, m10, m11, m12, m13, m14]).T)\n    _  = pm.Normal('likelihood', mu, Psi, observed=df[drivers].values)\n\n    idata = pm.sample(10_000, chains=4, nuts_sampler='numpyro', target_accept=.99, tune=2000,\n                      idata_kwargs={\"log_likelihood\": True}, random_seed=110)\n    idata.extend(pm.sample_posterior_predictive(idata))\n\n    return model, idata\n\n\nmodel2, idata2 = make_indirect_sem(priors={'eta': 2, 'lambda': [1, 1]})\n\nThere is quite a bit of extra structure now in our model. This structure articulates the path-dependence of the self-efficacy constructs on the support constructs. We’ve specified these as separate regressions with coefficient values for each of the support constructs, but additionally we need to ensure there is a correlation between the implied self-efficacy scores so we’ve wrapped the regression equations in two normal distributions with correlated residual terms. In this manner we preserve the multivariate relationships and allow the model to place weight on this prior if there is a strong co-determination effect. It’s perhaps easier to see in a picture than think through the code directly:\nThe main point to emphasize here is the expressive nature of the modelling paradigm. Our ability to construct and estimate different chains of dependence allows us to test and evaluate the direct and indirect effects due to the different “drivers” of life-satisfaction.\nAs before we can still recover posterior predictive check to evaluate granular level performance against the observed data points.\n\nmake_ppc(idata2, 100)\n\n\n\n\nBut additionally we can pull out the regression summary coefficients and report on the direct and indirect effects of our constructs\n\n\nsummary_df = az.summary(idata2, var_names=['beta_r', 'beta_r2'])\n\ndef calculate_effects(summary_df, var='SUP_P'):\n    #Indirect Paths\n    ## VAR -> SE_SOC ->LS\n    indirect_parent_soc = summary_df.loc[f'beta_r[{var}->SE_SOC]']['mean']*summary_df.loc['beta_r2[SE_SOCIAL]']['mean']\n\n    ## VAR -> SE_SOC ->LS\n    indirect_parent_acad = summary_df.loc[f'beta_r[{var}->SE_ACAD]']['mean']*summary_df.loc['beta_r2[SE_ACAD]']['mean']\n\n    ## Total Indirect Effects\n    total_indirect = indirect_parent_soc + indirect_parent_acad\n\n    ## Total Effects\n    total_effect = total_indirect + summary_df.loc[f'beta_r2[{var}]']['mean']\n\n    return pd.DataFrame([[indirect_parent_soc, indirect_parent_acad, total_indirect, total_effect]], \n                columns=[f'{var} -> SE_SOC ->LS', f'{var} -> SE_ACAD ->LS', f'Total Indirect Effects {var}', f'Total Effects {var}']\n                )\n\nindirect_p = calculate_effects(summary_df, 'SUP_P')\nindirect_f = calculate_effects(summary_df, 'SUP_F')\n\nresiduals_posterior_cov = get_posterior_resids(idata2, 2500)\nresiduals_posterior_corr = get_posterior_resids(idata2, 2500, 'corr')\n\nThe regression effects seem well sampled and show the strength of relationships between social self-efficacy scores and reported life-satisfaction.\n\npy$summary_df |> kable( caption= \"Regression Coefficients Amongst Latent Factors\", digits=2) |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\nRegression Coefficients Amongst Latent Factors\n \n  \n     \n    mean \n    sd \n    hdi_3% \n    hdi_97% \n    mcse_mean \n    mcse_sd \n    ess_bulk \n    ess_tail \n    r_hat \n  \n \n\n  \n    beta_r[SUP_F->SE_ACAD] \n    0.05 \n    0.04 \n    -0.04 \n    0.13 \n    0 \n    0 \n    35872 \n    29560 \n    1 \n  \n  \n    beta_r[SUP_P->SE_ACAD] \n    0.26 \n    0.05 \n    0.17 \n    0.36 \n    0 \n    0 \n    22718 \n    26763 \n    1 \n  \n  \n    beta_r[SUP_F->SE_SOC] \n    0.16 \n    0.04 \n    0.08 \n    0.22 \n    0 \n    0 \n    27895 \n    29498 \n    1 \n  \n  \n    beta_r[SUP_P->SE_SOC] \n    0.31 \n    0.04 \n    0.23 \n    0.40 \n    0 \n    0 \n    13525 \n    23161 \n    1 \n  \n  \n    beta_r2[SE_ACAD] \n    0.17 \n    0.12 \n    -0.06 \n    0.38 \n    0 \n    0 \n    28858 \n    29141 \n    1 \n  \n  \n    beta_r2[SE_SOCIAL] \n    0.58 \n    0.15 \n    0.30 \n    0.85 \n    0 \n    0 \n    23839 \n    28409 \n    1 \n  \n  \n    beta_r2[SUP_F] \n    0.02 \n    0.07 \n    -0.12 \n    0.15 \n    0 \n    0 \n    32759 \n    30795 \n    1 \n  \n  \n    beta_r2[SUP_P] \n    0.27 \n    0.09 \n    0.11 \n    0.44 \n    0 \n    0 \n    25497 \n    30201 \n    1 \n  \n\n\n\n\n\nBut now we can also pull out estimates of the indirect and total effects of parental and peer support on the outcome of life-satisfaction using the multiplicative path-tracing rules.\n\npy$indirect_p |> kable( caption= \"Total and Indirect Effects: Parental Support\", digits=2) |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\nTotal and Indirect Effects: Parental Support\n \n  \n    SUP_P -> SE_SOC ->LS \n    SUP_P -> SE_ACAD ->LS \n    Total Indirect Effects SUP_P \n    Total Effects SUP_P \n  \n \n\n  \n    0.18 \n    0.04 \n    0.22 \n    0.5 \n  \n\n\n\n\npy$indirect_f |> kable( caption= \"Total and Indirect Effects: Friend Support\", digits=4) |> kable_styling() |>  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\n\nTotal and Indirect Effects: Friend Support\n \n  \n    SUP_F -> SE_SOC ->LS \n    SUP_F -> SE_ACAD ->LS \n    Total Indirect Effects SUP_F \n    Total Effects SUP_F \n  \n \n\n  \n    0.0896 \n    0.0076 \n    0.0972 \n    0.1152 \n  \n\n\n\n\n\nNote that we’ve defined these effects as point estimates on the mean realizations of the regression coefficients. But we could have defined these quantities in the model context and sampled the posterior distribution too. However, the model was already complex enough that we kept it simpler for the clarity of exposition.\nFinally we can plot the residual variance-covariance for the Bayesian estimate of Structural equation model we hypothesized.\n\nplot_heatmap(py$residuals_posterior_cov, \"Residuals of the Sample Covariances and Model Implied Covariances\", \"A Visual Check of Bayesian Structural Model fit\")\n\n\n\n\nThe overall impression seems quite reasonable, but notably distinct from the fit of the simpler measurement model. In this way the flexibility of the Bayesian estimation strategy is not as constrained as the MLE variance-covariance estimation strategy and we can learn of meaningful interactions in a richer more theoretically compelling model while still calibrating against the observed data. This is borne out global model comparisons too.\n\ncompare_df = az.compare({'Measurement Model': idata, 'Structural Model': idata2});\naz.plot_compare(compare_df);\nplt.show()\n\n\n\n\nThe structural model is marginally worse by measures of global fit to the data, but this summary skips over the variety of questions we can answer with this new structure. The broad fidelity to observed data seen in the posterior predictive checks and the variance covariance residuals suggests that we ought to be willing to take this hit to global performance if we can answer questions left unanswered by the measurement model."
  },
  {
    "objectID": "talks/pycon_ireland_recording/pycon_ireland_recording.html",
    "href": "talks/pycon_ireland_recording/pycon_ireland_recording.html",
    "title": "Discrete Choice and Random Utility Models",
    "section": "",
    "text": "In November 2023 I gave a talk at PyCon Ireland on the usage of discrete choice models for the estimation of market share. The Slides can be found here\nBut the recording of the talk has been published on youtube here too.\n\n\n\nPyCon Presentation"
  },
  {
    "objectID": "talks/pydata_berlin_recording/pydata_berlin_2024.html",
    "href": "talks/pydata_berlin_recording/pydata_berlin_2024.html",
    "title": "Missing Data and Bayesian Imputation",
    "section": "",
    "text": "In April 2024 I gave a talk at PyData Berlin on the usage of bayesian imputation techniques in people analytics. The Slides can be found here\nBut the recording of the talk has been published on youtube here too.\n\n\n\nPyData Presentation"
  },
  {
    "objectID": "oss/pymc/sem_cfa_pymc.html",
    "href": "oss/pymc/sem_cfa_pymc.html",
    "title": "SEM and CFA in PyMC",
    "section": "",
    "text": "Structural Equation and Confirmatory Factor Analysis Models\nThis project started as an exploration into the nature of contemportary bayesian model development workflow as applied to psychometric models. The details were spelled out here. The project culminated in a publication to the official PyMC documentation that can be found online here"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "",
    "text": "Causal inference in practice needs to be credible.\nHuman beings (stakeholders too) are good intuitive causal reasoners and will be suspicious of grand causal claims. Your inferential strategies need to be transparent and defensible.\nThis is where you should leverage CausalPy and Bayesian causal modelling.\n\n\n\nBeing a Data Scientist in Industry\n\nThe “Good enough” rule and other varieties of Satisficing\n\nCausal Inference, Defensive coding and Credibility\nBayesian Causal Models and Design Patterns\nIV designs and Instruments\nPS designs and Weighting Schemes\nConclusion\n\n\n\n\n\n\n\n\n\nWhere do we get to if everyone does the minimum? An eye for an eye will leave everyone blind.\n\n\n\n\n\n\n\n\n\n\n\n\n\nDirectionally Correct, Magnitude Poor\n\n\n\n\nImportant investment decisions require a view of magnitude and direction.\nMost business decisions reflect some kind of investment\n\nBut time and effort are harder to place on a balance sheet\n\nOnly sound causal inference supports generalisable decision rules\n“Directionally Correct” is a Zero-interest-rate-phenomena.\n\n\n\n\n\n\n\n\n\n \n\n\n\nP-value based decision criteria on policy change questions are based on the null model of an asymptotic univariate distribution.\n\n\n\n\nMost aggregate data (e.g. Total Revenue) we see in industry result from a complex array of mixture distributions and any long-run aggregates take time to converge.\n\n\n\n\nManagement often doesn’t want to spend the time to validate the long-run characteristics of a phenomena that we would observe in a well powered A/B test.\n\n\n\n\nRisks underpowered experiments through a HIPPO-like decision rules and costly mistakes, ungeneralisable effects.\n\n\n\n\nChallenge: How to improve decision quality in a resource constrained/time-bound environment?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf the data can speak for itself, the answer is ussually blindingly obvious or has taken an inordinate amount of time to accumulate\n\n\n\n\nConversely, if we’ve modeled the data generating process we can answer an array of subtle questions about cause and effect that support effective decision-making.\n\n\n\n\nBuild credibility through linking theoretical estimand and empirical data while partnering across the business with subject matter experts.\n\n\n\n\n\n\n\n\n\n\n\n\n\nA python package for Bayesian Models and Causal Inference methods\nDeveloped and maintained by @PyMC Labs\nBroad Coverage of quasi-experimental designs.\n\n\n\n\n\n\n\n\n\n\n\n Causal question(s) of import can be interrogated just when we can pair a research design with an appropriate statistical model.\n\n\n\n\n\n\n\n \n\nThe treatment effect can be estimated cleanly \\[ y \\sim 1 + Z \\]\nThe treatment effect has to be estimated so as to avoid the bias due to X \\[ y \\sim 1 + \\overbrace{Z} \\] \\[ \\overbrace{Z} \\sim 1 + IV \\]"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#agenda",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#agenda",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Agenda",
    "text": "Agenda\n\nBeing a Data Scientist in Industry\nCausal Inference and Credibility\nBayesian Causal Models and Design Patterns\n\nComplementary Practices\n\nIV designs\n\nInstrument Choice\n\nPS designs\n\nWeighting Schemes\n\nConclusion: Sensitivity Analysis:\n\nUncertainty in the Model\nUncertainty about the Model"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#data-science-in-industry",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#data-science-in-industry",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Data Science in Industry",
    "text": "Data Science in Industry\nand other varieties of Satisficing\n\nWhere do we get to if everyone does the minimum? What is the compounding effect of poor decisions?"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#directionally-correct",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#directionally-correct",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Directionally Correct …",
    "text": "Directionally Correct …\nMaybe in a ZIRP world\n\n\n\n\n\nDirectionally Correct, Magnitude Poor\n\n\n\n\nImportant investment decisions require a view of magnitude and direction.\nMost business decisions reflect some kind of investment\n\nBut time and effort are harder to place on a balance sheet\n\nOnly sound causal inference supports generalisable decision rules\n“Directionally Correct” is a Zero-interest-rate-phenomena."
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#experiment-recipes-rules-and-responsibility",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#experiment-recipes-rules-and-responsibility",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Experiment Recipes, Rules and Responsibility",
    "text": "Experiment Recipes, Rules and Responsibility\n\n\n \n\n\n\nP-value based decision criteria on policy change questions are based on the null model of an asymptotic univariate distribution.\n\n\n\n\nMost aggregate data (e.g. Total Revenue) we see in industry result from a complex array of mixture distributions and any long-run aggregates take time to converge.\n\n\n\n\nManagement often doesn’t want to spend the time to validate the long-run characteristics of a phenomena that we would observe in a well powered A/B test.\n\n\n\n\nRisks underpowered experiments through a HIPPO-like decision rules and costly mistakes, ungeneralisable effects.\n\n\n\n\nChallenge: How to improve decision quality in a resource constrained/time-bound environment?"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#causal-inference-crediblility",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#causal-inference-crediblility",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Causal Inference, Crediblility",
    "text": "Causal Inference, Crediblility\n… and Quasi-Experimental Design\n\n\n\n\n\nAn Abdication of Responsibility\n\n\n\n\n\nIf the data can speak for itself, the answer is usually blindingly obvious or has taken an inordinate amount of time to accumulate\n\n\n\n\nMore generally, if we’ve modeled the data generating process we can answer an array of subtle questions about cause and effect that support effective decision-making.\n\n\n\n\nApproach: Communicate and pursue opportunities for natural experiments. Build credibility through linking theoretical estimand and empirical data while partnering across the business with subject matter experts."
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#causalpy-and-bayesian-inference",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#causalpy-and-bayesian-inference",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "CausalPy and Bayesian Inference",
    "text": "CausalPy and Bayesian Inference\n\n\n\n\n\nA python package for Bayesian Models and Causal Inference methods\nDeveloped and maintained by @PyMC Labs\nBroad Coverage of quasi-experimental designs."
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#causal-methods-and-models",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#causal-methods-and-models",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Causal Methods and Models",
    "text": "Causal Methods and Models\n\n\n\n\n Causal question(s) of import can be interrogated just when we can pair a research design with an appropriate statistical model."
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#canonical-dags-and-methods",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#canonical-dags-and-methods",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Canonical DAGs and Methods",
    "text": "Canonical DAGs and Methods\n\n\n \n\nThe treatment effect can be estimated cleanly \\[ y \\sim 1 + Z \\]\nThe treatment effect has to be estimated so as to avoid the bias due to X \\[ y \\sim 1 + \\widehat{Z} \\] \\[ \\hat{Z} \\sim 1 + IV \\]"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#iv-regression-in-code",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#iv-regression-in-code",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "IV Regression in Code",
    "text": "IV Regression in Code\nN = 100\ne1 = np.random.normal(0, 3, N)\ne2 = np.random.normal(0, 1, N)\nZ = np.random.uniform(0, 1, N)\n## Ensure the endogeneity of the the treatment variable\nX = -1 + 4 * Z + e2 + 2 * e1\ny = 2 + 3 * X + 3 * e1\n\ntest_data = pd.DataFrame({\"y\": y, \"X\": X, \"Z\": Z})\n\nsample_kwargs = {\n    \"tune\": 1000,\n    \"draws\": 2000,\n    \"chains\": 4,\n    \"cores\": 4,\n    \"target_accept\": 0.99,\n}\ninstruments_formula = \"X  ~ 1 + Z\"\nformula = \"y ~  1 + X\"\ninstruments_data = test_data[[\"X\", \"Z\"]]\ndata = test_data[[\"y\", \"X\"]]\niv = InstrumentalVariable(\n    instruments_data=instruments_data,\n    data=data,\n    instruments_formula=instruments_formula,\n    formula=formula,\n    model=InstrumentalVariableRegression(sample_kwargs=sample_kwargs),\n)\n\n\nCausal Inference in Python"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#iv-regression-in-causalpy",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#iv-regression-in-causalpy",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "IV Regression in CausalPy",
    "text": "IV Regression in CausalPy\nN = 100\ne1 = np.random.normal(0, 3, N)\ne2 = np.random.normal(0, 1, N)\nZ = np.random.uniform(0, 1, N)\n## Ensure the endogeneity of the the treatment variable\nX = -1 + 4 * Z + e2 + 2 * e1\ny = 2 + 3 * X + 3 * e1\n\ntest_data = pd.DataFrame({\"y\": y, \"X\": X, \"Z\": Z})\n\nsample_kwargs = {\n    \"tune\": 1000,\n    \"draws\": 2000,\n    \"chains\": 4,\n    \"cores\": 4,\n    \"target_accept\": 0.99,\n}\ninstruments_formula = \"X  ~ 1 + Z\"\nformula = \"y ~  1 + X\"\ninstruments_data = test_data[[\"X\", \"Z\"]]\ndata = test_data[[\"y\", \"X\"]]\niv = InstrumentalVariable(\n    instruments_data=instruments_data,\n    data=data,\n    instruments_formula=instruments_formula,\n    formula=formula,\n    model=InstrumentalVariableRegression(sample_kwargs=sample_kwargs),\n)"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#bayesian-structural-model",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#bayesian-structural-model",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Bayesian Structural Model",
    "text": "Bayesian Structural Model\nAnd Instrument Strength\n\\[\\begin{align*}\n\\left(\n\\begin{array}{cc}\nY \\\\\nZ\n\\end{array}\n\\right)\n& \\sim\n\\text{MultiNormal}(\\color{green} \\mu, \\color{purple} \\Sigma) \\\\\n\\color{green} \\mu & = \\left(\n\\begin{array}{cc}\n\\mu_{y} \\\\\n\\mu_{z}\n\\end{array}\n\\right)\n=\n\\left(\n\\begin{array}{cc}\n\\beta_{00} + \\color{blue} \\beta_{01}Z ... \\\\\n\\beta_{10} + \\beta_{11}IV ...\n\\end{array}\n\\right)\n\\end{align*}\n\\]\nThe treatment effect \\(\\color{blue}\\beta_{01}\\) of is the primary quantity of interest\n\n\\[ \\color{purple} \\Sigma  = \\begin{bmatrix}\n1 & \\color{blue} \\sigma \\\\\n\\color{blue} \\sigma & 1\n\\end{bmatrix}\n\\]\nThe Bayesian estimation strategy incorporates two structural equations and the success of the IV model relies of the correlation between terms."
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#concrete-example",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#concrete-example",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Concrete Example",
    "text": "Concrete Example\n\n\n\nCausal Inference in Python"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#returns-to-schooling-concrete-example",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#returns-to-schooling-concrete-example",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Returns to Schooling: Concrete Example",
    "text": "Returns to Schooling: Concrete Example\n\n\n\n\nRecipe of Assumptions:\n\nExclusion Restriction\nIndependence\nRelevance"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#returns-to-schooling-instrument",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#returns-to-schooling-instrument",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Returns to Schooling: Instrument",
    "text": "Returns to Schooling: Instrument\n\n\n\nCausal Inference in Python"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#returns-to-schooling-instrument-relevance",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#returns-to-schooling-instrument-relevance",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Returns to Schooling: Instrument Relevance",
    "text": "Returns to Schooling: Instrument Relevance\n\n\n\n\n\nVisual check of relevance for different instrument variables on the outcome\n\n\n\nWe want to argue for:\n\nthe relevance of our instrument i.e. that it has a non-trivial impact on the outcome of interest\nthat it influences the result only via the treatment condition.\nevaluating multiple instrument candidates"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#returns-to-schooling-addressing-confounding",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#returns-to-schooling-addressing-confounding",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Returns to Schooling: Addressing Confounding",
    "text": "Returns to Schooling: Addressing Confounding\n\n\n\n\nThe natural comparison with OLS shows:\n\nevidence of genuine confounding in the estimates of treatment effect\nCrucially it highlights the false precision in the OLS estimate.\n\n\n\n\n\nCausal Inference in Python"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#returns-to-schooling-quantifying-confounding",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#returns-to-schooling-quantifying-confounding",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Returns to Schooling: Quantifying Confounding",
    "text": "Returns to Schooling: Quantifying Confounding\n\n\n\n\n\nComparison between IV and OLS model parameter estimates\n\n\n\nThe natural comparison with OLS shows:\n\nevidence of genuine confounding in the estimates of treatment effect\nCrucially it highlights the false precision in the OLS estimate.\nModel comparison is at the heart of understanding confounding in causal infernece.\nIV models may be compared by structure but also by instruments used."
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#returns-to-schooling-justifying-instruments",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#returns-to-schooling-justifying-instruments",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Returns to Schooling: Justifying Instruments",
    "text": "Returns to Schooling: Justifying Instruments\n\n\n\n\n\nSamples from LKJ prior on Covariance structure with different prior settings\n\n\n\nThe strength of an instrument is determined by the correlation structure between instrument and outcome via the treatment solely:\n\nF-tests can be used to assess how the instrument relates to the outcome.\nIn the Bayesian setting we can directly estimate the correlation structure and apply sensitivity tests.\nStronger priors on correlation strength influence the outcomes and can be evaluated against the data through posterior predictive checks"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#returns-to-schooling-sensitivity-analysis-and-model-comparison",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#returns-to-schooling-sensitivity-analysis-and-model-comparison",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Returns to Schooling: Sensitivity Analysis and Model Comparison",
    "text": "Returns to Schooling: Sensitivity Analysis and Model Comparison\n\n\n\n\nThe strength of an instrument is determined by the correlation structure between instrument and outcome via the treatment solely:\n\nF-tests can be used to assess how the instrument relates to the outcome.\nIn the Bayesian setting we can directly estimate the correlation structure and apply sensitivity tests.\n\n\n\n\n\nCausal Inference in Python"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#returns-to-schooling-sensitivity-analysis-and-model-evaluation",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#returns-to-schooling-sensitivity-analysis-and-model-evaluation",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Returns to Schooling: Sensitivity Analysis and Model Evaluation",
    "text": "Returns to Schooling: Sensitivity Analysis and Model Evaluation\n\n\n\n\n\nParameter estimates across different model formulations\n\n\n\n\n\n\nPosterior Predictive checks and Marginal Effects on our best IV model"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#canonical-dags-and-methods-1",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#canonical-dags-and-methods-1",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Canonical DAGs and Methods",
    "text": "Canonical DAGs and Methods\n\n\n\n\n\nRandom Control Trial\n\n\n\n\n\nPropensity Score Adjustment\n\n\n\nThe treatment effect can be estimated cleanly \\[ y \\sim 1 + Z + X_1 ... X_N \\] \\[ p(Z) \\sim X_1 ... X_N \\]\nPropensity score adjustments reweight our outcome by the probability of treatment \\[ y \\sim  f(p(Z), Z)\\]"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#propensity-scores-in-causalpy",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#propensity-scores-in-causalpy",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Propensity Scores in CausalPy",
    "text": "Propensity Scores in CausalPy\ndf1 = pd.DataFrame(\n    np.random.multivariate_normal([0.5, 1], [[2, 1], [1, 1]], size=10000),\n    columns=[\"x1\", \"x2\"],\n)\ndf1[\"trt\"] = np.where(\n    -0.5 + 0.25 * df1[\"x1\"] + 0.75 * df1[\"x2\"] + np.random.normal(0, 1, size=10000) > 0,\n    1,\n    0,\n)\nTREATMENT_EFFECT = 2\ndf1[\"outcome\"] = (\n    TREATMENT_EFFECT * df1[\"trt\"]\n    + df1[\"x1\"]\n    + df1[\"x2\"]\n    + np.random.normal(0, 1, size=10000)\n)\n\nresult1 = cp.InversePropensityWeighting(\n    df1,\n    formula=\"trt ~ 1 + x1 + x2\",\n    outcome_variable=\"outcome\",\n    weighting_scheme=\"robust\",\n    model=cp.pymc_models.PropensityScore(\n        sample_kwargs={\n            \"draws\": 1000,\n            \"target_accept\": 0.95,\n            \"random_seed\": seed,\n            \"progressbar\": False,\n        },\n    ),\n)\n\nresult1"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#weighting-methods",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#weighting-methods",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Weighting Methods",
    "text": "Weighting Methods\n\n\n\nCausal Inference in Python"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#models-and-weighting-methods",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#models-and-weighting-methods",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Models and Weighting Methods",
    "text": "Models and Weighting Methods\n\n\n\n\n\n\n\nPropensity Weighting Adjusts Outcome Distribution to improve estimation of treatment effects"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#validating-balancing-effects",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#validating-balancing-effects",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Validating Balancing Effects",
    "text": "Validating Balancing Effects\n\nBalancing Covariate Distributions across Treatment GroupsBalanced covariate distributions is testable implication the propensity score design."
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#propensity-models",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#propensity-models",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Propensity Models",
    "text": "Propensity Models\n\n\nOverfitting Propensity Score models to the sample data confounds causal inference\nCausal Inference in Python"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#propensity-score-models",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#propensity-score-models",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Propensity Score Models",
    "text": "Propensity Score Models\n\nOverfitting Propensity Score models to the sample data confounds causal inference"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#conclusion",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#conclusion",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\nUncertainty in the Model\n\nCausal inference wrestles with the uncertain consequence(s) of assumed data generating processes.\nBayesian Causal Inference takes uncertainty quantification seriously\nPrior sensitivity Analysis further establishes the robustness of our estimates under uncertainty\n\nUncertainty about the Model\n\nThe combinatorial complexity of the possible models necessitates informed opinion to zero-in on plausible models\n\n\n\n\n\nCausalPy streamlines the deployment and analysis of these causal inference models with proper tools to asses their uncertainty, and communicate impact of policy changes.\nFaciliates model evaluation, comparison and combination - to build credible patterns of inference and justified approaches to critical decisions.\n\n\n\n\n\nCausal Inference in Python"
  },
  {
    "objectID": "talks/stem_talk_junior_infants/working_in_stem.html#introduction",
    "href": "talks/stem_talk_junior_infants/working_in_stem.html#introduction",
    "title": "Working as a Data Scientist",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\nWhat Do I do?\n\n\n\nI’m a data scientist\n\nBayesian statistician,\nLogician.\n\n\n\n\n\nToday I’m going to talk about Counting"
  },
  {
    "objectID": "talks/stem_talk_junior_infants/working_in_stem.html#introduction-1",
    "href": "talks/stem_talk_junior_infants/working_in_stem.html#introduction-1",
    "title": "Working as a Data Scientist",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\nWhat Do I do?\n\n\n\nI’m a data scientist\n\nBayesian statistician,\nLogician.\n\n\n\n\n\nToday I’m going to talk about Counting"
  },
  {
    "objectID": "talks/stem_talk_junior_infants/working_in_stem.html#introduction-who-am-i",
    "href": "talks/stem_talk_junior_infants/working_in_stem.html#introduction-who-am-i",
    "title": "Working as a Data Scientist",
    "section": "Introduction: Who Am I?",
    "text": "Introduction: Who Am I?\n\n\n\n\nData Scientist and Statistician\nStudied Mathematics at University\nWorks as Statistician in the Tech Industry\n\n\n\n\nWyatt’s Dad\nLoves a Cup of Tea and Biscuits\nFavourite Colour: Purple"
  },
  {
    "objectID": "talks/stem_talk_junior_infants/working_in_stem.html#what-a-data-scientist-does",
    "href": "talks/stem_talk_junior_infants/working_in_stem.html#what-a-data-scientist-does",
    "title": "Working as a Data Scientist",
    "section": "What a Data Scientist Does?",
    "text": "What a Data Scientist Does?\n\n\nMeasurement\n\n\nCounting"
  },
  {
    "objectID": "talks/stem_talk_junior_infants/working_in_stem.html#what-a-data-scientist-does-1",
    "href": "talks/stem_talk_junior_infants/working_in_stem.html#what-a-data-scientist-does-1",
    "title": "Working as a Data Scientist",
    "section": "What a Data Scientist Does?",
    "text": "What a Data Scientist Does?\nGuessing\n\n\n\n\n\n\n\n\n\n\nGirl\nBoy\nGiraffe\n\n\n\n\n1\n95\n119\n564\n\n\n2\n106\n104\n571\n\n\n3\n103\n103\n561\n\n\n4\n95\n113\n558\n\n\n5\n104\n101\n509\n\n\n6\n97\n103\n563\n\n\nNew Class Member\n?\n?\n?"
  },
  {
    "objectID": "talks/stem_talk_junior_infants/working_in_stem.html#what-a-data-scientist-does-2",
    "href": "talks/stem_talk_junior_infants/working_in_stem.html#what-a-data-scientist-does-2",
    "title": "Working as a Data Scientist",
    "section": "What a Data Scientist Does",
    "text": "What a Data Scientist Does\nPrediction\n\n\n\n\n\n\n\n\n\n\nGirl\nBoy\nGiraffe\n\n\n\n\n1\n95\n119\n564\n\n\n2\n106\n104\n571\n\n\n3\n103\n103\n561\n\n\n4\n95\n113\n558\n\n\n5\n104\n101\n509\n\n\n6\n97\n103\n563\n\n\nPrediction\n100\n107\n554"
  },
  {
    "objectID": "talks/stem_talk_junior_infants/working_in_stem.html#experiment",
    "href": "talks/stem_talk_junior_infants/working_in_stem.html#experiment",
    "title": "Working as a Data Scientist",
    "section": "Experiment",
    "text": "Experiment\nLet’s do an Experiment and Measure the Class Heights\n\n\n\nCounting"
  },
  {
    "objectID": "scratchwork/model_uncertainty.html",
    "href": "scratchwork/model_uncertainty.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "import networkx as nx\nimport pymc as pm\nimport pandas as pd\nimport numpy as np\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom networkx.drawing.nx_agraph import graphviz_layout\n\nfig, ax = plt.subplots(figsize=(20, 8))\ntriads = {\n    \"021D\": [(3, 1), (3, 2)],\n    \"021U\": [(5, 6), (4, 6) ,(6, 5)],\n    \"021C\": [(7, 8), (8, 9)],\n    \"111D\": [(10, 11), (10, 12), (11, 12)],\n    \"A11D\": [(13, 15), (15, 14), (14, 13)],\n}\n\ndef shift_positions(pos, x_shift, y_shift):\n    return {node: (x + x_shift, y + y_shift) for node, (x, y) in pos.items()}\n\npositions = {}\nx_shift = 0\ny_shift = 0\nshift_step = 2\ngraphs = []\nfor i, triad in zip(range(len(triads)), triads.values()):\n    G = nx.DiGraph()\n    G.add_nodes_from(list(np.unique(triad)))\n    G.add_edges_from(triad)\n    pos = nx.spring_layout(G)\n    shifted_pos = shift_positions(pos, x_shift, y_shift)\n    positions.update(shifted_pos)\n    x_shift += shift_step\n    y_shift += shift_step\n    graphs.append(G)\n\nfor G in graphs:\n    nx.draw(G, pos=positions, with_labels=False, node_size=500, ax=ax)\n\nax.fill_betweenx(range(10), x1=1, x2=5, alpha=0.1, label='focal region')\nax.set_title(\"Candidate Model Graphs\", fontweight='bold', fontsize=20)\nax.legend();\n\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create a list of 4 graphs\ngraphs = [\n    nx.cycle_graph(5),\n    nx.complete_graph(4),\n    nx.path_graph(6),\n    nx.star_graph(5)\n]\n\n# Define a function to shift the graph positions\ndef shift_positions(pos, x_shift, y_shift):\n    return {node: (x + x_shift, y + y_shift) for node, (x, y) in pos.items()}\n\n# Generate position dictionaries and shift them for each graph\npositions = {}\nx_shift = 0\ny_shift = 0\nshift_step = 4  # How much to shift each graph to avoid overlap\n\nfor i, G in enumerate(graphs):\n    # Generate positions for the current graph using spring layout\n    pos = nx.spring_layout(G)\n    \n    # Shift the positions to avoid overlap\n    shifted_pos = shift_positions(pos, x_shift, y_shift)\n    \n    # Update the global positions dictionary\n    positions.update(shifted_pos)\n    \n    # Increment the shift values for the next graph\n    x_shift += shift_step\n    y_shift += shift_step\n\n# Draw all the graphs on the same plot\nplt.figure(figsize=(8, 8))\n\nfor G in graphs:\n    nx.draw(G, pos=positions, with_labels=True, node_size=500)\n\nplt.show()"
  },
  {
    "objectID": "talks/pycon_ireland_causalpy/pycon_2024.html#preliminaries",
    "href": "talks/pycon_ireland_causalpy/pycon_2024.html#preliminaries",
    "title": "Uncertainty and Causal Inference in Python",
    "section": "Preliminaries",
    "text": "Preliminaries\n\n\n\n\n\n\nWho am I?\n\n\n\nI’m a data scientist at Personio\n\nBayesian statistician,\nReformed philosopher and logician.\n\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\nThe worked examples used here can be found here\n\n\n\n\nMy Website"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#agenda",
    "href": "talks/ukraine_sem/talk4ukraine.html#agenda",
    "title": "Structural Causal Models in PyMC",
    "section": "Agenda",
    "text": "Agenda\nAbstract -&gt; Concrete -&gt; Code\n\n\n\nValid Causal Structure\n\nForm and Content of Causal Inference\nAbstract Conception\nConcrete Conception\nThe Bayesian Phrasing\nImplementation in Code\n\nFactor Structures\n\nMeasurement Models: Abstract Conception\nBayesian Phrasing: Concrete Example\nBayesian Phrasing: Code in PyMC\nModel Fit and Diagnostics\n\n\n\n\nStructural Equation Models\n\nAdding Structural Components\nThe SEM Model in PyMC\nModel Comparison: CFA and SEM\n\nCausal Estimands and the Do-Operator\n\nThe Do-Calculus and Minimum Valid Structure\nWhat-if Structures and the Do-Operator\nThe Do-Operator in PyMC\nCausal Estimands under Intervention"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#preliminaries",
    "href": "talks/ukraine_sem/talk4ukraine.html#preliminaries",
    "title": "Structural Causal Models in PyMC",
    "section": "Preliminaries",
    "text": "Preliminaries\n\n\n\n\n\n\nWho am I?\n\n\n\nI’m a data scientist at Personio\n\nBayesian statistician,\nReformed philosopher and logician.\n\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\n\nThe worked examples used here can be found here\nDownload SEM Notebook\nSEM Data\n\n\n\n\n\nMy Website"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#form-and-content",
    "href": "talks/ukraine_sem/talk4ukraine.html#form-and-content",
    "title": "Structural Causal Models in PyMC",
    "section": "Form and Content",
    "text": "Form and Content\nDirected Acyclic Graphs and their Meaning\n\n\n\n“Every proposition has a content and a form. We get the picture of the pure form if we abstract from the meaning of the single words, or symbols (so far as they have independent meanings)… By syntax in this general sense of the word I mean the rules which tell us in which connections only a word gives sense, thus excluding nonsensical structures.” - Wittgenstein Some Remarks on Logical Form\n\n\n \\[ \\psi | \\neg \\psi | \\psi \\rightarrow \\phi \\]\nSets of Admissable Graphs and Well formed Fomulae\n\n\n\n\nSCMS in PyMC"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#form-and-content-of-causal-inference",
    "href": "talks/ukraine_sem/talk4ukraine.html#form-and-content-of-causal-inference",
    "title": "Structural Causal Models in PyMC",
    "section": "Form and Content of Causal Inference",
    "text": "Form and Content of Causal Inference\nDirected Acyclic Graphs and their Meaning\n\n\n\n“Every proposition has a content and a form. We get the picture of the pure form if we abstract from the meaning of the single words, or symbols (so far as they have independent meanings)… By syntax in this general sense of the word I mean the rules which tell us in which connections only a word [makes] sense, thus excluding nonsensical structures.” - Wittgenstein Some Remarks on Logical Form\n\n\n \\[ \\psi | \\neg \\psi | \\psi \\rightarrow \\phi \\]\nSets of Admissable Graphs and Well formed valid Fomulae"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#abstract-conception",
    "href": "talks/ukraine_sem/talk4ukraine.html#abstract-conception",
    "title": "Structural Causal Models in PyMC",
    "section": "Abstract Conception",
    "text": "Abstract Conception\nNon-Parametric Structural Diagrams\n\n\n\n\n\n\nNon-parametric Structural Causal Models highlight the aspects of the Data Generating processes that threaten the valid construction of a causal claim.\n\n\\[ Y \\sim f_{Y}(Z, X, U_{Y})  \\] \\[ Z \\sim f_{Z}(X, IV, U_{Z})\\] \\[ X \\sim f_{X}(U_{X})\\] \\[ IV \\sim f_{IV}(U_{IV})\\]"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#concrete-conception",
    "href": "talks/ukraine_sem/talk4ukraine.html#concrete-conception",
    "title": "Structural Causal Models in PyMC",
    "section": "Concrete Conception",
    "text": "Concrete Conception\nParametric Approximation via Regression\n\n\n\n\n\n\n\\[ y \\sim 1 + Z + u \\] Regression Approximation to estimating valid coefficients in systems of simultaneous equation via 2SLS.\n\n\\[ y \\sim 1 + \\widehat{Z} + u \\] \\[ \\widehat{Z} \\sim 1 + IV + u \\]"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#the-bayesian-phrasing",
    "href": "talks/ukraine_sem/talk4ukraine.html#the-bayesian-phrasing",
    "title": "Structural Causal Models in PyMC",
    "section": "The Bayesian Phrasing",
    "text": "The Bayesian Phrasing\n\n\n\\[\\begin{align*}\n\\left(\n\\begin{array}{cc}\nY \\\\\nZ\n\\end{array}\n\\right)\n& \\sim\n\\text{MultiNormal}(\\color{green} \\mu, \\color{purple} \\Sigma) \\\\\n\\color{green} \\mu & = \\left(\n\\begin{array}{cc}\n\\mu_{y} \\\\\n\\mu_{z}\n\\end{array}\n\\right)\n=\n\\left(\n\\begin{array}{cc}\n\\beta_{00} + \\color{blue} \\beta_{01}Z ... \\\\\n\\beta_{10} + \\beta_{11}IV ...\n\\end{array}\n\\right)\n\\end{align*}\n\\]\nThe treatment effect \\(\\color{blue}\\beta_{01}\\) of is the primary quantity of interest\n\n\\[ \\color{purple} \\Sigma  = \\begin{bmatrix}\n1 & \\color{purple} \\sigma \\\\\n\\color{purple} \\sigma & 1\n\\end{bmatrix}\n\\] But the estimation depends on the multivariate realisation of the data\n\n\n\n\nEven the “simple” IV design is a structural causal model.\n\n\n\n\nThe crucial component is the covariance structure of the joint-distribution and the instrument’s theoretical validity\n\n\n\n\nThe Bayesian Estimation strategy estimates the IV model by seeking a parameterisation where the potential outcomes are conditionally exchangeable.\n\n\\[ p((y_{1}, z_{1})^{T} ....(y_{q}, z_{q})^{T}) = \\dfrac{p(YZ | \\theta)p(\\theta)}{\\sum p_{i}(YZ)}  \\]\n\\[ = \\dfrac{p( (y_{1}, z_{1})^{T} ....(y_{q}, z_{q})^{T}) | \\Sigma , \\beta)p(\\Sigma , \\beta) }{\\sum p_{i}(YZ)} \\]"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#implementation-in-causalpy",
    "href": "talks/ukraine_sem/talk4ukraine.html#implementation-in-causalpy",
    "title": "Structural Causal Models in PyMC",
    "section": "Implementation in CausalPy",
    "text": "Implementation in CausalPy\n\n\n\nSCMS in PyMC"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#implementation-in-code",
    "href": "talks/ukraine_sem/talk4ukraine.html#implementation-in-code",
    "title": "Structural Causal Models in PyMC",
    "section": "Implementation in Code",
    "text": "Implementation in Code\nPyMC model context\n## PyMC model context\nwith pm.Model() as iv_model:\n    # Initialising Regression Priors\n    beta_t = pm.Normal(\n        name=\"beta_t\",\n        mu=priors[\"mus_t\"],\n        sigma=priors[\"sigma_t\"],\n        dims=\"instruments\")\n\n    beta_z = pm.Normal(\n        name=\"beta_z\",\n        mu=priors[\"mus_z\"],\n        sigma=priors[\"sigma_z\"],\n        dims=\"covariates\")\n    \n    # Covariance Prior Parameters\n    sd_dist = pm.Exponential.dist(priors[\"lkj_sd\"], shape=2)\n    chol, corr, sigmas = pm.LKJCholeskyCov(\n        name=\"chol_cov\",\n        eta=priors[\"eta\"],\n        n=2,\n        sd_dist=sd_dist,\n    )\n    # compute and store the covariance matrix\n    pm.Deterministic(name=\"cov\", \n    var=pt.dot(l=chol, r=chol.T))\n\n    # --- Parameterization ---\n    # focal regression\n    mu_y = pm.Deterministic(name=\"mu_y\", \n    var=pm.math.dot(X, beta_z))\n    # instrumental regression\n    mu_t = pm.Deterministic(name=\"mu_t\", \n    var=pm.math.dot(Z, beta_t))\n    \n    # Stack regressions for MvNormal\n    mu = pm.Deterministic(name=\"mu\", \n    var=pt.stack(tensors=(mu_y, mu_t), \n    axis=1))\n\n    # --- Likelihood ---\n    pm.MvNormal(\n        name=\"likelihood\",\n        mu=mu,\n        chol=chol,\n        observed=np.stack(arrays=(y, t), axis=1),\n        shape=(X.shape[0], 2),\n    )"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#measurement-models-and-factor-structures",
    "href": "talks/ukraine_sem/talk4ukraine.html#measurement-models-and-factor-structures",
    "title": "Structural Causal Models in PyMC",
    "section": "Measurement Models and Factor Structures",
    "text": "Measurement Models and Factor Structures\nThe Confirmatory Factor Model can also be phrased in the Bayesian way.\n\\[p(\\mathbf{x_{i}}^{T}.....\\mathbf{x_{q}}^{T} | \\text{Ksi}, \\Psi, \\tau, \\Lambda) \\sim Normal(\\tau + \\Lambda\\cdot \\text{Ksi}, \\Psi)\\]\n\\[ \\lambda_{1} ..... \\lambda_{2} ..... \\lambda_{3} \\in \\Lambda \\]\n\n\n\n\n\n  \n    \n       \n      motiv\n      harm\n      stabi\n      ppsych\n      ses\n      verbal\n      read\n      arith\n      spell\n    \n  \n  \n    \n      0\n      -7.907122\n      -5.075312\n      -3.138836\n      -17.800210\n      4.766450\n      -3.633360\n      -3.488981\n      -9.989121\n      -6.567873\n    \n    \n      1\n      1.751478\n      -4.155847\n      3.520752\n      7.009367\n      -6.048681\n      -7.693461\n      -4.520552\n      8.196238\n      8.778973\n    \n    \n      2\n      14.472570\n      -4.540677\n      4.070600\n      23.734260\n      -16.970670\n      -3.909941\n      -4.818170\n      7.529984\n      -5.688716\n    \n    \n      3\n      -1.165421\n      -5.668406\n      2.600437\n      1.493158\n      1.396363\n      21.409450\n      -3.138441\n      5.730547\n      -2.915676\n    \n    \n      4\n      -4.222899\n      -10.072150\n      -6.030737\n      -5.985864\n      -18.376400\n      -1.438816\n      -2.009742\n      -0.623953\n      -1.024624"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#measurement-model-graph",
    "href": "talks/ukraine_sem/talk4ukraine.html#measurement-model-graph",
    "title": "Structural Causal Models in PyMC",
    "section": "Measurement Model Graph",
    "text": "Measurement Model Graph"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#measurement-models",
    "href": "talks/ukraine_sem/talk4ukraine.html#measurement-models",
    "title": "Structural Causal Models in PyMC",
    "section": "Measurement Models",
    "text": "Measurement Models\nand Structured Priors\nThe Confirmatory Factor Model can also be phrased in the Bayesian way.\n\\[p(\\mathbf{x_{i}}^{T}.....\\mathbf{x_{q}}^{T} | \\text{Ksi}, \\Psi, \\tau, \\Lambda) \\sim Normal(\\tau + \\Lambda\\cdot \\text{Ksi}, \\Psi)\\]\n\\[ \\lambda_{11} ..... \\lambda_{21} ..... \\lambda_{31} \\in \\Lambda \\]\n\n\n\n\n\n\n\n \nmotiv\nharm\nstabi\nppsych\nses\nverbal\nread\narith\nspell\n\n\n\n\n0\n-7.907122\n-5.075312\n-3.138836\n-17.800210\n4.766450\n-3.633360\n-3.488981\n-9.989121\n-6.567873\n\n\n1\n1.751478\n-4.155847\n3.520752\n7.009367\n-6.048681\n-7.693461\n-4.520552\n8.196238\n8.778973\n\n\n2\n14.472570\n-4.540677\n4.070600\n23.734260\n-16.970670\n-3.909941\n-4.818170\n7.529984\n-5.688716\n\n\n3\n-1.165421\n-5.668406\n2.600437\n1.493158\n1.396363\n21.409450\n-3.138441\n5.730547\n-2.915676\n\n\n4\n-4.222899\n-10.072150\n-6.030737\n-5.985864\n-18.376400\n-1.438816\n-2.009742\n-0.623953\n-1.024624"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#measurement-models-1",
    "href": "talks/ukraine_sem/talk4ukraine.html#measurement-models-1",
    "title": "Structural Causal Models in PyMC",
    "section": "Measurement Models",
    "text": "Measurement Models\nand Factor Structures\nThe Confirmatory Factor Model can also be phrased in the Bayesian way.\n\\[p(\\mathbf{x_{i}}^{T}.....\\mathbf{x_{q}}^{T} | \\text{Ksi}, \\Psi, \\tau, \\Lambda) \\sim Normal(\\tau + \\Lambda\\cdot \\text{Ksi}, \\Psi)\\]\n\\[ \\lambda_{11} ..... \\lambda_{21} ..... \\lambda_{31} \\in \\Lambda \\]\n\n\n\n\n\n  \n    \n       \n      motiv\n      harm\n      stabi\n      ppsych\n      ses\n      verbal\n      read\n      arith\n      spell\n    \n  \n  \n    \n      0\n      -7.907122\n      -5.075312\n      -3.138836\n      -17.800210\n      4.766450\n      -3.633360\n      -3.488981\n      -9.989121\n      -6.567873\n    \n    \n      1\n      1.751478\n      -4.155847\n      3.520752\n      7.009367\n      -6.048681\n      -7.693461\n      -4.520552\n      8.196238\n      8.778973\n    \n    \n      2\n      14.472570\n      -4.540677\n      4.070600\n      23.734260\n      -16.970670\n      -3.909941\n      -4.818170\n      7.529984\n      -5.688716\n    \n    \n      3\n      -1.165421\n      -5.668406\n      2.600437\n      1.493158\n      1.396363\n      21.409450\n      -3.138441\n      5.730547\n      -2.915676\n    \n    \n      4\n      -4.222899\n      -10.072150\n      -6.030737\n      -5.985864\n      -18.376400\n      -1.438816\n      -2.009742\n      -0.623953\n      -1.024624"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#measurement-models-in-scms",
    "href": "talks/ukraine_sem/talk4ukraine.html#measurement-models-in-scms",
    "title": "Structural Causal Models in PyMC",
    "section": "Measurement Models in SCMs",
    "text": "Measurement Models in SCMs\n\n\n\\[ ADJUST \\sim f_{ADJUST}(motiv, harm, stabi, U)  \\] \\[ RISK \\sim f_{RISK}(ppsych, ses, verbal, U)\\] \\[ ACHIEVE \\sim f_{ACHIEVE}(read, arith, spell, U)\\]\nNon-parametric phrasing of SCMs under-specifies the relations of interest.\nCFA models estimates the multivariate correlation structure while imposing the focal causal structure and dependencies to “hide” detail in measurement error."
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#measurement-models-in-pymc",
    "href": "talks/ukraine_sem/talk4ukraine.html#measurement-models-in-pymc",
    "title": "Structural Causal Models in PyMC",
    "section": "Measurement Models in PyMC",
    "text": "Measurement Models in PyMC\n\ncoords = {\n    \"obs\": list(range(len(df))),\n    \"indicators\": [\"motiv\", \"harm\", \"stabi\", \"verbal\", \"ses\", \"ppsych\", \"read\", \"arith\", \"spell\"],\n    \"indicators_1\": [\"motiv\", \"harm\", \"stabi\"],\n    \"indicators_2\": [\"verbal\", \"ses\", \"ppsych\"],\n    \"indicators_3\": [\"read\", \"arith\", \"spell\"],\n    \"latent\": [\"adjust\", \"risk\", \"achieve\"],\n}\n\ndef make_lambda(indicators, name='lambdas1', priors=[1, 10]):\n    \"\"\" Takes an argument indicators which is a string in the coords dict\"\"\"\n    temp_name = name + '_'\n    lambdas_ = pm.Normal(temp_name, priors[0], priors[1], dims=(indicators))\n    # Force a fixed scale on the factor loadings for factor 1\n    lambdas_1 = pm.Deterministic(\n        name, pt.set_subtensor(lambdas_[0], 1), dims=(indicators)\n    )\n    return lambdas_1\n\nobs_idx = list(range(len(df)))\nwith pm.Model(coords=coords) as model:\n\n    # Set up Factor Loadings\n    lambdas_1 = make_lambda('indicators_1', 'lambdas1')\n    lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, 2])\n    lambdas_3 = make_lambda('indicators_3', 'lambdas3')\n    \n    # Specify covariance structure between latent factors\n    sd_dist = pm.Exponential.dist(1.0, shape=3)\n    chol, _, _ = pm.LKJCholeskyCov(\"chol_cov\", n=3, eta=2, sd_dist=sd_dist, compute_corr=True)\n    ksi = pm.MvNormal(\"ksi\", 0, chol=chol, dims=(\"obs\", \"latent\"))\n\n    # Construct Pseudo Observation matrix based on Factor Loadings\n    m1 = ksi[obs_idx, 0] * lambdas_1[0]\n    m2 = ksi[obs_idx, 0] * lambdas_1[1]\n    m3 = ksi[obs_idx, 0] * lambdas_1[2]\n    m4 = ksi[obs_idx, 1] * lambdas_2[0]\n    m5 = ksi[obs_idx, 1] * lambdas_2[1]\n    m6 = ksi[obs_idx, 1] * lambdas_2[2]\n    m7 = ksi[obs_idx, 2] * lambdas_3[0]\n    m8 = ksi[obs_idx, 2] * lambdas_3[1]\n    m9 = ksi[obs_idx, 2] * lambdas_3[2]\n\n    mu = pm.Deterministic(\"mu\", pm.math.stack([m1, m2, m3, m4, m5, m6, m7, m8, m9]).T)\n\n    ## Error Terms\n    Psi = pm.InverseGamma(\"Psi\", 2, 13, dims=\"indicators\")\n\n    # Likelihood\n    _ = pm.Normal(\n        \"likelihood\",\n        mu,\n        Psi,\n        observed=df[coords['indicators']].values,\n    )"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#functional-form-and-the-state-of-the-world",
    "href": "talks/ukraine_sem/talk4ukraine.html#functional-form-and-the-state-of-the-world",
    "title": "Structural Causal Models in PyMC",
    "section": "Functional Form and the State of the World",
    "text": "Functional Form and the State of the World\nCredible Maps\n\n\nCausal Structural Models generate valid causal claims just when:\n\nthe mathematical model is apt to account for risks of confounding in the assumed data generating process\nthe model parameters or theoretical estimands can be properly identified and estimated with the data available.\nthe assumed data generating process is a close enough approximation of the actual process\nThe set of admissable causal claims is constrained by the substantive extra-statistical requirement to credibly map model to the world\n\n\n\n\n\nTheory and Credibility by Ashworth et al\n\n\n\n\\[ \\Bigg( \\begin{array}{cc}\nY \\sim f_{Y}(Z, X, U_{Y}) \\\\\nZ \\sim f_{Z}(X, IV, U_{Z})\n\\end{array} \\Bigg) \\approxeq \\Bigg[ \\begin{array}{cc}\nY \\sim 1 + \\widehat{Z} + u \\\\\n\\widehat{Z} \\sim 1 + IV + u\n\\end{array} \\Bigg] \\\\\n\\approxeq \\dfrac{p( (y_{1}, z_{1})^{T} ....(y_{q}, z_{q})^{T}) | \\color{purple}\\Sigma ,\\color{blue}\\beta \\color{black} )p(\\color{purple}\\Sigma ,\\color{blue}\\beta \\color{black}) }{\\sum p_{i}(YZ)}\n\\]"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#model-fit-covariance-structures",
    "href": "talks/ukraine_sem/talk4ukraine.html#model-fit-covariance-structures",
    "title": "Structural Causal Models in PyMC",
    "section": "Model Fit: Covariance Structures",
    "text": "Model Fit: Covariance Structures\n The differences between the sample covariance and model predicted covariances are small."
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#model-fit-posterior-predictive-checks",
    "href": "talks/ukraine_sem/talk4ukraine.html#model-fit-posterior-predictive-checks",
    "title": "Structural Causal Models in PyMC",
    "section": "Model Fit: Posterior Predictive Checks",
    "text": "Model Fit: Posterior Predictive Checks\n The model can successfully retrodict the observed data, indicating good model fit."
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#model-implications-latent-scores",
    "href": "talks/ukraine_sem/talk4ukraine.html#model-implications-latent-scores",
    "title": "Structural Causal Models in PyMC",
    "section": "Model Implications: Latent Scores",
    "text": "Model Implications: Latent Scores\n\n\n\n\n\nBayesian Models sample the Latent constructs\nAssess the measurement profile of individual outliers under intervention.\nAllows us to interrogate what are the relations between these Latent components?\nHow do thet vary under pressure?"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#the-model",
    "href": "talks/ukraine_sem/talk4ukraine.html#the-model",
    "title": "Structural Causal Models in PyMC",
    "section": "The Model",
    "text": "The Model\n\\[\\underbrace{\\text{Ksi}}_{latent} = \\underbrace{\\mathbf{B}^{T}\\text{Ksi}}_{structural \\\\ component} + \\underbrace{\\Lambda\\cdot \\text{Ksi}}_{measurement \\\\ component}\\]\n# measurement model\nadjust =~ motiv + harm + stabi\nrisk =~ verbal + ses + ppsych\nachieve =~ read + arith + spell\n\n# focal regressions\nadjust ~ risk \nachieve ~ adjust + risk\n\n\n\nSCMS in PyMC"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#adding-structural-components",
    "href": "talks/ukraine_sem/talk4ukraine.html#adding-structural-components",
    "title": "Structural Causal Models in PyMC",
    "section": "Adding Structural Components",
    "text": "Adding Structural Components\nRegression Structures are Overlaid\n\n\nadjust =~ motiv + harm + stabi\nrisk =~ verbal + ses + ppsych\nachieve =~ read + arith + spell\n\nadjust ~ risk\nachieve ~ adjust + risk\n\n\n\\[SEM : \\underbrace{\\text{Ksi}^{*}}_{latent} = \\underbrace{\\mathbf{B}\\text{Ksi}}_{structural \\\\ component} + \\underbrace{\\Lambda\\cdot \\text{Ksi}}_{measurement \\\\ component}\\]\n\\[CFA : \\underbrace{\\text{Ksi}^{*}}_{latent} = \\underbrace{\\mathbf{0}\\text{Ksi}}_{structural \\\\ component} + \\underbrace{\\Lambda\\cdot \\text{Ksi}}_{measurement \\\\ component}\\]"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#model-estimates-factor-loadings",
    "href": "talks/ukraine_sem/talk4ukraine.html#model-estimates-factor-loadings",
    "title": "Structural Causal Models in PyMC",
    "section": "Model Estimates: Factor Loadings",
    "text": "Model Estimates: Factor Loadings\n\n\n \n\n\n\nThe weights accorded to each of the indicator metrics on each of the specified factors are scaled.\n\n\n\n\nConsistency in the factor loadings is a gauge of factor coherence.\n\n\n\n\nInvariance of the loading scale across groups supports the argument to robust constructs."
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#the-structural-matrix",
    "href": "talks/ukraine_sem/talk4ukraine.html#the-structural-matrix",
    "title": "Structural Causal Models in PyMC",
    "section": "The Structural Matrix",
    "text": "The Structural Matrix\n\\[ \\overbrace{\\text{Ksi}}^{latent} = \\begin{bmatrix} \\overbrace{2.5}^{adjust} & \\overbrace{3.2}^{risk} & \\overbrace{7.3}^{achieve} \\\\ ... & ... & ...\n\\\\ ... & ... & ...  \\\\ ... & ... \\\\ 3.9 & ...\\end{bmatrix} \\begin{bmatrix} 0 & \\beta_{12} & \\beta_{13}\n\\\\ \\beta_{21} & 0 & \\beta_{23}\n\\\\ \\beta_{31} & \\beta_{32} & 0 \\end{bmatrix} = \\underbrace{\\mathbf{B}}_{Regression \\\\ Coefficients} \\]"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#the-generative-model",
    "href": "talks/ukraine_sem/talk4ukraine.html#the-generative-model",
    "title": "Structural Causal Models in PyMC",
    "section": "The Generative Model",
    "text": "The Generative Model\n\n\n\n\nSCMS in PyMC"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#the-generative-model-in-pymc",
    "href": "talks/ukraine_sem/talk4ukraine.html#the-generative-model-in-pymc",
    "title": "Structural Causal Models in PyMC",
    "section": "The Generative Model in PyMC",
    "text": "The Generative Model in PyMC\n\ncoords = {\n    \"obs\": list(range(len(df))),\n    \"indicators\": [\"motiv\", \"harm\", \"stabi\", \"verbal\", \"ses\", \"ppsych\", \"read\", \"arith\", \"spell\"],\n    \"indicators_1\": [\"motiv\", \"harm\", \"stabi\"],\n    \"indicators_2\": [\"verbal\", \"ses\", \"ppsych\"],\n    \"indicators_3\": [\"read\", \"arith\", \"spell\"],\n    \"latent\": [\"adjust\", \"risk\", \"achieve\"],\n    \"paths\": [\"risk->adjust\", \"risk-->achieve\", \"adjust-->achieve\"]\n    }\n\n    obs_idx = list(range(len(df)))\n    with pm.Model(coords=coords) as model:\n\n        # Set up Factor Loadings\n        lambdas_1 = make_lambda('indicators_1', 'lambdas1', priors=[1, .5]) #adjust\n        lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, .5]) # risk\n        lambdas_3 = make_lambda('indicators_3', 'lambdas3', priors=[1, .5]) # achieve\n        \n        # Specify covariance structure between latent factors\n        sd_dist = pm.Exponential.dist(1.0, shape=3)\n        chol, _, _ = pm.LKJCholeskyCov(\"chol_cov\", n=3, eta=2, sd_dist=sd_dist, compute_corr=True)\n        ksi = pm.MvNormal(\"ksi\", 0, chol=chol, dims=(\"obs\", \"latent\"))\n\n        ## Build Regression Components\n        coefs = pm.Normal('betas', 0, .5, dims='paths')\n        zeros = pt.zeros((3, 3))\n        ## adjust ~ risk\n        zeros1 = pt.set_subtensor(zeros[1, 0], coefs[0])\n        ## achieve ~ risk + adjust\n        zeros2 = pt.set_subtensor(zeros1[1, 2], coefs[1])\n        coefs_ = pt.set_subtensor(zeros2[0, 2], coefs[2])\n        \n        structural_relations = pm.Deterministic('endogenous_structural_paths', \n        pm.math.dot(ksi, coefs_))\n\n        # Construct Pseudo Observation matrix based on Factor Loadings\n        m1 = ksi[obs_idx, 0] * lambdas_1[0] +  structural_relations[obs_idx, 0] #adjust\n        m2 = ksi[obs_idx, 0] * lambdas_1[1] +  structural_relations[obs_idx, 0] #adjust\n        m3 = ksi[obs_idx, 0] * lambdas_1[2] +  structural_relations[obs_idx, 0] #adjust\n        m4 = ksi[obs_idx, 1] * lambdas_2[0] +  structural_relations[obs_idx, 1]  #risk\n        m5 = ksi[obs_idx, 1] * lambdas_2[1] +  structural_relations[obs_idx, 1]  #risk\n        m6 = ksi[obs_idx, 1] * lambdas_2[2] +  structural_relations[obs_idx, 1]  #risk\n        m7 = ksi[obs_idx, 2] * lambdas_3[0] +  structural_relations[obs_idx, 2]  #achieve\n        m8 = ksi[obs_idx, 2] * lambdas_3[1] +  structural_relations[obs_idx, 2]  #achieve\n        m9 = ksi[obs_idx, 2] * lambdas_3[2] +  structural_relations[obs_idx, 2]  #achieve\n\n        mu = pm.Deterministic(\"mu\", pm.math.stack([m1, m2, m3, m4, m5, m6, m7, m8, m9]).T)\n\n\n        ## Error Terms\n        Psi = pm.InverseGamma(\"Psi\", 2, 13, dims=\"indicators\")\n\n        # Likelihood\n        _ = pm.Normal(\n            \"likelihood\",\n            mu,\n            Psi,\n            observed=df[coords['indicators']].values,\n        )"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#model-evaluation",
    "href": "talks/ukraine_sem/talk4ukraine.html#model-evaluation",
    "title": "Structural Causal Models in PyMC",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\n\n\n\nSCMS in PyMC"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#model-evaluation-residuals",
    "href": "talks/ukraine_sem/talk4ukraine.html#model-evaluation-residuals",
    "title": "Structural Causal Models in PyMC",
    "section": "Model Evaluation: Residuals",
    "text": "Model Evaluation: Residuals\n\n\n\n\n\n\n\n\n\n\nSCMS in PyMC"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#model-comparison",
    "href": "talks/ukraine_sem/talk4ukraine.html#model-comparison",
    "title": "Structural Causal Models in PyMC",
    "section": "Model Comparison",
    "text": "Model Comparison\nLocal and Global Fit\n\n\n\n\n\nLocal Model checks\n\n\n\n\n\n\nModel ranked on Leave one out cross validation\n\n\nSEM model achieves better performance on the local and global model checks than the CFA model"
  },
  {
    "objectID": "talks/lbs_sem_causal/lbs_sem_causal.html",
    "href": "talks/lbs_sem_causal/lbs_sem_causal.html",
    "title": "Learning Bayesian Statistics: SEM and CFA",
    "section": "",
    "text": "In November 2024 I recorded a podcast interview about how to develop structural equation models in PyMC and the relationship between SEM and causal inference more generally. You can listen to the podcast here:\n\n\n\nYoutube Webinair"
  },
  {
    "objectID": "scratchwork/bayesian_sem.html",
    "href": "scratchwork/bayesian_sem.html",
    "title": "Measurment Model",
    "section": "",
    "text": "import pymc as pm\nimport pandas as pd\nimport numpy as np\nfrom pytensor import tensor as pt\nimport arviz as az\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom pymc import do, observe\ndf = pd.read_csv('sem_data.csv')\ndf.head()\n\n\n\n\n\n\n\n\nmotiv\nharm\nstabi\nppsych\nses\nverbal\nread\narith\nspell\n\n\n\n\n0\n-7.907122\n-5.075312\n-3.138836\n-17.800210\n4.766450\n-3.633360\n-3.488981\n-9.989121\n-6.567873\n\n\n1\n1.751478\n-4.155847\n3.520752\n7.009367\n-6.048681\n-7.693461\n-4.520552\n8.196238\n8.778973\n\n\n2\n14.472570\n-4.540677\n4.070600\n23.734260\n-16.970670\n-3.909941\n-4.818170\n7.529984\n-5.688716\n\n\n3\n-1.165421\n-5.668406\n2.600437\n1.493158\n1.396363\n21.409450\n-3.138441\n5.730547\n-2.915676\n\n\n4\n-4.222899\n-10.072150\n-6.030737\n-5.985864\n-18.376400\n-1.438816\n-2.009742\n-0.623953\n-1.024624\ncoords = {\n    \"obs\": list(range(len(df))),\n    \"indicators\": [\"motiv\", \"harm\", \"stabi\", \"verbal\", \"ses\", \"ppsych\", \"read\", \"arith\", \"spell\"],\n    \"indicators_1\": [\"motiv\", \"harm\", \"stabi\"],\n    \"indicators_2\": [\"verbal\", \"ses\", \"ppsych\"],\n    \"indicators_3\": [\"read\", \"arith\", \"spell\"],\n    \"latent\": [\"adjust\", \"risk\", \"achieve\"],\n}\n\ndef make_lambda(indicators, name='lambdas1', priors=[1, 10]):\n    \"\"\" Takes an argument indicators which is a string in the coords dict\"\"\"\n    temp_name = name + '_'\n    lambdas_ = pm.Normal(temp_name, priors[0], priors[1], dims=(indicators))\n    # Force a fixed scale on the factor loadings for factor 1\n    lambdas_1 = pm.Deterministic(\n        name, pt.set_subtensor(lambdas_[0], 1), dims=(indicators)\n    )\n    return lambdas_1\nobs_idx = list(range(len(df)))\nwith pm.Model(coords=coords) as model:\n\n    # Set up Factor Loadings\n    lambdas_1 = make_lambda('indicators_1', 'lambdas1')\n    lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, 2])\n    lambdas_3 = make_lambda('indicators_3', 'lambdas3')\n    # Specify covariance structure between latent factors\n    kappa = 0\n    sd_dist = pm.Exponential.dist(1.0, shape=3)\n    chol, _, _ = pm.LKJCholeskyCov(\"chol_cov\", n=3, eta=2, sd_dist=sd_dist, compute_corr=True)\n    ksi = pm.MvNormal(\"ksi\", kappa, chol=chol, dims=(\"obs\", \"latent\"))\n\n    # Construct Pseudo Observation matrix based on Factor Loadings\n    m1 = ksi[obs_idx, 0] * lambdas_1[0]\n    m2 = ksi[obs_idx, 0] * lambdas_1[1]\n    m3 = ksi[obs_idx, 0] * lambdas_1[2]\n    m4 = ksi[obs_idx, 1] * lambdas_2[0]\n    m5 = ksi[obs_idx, 1] * lambdas_2[1]\n    m6 = ksi[obs_idx, 1] * lambdas_2[2]\n    m7 = ksi[obs_idx, 2] * lambdas_3[0]\n    m8 = ksi[obs_idx, 2] * lambdas_3[1]\n    m9 = ksi[obs_idx, 2] * lambdas_3[2]\n\n    mu = pm.Deterministic(\"mu\", pm.math.stack([m1, m2, m3, m4, m5, m6, m7, m8, m9]).T)\n\n    ## Error Terms\n    Psi = pm.InverseGamma(\"Psi\", 2, 13, dims=\"indicators\")\n\n    # Likelihood\n    _ = pm.Normal(\n        \"likelihood\",\n        mu,\n        Psi,\n        observed=df[coords['indicators']].values,\n    )\n\n    idata = pm.sample(\n        draws=1000,\n        chains=4,\n        nuts_sampler=\"numpyro\", target_accept=0.95, idata_kwargs={\"log_likelihood\": True}, \n        tune=2000,\n        random_seed=150\n    )\n    idata.extend(pm.sample_posterior_predictive(idata))\n\npm.model_to_graphviz(model)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\nSampling: [likelihood]\naz.summary(idata, var_names=[\"lambdas1\", \"lambdas2\", \"lambdas3\"])\n\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nlambdas1[motiv]\n1.000\n0.000\n1.000\n1.000\n0.000\n0.000\n4000.0\n4000.0\nNaN\n\n\nlambdas1[harm]\n0.895\n0.043\n0.814\n0.973\n0.002\n0.001\n521.0\n1401.0\n1.01\n\n\nlambdas1[stabi]\n0.703\n0.047\n0.617\n0.791\n0.002\n0.001\n801.0\n1571.0\n1.01\n\n\nlambdas2[verbal]\n1.000\n0.000\n1.000\n1.000\n0.000\n0.000\n4000.0\n4000.0\nNaN\n\n\nlambdas2[ses]\n0.842\n0.087\n0.679\n1.000\n0.004\n0.003\n453.0\n1294.0\n1.01\n\n\nlambdas2[ppsych]\n-0.801\n0.084\n-0.969\n-0.653\n0.004\n0.003\n521.0\n1276.0\n1.00\n\n\nlambdas3[read]\n1.000\n0.000\n1.000\n1.000\n0.000\n0.000\n4000.0\n4000.0\nNaN\n\n\nlambdas3[arith]\n0.841\n0.035\n0.773\n0.906\n0.001\n0.001\n1955.0\n2586.0\n1.00\n\n\nlambdas3[spell]\n0.981\n0.029\n0.927\n1.037\n0.001\n0.001\n1123.0\n1700.0\n1.00\naz.plot_trace(idata, var_names=['lambdas1', 'lambdas2', 'lambdas3'])\nplt.tight_layout();\n\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/density_utils.py:488: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/density_utils.py:488: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/density_utils.py:488: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\ndef make_factor_loadings_df(idata):\n    factor_loadings = pd.DataFrame(\n        az.summary(\n            idata, var_names=[\"lambdas1\", \"lambdas2\", \"lambdas3\"]\n        )[\"mean\"]\n    ).reset_index()\n    factor_loadings[\"factor\"] = factor_loadings[\"index\"].str.split(\"[\", expand=True)[0]\n    factor_loadings.columns = [\"factor_loading\", \"factor_loading_weight\", \"factor\"]\n    factor_loadings[\"factor_loading_weight_sq\"] = factor_loadings[\"factor_loading_weight\"] ** 2\n    factor_loadings[\"sum_sq_loadings\"] = factor_loadings.groupby(\"factor\")[\n        \"factor_loading_weight_sq\"\n    ].transform(sum)\n    factor_loadings[\"error_variances\"] = az.summary(idata, var_names=[\"Psi\"])[\"mean\"].values\n    factor_loadings[\"total_indicator_variance\"] = (\n        factor_loadings[\"factor_loading_weight_sq\"] + factor_loadings[\"error_variances\"]\n    )\n    factor_loadings[\"total_variance\"] = factor_loadings[\"total_indicator_variance\"].sum()\n    factor_loadings[\"indicator_explained_variance\"] = (\n        factor_loadings[\"factor_loading_weight_sq\"] / factor_loadings[\"total_variance\"]\n    )\n    factor_loadings[\"factor_explained_variance\"] = (\n        factor_loadings[\"sum_sq_loadings\"] / factor_loadings[\"total_variance\"]\n    )\n    num_cols = [c for c in factor_loadings.columns if not c in [\"factor_loading\", \"factor\"]]\n    return factor_loadings\n\n\npd.set_option(\"display.max_colwidth\", 15)\nfactor_loadings = make_factor_loadings_df(idata)\nnum_cols = [c for c in factor_loadings.columns if not c in [\"factor_loading\", \"factor\"]]\nfactor_loadings.style.format(\"{:.3f}\", subset=num_cols).background_gradient(\n    axis=0, subset=[\"indicator_explained_variance\", \"factor_explained_variance\"]\n)\n\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/var/folders/__/ng_3_9pn1f11ftyml_qr69vh0000gn/T/ipykernel_51797/2929805399.py:12: FutureWarning: The provided callable &lt;built-in function sum&gt; is currently using SeriesGroupBy.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n  ].transform(sum)\n\n\n\n\n\n\n\n \nfactor_loading\nfactor_loading_weight\nfactor\nfactor_loading_weight_sq\nsum_sq_loadings\nerror_variances\ntotal_indicator_variance\ntotal_variance\nindicator_explained_variance\nfactor_explained_variance\n\n\n\n\n0\nlambdas1[motiv]\n1.000\nlambdas1\n1.000\n2.295\n3.676\n4.676\n60.931\n0.016\n0.038\n\n\n1\nlambdas1[harm]\n0.895\nlambdas1\n0.801\n2.295\n5.623\n6.424\n60.931\n0.013\n0.038\n\n\n2\nlambdas1[stabi]\n0.703\nlambdas1\n0.494\n2.295\n7.605\n8.099\n60.931\n0.008\n0.038\n\n\n3\nlambdas2[verbal]\n1.000\nlambdas2\n1.000\n2.351\n6.915\n7.915\n60.931\n0.016\n0.039\n\n\n4\nlambdas2[ses]\n0.842\nlambdas2\n0.709\n2.351\n8.027\n8.736\n60.931\n0.012\n0.039\n\n\n5\nlambdas2[ppsych]\n-0.801\nlambdas2\n0.642\n2.351\n8.242\n8.884\n60.931\n0.011\n0.039\n\n\n6\nlambdas3[read]\n1.000\nlambdas3\n1.000\n2.670\n3.409\n4.409\n60.931\n0.016\n0.044\n\n\n7\nlambdas3[arith]\n0.841\nlambdas3\n0.707\n2.670\n6.168\n6.875\n60.931\n0.012\n0.044\n\n\n8\nlambdas3[spell]\n0.981\nlambdas3\n0.962\n2.670\n3.951\n4.913\n60.931\n0.016\n0.044\nfig, axs = plt.subplots(1, 2, figsize=(20, 9))\naxs = axs.flatten()\naz.plot_energy(idata, ax=axs[0])\naz.plot_forest(idata, var_names=[\"lambdas1\", \"lambdas2\", \"lambdas3\"], combined=True, ax=axs[1]);\nplt.tight_layout();\ndrivers = coords['indicators']\ndef get_posterior_resids(idata, samples=100, metric=\"cov\"):\n    resids = []\n    for i in range(100):\n        if metric == \"cov\":\n            model_cov = pd.DataFrame(\n                az.extract(idata[\"posterior_predictive\"])[\"likelihood\"][:, :, i]\n            ).cov()\n            obs_cov = df[drivers].cov()\n        else:\n            model_cov = pd.DataFrame(\n                az.extract(idata[\"posterior_predictive\"])[\"likelihood\"][:, :, i]\n            ).corr()\n            obs_cov = df[drivers].corr()\n        model_cov.index = obs_cov.index\n        model_cov.columns = obs_cov.columns\n        residuals = model_cov - obs_cov\n        resids.append(residuals.values.flatten())\n\n    residuals_posterior = pd.DataFrame(pd.DataFrame(resids).mean().values.reshape(9, 9))\n    residuals_posterior.index = obs_cov.index\n    residuals_posterior.columns = obs_cov.index\n    return residuals_posterior\n\n\nresiduals_posterior_cov = get_posterior_resids(idata, 1000)\nresiduals_posterior_corr = get_posterior_resids(idata, 1000, metric=\"corr\")\nfig, ax = plt.subplots(figsize=(20, 10))\nmask = np.triu(np.ones_like(residuals_posterior_corr, dtype=bool))\nax = sns.heatmap(residuals_posterior_corr, annot=True, cmap=\"bwr\", mask=mask)\nax.set_title(\"Residuals between Model Implied and Sample Correlations \\n CFA\", fontsize=25);\ndef make_ppc(\n    idata,\n    samples=100,\n    drivers=drivers,\n    dims=(3, 3),\n):\n    fig, axs = plt.subplots(dims[0], dims[1], figsize=(20, 10))\n    axs = axs.flatten()\n    for i in range(len(drivers)):\n        for j in range(samples):\n            temp = az.extract(idata[\"posterior_predictive\"].sel({\"likelihood_dim_3\": i}))[\n                \"likelihood\"\n            ].values[:, j]\n            temp = pd.DataFrame(temp, columns=[\"likelihood\"])\n            if j == 0:\n                axs[i].hist(df[drivers[i]], alpha=0.3, ec=\"black\", bins=20, label=\"Observed Scores\")\n                axs[i].hist(\n                    temp[\"likelihood\"], color=\"purple\", alpha=0.1, bins=20, label=\"Predicted Scores\"\n                )\n            else:\n                axs[i].hist(df[drivers[i]], alpha=0.3, ec=\"black\", bins=20)\n                axs[i].hist(temp[\"likelihood\"], color=\"purple\", alpha=0.1, bins=20)\n            axs[i].set_title(f\"Posterior Predictive Checks {drivers[i]}\")\n            axs[i].legend()\n    plt.tight_layout()\n    plt.show()\n\n\nmake_ppc(idata)"
  },
  {
    "objectID": "scratchwork/bayesian_sem.html#measurment-model",
    "href": "scratchwork/bayesian_sem.html#measurment-model",
    "title": "Examined Algorithms",
    "section": "Measurment Model",
    "text": "Measurment Model\n\ncoords = {\n    \"obs\": list(range(len(df))),\n    \"indicators\": [\"motiv\", \"harm\", \"stabi\", \"verbal\", \"ses\", \"ppsych\", \"read\", \"arith\", \"spell\"],\n    \"indicators_1\": [\"motiv\", \"harm\", \"stabi\"],\n    \"indicators_2\": [\"verbal\", \"ses\", \"ppsych\"],\n    \"indicators_3\": [\"read\", \"arith\", \"spell\"],\n    \"latent\": [\"adjust\", \"risk\", \"achieve\"],\n}\n\ndef make_lambda(indicators, name='lambdas1', priors=[1, 10]):\n    \"\"\" Takes an argument indicators which is a string in the coords dict\"\"\"\n    temp_name = name + '_'\n    lambdas_ = pm.Normal(temp_name, priors[0], priors[1], dims=(indicators))\n    # Force a fixed scale on the factor loadings for factor 1\n    lambdas_1 = pm.Deterministic(\n        name, pt.set_subtensor(lambdas_[0], 1), dims=(indicators)\n    )\n    return lambdas_1\n\n\nobs_idx = list(range(len(df)))\nwith pm.Model(coords=coords) as model:\n\n    # Set up Factor Loadings\n    lambdas_1 = make_lambda('indicators_1', 'lambdas1')\n    lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, 2])\n    lambdas_3 = make_lambda('indicators_3', 'lambdas3')\n    # Specify covariance structure between latent factors\n    kappa = 0\n    sd_dist = pm.Exponential.dist(1.0, shape=3)\n    chol, _, _ = pm.LKJCholeskyCov(\"chol_cov\", n=3, eta=2, sd_dist=sd_dist, compute_corr=True)\n    ksi = pm.MvNormal(\"ksi\", kappa, chol=chol, dims=(\"obs\", \"latent\"))\n\n    # Construct Pseudo Observation matrix based on Factor Loadings\n    m1 = ksi[obs_idx, 0] * lambdas_1[0]\n    m2 = ksi[obs_idx, 0] * lambdas_1[1]\n    m3 = ksi[obs_idx, 0] * lambdas_1[2]\n    m4 = ksi[obs_idx, 1] * lambdas_2[0]\n    m5 = ksi[obs_idx, 1] * lambdas_2[1]\n    m6 = ksi[obs_idx, 1] * lambdas_2[2]\n    m7 = ksi[obs_idx, 2] * lambdas_3[0]\n    m8 = ksi[obs_idx, 2] * lambdas_3[1]\n    m9 = ksi[obs_idx, 2] * lambdas_3[2]\n\n    mu = pm.Deterministic(\"mu\", pm.math.stack([m1, m2, m3, m4, m5, m6, m7, m8, m9]).T)\n\n    ## Error Terms\n    Psi = pm.InverseGamma(\"Psi\", 2, 13, dims=\"indicators\")\n\n    # Likelihood\n    _ = pm.Normal(\n        \"likelihood\",\n        mu,\n        Psi,\n        observed=df[coords['indicators']].values,\n    )\n\n    idata = pm.sample(\n        draws=1000,\n        chains=4,\n        nuts_sampler=\"numpyro\", target_accept=0.95, idata_kwargs={\"log_likelihood\": True}, \n        tune=2000,\n        random_seed=150\n    )\n    idata.extend(pm.sample_posterior_predictive(idata))\n\npm.model_to_graphviz(model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naz.summary(idata, var_names=[\"lambdas1\", \"lambdas2\", \"lambdas3\"])\n\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      lambdas1[motiv]\n      1.000\n      0.000\n      1.000\n      1.000\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      lambdas1[harm]\n      0.895\n      0.043\n      0.814\n      0.973\n      0.002\n      0.001\n      521.0\n      1401.0\n      1.01\n    \n    \n      lambdas1[stabi]\n      0.703\n      0.047\n      0.617\n      0.791\n      0.002\n      0.001\n      801.0\n      1571.0\n      1.01\n    \n    \n      lambdas2[verbal]\n      1.000\n      0.000\n      1.000\n      1.000\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      lambdas2[ses]\n      0.842\n      0.087\n      0.679\n      1.000\n      0.004\n      0.003\n      453.0\n      1294.0\n      1.01\n    \n    \n      lambdas2[ppsych]\n      -0.801\n      0.084\n      -0.969\n      -0.653\n      0.004\n      0.003\n      521.0\n      1276.0\n      1.00\n    \n    \n      lambdas3[read]\n      1.000\n      0.000\n      1.000\n      1.000\n      0.000\n      0.000\n      4000.0\n      4000.0\n      NaN\n    \n    \n      lambdas3[arith]\n      0.841\n      0.035\n      0.773\n      0.906\n      0.001\n      0.001\n      1955.0\n      2586.0\n      1.00\n    \n    \n      lambdas3[spell]\n      0.981\n      0.029\n      0.927\n      1.037\n      0.001\n      0.001\n      1123.0\n      1700.0\n      1.00\n    \n  \n\n\n\n\n\naz.plot_trace(idata, var_names=['lambdas1', 'lambdas2', 'lambdas3'])\nplt.tight_layout();\n\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/density_utils.py:488: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/density_utils.py:488: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/density_utils.py:488: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n\n\n\n\n\n\ndef make_factor_loadings_df(idata):\n    factor_loadings = pd.DataFrame(\n        az.summary(\n            idata, var_names=[\"lambdas1\", \"lambdas2\", \"lambdas3\"]\n        )[\"mean\"]\n    ).reset_index()\n    factor_loadings[\"factor\"] = factor_loadings[\"index\"].str.split(\"[\", expand=True)[0]\n    factor_loadings.columns = [\"factor_loading\", \"factor_loading_weight\", \"factor\"]\n    factor_loadings[\"factor_loading_weight_sq\"] = factor_loadings[\"factor_loading_weight\"] ** 2\n    factor_loadings[\"sum_sq_loadings\"] = factor_loadings.groupby(\"factor\")[\n        \"factor_loading_weight_sq\"\n    ].transform(sum)\n    factor_loadings[\"error_variances\"] = az.summary(idata, var_names=[\"Psi\"])[\"mean\"].values\n    factor_loadings[\"total_indicator_variance\"] = (\n        factor_loadings[\"factor_loading_weight_sq\"] + factor_loadings[\"error_variances\"]\n    )\n    factor_loadings[\"total_variance\"] = factor_loadings[\"total_indicator_variance\"].sum()\n    factor_loadings[\"indicator_explained_variance\"] = (\n        factor_loadings[\"factor_loading_weight_sq\"] / factor_loadings[\"total_variance\"]\n    )\n    factor_loadings[\"factor_explained_variance\"] = (\n        factor_loadings[\"sum_sq_loadings\"] / factor_loadings[\"total_variance\"]\n    )\n    num_cols = [c for c in factor_loadings.columns if not c in [\"factor_loading\", \"factor\"]]\n    return factor_loadings\n\n\npd.set_option(\"display.max_colwidth\", 15)\nfactor_loadings = make_factor_loadings_df(idata)\nnum_cols = [c for c in factor_loadings.columns if not c in [\"factor_loading\", \"factor\"]]\nfactor_loadings.style.format(\"{:.3f}\", subset=num_cols).background_gradient(\n    axis=0, subset=[\"indicator_explained_variance\", \"factor_explained_variance\"]\n)\n\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/var/folders/__/ng_3_9pn1f11ftyml_qr69vh0000gn/T/ipykernel_19714/2929805399.py:12: FutureWarning: The provided callable <built-in function sum> is currently using SeriesGroupBy.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n  ].transform(sum)\n\n\n\n\n\n  \n    \n       \n      factor_loading\n      factor_loading_weight\n      factor\n      factor_loading_weight_sq\n      sum_sq_loadings\n      error_variances\n      total_indicator_variance\n      total_variance\n      indicator_explained_variance\n      factor_explained_variance\n    \n  \n  \n    \n      0\n      lambdas1[motiv]\n      1.000\n      lambdas1\n      1.000\n      2.295\n      3.676\n      4.676\n      60.931\n      0.016\n      0.038\n    \n    \n      1\n      lambdas1[harm]\n      0.895\n      lambdas1\n      0.801\n      2.295\n      5.623\n      6.424\n      60.931\n      0.013\n      0.038\n    \n    \n      2\n      lambdas1[stabi]\n      0.703\n      lambdas1\n      0.494\n      2.295\n      7.605\n      8.099\n      60.931\n      0.008\n      0.038\n    \n    \n      3\n      lambdas2[verbal]\n      1.000\n      lambdas2\n      1.000\n      2.351\n      6.915\n      7.915\n      60.931\n      0.016\n      0.039\n    \n    \n      4\n      lambdas2[ses]\n      0.842\n      lambdas2\n      0.709\n      2.351\n      8.027\n      8.736\n      60.931\n      0.012\n      0.039\n    \n    \n      5\n      lambdas2[ppsych]\n      -0.801\n      lambdas2\n      0.642\n      2.351\n      8.242\n      8.884\n      60.931\n      0.011\n      0.039\n    \n    \n      6\n      lambdas3[read]\n      1.000\n      lambdas3\n      1.000\n      2.670\n      3.409\n      4.409\n      60.931\n      0.016\n      0.044\n    \n    \n      7\n      lambdas3[arith]\n      0.841\n      lambdas3\n      0.707\n      2.670\n      6.168\n      6.875\n      60.931\n      0.012\n      0.044\n    \n    \n      8\n      lambdas3[spell]\n      0.981\n      lambdas3\n      0.962\n      2.670\n      3.951\n      4.913\n      60.931\n      0.016\n      0.044\n    \n  \n\n\n\n\nfig, axs = plt.subplots(1, 2, figsize=(20, 9))\naxs = axs.flatten()\naz.plot_energy(idata, ax=axs[0])\naz.plot_forest(idata, var_names=[\"lambdas1\", \"lambdas2\", \"lambdas3\"], combined=True, ax=axs[1]);\nplt.tight_layout();\n\n\n\n\n\ndrivers = coords['indicators']\ndef get_posterior_resids(idata, samples=100, metric=\"cov\"):\n    resids = []\n    for i in range(100):\n        if metric == \"cov\":\n            model_cov = pd.DataFrame(\n                az.extract(idata[\"posterior_predictive\"])[\"likelihood\"][:, :, i]\n            ).cov()\n            obs_cov = df[drivers].cov()\n        else:\n            model_cov = pd.DataFrame(\n                az.extract(idata[\"posterior_predictive\"])[\"likelihood\"][:, :, i]\n            ).corr()\n            obs_cov = df[drivers].corr()\n        model_cov.index = obs_cov.index\n        model_cov.columns = obs_cov.columns\n        residuals = model_cov - obs_cov\n        resids.append(residuals.values.flatten())\n\n    residuals_posterior = pd.DataFrame(pd.DataFrame(resids).mean().values.reshape(9, 9))\n    residuals_posterior.index = obs_cov.index\n    residuals_posterior.columns = obs_cov.index\n    return residuals_posterior\n\n\nresiduals_posterior_cov = get_posterior_resids(idata, 1000)\nresiduals_posterior_corr = get_posterior_resids(idata, 1000, metric=\"corr\")\n\n\nfig, ax = plt.subplots(figsize=(20, 10))\nmask = np.triu(np.ones_like(residuals_posterior_corr, dtype=bool))\nax = sns.heatmap(residuals_posterior_corr, annot=True, cmap=\"bwr\", mask=mask)\nax.set_title(\"Residuals between Model Implied and Sample Correlations\", fontsize=25);\n\n\n\n\n\ndef make_ppc(\n    idata,\n    samples=100,\n    drivers=drivers,\n    dims=(3, 3),\n):\n    fig, axs = plt.subplots(dims[0], dims[1], figsize=(20, 10))\n    axs = axs.flatten()\n    for i in range(len(drivers)):\n        for j in range(samples):\n            temp = az.extract(idata[\"posterior_predictive\"].sel({\"likelihood_dim_3\": i}))[\n                \"likelihood\"\n            ].values[:, j]\n            temp = pd.DataFrame(temp, columns=[\"likelihood\"])\n            if j == 0:\n                axs[i].hist(df[drivers[i]], alpha=0.3, ec=\"black\", bins=20, label=\"Observed Scores\")\n                axs[i].hist(\n                    temp[\"likelihood\"], color=\"purple\", alpha=0.1, bins=20, label=\"Predicted Scores\"\n                )\n            else:\n                axs[i].hist(df[drivers[i]], alpha=0.3, ec=\"black\", bins=20)\n                axs[i].hist(temp[\"likelihood\"], color=\"purple\", alpha=0.1, bins=20)\n            axs[i].set_title(f\"Posterior Predictive Checks {drivers[i]}\")\n            axs[i].legend()\n    plt.tight_layout()\n    plt.show()\n\n\n#make_ppc(idata)"
  },
  {
    "objectID": "scratchwork/bayesian_sem.html#sem-model",
    "href": "scratchwork/bayesian_sem.html#sem-model",
    "title": "Measurment Model",
    "section": "SEM Model",
    "text": "SEM Model\n\nimport numpy as np\nimport pandas as pd\ndata = np.random.multivariate_normal([0,0,0], [[1, 0, 0], [0,1, 0], [0, 0, 1]], size=(500))\ndata = np.ones((500, 3))\nbetas = np.zeros((3, 3))\nbetas[1, 0] = 0.5\nbetas[1, 2] = 0.5\nbetas[0, 2] = 0.7\n\nB_matrix = pd.DataFrame(betas, index=['adjust_coef', 'risk_coef', 'achieve_coef'], \n                                columns=['adjust_target', 'risk_target', 'achieve_target'])\ndata = pd.DataFrame(data, columns=['adjust', 'risk', 'achieve'])\nB_matrix\n\n\n\n\n\n\n\n\nadjust_target\nrisk_target\nachieve_target\n\n\n\n\nadjust_coef\n0.0\n0.0\n0.7\n\n\nrisk_coef\n0.5\n0.0\n0.5\n\n\nachieve_coef\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\ndata.head()\n\n\n\n\n\n\n\n\nadjust\nrisk\nachieve\n\n\n\n\n0\n1.0\n1.0\n1.0\n\n\n1\n1.0\n1.0\n1.0\n\n\n2\n1.0\n1.0\n1.0\n\n\n3\n1.0\n1.0\n1.0\n\n\n4\n1.0\n1.0\n1.0\n\n\n\n\n\n\n\n\nnp.dot(data, B_matrix)\n\narray([[0.5, 0. , 1.2],\n       [0.5, 0. , 1.2],\n       [0.5, 0. , 1.2],\n       ...,\n       [0.5, 0. , 1.2],\n       [0.5, 0. , 1.2],\n       [0.5, 0. , 1.2]])\n\n\n\n\n\ndef make_sem():\n    coords = {\n    \"obs\": list(range(len(df))),\n    \"indicators\": [\"motiv\", \"harm\", \"stabi\", \"verbal\", \"ses\", \"ppsych\", \"read\", \"arith\", \"spell\"],\n    \"indicators_1\": [\"motiv\", \"harm\", \"stabi\"],\n    \"indicators_2\": [\"verbal\", \"ses\", \"ppsych\"],\n    \"indicators_3\": [\"read\", \"arith\", \"spell\"],\n    \"latent\": [\"adjust\", \"risk\", \"achieve\"],\n    \"paths\": [\"risk-&gt;adjust\", \"risk--&gt;achieve\", \"adjust--&gt;achieve\"]\n    }\n\n    obs_idx = list(range(len(df)))\n    with pm.Model(coords=coords) as model:\n\n        # Set up Factor Loadings\n        lambdas_1 = make_lambda('indicators_1', 'lambdas1', priors=[1, .5]) #adjust\n        lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, .5]) # risk\n        lambdas_3 = make_lambda('indicators_3', 'lambdas3', priors=[1, .5]) # achieve\n        \n        # Specify covariance structure between latent factors\n        sd_dist = pm.Exponential.dist(1.0, shape=3)\n        chol, _, _ = pm.LKJCholeskyCov(\"chol_cov\", n=3, eta=2, sd_dist=sd_dist, compute_corr=True)\n        ksi = pm.MvNormal(\"ksi\", 0, chol=chol, dims=(\"obs\", \"latent\"))\n\n        ## Build Regression Components\n        coefs = pm.Normal('betas', 0, .5, dims='paths')\n        zeros = pt.zeros((3, 3))\n        ## adjust ~ risk\n        zeros1 = pt.set_subtensor(zeros[1, 0], coefs[0])\n        ## achieve ~ risk + adjust\n        zeros2 = pt.set_subtensor(zeros1[1, 2], coefs[1])\n        coefs_ = pt.set_subtensor(zeros2[0, 2], coefs[2])\n        \n        structural_relations = pm.Deterministic('endogenous_structural_paths', pm.math.dot(ksi, coefs_))\n\n        # Construct Pseudo Observation matrix based on Factor Loadings\n        m1 = ksi[obs_idx, 0] * lambdas_1[0] +  structural_relations[obs_idx, 0] #adjust\n        m2 = ksi[obs_idx, 0] * lambdas_1[1] +  structural_relations[obs_idx, 0] #adjust\n        m3 = ksi[obs_idx, 0] * lambdas_1[2] +  structural_relations[obs_idx, 0] #adjust\n        m4 = ksi[obs_idx, 1] * lambdas_2[0] +  structural_relations[obs_idx, 1]  #risk\n        m5 = ksi[obs_idx, 1] * lambdas_2[1] +  structural_relations[obs_idx, 1]  #risk\n        m6 = ksi[obs_idx, 1] * lambdas_2[2] +  structural_relations[obs_idx, 1]  #risk\n        m7 = ksi[obs_idx, 2] * lambdas_3[0] +  structural_relations[obs_idx, 2]  #achieve\n        m8 = ksi[obs_idx, 2] * lambdas_3[1] +  structural_relations[obs_idx, 2]  #achieve\n        m9 = ksi[obs_idx, 2] * lambdas_3[2] +  structural_relations[obs_idx, 2]  #achieve\n\n        mu = pm.Deterministic(\"mu\", pm.math.stack([m1, m2, m3, m4, m5, m6, m7, m8, m9]).T)\n\n\n        ## Error Terms\n        Psi = pm.InverseGamma(\"Psi\", 2, 13, dims=\"indicators\")\n\n        # Likelihood\n        _ = pm.Normal(\n            \"likelihood\",\n            mu,\n            Psi,\n            observed=df[coords['indicators']].values,\n        )\n\n        idata = pm.sample(\n            draws=2500,\n            chains=4,\n            #nuts_sampler=\"numpyro\", \n            target_accept=0.90, idata_kwargs={\"log_likelihood\": True}, \n            random_seed=150,\n            tune=1000,\n        )\n        idata.extend(pm.sample_posterior_predictive(idata))\n    return idata, model\n\nidata_sem, model_sem = make_sem()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [lambdas1_, lambdas2_, lambdas3_, chol_cov, ksi, betas, Psi]\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n\n\n\n\n\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 2_500 draw iterations (4_000 + 10_000 draws total) took 164 seconds.\nThere were 1 divergences after tuning. Increase `target_accept` or reparameterize.\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\nSampling: [likelihood]\n\n\n\n\n\n\n\n\n\naz.summary(idata_sem, var_names=[\"lambdas1\", \"lambdas2\", \"lambdas3\", \"betas\"])\n\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nlambdas1[motiv]\n1.000\n0.000\n1.000\n1.000\n0.000\n0.000\n10000.0\n10000.0\nNaN\n\n\nlambdas1[harm]\n0.850\n0.045\n0.766\n0.933\n0.001\n0.001\n997.0\n2361.0\n1.00\n\n\nlambdas1[stabi]\n0.651\n0.052\n0.556\n0.748\n0.002\n0.001\n988.0\n3122.0\n1.00\n\n\nlambdas2[verbal]\n1.000\n0.000\n1.000\n1.000\n0.000\n0.000\n10000.0\n10000.0\nNaN\n\n\nlambdas2[ses]\n0.833\n0.081\n0.681\n0.986\n0.002\n0.001\n2282.0\n4694.0\n1.00\n\n\nlambdas2[ppsych]\n-0.764\n0.079\n-0.920\n-0.623\n0.002\n0.001\n2399.0\n4215.0\n1.00\n\n\nlambdas3[read]\n1.000\n0.000\n1.000\n1.000\n0.000\n0.000\n10000.0\n10000.0\nNaN\n\n\nlambdas3[arith]\n0.610\n0.057\n0.498\n0.712\n0.003\n0.002\n381.0\n1068.0\n1.00\n\n\nlambdas3[spell]\n0.825\n0.037\n0.755\n0.894\n0.001\n0.001\n757.0\n1717.0\n1.00\n\n\nbetas[risk-&gt;adjust]\n0.202\n0.139\n-0.065\n0.457\n0.010\n0.007\n186.0\n202.0\n1.02\n\n\nbetas[risk--&gt;achieve]\n0.207\n0.178\n-0.129\n0.534\n0.012\n0.009\n208.0\n487.0\n1.01\n\n\nbetas[adjust--&gt;achieve]\n0.747\n0.104\n0.556\n0.942\n0.006\n0.004\n311.0\n810.0\n1.01\n\n\n\n\n\n\n\n\nmake_ppc(idata_sem)\n\n\n\n\n\n\n\n\n\nresiduals_posterior_cov = get_posterior_resids(idata_sem, 1000)\nresiduals_posterior_corr = get_posterior_resids(idata_sem, 1000, metric=\"corr\")\n\n\nfig, ax = plt.subplots(figsize=(20, 10))\nmask = np.triu(np.ones_like(residuals_posterior_corr, dtype=bool))\nax = sns.heatmap(residuals_posterior_corr, annot=True, cmap=\"bwr\", mask=mask)\nax.set_title(\"Residuals between Model Implied and Sample Correlations \\n SEM model\", fontsize=25);\n\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(1, 3, figsize=(15, 10))\naxs = axs.flatten()\nax = axs[0]\nax1 = axs[1]\nax2 = axs[2]\naz.plot_forest(idata_sem, var_names=[\"ksi\"], combined=True, ax=ax, coords={\"latent\": [\"adjust\"]})\naz.plot_forest(\n    idata_sem,\n    var_names=[\"ksi\"],\n    combined=True,\n    ax=ax1,\n    colors=\"forestgreen\",\n    coords={\"latent\": [\"risk\"]},\n)\naz.plot_forest(\n    idata_sem,\n    var_names=[\"ksi\"],\n    combined=True,\n    ax=ax2,\n    colors=\"slateblue\",\n    coords={\"latent\": [\"achieve\"]},\n)\nax.set_yticklabels([])\nax.set_xlabel(\"ADJUST\")\nax1.set_yticklabels([])\nax1.set_xlabel(\"RISK\")\nax2.set_yticklabels([])\nax2.set_xlabel(\"ACHIEVE\")\nax.axvline(-2, color=\"red\")\nax1.axvline(-2, color=\"red\")\nax2.axvline(-2, color=\"red\")\nax.set_title(\"Individual On Latent Factor ADJUST\")\nax1.set_title(\"Individual On Latent Factor RISK\")\nax2.set_title(\"Individual On Latent Factor ACHIEVE\")\nplt.show();\n\n\n\n\n\n\n\n\n\ncompare_df = az.compare({'SEM': idata_sem, 'CFA': idata}, 'waic')\ncompare_df\n\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/stats.py:1653: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. \nSee http://arxiv.org/abs/1507.04544 for details\n  warnings.warn(\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/stats.py:1653: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. \nSee http://arxiv.org/abs/1507.04544 for details\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nrank\nelpd_waic\np_waic\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\nSEM\n0\n-14525.202949\n905.303983\n0.000000\n1.0\n49.632750\n0.000000\nTrue\nlog\n\n\nCFA\n1\n-14748.968775\n939.024032\n223.765826\n0.0\n48.571539\n12.080799\nTrue\nlog\n\n\n\n\n\n\n\n\naz.plot_compare(compare_df);"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#the-do-operator-and-sensitivity-analysis",
    "href": "talks/ukraine_sem/talk4ukraine.html#the-do-operator-and-sensitivity-analysis",
    "title": "Structural Causal Models in PyMC",
    "section": "The Do-Operator and Sensitivity Analysis",
    "text": "The Do-Operator and Sensitivity Analysis\n\n\n\n\n\nCausal Hierarchy\n\n\n\nThe graph algebra of the do-calculus sets rules on admissable structures required to warrant valid causal claims when accounting for different species of confounding.\nThey complement the development and analysis of SEM models, providing adequacy conditions for a causal interpretation of these substantive structural relations.\nThe justification of the causal claims implicit in a SEM model need to be made more explicitly by the researcher with reference to evaluation of model fit to data."
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#factor-structures-and-the-do-calculus",
    "href": "talks/ukraine_sem/talk4ukraine.html#factor-structures-and-the-do-calculus",
    "title": "Structural Causal Models in PyMC",
    "section": "Factor Structures and the Do-Calculus",
    "text": "Factor Structures and the Do-Calculus\n\n“[T]ypically the causal assumptions are less established, though they should be defensible and consistent with the current state of knowledge. The analysis is done under the speculation of “what if these causal assumptions were true.” These latter analyses are useful because there are often ways of testing the model, or parts of it. These tests can be helpful in rejecting one or more of the causal assumptions, thereby revealing flaws in specification. Of course, passing these tests does not prove the validity of the causal assumptions, but it lends credibility to them. If we repeatedly test the model in diverse data sets and find good matches to the data, then the causal assumptions further gain in their credibility”\n\n\nBollen and Pearl in Eight myths about causality and structural equation models\n\n\n\n\nSCMS in PyMC"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#the-structural-matrix-1",
    "href": "talks/ukraine_sem/talk4ukraine.html#the-structural-matrix-1",
    "title": "Structural Causal Models in PyMC",
    "section": "The Structural Matrix",
    "text": "The Structural Matrix\nThe specification of the structural matrix encodes the candidate regression relations which are to be evaluated in the SEM fit.\n\\[ \\overbrace{\\text{Ksi}}^{latent} = \\begin{bmatrix} \\overbrace{2.5}^{adjust} & \\overbrace{3.2}^{risk} & \\overbrace{7.3}^{achieve} \\\\ ... & ... & ...\n\\\\ ... & ... & ...  \\\\ ... & ... \\\\ 3.9 & ...\\end{bmatrix} \\begin{bmatrix} 0 & 0 & \\color{blue}{\\beta_{13}}\n\\\\ \\color{blue}{\\beta_{21}} & 0 & \\color{blue}{\\beta_{23}}\n\\\\ 0 & 0 & 0 \\end{bmatrix} = \\underbrace{\\mathbf{B}}_{Regression \\\\ Coefficients} \\]\n\nadjust ~ risk\n\n\nachieve ~ adjust + risk"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#what-if-structures-and-the-do-calculus",
    "href": "talks/ukraine_sem/talk4ukraine.html#what-if-structures-and-the-do-calculus",
    "title": "Structural Causal Models in PyMC",
    "section": "What-if Structures and the Do-Calculus",
    "text": "What-if Structures and the Do-Calculus\n\n\n\n“[T]ypically the causal assumptions are less established, though they should be defensible and consistent with the current state of knowledge. The analysis is done under the speculation of “what if these causal assumptions were true.” These latter analyses are useful because there are often ways of testing the model, or parts of it. These tests can be helpful in rejecting one or more of the causal assumptions, thereby revealing flaws in specification. Of course, passing these tests does not prove the validity of the causal assumptions, but it lends credibility to them.”\n\n\nBollen and Pearl in Eight myths about causality and structural equation models\n\n\n\n\n\n\n\n\nSCMS in PyMC"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#the-do-calculus-and-sensitivity-analysis",
    "href": "talks/ukraine_sem/talk4ukraine.html#the-do-calculus-and-sensitivity-analysis",
    "title": "Structural Causal Models in PyMC",
    "section": "The Do-Calculus and Sensitivity Analysis",
    "text": "The Do-Calculus and Sensitivity Analysis\n\n\n\n\n\nCausal Hierarchy\n\n\n\nThe graph algebra of the do-calculus sets rules on admissable structures required to warrant valid causal claims when accounting for different species of confounding.\nThey complement the development and analysis of SEM models, providing minimalist admissablility conditions for a causal interpretation of these structural relations.\nThe substantive justification of the causal claims implicit in a SEM model need to be made more explicitly by the researcher with reference to evaluation of model fit to data by comparing to competing models."
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#what-if-structures-and-the-do-operator",
    "href": "talks/ukraine_sem/talk4ukraine.html#what-if-structures-and-the-do-operator",
    "title": "Structural Causal Models in PyMC",
    "section": "What-if Structures and the Do-Operator",
    "text": "What-if Structures and the Do-Operator\n\n\n\n“[T]ypically the causal assumptions are less established, though they should be defensible and consistent with the current state of knowledge. The analysis is done under the speculation of “what if these causal assumptions were true.” These latter analyses are useful because there are often ways of testing the model, or parts of it. These tests can be helpful in rejecting one or more of the causal assumptions, thereby revealing flaws in specification. Of course, passing these tests does not prove the validity of the causal assumptions, but it lends credibility to them.”\n\n\nBollen and Pearl in Eight myths about causality and structural equation models\n\n\n\n\nThe Do-Operator uses graph mutilation techniques to interrogate the impact of different data generating conditions including the analysis of causal claims."
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#model-graphs-in-pymc",
    "href": "talks/ukraine_sem/talk4ukraine.html#model-graphs-in-pymc",
    "title": "Structural Causal Models in PyMC",
    "section": "Model Graphs in PyMC",
    "text": "Model Graphs in PyMC\n\n\nidata_sem, model_sem = make_sem()\npm.model_to_graphviz(model_sem)\n\n\nmodel_beta0 = do(model_sem, {\"betas\": [0, 0 , 0]}, \nprune_vars=True)\npm.model_to_graphviz(model_beta0)\n\n\n\n\n\n\nSCMS in PyMC"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#graphs-mutilation-in-pymc",
    "href": "talks/ukraine_sem/talk4ukraine.html#graphs-mutilation-in-pymc",
    "title": "Structural Causal Models in PyMC",
    "section": "Graphs Mutilation in PyMC",
    "text": "Graphs Mutilation in PyMC\n\n\n## Model Estimated on Data\nidata_sem, model_sem = make_sem()\npm.model_to_graphviz(model_sem)\n\n\n## The Do-Operator in Action\nmodel_beta0 = do(model_sem, \n{\"betas\": [0, 0 , 0]}, \nprune_vars=True)\npm.model_to_graphviz(model_beta0)"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#causal-estimands-under-intervention",
    "href": "talks/ukraine_sem/talk4ukraine.html#causal-estimands-under-intervention",
    "title": "Structural Causal Models in PyMC",
    "section": "Causal Estimands under Intervention",
    "text": "Causal Estimands under Intervention\nmodel_beta0 = do(model_sem, {\"betas\": [0, 0 , 0]}, prune_vars=True)\nmodel_beta1 = do(model_sem, {\"betas\": [.6, .3, .7]}, prune_vars=True)\n\n# Sample new data assuming path parameters 0: P(Y | c, do(beta=0))\nidata_z0 = pm.sample_posterior_predictive(\n    idata_sem,\n    model=model_beta0,\n    predictions=False,\n    var_names=[\"likelihood\", \"betas\",],\n)\n# Sample new data assuming substantive specification: P(Y | c, do(z=[.6, .3, .7]))\nidata_z1 = pm.sample_posterior_predictive(\n    idata_sem,\n    model=model_beta1,\n    predictions=False,\n    var_names=[\"likelihood\", \"betas\"],\n)"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#causal-estimands-under-intervention-1",
    "href": "talks/ukraine_sem/talk4ukraine.html#causal-estimands-under-intervention-1",
    "title": "Structural Causal Models in PyMC",
    "section": "Causal Estimands under Intervention",
    "text": "Causal Estimands under Intervention\n\\[ E\\Big[(Y| do(\\beta=0)) - (Y | do(\\beta \\neq 0) \\Big] \\]"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#model-fit-sensitivity-under-intervention",
    "href": "talks/ukraine_sem/talk4ukraine.html#model-fit-sensitivity-under-intervention",
    "title": "Structural Causal Models in PyMC",
    "section": "Model Fit: Sensitivity under Intervention",
    "text": "Model Fit: Sensitivity under Intervention\n\n\n\n\n\nThe plots are scaled identically here between -1 and 1. Highlighting significantly worse model fit under implausibly zero’d out beta coefficients."
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#conclusion",
    "href": "talks/ukraine_sem/talk4ukraine.html#conclusion",
    "title": "Structural Causal Models in PyMC",
    "section": "Conclusion",
    "text": "Conclusion\nCredibility versus Certification\n\n\n\n“SEM is an inference engine that takes in two inputs, qualitative causal assumptions and empirical data, and derives two logical consequences of these inputs: quantitative causal conclusions and statistical measures of fit for the testable implications of the assumptions. Failure to fit the data casts doubt on the strong causal assumptions of zero coefficients or zero covariances and guides the researcher to diagnose, or repair the structural misspecifications. Fitting the data does not “prove” the causal assumptions, but it makes them tentatively more plausible.”\n\n\nBollen and Pearl in Eight myths about causality and structural equation models\n\n\n\n\n\nAn Inference Engine adding ***’s to our conclusion"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#measurement-models-2",
    "href": "talks/ukraine_sem/talk4ukraine.html#measurement-models-2",
    "title": "Structural Causal Models in PyMC",
    "section": "Measurement Models",
    "text": "Measurement Models\nand Factor Structures\nThe Confirmatory Factor Model can also be phrased in the Bayesian way.\n\\[p(\\mathbf{x_{i}}^{T}.....\\mathbf{x_{q}}^{T} | \\text{Ksi}, \\Psi, \\tau, \\Lambda) \\sim Normal(\\tau + \\Lambda\\cdot \\text{Ksi}, \\Psi)\\]\n\\[ \\lambda_{11} ..... \\lambda_{21} ..... \\lambda_{31} \\in \\Lambda \\]\n\n\n\n\n\n  \n    \n       \n      motiv\n      harm\n      stabi\n      ppsych\n      ses\n      verbal\n      read\n      arith\n      spell\n    \n  \n  \n    \n      0\n      -7.907122\n      -5.075312\n      -3.138836\n      -17.800210\n      4.766450\n      -3.633360\n      -3.488981\n      -9.989121\n      -6.567873\n    \n    \n      1\n      1.751478\n      -4.155847\n      3.520752\n      7.009367\n      -6.048681\n      -7.693461\n      -4.520552\n      8.196238\n      8.778973\n    \n    \n      2\n      14.472570\n      -4.540677\n      4.070600\n      23.734260\n      -16.970670\n      -3.909941\n      -4.818170\n      7.529984\n      -5.688716\n    \n    \n      3\n      -1.165421\n      -5.668406\n      2.600437\n      1.493158\n      1.396363\n      21.409450\n      -3.138441\n      5.730547\n      -2.915676\n    \n    \n      4\n      -4.222899\n      -10.072150\n      -6.030737\n      -5.985864\n      -18.376400\n      -1.438816\n      -2.009742\n      -0.623953\n      -1.024624"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#causal-estimands-and-the-do-operator",
    "href": "talks/ukraine_sem/talk4ukraine.html#causal-estimands-and-the-do-operator",
    "title": "Structural Causal Models in PyMC",
    "section": "Causal Estimands and the Do-Operator",
    "text": "Causal Estimands and the Do-Operator\n\n\n\n\n\nCausal Hierarchy\n\n\n\nThe graph algebra of the do-calculus sets rules on admissable structures required to warrant valid causal claims when accounting for different species of confounding.\nThey complement the development and analysis of SEM models, providing minimalist admissablility conditions for a causal interpretation of these structural relations.\nThe substantive justification of the causal claims implicit in a SEM model need to be made more explicitly by the researcher with reference to evaluation of model fit to data by comparing to competing models."
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#graph-mutilation-in-pymc",
    "href": "talks/ukraine_sem/talk4ukraine.html#graph-mutilation-in-pymc",
    "title": "Structural Causal Models in PyMC",
    "section": "Graph Mutilation in PyMC",
    "text": "Graph Mutilation in PyMC\n\n\n## Model Estimated on Data\nidata_sem, model_sem = make_sem()\npm.model_to_graphviz(model_sem)\n\n\n## The Do-Operator in Action\nmodel_beta0 = do(model_sem, \n{\"betas\": [0, 0 , 0]}, \nprune_vars=True)\npm.model_to_graphviz(model_beta0)"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#the-sem-model-in-pymc",
    "href": "talks/ukraine_sem/talk4ukraine.html#the-sem-model-in-pymc",
    "title": "Structural Causal Models in PyMC",
    "section": "The SEM Model in PyMC",
    "text": "The SEM Model in PyMC\nDefining the DAG\n\ncoords = {\n    \"obs\": list(range(len(df))),\n    \"indicators\": [\"motiv\", \"harm\", \"stabi\", \"verbal\", \"ses\", \"ppsych\", \"read\", \"arith\", \"spell\"],\n    \"indicators_1\": [\"motiv\", \"harm\", \"stabi\"],\n    \"indicators_2\": [\"verbal\", \"ses\", \"ppsych\"],\n    \"indicators_3\": [\"read\", \"arith\", \"spell\"],\n    \"latent\": [\"adjust\", \"risk\", \"achieve\"],\n    \"paths\": [\"risk-&gt;adjust\", \"risk--&gt;achieve\", \"adjust--&gt;achieve\"]\n    }\n\n    obs_idx = list(range(len(df)))\n    with pm.Model(coords=coords) as model:\n\n        # Set up Factor Loadings\n        lambdas_1 = make_lambda('indicators_1', 'lambdas1', priors=[1, .5]) #adjust\n        lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, .5]) # risk\n        lambdas_3 = make_lambda('indicators_3', 'lambdas3', priors=[1, .5]) # achieve\n        \n        # Specify covariance structure between latent factors\n        sd_dist = pm.Exponential.dist(1.0, shape=3)\n        chol, _, _ = pm.LKJCholeskyCov(\"chol_cov\", n=3, eta=2, sd_dist=sd_dist, compute_corr=True)\n        ksi = pm.MvNormal(\"ksi\", 0, chol=chol, dims=(\"obs\", \"latent\"))\n\n        ## Build Regression Components\n        coefs = pm.Normal('betas', 0, .5, dims='paths')\n        zeros = pt.zeros((3, 3))\n        ## adjust ~ risk\n        zeros1 = pt.set_subtensor(zeros[1, 0], coefs[0])\n        ## achieve ~ risk + adjust\n        zeros2 = pt.set_subtensor(zeros1[1, 2], coefs[1])\n        coefs_ = pt.set_subtensor(zeros2[0, 2], coefs[2])\n        \n        structural_relations = pm.Deterministic('endogenous_structural_paths', \n        pm.math.dot(ksi, coefs_))\n\n        # Construct Pseudo Observation matrix based on Factor Loadings\n        m1 = ksi[obs_idx, 0] * lambdas_1[0] +  structural_relations[obs_idx, 0] #adjust\n        m2 = ksi[obs_idx, 0] * lambdas_1[1] +  structural_relations[obs_idx, 0] #adjust\n        m3 = ksi[obs_idx, 0] * lambdas_1[2] +  structural_relations[obs_idx, 0] #adjust\n        m4 = ksi[obs_idx, 1] * lambdas_2[0] +  structural_relations[obs_idx, 1]  #risk\n        m5 = ksi[obs_idx, 1] * lambdas_2[1] +  structural_relations[obs_idx, 1]  #risk\n        m6 = ksi[obs_idx, 1] * lambdas_2[2] +  structural_relations[obs_idx, 1]  #risk\n        m7 = ksi[obs_idx, 2] * lambdas_3[0] +  structural_relations[obs_idx, 2]  #achieve\n        m8 = ksi[obs_idx, 2] * lambdas_3[1] +  structural_relations[obs_idx, 2]  #achieve\n        m9 = ksi[obs_idx, 2] * lambdas_3[2] +  structural_relations[obs_idx, 2]  #achieve\n\n        mu = pm.Deterministic(\"mu\", pm.math.stack([m1, m2, m3, m4, m5, m6, m7, m8, m9]).T)\n\n\n        ## Error Terms\n        Psi = pm.InverseGamma(\"Psi\", 2, 13, dims=\"indicators\")\n\n        # Likelihood\n        _ = pm.Normal(\n            \"likelihood\",\n            mu,\n            Psi,\n            observed=df[coords['indicators']].values,\n        )"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#confirmatory-factor-structures",
    "href": "talks/ukraine_sem/talk4ukraine.html#confirmatory-factor-structures",
    "title": "Structural Causal Models in PyMC",
    "section": "Confirmatory Factor Structures",
    "text": "Confirmatory Factor Structures\nand Conditional Exchangeability\nAssumptions of exchangeability in survey measurement and De-Finetti’s theorem \\[p(Y​,T, X_{1}…,X_n​)  =  \\int p(Y, T, X_{1} ... X_n ​∣\\theta)p(\\theta)d(\\theta)\\]\nensures the observed data can be represented as conditionally independent mixture distribution where in general:\n\\[X_{i} \\perp\\!\\!\\!\\!\\perp X_{j} | \\theta \\text {        } \\forall i \\neq j\\]\nand in paticular latent factors \\(\\Lambda\\) are hypothetical common causes argued to induce conditional exchangeability among the observed data.\n\\[ Y(0), Y(1) \\perp\\!\\!\\!\\!\\perp T | \\Lambda\\]"
  },
  {
    "objectID": "talks/ukraine_sem/talk4ukraine.html#factor-structures-as-measurement-models",
    "href": "talks/ukraine_sem/talk4ukraine.html#factor-structures-as-measurement-models",
    "title": "Structural Causal Models in PyMC",
    "section": "Factor Structures as Measurement Models",
    "text": "Factor Structures as Measurement Models\n\n\n\\[ ADJUST \\sim f_{ADJUST}(motiv, harm, stabi, U)  \\] \\[ RISK \\sim f_{RISK}(ppsych, ses, verbal, U)\\] \\[ ACHIEVE \\sim f_{ACHIEVE}(read, arith, spell, U)\\]\nNon-parametric phrasing of SCMs under-specifies the relations of interest.\nCFA models estimates the multivariate correlation structure while imposing the focal causal structure and dependencies to “hide” detail in measurement error."
  },
  {
    "objectID": "talks/mister_p/mister_p.html#hierarchical-regression-and-ppls",
    "href": "talks/mister_p/mister_p.html#hierarchical-regression-and-ppls",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Hierarchical Regression and PPLs",
    "text": "Hierarchical Regression and PPLs\nDifferences in Strata drawn at Random?\n\\[ y = \\beta_{0} + (\\beta_{1} + \\color{red}\\gamma_{1,g}\\color{black})X_{1} + \\beta_{2}X_{2}\\]\nwhere\n\\[\\beta_{i} \\sim Normal(0, \\color{purple}\\sigma_{i}\\color{black})\\]\nand\n\\[\\color{red}{\\gamma_{1,g}}\\color{black} \\sim Normal(0, \\color{purple}\\sigma_{i}\\color{black}) \\text{ for } g \\in G\\]\n\\[\\sigma_{i} \\sim InverseGamma(?, ?)\\]"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#hierarchical-regression-and-ppls-1",
    "href": "talks/mister_p/mister_p.html#hierarchical-regression-and-ppls-1",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Hierarchical Regression and PPLs",
    "text": "Hierarchical Regression and PPLs\nVectorisation and Python Implementation\n\nimport numpy as np\nimport pandas as pd\n\nb0, b1, b2 = 1, 2, 4\nb_g = np.random.normal(0, 1, 5)\nX = np.random.normal(0, 1, size=(100, 2))\ngroups = np.array(['A', 'B', 'C', 'D', 'E']*20)\n\n# Factorize and index\ngrp_indx, uniques = pd.factorize(groups)\ny = b0 + (b1 +b_g[grp_indx])*X[:, 0] + b2*X[:, 1]"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#hierarchical-regression-and-ppls-2",
    "href": "talks/mister_p/mister_p.html#hierarchical-regression-and-ppls-2",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Hierarchical Regression and PPLs",
    "text": "Hierarchical Regression and PPLs\nVectorisation and Python Implementation\n\n\n\n\n\n\n\n\n\ng\nb_g\nX0\nX1\ny\n\n\n\n\n0\nA\n0.73\n-1.36\n-0.50\n-4.70\n\n\n1\nB\n1.93\n-1.02\n-0.65\n-5.62\n\n\n2\nC\n0.38\n-1.51\n-0.50\n-4.58\n\n\n3\nD\n2.26\n-0.58\n0.42\n0.22\n\n\n4\nE\n-0.85\n-1.02\n0.18\n0.55\n\n\n5\nA\n0.73\n2.01\n-0.64\n3.92\n\n\n6\nB\n1.93\n0.03\n-0.45\n-0.67\n\n\n7\nC\n0.38\n1.55\n-0.00\n4.67\n\n\n8\nD\n2.26\n0.02\n-2.46\n-8.75\n\n\n9\nE\n-0.85\n-3.36\n-0.02\n-2.94"
  },
  {
    "objectID": "talks/mister_p/mister_p.html#regression-as-weighting-adjustment-1",
    "href": "talks/mister_p/mister_p.html#regression-as-weighting-adjustment-1",
    "title": "Multilevel Regression and Post-Stratification",
    "section": "Regression as Weighting Adjustment",
    "text": "Regression as Weighting Adjustment\nRegression automates the more manual re-weighting\n\nWeighted Average"
  },
  {
    "objectID": "talks/value_capture/value_capture.html",
    "href": "talks/value_capture/value_capture.html",
    "title": "Value Capture: The Horror",
    "section": "",
    "text": "Value Capture: Balancing the Horror\nA presentation at the monthly experimentation reading group. You can download the pdf here: Download the pdf"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Examined Algorithms",
    "section": "",
    "text": "I’m a data scientist specialising in probabilistic modelling for the study of risk and causal inference. I have experience in model development, deployment, multivariate testing and monitoring.\nI’m interested in questions of inference and measurement in the face of natural variation and confounding.\nMy academic background is in mathematical logic and philosophy where I mostly imagined possible worlds and modal logics."
  },
  {
    "objectID": "index.html#recent-experience",
    "href": "index.html#recent-experience",
    "title": "Examined Algorithms",
    "section": "",
    "text": "Personio | Staff Data Scientist | December 2021 - Present\n\nWorking with Product and Engineering to measure risk and quantify impact. Revenue Ops to forecasts and optimise processes.\n\nPyMC | Open Source Contributor | November 2021 - Present\n\nHelping to document some of the more esoteric applications of Bayesian modelling in PyMC."
  },
  {
    "objectID": "scratchwork/bayesian_sem.html#counterfactual-analysis",
    "href": "scratchwork/bayesian_sem.html#counterfactual-analysis",
    "title": "Measurment Model",
    "section": "Counterfactual Analysis",
    "text": "Counterfactual Analysis\n\nmodel_sem\n\n\\[\n            \\begin{array}{rcl}\n            \\text{lambdas1\\_} &\\sim & \\operatorname{Normal}(1,~0.5)\\\\\\text{lambdas2\\_} &\\sim & \\operatorname{Normal}(1,~0.5)\\\\\\text{lambdas3\\_} &\\sim & \\operatorname{Normal}(1,~0.5)\\\\\\text{chol\\_cov} &\\sim & \\operatorname{\\_lkjcholeskycov}(3,~2,~\\operatorname{Exponential}(f()))\\\\\\text{ksi} &\\sim & \\operatorname{MultivariateNormal}(f(\\text{chol\\_cov}),~f(\\text{chol\\_cov}))\\\\\\text{betas} &\\sim & \\operatorname{Normal}(0,~0.5)\\\\\\text{Psi} &\\sim & \\operatorname{InverseGamma}(2,~13)\\\\\\text{lambdas1} &\\sim & \\operatorname{Deterministic}(f(\\text{lambdas1\\_}))\\\\\\text{lambdas2} &\\sim & \\operatorname{Deterministic}(f(\\text{lambdas2\\_}))\\\\\\text{lambdas3} &\\sim & \\operatorname{Deterministic}(f(\\text{lambdas3\\_}))\\\\\\text{chol\\_cov\\_corr} &\\sim & \\operatorname{Deterministic}(f(\\text{chol\\_cov}))\\\\\\text{chol\\_cov\\_stds} &\\sim & \\operatorname{Deterministic}(f(\\text{chol\\_cov}))\\\\\\text{endogenous\\_structural\\_paths} &\\sim & \\operatorname{Deterministic}(f(\\text{ksi},~\\text{betas}))\\\\\\text{mu} &\\sim & \\operatorname{Deterministic}(f(\\text{ksi},~\\text{betas},~\\text{lambdas3\\_},~\\text{lambdas2\\_},~\\text{lambdas1\\_}))\\\\\\text{likelihood} &\\sim & \\operatorname{Normal}(\\text{mu},~\\text{Psi})\n            \\end{array}\n            \\]\n\n\n\nmodel_beta0 = do(model_sem, {\"betas\": [0, 0 , 0]}, prune_vars=True)\nmodel_beta1 = do(model_sem, {\"betas\": [.6, .3, .7]}, prune_vars=True)\nmodel_beta0\n\n\\[\n            \\begin{array}{rcl}\n            \\text{lambdas1\\_} &\\sim & \\operatorname{Normal}(1,~0.5)\\\\\\text{lambdas2\\_} &\\sim & \\operatorname{Normal}(1,~0.5)\\\\\\text{lambdas3\\_} &\\sim & \\operatorname{Normal}(1,~0.5)\\\\\\text{chol\\_cov} &\\sim & \\operatorname{\\_lkjcholeskycov}(3,~2,~\\operatorname{Exponential}(f()))\\\\\\text{ksi} &\\sim & \\operatorname{MultivariateNormal}(f(\\text{chol\\_cov}),~f(\\text{chol\\_cov}))\\\\\\text{Psi} &\\sim & \\operatorname{InverseGamma}(2,~13)\\\\\\text{endogenous\\_structural\\_paths} &\\sim & \\operatorname{Deterministic}(f(\\text{ksi}))\\\\\\text{lambdas3} &\\sim & \\operatorname{Deterministic}(f(\\text{lambdas3\\_}))\\\\\\text{lambdas2} &\\sim & \\operatorname{Deterministic}(f(\\text{lambdas2\\_}))\\\\\\text{lambdas1} &\\sim & \\operatorname{Deterministic}(f(\\text{lambdas1\\_}))\\\\\\text{mu} &\\sim & \\operatorname{Deterministic}(f(\\text{ksi},~\\text{lambdas3\\_},~\\text{lambdas2\\_},~\\text{lambdas1\\_}))\\\\\\text{likelihood} &\\sim & \\operatorname{Normal}(f(\\text{ksi},~\\text{lambdas1\\_},~\\text{lambdas2\\_},~\\text{lambdas3\\_}),~\\text{Psi})\n            \\end{array}\n            \\]\n\n\n\n# Sample new sales data assuming Google Ads off: P(Y | c, do(z=0))\nidata_z0 = pm.sample_posterior_predictive(\n    idata_sem,\n    model=model_beta0,\n    predictions=False,\n    var_names=[\"likelihood\", \"betas\",],\n)\n\n\n# Sample new sales data assuming Google Ads off: P(Y | c, do(z=1))\nidata_z1 = pm.sample_posterior_predictive(\n    idata_sem,\n    model=model_beta1,\n    predictions=False,\n    var_names=[\"likelihood\", \"betas\"],\n)\n\nSampling: [likelihood]\n\n\n\n\n\n\n\n\nSampling: [likelihood]\n\n\n\n\n\n\n\n\n\npd.DataFrame((idata_z0['posterior_predictive']['likelihood'] - idata_z1['posterior_predictive']['likelihood']).mean(dim=('chain', 'draw')).values)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n0\n-0.550044\n-0.611088\n-0.628768\n0.086788\n-0.010003\n-0.108678\n4.889491\n4.983223\n4.915345\n\n\n1\n2.344756\n2.603190\n2.575534\n0.071091\n0.038979\n-0.105043\n-0.486688\n-0.592597\n-0.573594\n\n\n2\n4.672426\n4.526858\n4.723740\n-0.134112\n-0.107309\n-0.067223\n-5.387882\n-5.387973\n-5.491420\n\n\n3\n-2.819183\n-2.772283\n-2.644945\n0.060841\n0.001182\n-0.242429\n0.159944\n0.115791\n0.183742\n\n\n4\n1.863068\n1.877680\n1.869703\n-0.162434\n0.054977\n-0.172869\n3.982492\n4.081118\n4.022108\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n-2.877307\n-2.741793\n-2.712314\n0.295789\n0.118695\n-0.106244\n-3.244289\n-3.269911\n-3.144995\n\n\n496\n-1.806035\n-1.866611\n-1.708778\n-0.034715\n-0.125841\n0.024298\n-3.121126\n-3.169021\n-3.109019\n\n\n497\n0.062248\n0.152307\n0.047685\n-0.045339\n0.058982\n0.127697\n-1.064855\n-1.074607\n-0.996607\n\n\n498\n1.900756\n1.845868\n1.797741\n-0.037267\n-0.175881\n-0.083982\n-9.418460\n-9.405972\n-9.496412\n\n\n499\n6.329182\n6.267390\n6.253532\n0.038038\n-0.052934\n-0.127131\n14.509770\n14.682946\n14.486019\n\n\n\n\n500 rows × 9 columns\n\n\n\n\nresiduals_posterior_corr0 = get_posterior_resids(idata_z0, 1000, metric=\"corr\")\n\nresiduals_posterior_corr1 = get_posterior_resids(idata_z1, 1000, metric=\"corr\")\n\n\nresiduals_posterior_corr0 - residuals_posterior_corr1\n\n\n\n\n\n\n\n\nmotiv\nharm\nstabi\nverbal\nses\nppsych\nread\narith\nspell\n\n\n\n\nmotiv\n0.000000\n-0.082205\n-0.130390\n-0.242107\n-0.193577\n0.184254\n-0.897010\n-0.817537\n-0.876321\n\n\nharm\n-0.082205\n0.000000\n-0.154298\n-0.251194\n-0.202228\n0.193474\n-0.795131\n-0.721661\n-0.772816\n\n\nstabi\n-0.130390\n-0.154298\n0.000000\n-0.259143\n-0.209786\n0.191927\n-0.667100\n-0.600199\n-0.653357\n\n\nverbal\n-0.242107\n-0.251194\n-0.259143\n0.000000\n0.006914\n0.000030\n-0.232936\n-0.244641\n-0.239960\n\n\nses\n-0.193577\n-0.202228\n-0.209786\n0.006914\n0.000000\n0.006726\n-0.190744\n-0.210943\n-0.196132\n\n\nppsych\n0.184254\n0.193474\n0.191927\n0.000030\n0.006726\n0.000000\n0.179593\n0.192383\n0.185316\n\n\nread\n-0.897010\n-0.795131\n-0.667100\n-0.232936\n-0.190744\n0.179593\n0.000000\n-0.182936\n-0.086550\n\n\narith\n-0.817537\n-0.721661\n-0.600199\n-0.244641\n-0.210943\n0.192383\n-0.182936\n0.000000\n-0.228749\n\n\nspell\n-0.876321\n-0.772816\n-0.653357\n-0.239960\n-0.196132\n0.185316\n-0.086550\n-0.228749\n0.000000\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(20, 10))\nmask = np.triu(np.ones_like(residuals_posterior_corr1, dtype=bool))\nax = sns.heatmap(residuals_posterior_corr1, annot=True, cmap=\"bwr\", mask=mask)\nax.set_title(\"Residuals between Model Implied and Sample Correlations\", fontsize=25);\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(20, 10))\nmask = np.triu(np.ones_like(residuals_posterior_corr0, dtype=bool))\nax = sns.heatmap(residuals_posterior_corr0, annot=True, cmap=\"bwr\", mask=mask)\nax.set_title(\"Residuals between Model Implied and Sample Correlations\", fontsize=25);"
  },
  {
    "objectID": "notes/certain_things/Wedding/Tattoo Idea.html",
    "href": "notes/certain_things/Wedding/Tattoo Idea.html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "Collage of Hamiltonian MCMC sampling the posterior distribution overlaid with a Sisysphus character pushing a rock up a hill spliced with Bayes Theorem."
  },
  {
    "objectID": "posts/post-with-code/discrete_choice_mlogit/mlogit.html",
    "href": "posts/post-with-code/discrete_choice_mlogit/mlogit.html",
    "title": "R Notebook",
    "section": "",
    "text": "This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code.\nTry executing this chunk by clicking the Run button within the chunk or by placing your cursor inside it and pressing Cmd+Shift+Enter.\n\nlibrary(mlogit)\nlibrary(bayesm)\n\ndf = read.csv('TravelMode.csv')\n#df$choice &lt;- df$choice == 'no'\n#df$mode &lt;- as.factor(df$mode)\ndf\n\n\nlibrary(dplyr)\n\ndf |&gt; group_by(mode) |&gt; summarize(vcost=median(vcost), \n                                  wait = median(wait),\n                                  travel = median(travel))\n\n\ndf_long = mlogit.data(df, choice='choice', shape='long', alt.var = \"mode\")\ndf_long\n\n\nmnl = mlogit(choice ~ vcost + wait + travel | 1, data=df_long)\nmnl |&gt; summary()\n\n\nnmnl = mlogit(choice ~ vcost + travel + wait, \n              data=df_long, reflevel = 'air', \n              nests=list(land = c('car', 'bus', 'train'), \n                         air = c('air')\n                         ), \n              un.nest.el = TRUE\n              )\nsummary(nmnl)\n\n\noptions(scipen=999)\ndata(\"Heating\", package = \"mlogit\")\nH &lt;- dfidx(Heating, choice = \"depvar\", varying = c(3:12))\nm &lt;- mlogit(depvar ~ ic + oc | 1, H, \n            nests=list(room = c(\"er\", \"gr\"), \n                       central = c(\"ec\", \"gc\", \"hp\")))\nsummary(m)\n\nAdd a new chunk by clicking the Insert Chunk button on the toolbar or by pressing Cmd+Option+I.\nWhen you save the notebook, an HTML file containing the code and output will be saved alongside it (click the Preview button or press Cmd+Shift+K to preview the HTML file).\nThe preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike Knit, Preview does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed."
  },
  {
    "objectID": "talks/ukraine_sem/notebook/bayesian_sem.html",
    "href": "talks/ukraine_sem/notebook/bayesian_sem.html",
    "title": "Measurment Model",
    "section": "",
    "text": "import pymc as pm\nimport pandas as pd\nimport numpy as np\nfrom pytensor import tensor as pt\nimport arviz as az\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom pymc import do, observe\ndf = pd.read_csv('sem_data.csv')\ndf.head()\n\n\n\n\n\n\n\n\nmotiv\nharm\nstabi\nppsych\nses\nverbal\nread\narith\nspell\n\n\n\n\n0\n-7.907122\n-5.075312\n-3.138836\n-17.800210\n4.766450\n-3.633360\n-3.488981\n-9.989121\n-6.567873\n\n\n1\n1.751478\n-4.155847\n3.520752\n7.009367\n-6.048681\n-7.693461\n-4.520552\n8.196238\n8.778973\n\n\n2\n14.472570\n-4.540677\n4.070600\n23.734260\n-16.970670\n-3.909941\n-4.818170\n7.529984\n-5.688716\n\n\n3\n-1.165421\n-5.668406\n2.600437\n1.493158\n1.396363\n21.409450\n-3.138441\n5.730547\n-2.915676\n\n\n4\n-4.222899\n-10.072150\n-6.030737\n-5.985864\n-18.376400\n-1.438816\n-2.009742\n-0.623953\n-1.024624\ncoords = {\n    \"obs\": list(range(len(df))),\n    \"indicators\": [\"motiv\", \"harm\", \"stabi\", \"verbal\", \"ses\", \"ppsych\", \"read\", \"arith\", \"spell\"],\n    \"indicators_1\": [\"motiv\", \"harm\", \"stabi\"],\n    \"indicators_2\": [\"verbal\", \"ses\", \"ppsych\"],\n    \"indicators_3\": [\"read\", \"arith\", \"spell\"],\n    \"latent\": [\"adjust\", \"risk\", \"achieve\"],\n}\n\ndef make_lambda(indicators, name='lambdas1', priors=[1, 10]):\n    \"\"\" Takes an argument indicators which is a string in the coords dict\"\"\"\n    temp_name = name + '_'\n    lambdas_ = pm.Normal(temp_name, priors[0], priors[1], dims=(indicators))\n    # Force a fixed scale on the factor loadings for factor 1\n    lambdas_1 = pm.Deterministic(\n        name, pt.set_subtensor(lambdas_[0], 1), dims=(indicators)\n    )\n    return lambdas_1\nobs_idx = list(range(len(df)))\nwith pm.Model(coords=coords) as model:\n\n    # Set up Factor Loadings\n    lambdas_1 = make_lambda('indicators_1', 'lambdas1')\n    lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, 2])\n    lambdas_3 = make_lambda('indicators_3', 'lambdas3')\n    # Specify covariance structure between latent factors\n    kappa = 0\n    sd_dist = pm.Exponential.dist(1.0, shape=3)\n    chol, _, _ = pm.LKJCholeskyCov(\"chol_cov\", n=3, eta=2, sd_dist=sd_dist, compute_corr=True)\n    ksi = pm.MvNormal(\"ksi\", kappa, chol=chol, dims=(\"obs\", \"latent\"))\n\n    # Construct Pseudo Observation matrix based on Factor Loadings\n    m1 = ksi[obs_idx, 0] * lambdas_1[0]\n    m2 = ksi[obs_idx, 0] * lambdas_1[1]\n    m3 = ksi[obs_idx, 0] * lambdas_1[2]\n    m4 = ksi[obs_idx, 1] * lambdas_2[0]\n    m5 = ksi[obs_idx, 1] * lambdas_2[1]\n    m6 = ksi[obs_idx, 1] * lambdas_2[2]\n    m7 = ksi[obs_idx, 2] * lambdas_3[0]\n    m8 = ksi[obs_idx, 2] * lambdas_3[1]\n    m9 = ksi[obs_idx, 2] * lambdas_3[2]\n\n    mu = pm.Deterministic(\"mu\", pm.math.stack([m1, m2, m3, m4, m5, m6, m7, m8, m9]).T)\n\n    ## Error Terms\n    Psi = pm.InverseGamma(\"Psi\", 2, 13, dims=\"indicators\")\n\n    # Likelihood\n    _ = pm.Normal(\n        \"likelihood\",\n        mu,\n        Psi,\n        observed=df[coords['indicators']].values,\n    )\n\n    idata = pm.sample(\n        draws=1000,\n        chains=4,\n        nuts_sampler=\"numpyro\", target_accept=0.95, idata_kwargs={\"log_likelihood\": True}, \n        tune=2000,\n        random_seed=150\n    )\n    idata.extend(pm.sample_posterior_predictive(idata))\n\npm.model_to_graphviz(model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[76], line 38\n     30     # Likelihood\n     31     _ = pm.Normal(\n     32         \"likelihood\",\n     33         mu,\n     34         Psi,\n     35         observed=df[coords['indicators']].values,\n     36     )\n---&gt; 38     idata = pm.sample(\n     39         draws=1000,\n     40         chains=4,\n     41         nuts_sampler=\"numpyro\", target_accept=0.95, idata_kwargs={\"log_likelihood\": True}, \n     42         tune=2000,\n     43         random_seed=150\n     44     )\n     45     idata.extend(pm.sample_posterior_predictive(idata))\n     47 pm.model_to_graphviz(model)\n\nFile ~/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/pymc/sampling/mcmc.py:727, in sample(draws, tune, chains, cores, random_seed, progressbar, progressbar_theme, step, var_names, nuts_sampler, initvals, init, jitter_max_retries, n_init, trace, discard_tuned_samples, compute_convergence_checks, keep_warning_stat, return_inferencedata, idata_kwargs, nuts_sampler_kwargs, callback, mp_ctx, blas_cores, model, **kwargs)\n    722         raise ValueError(\n    723             \"Model can not be sampled with NUTS alone. Your model is probably not continuous.\"\n    724         )\n    726     with joined_blas_limiter():\n--&gt; 727         return _sample_external_nuts(\n    728             sampler=nuts_sampler,\n    729             draws=draws,\n    730             tune=tune,\n    731             chains=chains,\n    732             target_accept=kwargs.pop(\"nuts\", {}).get(\"target_accept\", 0.8),\n    733             random_seed=random_seed,\n    734             initvals=initvals,\n    735             model=model,\n    736             var_names=var_names,\n    737             progressbar=progressbar,\n    738             idata_kwargs=idata_kwargs,\n    739             compute_convergence_checks=compute_convergence_checks,\n    740             nuts_sampler_kwargs=nuts_sampler_kwargs,\n    741             **kwargs,\n    742         )\n    744 if isinstance(step, list):\n    745     step = CompoundStep(step)\n\nFile ~/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/pymc/sampling/mcmc.py:356, in _sample_external_nuts(sampler, draws, tune, chains, target_accept, random_seed, initvals, model, var_names, progressbar, idata_kwargs, compute_convergence_checks, nuts_sampler_kwargs, **kwargs)\n    353 elif sampler in (\"numpyro\", \"blackjax\"):\n    354     import pymc.sampling.jax as pymc_jax\n--&gt; 356     idata = pymc_jax.sample_jax_nuts(\n    357         draws=draws,\n    358         tune=tune,\n    359         chains=chains,\n    360         target_accept=target_accept,\n    361         random_seed=random_seed,\n    362         initvals=initvals,\n    363         model=model,\n    364         var_names=var_names,\n    365         progressbar=progressbar,\n    366         nuts_sampler=sampler,\n    367         idata_kwargs=idata_kwargs,\n    368         compute_convergence_checks=compute_convergence_checks,\n    369         **nuts_sampler_kwargs,\n    370     )\n    371     return idata\n    373 else:\n\nFile ~/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/pymc/sampling/jax.py:615, in sample_jax_nuts(draws, tune, chains, target_accept, random_seed, initvals, jitter, model, var_names, nuts_kwargs, progressbar, keep_untransformed, chain_method, postprocessing_backend, postprocessing_vectorize, postprocessing_chunks, idata_kwargs, compute_convergence_checks, nuts_sampler)\n    612     raise ValueError(f\"{nuts_sampler=} not recognized\")\n    614 tic1 = datetime.now()\n--&gt; 615 raw_mcmc_samples, sample_stats, library = sampler_fn(\n    616     model=model,\n    617     target_accept=target_accept,\n    618     tune=tune,\n    619     draws=draws,\n    620     chains=chains,\n    621     chain_method=chain_method,\n    622     progressbar=progressbar,\n    623     random_seed=random_seed,\n    624     initial_points=initial_points,\n    625     nuts_kwargs=nuts_kwargs,\n    626 )\n    627 tic2 = datetime.now()\n    629 if idata_kwargs is None:\n\nFile ~/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/pymc/sampling/jax.py:469, in _sample_numpyro_nuts(model, target_accept, tune, draws, chains, chain_method, progressbar, random_seed, initial_points, nuts_kwargs)\n    455 pmap_numpyro.run(\n    456     map_seed,\n    457     init_params=initial_points,\n   (...)\n    465     ),\n    466 )\n    468 raw_mcmc_samples = pmap_numpyro.get_samples(group_by_chain=True)\n--&gt; 469 sample_stats = _numpyro_stats_to_dict(pmap_numpyro)\n    470 return raw_mcmc_samples, sample_stats, numpyro\n\nFile ~/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/pymc/sampling/jax.py:409, in _numpyro_stats_to_dict(posterior)\n    407     data[name] = value\n    408     if stat == \"num_steps\":\n--&gt; 409         data[\"tree_depth\"] = np.log2(value).astype(int) + 1\n    410 return data\n\nFile ~/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/jax/_src/array.py:425, in ArrayImpl.__array__(self, dtype, context, copy)\n    422 def __array__(self, dtype=None, context=None, copy=None):\n    423   # copy argument is supported by np.asarray starting in numpy 2.0\n    424   kwds = {} if copy is None else {'copy': copy}\n--&gt; 425   return np.asarray(self._value, dtype=dtype, **kwds)\n\nFile ~/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/jax/_src/profiler.py:333, in annotate_function.&lt;locals&gt;.wrapper(*args, **kwargs)\n    330 @wraps(func)\n    331 def wrapper(*args, **kwargs):\n    332   with TraceAnnotation(name, **decorator_kwargs):\n--&gt; 333     return func(*args, **kwargs)\n    334   return wrapper\n\nFile ~/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/jax/_src/array.py:645, in ArrayImpl._value(self)\n    643 npy_value = np.empty(self.shape, self.dtype)\n    644 for i, ind in _cached_index_calc(self.sharding, self.shape):\n--&gt; 645   npy_value[ind] = self._arrays[i]._single_device_array_to_np_array()\n    646 self._npy_value = npy_value\n    647 self._npy_value.flags.writeable = False\n\nKeyboardInterrupt:\naz.summary(idata, var_names=[\"lambdas1\", \"lambdas2\", \"lambdas3\"])\n\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nlambdas1[motiv]\n1.000\n0.000\n1.000\n1.000\n0.000\n0.000\n4000.0\n4000.0\nNaN\n\n\nlambdas1[harm]\n0.895\n0.043\n0.814\n0.973\n0.002\n0.001\n521.0\n1401.0\n1.01\n\n\nlambdas1[stabi]\n0.703\n0.047\n0.617\n0.791\n0.002\n0.001\n801.0\n1571.0\n1.01\n\n\nlambdas2[verbal]\n1.000\n0.000\n1.000\n1.000\n0.000\n0.000\n4000.0\n4000.0\nNaN\n\n\nlambdas2[ses]\n0.842\n0.087\n0.679\n1.000\n0.004\n0.003\n453.0\n1294.0\n1.01\n\n\nlambdas2[ppsych]\n-0.801\n0.084\n-0.969\n-0.653\n0.004\n0.003\n521.0\n1276.0\n1.00\n\n\nlambdas3[read]\n1.000\n0.000\n1.000\n1.000\n0.000\n0.000\n4000.0\n4000.0\nNaN\n\n\nlambdas3[arith]\n0.841\n0.035\n0.773\n0.906\n0.001\n0.001\n1955.0\n2586.0\n1.00\n\n\nlambdas3[spell]\n0.981\n0.029\n0.927\n1.037\n0.001\n0.001\n1123.0\n1700.0\n1.00\naz.plot_trace(idata, var_names=['lambdas1', 'lambdas2', 'lambdas3'])\nplt.tight_layout();\n\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/density_utils.py:488: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/density_utils.py:488: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/density_utils.py:488: UserWarning: Your data appears to have a single value or no finite values\n  warnings.warn(\"Your data appears to have a single value or no finite values\")\ndef make_factor_loadings_df(idata):\n    factor_loadings = pd.DataFrame(\n        az.summary(\n            idata, var_names=[\"lambdas1\", \"lambdas2\", \"lambdas3\"]\n        )[\"mean\"]\n    ).reset_index()\n    factor_loadings[\"factor\"] = factor_loadings[\"index\"].str.split(\"[\", expand=True)[0]\n    factor_loadings.columns = [\"factor_loading\", \"factor_loading_weight\", \"factor\"]\n    factor_loadings[\"factor_loading_weight_sq\"] = factor_loadings[\"factor_loading_weight\"] ** 2\n    factor_loadings[\"sum_sq_loadings\"] = factor_loadings.groupby(\"factor\")[\n        \"factor_loading_weight_sq\"\n    ].transform(sum)\n    factor_loadings[\"error_variances\"] = az.summary(idata, var_names=[\"Psi\"])[\"mean\"].values\n    factor_loadings[\"total_indicator_variance\"] = (\n        factor_loadings[\"factor_loading_weight_sq\"] + factor_loadings[\"error_variances\"]\n    )\n    factor_loadings[\"total_variance\"] = factor_loadings[\"total_indicator_variance\"].sum()\n    factor_loadings[\"indicator_explained_variance\"] = (\n        factor_loadings[\"factor_loading_weight_sq\"] / factor_loadings[\"total_variance\"]\n    )\n    factor_loadings[\"factor_explained_variance\"] = (\n        factor_loadings[\"sum_sq_loadings\"] / factor_loadings[\"total_variance\"]\n    )\n    num_cols = [c for c in factor_loadings.columns if not c in [\"factor_loading\", \"factor\"]]\n    return factor_loadings\n\n\npd.set_option(\"display.max_colwidth\", 15)\nfactor_loadings = make_factor_loadings_df(idata)\nnum_cols = [c for c in factor_loadings.columns if not c in [\"factor_loading\", \"factor\"]]\nfactor_loadings.style.format(\"{:.3f}\", subset=num_cols).background_gradient(\n    axis=0, subset=[\"indicator_explained_variance\", \"factor_explained_variance\"]\n)\n\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/var/folders/__/ng_3_9pn1f11ftyml_qr69vh0000gn/T/ipykernel_52977/2929805399.py:12: FutureWarning: The provided callable &lt;built-in function sum&gt; is currently using SeriesGroupBy.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n  ].transform(sum)\n\n\n\n\n\n\n\n \nfactor_loading\nfactor_loading_weight\nfactor\nfactor_loading_weight_sq\nsum_sq_loadings\nerror_variances\ntotal_indicator_variance\ntotal_variance\nindicator_explained_variance\nfactor_explained_variance\n\n\n\n\n0\nlambdas1[motiv]\n1.000\nlambdas1\n1.000\n2.295\n3.676\n4.676\n60.931\n0.016\n0.038\n\n\n1\nlambdas1[harm]\n0.895\nlambdas1\n0.801\n2.295\n5.623\n6.424\n60.931\n0.013\n0.038\n\n\n2\nlambdas1[stabi]\n0.703\nlambdas1\n0.494\n2.295\n7.605\n8.099\n60.931\n0.008\n0.038\n\n\n3\nlambdas2[verbal]\n1.000\nlambdas2\n1.000\n2.351\n6.915\n7.915\n60.931\n0.016\n0.039\n\n\n4\nlambdas2[ses]\n0.842\nlambdas2\n0.709\n2.351\n8.027\n8.736\n60.931\n0.012\n0.039\n\n\n5\nlambdas2[ppsych]\n-0.801\nlambdas2\n0.642\n2.351\n8.242\n8.884\n60.931\n0.011\n0.039\n\n\n6\nlambdas3[read]\n1.000\nlambdas3\n1.000\n2.670\n3.409\n4.409\n60.931\n0.016\n0.044\n\n\n7\nlambdas3[arith]\n0.841\nlambdas3\n0.707\n2.670\n6.168\n6.875\n60.931\n0.012\n0.044\n\n\n8\nlambdas3[spell]\n0.981\nlambdas3\n0.962\n2.670\n3.951\n4.913\n60.931\n0.016\n0.044\nfig, axs = plt.subplots(1, 2, figsize=(20, 9))\naxs = axs.flatten()\naz.plot_energy(idata, ax=axs[0])\naz.plot_forest(idata, var_names=[\"lambdas1\", \"lambdas2\", \"lambdas3\"], combined=True, ax=axs[1]);\nplt.tight_layout();\ndrivers = coords['indicators']\ndef get_posterior_resids(idata, samples=100, metric=\"cov\"):\n    resids = []\n    for i in range(100):\n        if metric == \"cov\":\n            model_cov = pd.DataFrame(\n                az.extract(idata[\"posterior_predictive\"])[\"likelihood\"][:, :, i]\n            ).cov()\n            obs_cov = df[drivers].cov()\n        else:\n            model_cov = pd.DataFrame(\n                az.extract(idata[\"posterior_predictive\"])[\"likelihood\"][:, :, i]\n            ).corr()\n            obs_cov = df[drivers].corr()\n        model_cov.index = obs_cov.index\n        model_cov.columns = obs_cov.columns\n        residuals = model_cov - obs_cov\n        resids.append(residuals.values.flatten())\n\n    residuals_posterior = pd.DataFrame(pd.DataFrame(resids).mean().values.reshape(9, 9))\n    residuals_posterior.index = obs_cov.index\n    residuals_posterior.columns = obs_cov.index\n    return residuals_posterior\n\n\nresiduals_posterior_cov = get_posterior_resids(idata, 1000)\nresiduals_posterior_corr = get_posterior_resids(idata, 1000, metric=\"corr\")\nfig, ax = plt.subplots(figsize=(20, 10))\nmask = np.triu(np.ones_like(residuals_posterior_corr, dtype=bool))\nax = sns.heatmap(residuals_posterior_corr, annot=True, cmap=\"bwr\", mask=mask)\nax.set_title(\"Residuals between Model Implied and Sample Correlations \\n CFA\", fontsize=25);\ndef make_ppc(\n    idata,\n    samples=100,\n    drivers=drivers,\n    dims=(3, 3),\n):\n    fig, axs = plt.subplots(dims[0], dims[1], figsize=(20, 10))\n    axs = axs.flatten()\n    for i in range(len(drivers)):\n        for j in range(samples):\n            temp = az.extract(idata[\"posterior_predictive\"].sel({\"likelihood_dim_3\": i}))[\n                \"likelihood\"\n            ].values[:, j]\n            temp = pd.DataFrame(temp, columns=[\"likelihood\"])\n            if j == 0:\n                axs[i].hist(df[drivers[i]], alpha=0.3, ec=\"black\", bins=20, label=\"Observed Scores\")\n                axs[i].hist(\n                    temp[\"likelihood\"], color=\"purple\", alpha=0.1, bins=20, label=\"Predicted Scores\"\n                )\n            else:\n                axs[i].hist(df[drivers[i]], alpha=0.3, ec=\"black\", bins=20)\n                axs[i].hist(temp[\"likelihood\"], color=\"purple\", alpha=0.1, bins=20)\n            axs[i].set_title(f\"Posterior Predictive Checks {drivers[i]}\")\n            axs[i].legend()\n    plt.tight_layout()\n    plt.show()\n\n\nmake_ppc(idata)"
  },
  {
    "objectID": "talks/ukraine_sem/notebook/bayesian_sem.html#sem-model",
    "href": "talks/ukraine_sem/notebook/bayesian_sem.html#sem-model",
    "title": "Measurment Model",
    "section": "SEM Model",
    "text": "SEM Model\n\nMaking the Structural Matrix\n\nimport numpy as np\nimport pandas as pd\ndata = np.random.multivariate_normal([0,0,0], [[1, 0, 0], [0,1, 0], [0, 0, 1]], size=(500))\ndata = np.ones((500, 3))\nbetas = np.zeros((3, 3))\nbetas[1, 0] = 0.5\nbetas[1, 2] = 0.5\nbetas[0, 2] = 0.7\n\nB_matrix = pd.DataFrame(betas, index=['adjust_coef', 'risk_coef', 'achieve_coef'], \n                                columns=['adjust_target', 'risk_target', 'achieve_target'])\ndata = pd.DataFrame(data, columns=['adjust', 'risk', 'achieve'])\nB_matrix\n\n\n\n\n\n\n\n\nadjust_target\nrisk_target\nachieve_target\n\n\n\n\nadjust_coef\n0.0\n0.0\n0.7\n\n\nrisk_coef\n0.5\n0.0\n0.5\n\n\nachieve_coef\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\ndata.head()\n\n\n\n\n\n\n\n\nadjust\nrisk\nachieve\n\n\n\n\n0\n1.0\n1.0\n1.0\n\n\n1\n1.0\n1.0\n1.0\n\n\n2\n1.0\n1.0\n1.0\n\n\n3\n1.0\n1.0\n1.0\n\n\n4\n1.0\n1.0\n1.0\n\n\n\n\n\n\n\n\nnp.dot(data, B_matrix)\n\narray([[0.5, 0. , 1.2],\n       [0.5, 0. , 1.2],\n       [0.5, 0. , 1.2],\n       ...,\n       [0.5, 0. , 1.2],\n       [0.5, 0. , 1.2],\n       [0.5, 0. , 1.2]])\n\n\n\n\nFitting the Model\n\n\n\ndef make_sem():\n    coords = {\n    \"obs\": list(range(len(df))),\n    \"indicators\": [\"motiv\", \"harm\", \"stabi\", \"verbal\", \"ses\", \"ppsych\", \"read\", \"arith\", \"spell\"],\n    \"indicators_1\": [\"motiv\", \"harm\", \"stabi\"],\n    \"indicators_2\": [\"verbal\", \"ses\", \"ppsych\"],\n    \"indicators_3\": [\"read\", \"arith\", \"spell\"],\n    \"latent\": [\"adjust\", \"risk\", \"achieve\"],\n    \"paths\": [\"risk-&gt;adjust\", \"risk--&gt;achieve\", \"adjust--&gt;achieve\"]\n    }\n\n    obs_idx = list(range(len(df)))\n    with pm.Model(coords=coords) as model:\n\n        # Set up Factor Loadings\n        lambdas_1 = make_lambda('indicators_1', 'lambdas1', priors=[1, .5]) #adjust\n        lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, .5]) # risk\n        lambdas_3 = make_lambda('indicators_3', 'lambdas3', priors=[1, .5]) # achieve\n        \n        # Specify covariance structure between latent factors\n        sd_dist = pm.Exponential.dist(1.0, shape=3)\n        chol, _, _ = pm.LKJCholeskyCov(\"chol_cov\", n=3, eta=2, sd_dist=sd_dist, compute_corr=True)\n        ksi = pm.MvNormal(\"ksi\", 0, chol=chol, dims=(\"obs\", \"latent\"))\n\n        ## Build Regression Components\n        coefs = pm.Normal('betas', 0, .5, dims='paths')\n        zeros = pt.zeros((3, 3))\n        ## adjust ~ risk\n        zeros1 = pt.set_subtensor(zeros[1, 0], coefs[0])\n        ## achieve ~ risk + adjust\n        zeros2 = pt.set_subtensor(zeros1[1, 2], coefs[1])\n        coefs_ = pt.set_subtensor(zeros2[0, 2], coefs[2])\n        \n        structural_relations = pm.Deterministic('endogenous_structural_paths', pm.math.dot(ksi, coefs_))\n\n        # Construct Pseudo Observation matrix based on Factor Loadings\n        m1 = ksi[obs_idx, 0] * lambdas_1[0] +  structural_relations[obs_idx, 0] #adjust\n        m2 = ksi[obs_idx, 0] * lambdas_1[1] +  structural_relations[obs_idx, 0] #adjust\n        m3 = ksi[obs_idx, 0] * lambdas_1[2] +  structural_relations[obs_idx, 0] #adjust\n        m4 = ksi[obs_idx, 1] * lambdas_2[0] +  structural_relations[obs_idx, 1]  #risk\n        m5 = ksi[obs_idx, 1] * lambdas_2[1] +  structural_relations[obs_idx, 1]  #risk\n        m6 = ksi[obs_idx, 1] * lambdas_2[2] +  structural_relations[obs_idx, 1]  #risk\n        m7 = ksi[obs_idx, 2] * lambdas_3[0] +  structural_relations[obs_idx, 2]  #achieve\n        m8 = ksi[obs_idx, 2] * lambdas_3[1] +  structural_relations[obs_idx, 2]  #achieve\n        m9 = ksi[obs_idx, 2] * lambdas_3[2] +  structural_relations[obs_idx, 2]  #achieve\n\n        mu = pm.Deterministic(\"mu\", pm.math.stack([m1, m2, m3, m4, m5, m6, m7, m8, m9]).T)\n\n\n        ## Error Terms\n        Psi = pm.InverseGamma(\"Psi\", 2, 13, dims=\"indicators\")\n\n        # Likelihood\n        _ = pm.Normal(\n            \"likelihood\",\n            mu,\n            Psi,\n            observed=df[coords['indicators']].values,\n        )\n\n        idata = pm.sample(\n            draws=2500,\n            chains=4,\n            #nuts_sampler=\"numpyro\", \n            target_accept=0.90, idata_kwargs={\"log_likelihood\": True}, \n            random_seed=150,\n            tune=1000,\n        )\n        idata.extend(pm.sample_posterior_predictive(idata))\n    return idata, model\n\nidata_sem, model_sem = make_sem()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [lambdas1_, lambdas2_, lambdas3_, chol_cov, ksi, betas, Psi]\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n\n\n\n\n\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 2_500 draw iterations (4_000 + 10_000 draws total) took 170 seconds.\nThere were 1 divergences after tuning. Increase `target_accept` or reparameterize.\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\nSampling: [likelihood]\n\n\n\n\n\n\n\n\n\npm.model_to_graphviz(model_sem)\n\n\n\n\n\n\n\n\n\naz.summary(idata_sem, var_names=[\"lambdas1\", \"lambdas2\", \"lambdas3\", \"betas\"])\n\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nlambdas1[motiv]\n1.000\n0.000\n1.000\n1.000\n0.000\n0.000\n10000.0\n10000.0\nNaN\n\n\nlambdas1[harm]\n0.850\n0.045\n0.766\n0.933\n0.001\n0.001\n997.0\n2361.0\n1.00\n\n\nlambdas1[stabi]\n0.651\n0.052\n0.556\n0.748\n0.002\n0.001\n988.0\n3122.0\n1.00\n\n\nlambdas2[verbal]\n1.000\n0.000\n1.000\n1.000\n0.000\n0.000\n10000.0\n10000.0\nNaN\n\n\nlambdas2[ses]\n0.833\n0.081\n0.681\n0.986\n0.002\n0.001\n2282.0\n4694.0\n1.00\n\n\nlambdas2[ppsych]\n-0.764\n0.079\n-0.920\n-0.623\n0.002\n0.001\n2399.0\n4215.0\n1.00\n\n\nlambdas3[read]\n1.000\n0.000\n1.000\n1.000\n0.000\n0.000\n10000.0\n10000.0\nNaN\n\n\nlambdas3[arith]\n0.610\n0.057\n0.498\n0.712\n0.003\n0.002\n381.0\n1068.0\n1.00\n\n\nlambdas3[spell]\n0.825\n0.037\n0.755\n0.894\n0.001\n0.001\n757.0\n1717.0\n1.00\n\n\nbetas[risk-&gt;adjust]\n0.202\n0.139\n-0.065\n0.457\n0.010\n0.007\n186.0\n202.0\n1.02\n\n\nbetas[risk--&gt;achieve]\n0.207\n0.178\n-0.129\n0.534\n0.012\n0.009\n208.0\n487.0\n1.01\n\n\nbetas[adjust--&gt;achieve]\n0.747\n0.104\n0.556\n0.942\n0.006\n0.004\n311.0\n810.0\n1.01\n\n\n\n\n\n\n\n\nmake_ppc(idata_sem)\n\n\n\n\n\n\n\n\n\nresiduals_posterior_cov = get_posterior_resids(idata_sem, 1000)\nresiduals_posterior_corr = get_posterior_resids(idata_sem, 1000, metric=\"corr\")\n\n\nfig, ax = plt.subplots(figsize=(20, 10))\nmask = np.triu(np.ones_like(residuals_posterior_corr, dtype=bool))\nax = sns.heatmap(residuals_posterior_corr, annot=True, cmap=\"bwr\", mask=mask)\nax.set_title(\"Residuals between Model Implied and Sample Correlations \\n SEM model\", fontsize=25);\n\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(1, 3, figsize=(15, 10))\naxs = axs.flatten()\nax = axs[0]\nax1 = axs[1]\nax2 = axs[2]\naz.plot_forest(idata_sem, var_names=[\"ksi\"], combined=True, ax=ax, coords={\"latent\": [\"adjust\"]})\naz.plot_forest(\n    idata_sem,\n    var_names=[\"ksi\"],\n    combined=True,\n    ax=ax1,\n    colors=\"forestgreen\",\n    coords={\"latent\": [\"risk\"]},\n)\naz.plot_forest(\n    idata_sem,\n    var_names=[\"ksi\"],\n    combined=True,\n    ax=ax2,\n    colors=\"slateblue\",\n    coords={\"latent\": [\"achieve\"]},\n)\nax.set_yticklabels([])\nax.set_xlabel(\"ADJUST\")\nax1.set_yticklabels([])\nax1.set_xlabel(\"RISK\")\nax2.set_yticklabels([])\nax2.set_xlabel(\"ACHIEVE\")\nax.axvline(-2, color=\"red\")\nax1.axvline(-2, color=\"red\")\nax2.axvline(-2, color=\"red\")\nax.set_title(\"Individual On Latent Factor ADJUST\")\nax1.set_title(\"Individual On Latent Factor RISK\")\nax2.set_title(\"Individual On Latent Factor ACHIEVE\")\nplt.show();\n\n\n\n\n\n\n\n\n\ncompare_df = az.compare({'SEM': idata_sem, 'CFA': idata}, 'waic')\ncompare_df\n\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/stats.py:1653: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. \nSee http://arxiv.org/abs/1507.04544 for details\n  warnings.warn(\n/Users/nathanielforde/mambaforge/envs/bayesian_causal_book/lib/python3.10/site-packages/arviz/stats/stats.py:1653: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. \nSee http://arxiv.org/abs/1507.04544 for details\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nrank\nelpd_waic\np_waic\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\nSEM\n0\n-14525.202949\n905.303983\n0.000000\n1.0\n49.632750\n0.000000\nTrue\nlog\n\n\nCFA\n1\n-14748.968775\n939.024032\n223.765826\n0.0\n48.571539\n12.080799\nTrue\nlog\n\n\n\n\n\n\n\n\naz.plot_compare(compare_df);"
  },
  {
    "objectID": "talks/ukraine_sem/notebook/bayesian_sem.html#counterfactual-analysis",
    "href": "talks/ukraine_sem/notebook/bayesian_sem.html#counterfactual-analysis",
    "title": "Measurment Model",
    "section": "Counterfactual Analysis",
    "text": "Counterfactual Analysis\n\nmodel_sem\n\n\\[\n            \\begin{array}{rcl}\n            \\text{lambdas1\\_} &\\sim & \\operatorname{Normal}(1,~0.5)\\\\\\text{lambdas2\\_} &\\sim & \\operatorname{Normal}(1,~0.5)\\\\\\text{lambdas3\\_} &\\sim & \\operatorname{Normal}(1,~0.5)\\\\\\text{chol\\_cov} &\\sim & \\operatorname{\\_lkjcholeskycov}(3,~2,~\\operatorname{Exponential}(f()))\\\\\\text{ksi} &\\sim & \\operatorname{MultivariateNormal}(f(\\text{chol\\_cov}),~f(\\text{chol\\_cov}))\\\\\\text{betas} &\\sim & \\operatorname{Normal}(0,~0.5)\\\\\\text{Psi} &\\sim & \\operatorname{InverseGamma}(2,~13)\\\\\\text{lambdas1} &\\sim & \\operatorname{Deterministic}(f(\\text{lambdas1\\_}))\\\\\\text{lambdas2} &\\sim & \\operatorname{Deterministic}(f(\\text{lambdas2\\_}))\\\\\\text{lambdas3} &\\sim & \\operatorname{Deterministic}(f(\\text{lambdas3\\_}))\\\\\\text{chol\\_cov\\_corr} &\\sim & \\operatorname{Deterministic}(f(\\text{chol\\_cov}))\\\\\\text{chol\\_cov\\_stds} &\\sim & \\operatorname{Deterministic}(f(\\text{chol\\_cov}))\\\\\\text{endogenous\\_structural\\_paths} &\\sim & \\operatorname{Deterministic}(f(\\text{ksi},~\\text{betas}))\\\\\\text{mu} &\\sim & \\operatorname{Deterministic}(f(\\text{ksi},~\\text{betas},~\\text{lambdas3\\_},~\\text{lambdas2\\_},~\\text{lambdas1\\_}))\\\\\\text{likelihood} &\\sim & \\operatorname{Normal}(\\text{mu},~\\text{Psi})\n            \\end{array}\n            \\]\n\n\n\nmodel_beta0 = do(model_sem, {\"betas\": [0, 0 , 0]}, prune_vars=True)\nmodel_beta1 = do(model_sem, {\"betas\": [.6, .3, .7]}, prune_vars=True)\nmodel_beta0\n\n\\[\n            \\begin{array}{rcl}\n            \\text{lambdas1\\_} &\\sim & \\operatorname{Normal}(1,~0.5)\\\\\\text{lambdas2\\_} &\\sim & \\operatorname{Normal}(1,~0.5)\\\\\\text{lambdas3\\_} &\\sim & \\operatorname{Normal}(1,~0.5)\\\\\\text{chol\\_cov} &\\sim & \\operatorname{\\_lkjcholeskycov}(3,~2,~\\operatorname{Exponential}(f()))\\\\\\text{ksi} &\\sim & \\operatorname{MultivariateNormal}(f(\\text{chol\\_cov}),~f(\\text{chol\\_cov}))\\\\\\text{Psi} &\\sim & \\operatorname{InverseGamma}(2,~13)\\\\\\text{endogenous\\_structural\\_paths} &\\sim & \\operatorname{Deterministic}(f(\\text{ksi}))\\\\\\text{lambdas3} &\\sim & \\operatorname{Deterministic}(f(\\text{lambdas3\\_}))\\\\\\text{lambdas2} &\\sim & \\operatorname{Deterministic}(f(\\text{lambdas2\\_}))\\\\\\text{lambdas1} &\\sim & \\operatorname{Deterministic}(f(\\text{lambdas1\\_}))\\\\\\text{mu} &\\sim & \\operatorname{Deterministic}(f(\\text{ksi},~\\text{lambdas3\\_},~\\text{lambdas2\\_},~\\text{lambdas1\\_}))\\\\\\text{likelihood} &\\sim & \\operatorname{Normal}(f(\\text{ksi},~\\text{lambdas1\\_},~\\text{lambdas2\\_},~\\text{lambdas3\\_}),~\\text{Psi})\n            \\end{array}\n            \\]\n\n\n\npm.model_to_graphviz(model_beta0)\n\n\n\n\n\n\n\n\n\n# Sample new sales data assuming Google Ads off: P(Y | c, do(z=0))\nidata_z0 = pm.sample_posterior_predictive(\n    idata_sem,\n    model=model_beta0,\n    predictions=False,\n    var_names=[\"likelihood\", \"betas\",],\n)\n\n\n# Sample new sales data assuming Google Ads off: P(Y | c, do(z=1))\nidata_z1 = pm.sample_posterior_predictive(\n    idata_sem,\n    model=model_beta1,\n    predictions=False,\n    var_names=[\"likelihood\", \"betas\"],\n)\n\nSampling: [likelihood]\n\n\n\n\n\n\n\n\nSampling: [likelihood]\n\n\n\n\n\n\n\n\n\npd.DataFrame((idata_z0['posterior_predictive']['likelihood'] - idata_z1['posterior_predictive']['likelihood']).mean(dim=('chain', 'draw')).values)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n0\n-0.463197\n-0.358005\n-0.286565\n-0.114874\n-0.042991\n0.110666\n4.936211\n5.107738\n4.972490\n\n\n1\n2.377638\n2.422167\n2.359539\n-0.063802\n-0.151001\n-0.018147\n-0.475168\n-0.303663\n-0.511428\n\n\n2\n4.663123\n4.648260\n4.735318\n0.092329\n-0.014905\n-0.087927\n-5.440901\n-5.441826\n-5.468720\n\n\n3\n-2.977008\n-2.997935\n-2.989136\n-0.039031\n0.041482\n-0.025136\n0.108632\n0.264840\n0.129751\n\n\n4\n1.826385\n1.823625\n1.722291\n0.008998\n-0.087842\n0.038139\n3.982011\n4.043272\n3.869997\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n-2.844905\n-3.038198\n-2.995460\n-0.007683\n0.025422\n0.177166\n-3.240432\n-3.093730\n-3.218599\n\n\n496\n-1.817311\n-1.728041\n-1.782307\n0.086392\n-0.032205\n-0.147835\n-3.111101\n-3.096323\n-3.237516\n\n\n497\n0.055163\n0.142330\n0.268756\n-0.165110\n-0.045748\n0.036947\n-1.071530\n-1.102467\n-1.087500\n\n\n498\n1.989527\n1.991247\n1.913922\n-0.025677\n0.027728\n-0.097815\n-9.445563\n-9.406408\n-9.387214\n\n\n499\n6.370377\n6.422907\n6.339601\n-0.023407\n-0.002096\n0.105869\n14.494246\n14.581555\n14.492164\n\n\n\n\n500 rows × 9 columns\n\n\n\n\npd.DataFrame((idata_z0['posterior_predictive']['likelihood'] - idata_z1['posterior_predictive']['likelihood']).mean(dim=('chain', 'draw')).values).median()\n\n0   -0.105562\n1   -0.085492\n2   -0.135569\n3   -0.006139\n4   -0.004415\n5   -0.002360\n6   -0.148865\n7   -0.163581\n8   -0.169653\ndtype: float64\n\n\n\nresiduals_posterior_corr0 = get_posterior_resids(idata_z0, 1000, metric=\"corr\")\n\nresiduals_posterior_corr1 = get_posterior_resids(idata_z1, 1000, metric=\"corr\")\n\n\nresiduals_posterior_corr0 - residuals_posterior_corr1\n\n\n\n\n\n\n\n\nmotiv\nharm\nstabi\nverbal\nses\nppsych\nread\narith\nspell\n\n\n\n\nmotiv\n0.000000\n-0.080177\n-0.131360\n-0.243116\n-0.192810\n0.178007\n-0.899440\n-0.816758\n-0.882780\n\n\nharm\n-0.080177\n0.000000\n-0.153191\n-0.252675\n-0.206456\n0.188974\n-0.799793\n-0.729367\n-0.786353\n\n\nstabi\n-0.131360\n-0.153191\n0.000000\n-0.262698\n-0.217575\n0.197371\n-0.678641\n-0.611510\n-0.658578\n\n\nverbal\n-0.243116\n-0.252675\n-0.262698\n0.000000\n-0.010569\n-0.003814\n-0.241953\n-0.253102\n-0.250916\n\n\nses\n-0.192810\n-0.206456\n-0.217575\n-0.010569\n0.000000\n0.001399\n-0.196381\n-0.206370\n-0.204542\n\n\nppsych\n0.178007\n0.188974\n0.197371\n-0.003814\n0.001399\n0.000000\n0.175259\n0.185912\n0.186708\n\n\nread\n-0.899440\n-0.799793\n-0.678641\n-0.241953\n-0.196381\n0.175259\n0.000000\n-0.178436\n-0.087744\n\n\narith\n-0.816758\n-0.729367\n-0.611510\n-0.253102\n-0.206370\n0.185912\n-0.178436\n0.000000\n-0.219602\n\n\nspell\n-0.882780\n-0.786353\n-0.658578\n-0.250916\n-0.204542\n0.186708\n-0.087744\n-0.219602\n0.000000\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(20, 10))\nmask = np.triu(np.ones_like(residuals_posterior_corr1, dtype=bool))\nax = sns.heatmap(residuals_posterior_corr1, annot=True, cmap=\"bwr\", mask=mask, vmax=1, vmin=-1)\nax.set_title(\"Residuals between Model Implied and Sample Correlations \\n Counterfactual Setting \", fontsize=25);\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(20, 10))\nmask = np.triu(np.ones_like(residuals_posterior_corr0, dtype=bool))\nax = sns.heatmap(residuals_posterior_corr0, annot=True, cmap=\"bwr\", mask=mask, vmax=1, vmin=-1)\nax.set_title(\"Residuals between Model Implied and Sample Correlations \\n 0'd setting\", fontsize=25);"
  },
  {
    "objectID": "notes/certain_things/Public/Philosophy/Repeat, Repeat, Repeat....html",
    "href": "notes/certain_things/Public/Philosophy/Repeat, Repeat, Repeat....html",
    "title": "Examined Algorithms",
    "section": "",
    "text": "It’s hard to cling to the sophistry of control when plan after plan becomes subtly derailed by awkward and intransigent truths. If every utterance from your infertile mind is rooted out and rendered false in portrait, there may be a mismatch between expectation and reality. The benefit of working at the coal face is how this friction between facts and fiction is so brutally exposed.\nComforting facades are quickly shorn of their veneer, leaving only the rough hewn details that don’t press against the jagged edges of reality. Learning is a process of painful revelation. But not every exposure is equally illuminating. What then, do we lose when we side-step the sharp edges and retreat to the sanctity of comfortable ignorance?",
    "crumbs": [
      "Philosophy",
      "Repeat, Repeat, Repeat..."
    ]
  },
  {
    "objectID": "oss/pymc_marketing/nested_logit.html",
    "href": "oss/pymc_marketing/nested_logit.html",
    "title": "Bayesian Nested Logit in PyMC Marketing",
    "section": "",
    "text": "Discrete Choice Models for Consumer Choice\nIn this project I sought to add the functionality for bayesian versions to the PyMC Marketing package. I added classes for the estimation of the multinomial logit and nested logit discrete choice models. We demonstrated how these classes can be used to esitmate patterns of consumer preference. We built a formula style interface for specifying the model set up.\nutility_formulas = [\n    \"gc ~ ic_gc + oc_gc | income + rooms \",\n    \"ec ~ ic_ec + oc_ec | income + rooms \",\n    \"gr ~ ic_gr + oc_gr | income + rooms \",\n    \"er ~ ic_er + oc_er | income + rooms \",\n    \"hp ~ ic_hp + oc_hp | income + rooms \",\n]\n\n\nnesting_structure = {\"central\": [\"gc\", \"ec\"], \"room\": [\"hp\", \"gr\", \"er\"]}\n\n\nnstL_1 = NestedLogit(\n    df,\n    utility_formulas,\n    \"depvar\",\n    covariates=[\"ic\", \"oc\"],\n    nesting_structure=nesting_structure,\n    model_config={\n        \"alphas_\": Prior(\"Normal\", mu=0, sigma=5, dims=\"alts\"),\n        \"betas\": Prior(\"Normal\", mu=0, sigma=1, dims=\"alt_covariates\"),\n        \"betas_fixed_\": Prior(\"Normal\", mu=0, sigma=1, dims=\"fixed_covariates\"),\n        \"lambdas_nests\": Prior(\"Beta\", alpha=2, beta=2, dims=\"nests\"),\n    },\n)\nnstL_1\n\nIn particular we highlight the differences between proportional and non-proportional patterns of product substitution using these models. The demonstration of the multinomial logit can be found here and the nested logit here\n\n\n\nThe Nested Logit Model Structure with Conditional and Marginal Probabilities across Nests"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-adventure-begins-a-tale-of-two-choices",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-adventure-begins-a-tale-of-two-choices",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "The Adventure Begins: A Tale of Two Choices",
    "text": "The Adventure Begins: A Tale of Two Choices\n\n\nThe Model Developer’s Journey\n“Which path shall I take?”\n\nMultinomial Logit (MNL) - The familiar trail\nNested Logit - The advanced expedition\nMixed Logit - The future frontier\n\nEvery choice shapes the model’s destiny…\n\nThe Consumer’s Dilemma\n“What drives human choice?”\n\nUtility maximization\nRational decision-making\nIndependence of Irrelevant Alternatives (IIA)\nExpected value calculations\n\nEach purchase tells a story of preference…"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-1-the-foundation---understanding-choice-architecture",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-1-the-foundation---understanding-choice-architecture",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "Chapter 1: The Foundation - Understanding Choice Architecture",
    "text": "Chapter 1: The Foundation - Understanding Choice Architecture\nThe Mathematical Foundation of Decision\nThe utility function forms the cornerstone of choice modeling:\n\\(U_{ij} = V_{ij} + \\epsilon_{ij}\\)\nWhere: - \\(V_{ij}\\) represents the systematic utility for individual \\(i\\) choosing alternative \\(j\\) - \\(\\epsilon_{ij}\\) captures the random component of choice\nThe Choose-Your-Own-Adventure Moment: Do we assume independence in our error terms, or do we allow for correlation?"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-2-the-multinomial-logit---the-classic-tale",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-2-the-multinomial-logit---the-classic-tale",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "Chapter 2: The Multinomial Logit - The Classic Tale",
    "text": "Chapter 2: The Multinomial Logit - The Classic Tale\nThe Simple Path Forward\nThe probability of choosing alternative \\(j\\) follows the elegant logistic form:\n\\(P_{ij} = \\frac{\\exp(V_{ij})}{\\sum_{k=1}^{J} \\exp(V_{ik})}\\)\nKey Contributions in PR #1654\n\nNew Model Class: MNLogit with Wilkinson-style formula interface\nAlternative-Specific Attributes: Price, quality, brand effects\nIndividual Fixed Attributes: Income, demographics, preferences\nCausal Inference Ready: Built-in counterfactual analysis"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-3-the-iia-problem---a-plot-twist",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-3-the-iia-problem---a-plot-twist",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "Chapter 3: The IIA Problem - A Plot Twist",
    "text": "Chapter 3: The IIA Problem - A Plot Twist\n\\[\\dfrac{P_{j}}{P_{i}} = \\dfrac{softmax(U_{j})}{softmax(U_{i})} = \\dfrac{ \\dfrac{e^{U_{j}}}{\\sum_{i}^{n}e^{U_{k}}}}{\\dfrac{e^{U_{i}}}{\\sum_{i}^{n}e^{U_{k}}}} = \\dfrac{e^{U_{j}}}{e^{U_{i}}} = e^{U_{j} - U_{k}}\\]"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-4-the-nested-logit---advanced-expedition",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-4-the-nested-logit---advanced-expedition",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "Chapter 4: The Nested Logit - Advanced Expedition",
    "text": "Chapter 4: The Nested Logit - Advanced Expedition\nBreaking Free from Independence\nThe nested logit introduces a two-stage choice process:\nStage 1 - Choose a nest: \\(P_i(\\text{nest}) = \\frac{\\exp(\\lambda_n I_{in})}{\\sum_{m} \\exp(\\lambda_m I_{im})}\\)\nStage 2 - Choose within nest: \\(P_{ij|\\text{nest}} = \\frac{\\exp(V_{ij}/\\lambda_n)}{\\sum_{k \\in \\text{nest}} \\exp(V_{ik}/\\lambda_n)}\\)\nWhere \\(I_{in} = \\ln\\left(\\sum_{j \\in \\text{nest}} \\exp(V_{ij}/\\lambda_n)\\right)\\) is the inclusive value"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-5-implementation-mastery---the-developers-toolkit",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-5-implementation-mastery---the-developers-toolkit",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "Chapter 5: Implementation Mastery - The Developer’s Toolkit",
    "text": "Chapter 5: Implementation Mastery - The Developer’s Toolkit\nNew Capabilities Added\nCore Models: - MNLogit: Multinomial logit with formula interface - NestedLogit: Up to 3-level nesting structures\nFormula Syntax: - Alternative-specific effects: car ~ cost + time - Fixed individual effects: car ~ cost + time | income + age - Flexible nesting: Public transport vs private transport\nNesting Structure: - Hierarchical grouping of alternatives - Realistic substitution patterns - Flexible depth up to 3 levels"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-6-the-data-adventure---real-world-applications",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-6-the-data-adventure---real-world-applications",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "Chapter 6: The Data Adventure - Real-World Applications",
    "text": "Chapter 6: The Data Adventure - Real-World Applications\nTwo New Datasets Journey\nCrackers Choice Dataset (3,157 observations): - Consumer packaged goods decisions - Price sensitivity analysis - Brand preference modeling\nHeating System Choice (901 observations): - Durable goods selection - Installation cost impacts - Energy efficiency preferences\nChoose Your Analysis Path\n“Will you explore price elasticity in fast-moving consumer goods, or investigate long-term investment decisions in home heating?”"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-7-causal-inference---the-power-of-counterfactuals",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-7-causal-inference---the-power-of-counterfactuals",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "Chapter 7: Causal Inference - The Power of Counterfactuals",
    "text": "Chapter 7: Causal Inference - The Power of Counterfactuals\nBeyond Prediction: Understanding Impact\nThe MNL Limitation: - Price increase on Product A - MNL: Proportional reallocation to all other products - Original shares: [0.4, 0.3, 0.3] for products A, B, C - New shares (MNL): [0.32, 0.34, 0.34] - Unrealistic\nThe Nested Logit Solution: - Realistic substitution within nests - Products A & B in same nest, C in different nest - New shares (Nested): [0.32, 0.38, 0.30] - More realistic\nThe Adventure Continues\n“Each intervention reveals new paths through the choice landscape…”"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-8-technical-architecture---the-implementation-story",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-8-technical-architecture---the-implementation-story",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "Chapter 8: Technical Architecture - The Implementation Story",
    "text": "Chapter 8: Technical Architecture - The Implementation Story\nCode Structure Contributions\nFiles Added (10,525+ lines): - mnl_logit.py: Core multinomial logit implementation - nested_logit.py: Nested logit with flexible nesting - Two comprehensive Jupyter notebooks - Complete test suites with 93.99% coverage\nKey Design Decisions: - Wilkinson formula syntax for intuitive specification - Wide-format data input (industry standard) - Bayesian parameter identification with proper constraints - Built-in visualization and counterfactual tools"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-9-the-mathematical-journey---parameter-identification",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-9-the-mathematical-journey---parameter-identification",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "Chapter 9: The Mathematical Journey - Parameter Identification",
    "text": "Chapter 9: The Mathematical Journey - Parameter Identification\nSolving the Identification Challenge\nMultinomial Logit Constraints: \\(\\alpha_{J} = 0 \\text{ (reference category)}\\) \\(\\beta_{\\text{fixed},J} = 0 \\text{ (reference category)}\\)\nNested Logit Complexity: - Nest-specific scale parameters: \\(0 &lt; \\lambda_n \\leq 1\\) - Cross-nest correlation structure - Hierarchical parameter identification\nThe Bayesian Advantage\n“In the realm of uncertainty, Bayesian methods provide the compass for navigating parameter space…”"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-10-benchmarking-excellence---validation-journey",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-10-benchmarking-excellence---validation-journey",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "Chapter 10: Benchmarking Excellence - Validation Journey",
    "text": "Chapter 10: Benchmarking Excellence - Validation Journey\nStanding on Giants’ Shoulders\nR’s mlogit Package Comparison: - Parameter recovery validation - Likelihood convergence verification - Prediction accuracy assessment\nPython Ecosystem Integration: - Beyond pylogit capabilities - PyMC probabilistic programming power - Seamless scikit-learn compatibility\nChoose Your Validation Adventure\n“Will you trust the classical frequentist path, or embrace the Bayesian uncertainty quantification?”"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-11-future-pathways---the-adventure-continues",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-11-future-pathways---the-adventure-continues",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "Chapter 11: Future Pathways - The Adventure Continues",
    "text": "Chapter 11: Future Pathways - The Adventure Continues\nThe Mixed Logit Frontier\nNext Chapter Preview: - Random coefficient models - Unobserved heterogeneity - Hierarchical Bayesian structures\nThe Developer’s Next Choice: - Extend to mixed logit models with random coefficients - Incorporate taste heterogeneity across populations - Add temporal dynamics to choice modeling"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-12-the-gallery-of-adventures---visual-storytelling",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#chapter-12-the-gallery-of-adventures---visual-storytelling",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "Chapter 12: The Gallery of Adventures - Visual Storytelling",
    "text": "Chapter 12: The Gallery of Adventures - Visual Storytelling\nTwo New Notebook Expeditions\nMultinomial Logit Journey: - Cracker choice modeling - Price intervention analysis - IIA demonstration\nNested Logit Expedition: - Heating system decisions - Realistic substitution patterns - Advanced nesting structures\nThe Visual Narrative\n“Every plot tells a story, every coefficient reveals a preference, every counterfactual opens a new chapter…”"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#epilogue-the-choice-is-yours",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#epilogue-the-choice-is-yours",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "Epilogue: The Choice is Yours",
    "text": "Epilogue: The Choice is Yours\nWhat This Adventure Brings to PyMC-Marketing\n\n\nFor Model Developers: - Intuitive formula interface - Flexible nesting structures - Comprehensive testing suite - Rich documentation and examples\nChoose Your Development Path: - Extend to mixed logit models - Add dynamic choice modeling - Integrate with marketing mix models\n\nFor Business Applications: - Realistic market simulation - Causal intervention analysis - Price optimization support - Product portfolio decisions\nChoose Your Business Adventure: - Pricing strategy optimization - Product launch planning - Market share forecasting - Competitive response modeling"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-final-page-technical-summary",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-final-page-technical-summary",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "The Final Page: Technical Summary",
    "text": "The Final Page: Technical Summary\nContribution Statistics\n\n52 commits of dedicated development\n10,525+ lines of new functionality\n14 files modified across the codebase\n93.99% test coverage achieved\n2 comprehensive example notebooks\nBeyond parity with R’s mlogit and Python’s pylogit\n\nThe Adventure’s Impact\n“In the grand choose-your-own-adventure of statistical modeling, this contribution opens new chapters in the story of human choice understanding. Whether you’re a developer seeking elegant APIs or a business analyst pursuing causal insights, the path forward is now illuminated by Bayesian light.”"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#appendix-the-technical-compass",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#appendix-the-technical-compass",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "Appendix: The Technical Compass",
    "text": "Appendix: The Technical Compass\nKey Formula Reference\nMultinomial Logit Probability: \\(P_{ij} = \\frac{\\exp(\\alpha_j + \\mathbf{x}_{ij}'\\boldsymbol{\\beta} + \\mathbf{z}_i'\\boldsymbol{\\gamma}_j)}{\\sum_{k=1}^{J} \\exp(\\alpha_k + \\mathbf{x}_{ik}'\\boldsymbol{\\beta} + \\mathbf{z}_i'\\boldsymbol{\\gamma}_k)}\\)\nNested Logit Probability: \\(P_{ij} = P_{i|\\text{nest}} \\times P_{ij|\\text{nest}} = \\frac{\\exp(\\lambda_n I_{in})}{\\sum_m \\exp(\\lambda_m I_{im})} \\times \\frac{\\exp(V_{ij}/\\lambda_n)}{\\sum_{k \\in \\text{nest}} \\exp(V_{ik}/\\lambda_n)}\\)\nInclusive Value: \\(I_{in} = \\ln\\left(\\sum_{j \\in \\text{nest}} \\exp(V_{ij}/\\lambda_n)\\right)\\)\nThe mathematical foundation upon which all choice adventures are built…"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-pitch-choosing-the-right-trail-matters",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-pitch-choosing-the-right-trail-matters",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "The Pitch: Choosing the Right Trail Matters",
    "text": "The Pitch: Choosing the Right Trail Matters\nConsumer choice is everywhere — from cereal to cars to climate systems. Business success hinges on understanding these choices: pricing, product design, market segmentation.Classic models fall short with unrealistic assumptions."
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-pitch-choice-matters",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-pitch-choice-matters",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "The Pitch: Choice Matters",
    "text": "The Pitch: Choice Matters\nConsumer choice is everywhere — from cereal to cars to climate systems. Business success hinges on understanding these choices are driven by pricing, product design, market segmentation strategies.\nPyMC-Marketing allows you to simulate product interventions safely, and learn the expected impact of new product strategies"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#preliminaries",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#preliminaries",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "Preliminaries",
    "text": "Preliminaries\n\n\n\n\n\n\nWho am I?\n\n\n\nI’m a data scientist at Personio\n\nBayesian statistician,\nReformed philosopher and logician.\n\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\nThe worked examples used here can be found here\n\n\n\n\nMy Website"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-adventure-begins",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-adventure-begins",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "The Adventure Begins:",
    "text": "The Adventure Begins:\nChoosing our Own Path\n\n\n\n\n\n\n21 Discrete Possible Endings!\n\n\n\n\nThe Luxury of being able to explore all paths\n\n\n\n\nDeterministic Routing and preferred Endings\n\n\n\n\nPeeking ahead and “cheating” fate."
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#choice",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#choice",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "Choice",
    "text": "Choice\n\n\nThe Model Developer’s Journey\n“Which path shall I take?”\n\nMultinomial Logit (MNL) - The familiar trail\nNested Logit - The advanced expedition\nMixed Logit - The future frontier\n\nEvery choice shapes the model’s destiny…\n\nThe Consumer’s Dilemma\n“What drives human choice?”\n\nUtility maximization\nRational decision-making\nIndependence of Irrelevant Alternatives (IIA)\nExpected value calculations\n\nEach purchase tells a story of preference…"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-difficulty-scaling",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-difficulty-scaling",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "The Difficulty Scaling",
    "text": "The Difficulty Scaling\nGenuine Uncertainty and the Multiplicity of Paths\n\n\n\n\n\n\nContinuum Possible Endings\n\n\n\n\nImpossibility of precise Survey\n\n\n\n\nProbabilistic Choice of paths\n\n\n\n\nCertainty (if any) only in Expectation"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-scaling-difficulty",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-scaling-difficulty",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "The Scaling Difficulty",
    "text": "The Scaling Difficulty\nGenuine Uncertainty and the Multiplicity of Paths\n\n\n\n\n\nGrowing up is grappling with uncertainty\n\n\n\n\n\nContinuum Possible Endings\n\n\n\n\nImpossibility of precise Survey and Oversight\n\n\n\n\nProbabilistic Choice over Paths\n\n\n\n\nCertainty (if any) only in Expectation and Breadth of Possibility"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#sampling-the-path-trajectories",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#sampling-the-path-trajectories",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "Sampling the Path Trajectories",
    "text": "Sampling the Path Trajectories\nBayesian Inference over Unrealised Worlds\n\nInference: What is the most plausible world given the data?\n\n\\[ p(\\theta_{w_{i}} | Y) = \\dfrac{p(\\theta_{w_{i}})p(Y | \\theta_{i})}{\\sum_{j}^{N} p(\\theta_{w_j})p(Y | \\theta_{w_j}) }\\]\n\nCounterfactual Inference: What plausibly happens in nearby worlds?\n\n\n\n\n\n\\(\\mathbf{\\theta_{w_{1}}} \\rightsquigarrow\\)\n\\(\\mathbf{\\theta_{w_{2}}} \\rightsquigarrow\\)\n\\(\\mathbf{\\theta_{w_{3}}} \\rightsquigarrow\\)\n\n\n\n\\(f(\\alpha_{w_1}, \\beta_{w_1}^{0}, \\beta_{w_1}^{1}) \\rightsquigarrow\\)\n\\(f(\\alpha_{w_2}, \\beta_{w_2}^{0}, \\beta_{w_2}^{1}) \\rightsquigarrow\\)\n\\(f(\\alpha_{w_3}, \\beta_{w_3}^{0}, \\beta_{w_3}^{1}) \\rightsquigarrow\\)\n\n\n\n\\(p(Y |w_1)\\)  \\(\\downarrow\\)\n\\(p(Y | w_2)\\)  \\(\\downarrow\\)\n\\(p(Y | w3)\\) \\(\\downarrow\\)"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-path-before-us",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-path-before-us",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "The Path Before Us",
    "text": "The Path Before Us\n\nThe Consumer’s Dilemma: What drives us to Choose in the face such vast Possibility?"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#revealed-preference",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#revealed-preference",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "Revealed Preference",
    "text": "Revealed Preference\nUtility driven Choice\n\n\n\n\n“How can we learn what drives human choice?”\n\nUtility maximization\nRational decision-making\nExpected value calculations\nImpulse and advertising?\n\nEach path taken tells a story of preference and reveals something about how the attributes of each alternative tempt or repel the chooser"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#world-state-informs-choice",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#world-state-informs-choice",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "World State Informs Choice",
    "text": "World State Informs Choice\nThe Mathematical Foundation of Decision\nThe utility function forms the cornerstone of choice modeling:\n\\[\\color{red}U_{ij} = \\color{blue}\\alpha_{ij} + \\color{blue}\\beta_{ij}^{1} \\color{black}\\cdot X_{ij}^{1} + \\color{blue} \\beta_{ij}^{2} \\color{black}\\cdot X_{ij}^{2} \\]\n\\[P_{ij} = \\frac{\\exp(\\color{red}U_{ij})}{\\sum_{k=1}^{J} \\exp(\\color{red}U_{ik})} \\]\nWhere:\n\n\\(U_{ij}\\) represents the systematic utility for individual \\(i\\) choosing alternative \\(j\\) at a particular world state \\(w = \\{ \\color{blue}\\alpha_{ij}, \\color{blue}\\beta_{ij}^{1}, \\color{blue}\\beta_{ij}^{2} X_{ij}^{1}, X_{ij}^{2} \\}\\)\n\\(X_{ij}\\) represents observed covariates (product attributes) and \\(\\color{blue}\\theta\\) structural parameters within the world \\(w\\).\nCollectively, the mathematical structure and world-state are combined to make a theory of the data generating process for the world \\(w\\)."
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#utility-and-marginal-effects",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#utility-and-marginal-effects",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "Utility and Marginal Effects",
    "text": "Utility and Marginal Effects\n\nLight shaft versus Glowing Fire?"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-simple-path-forward",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-simple-path-forward",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "The Simple Path Forward",
    "text": "The Simple Path Forward\nThe probability of choosing alternative \\(j\\) follows the elegant logistic form:\n\\(\\frac{\\exp(\\color{red}U_{ij})}{\\sum_{k=1}^{J} \\exp(\\color{red}U_{ik})} = P_{ij} \\Rightarrow s_{j}(\\color{blue}\\theta)=P(u_{j}&gt;u_{k};\\forall_{k̸=j})\\)\nKey Contributions in PR #1654\n\nNew Model Class: MNLogit with Wilkinson-style formula interface\nAlternative-Specific Attributes: Price, quality, brand effects\nIndividual Fixed Attributes: Income, demographics, preferences\nCausal Inference Ready: Built-in counterfactual analysis"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#relative-utility-and-marginal-effects",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#relative-utility-and-marginal-effects",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "Relative Utility and Marginal Effects",
    "text": "Relative Utility and Marginal Effects\nPaths Diverge\nDo you value the company of others? Do you fear it? What about the average cave dweller?\n\n \\[ u(\\text{Light shaft + Silence}) - u(\\text{Glowing Fire + Conversational Echoes}) &gt; 0?\\]"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#agenda",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#agenda",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "Agenda",
    "text": "Agenda\n\nBayesian Background: The Choice Setting\nThe Choice Data: PyMC-Marketing and Consumer Choice\nThe Tempting Path: Mulitnomial Logit and the IIA Problem\nThe Branching Path: Nested Logit and Market Structure\nYour Path: Choose your own Adventure"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#pymc-marketing",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#pymc-marketing",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "PyMC-Marketing",
    "text": "PyMC-Marketing\n\n\n\n\nBayesian Marketing Analytics\n\nMedia Mix Modelling, Customer Lifetime Value and Consumer Choice.\nUncertainty-aware decisions: model not just what people choose, but how sure we are.\nCausal inference ready: run interventions, not just predictions.\nModern Bayesian engine for scalability, flexibility, and transparency.\nIntuitive syntax with powerful underlying math."
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#choice-data",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#choice-data",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "Choice Data",
    "text": "Choice Data\n\nChoice Scenarios specified with attributes and choice outcomes for each discrete alternative"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#multinomial-logit",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#multinomial-logit",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "Multinomial Logit",
    "text": "Multinomial Logit"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#multinomial-logit-the-simple-path-forward",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#multinomial-logit-the-simple-path-forward",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "Multinomial Logit: The Simple Path Forward",
    "text": "Multinomial Logit: The Simple Path Forward\n\n\nThe probability of choosing alternative \\(j\\) follows the elegant logistic form:\n\\[\\frac{\\exp(\\color{red}U_{ij})}{\\sum_{k=1}^{J} \\exp(\\color{red}U_{ik})} = P_{ij} \\Rightarrow s_{j}(\\color{blue}\\theta_{w})=P(u_{j}&gt;u_{k};\\forall_{k̸=j})\\]\nA simple model with a compelling interpretation. Too simple?"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#pymc-marketing-implementation",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#pymc-marketing-implementation",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "PyMC-Marketing Implementation",
    "text": "PyMC-Marketing Implementation\nTrace Paths through Possible Worlds\n\nutility_formulas = [\n    \"gc ~ ic_gc + oc_gc | income + rooms + agehed\",\n    \"gr ~ ic_gr + oc_gr | income + rooms + agehed\",\n    \"ec ~ ic_ec + oc_ec | income + rooms + agehed\",\n    \"er ~ ic_er + oc_er | income + rooms + agehed\",\n    \"hp ~ ic_hp + oc_hp | income + rooms + agehed\",\n]\n\nmnl = MNLogit(df, utility_formulas, \"depvar\", covariates=[\"ic\", \"oc\"])\nmnl.sample()"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#utility-matrix",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#utility-matrix",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "Utility Matrix",
    "text": "Utility Matrix\nKey Aspects\n\n\n\\[ \\begin{split} \\begin{split} \\begin{pmatrix}\nu_{gc}   \\\\\nu_{gr}   \\\\\nu_{ec}   \\\\\nu_{er}   \\\\\nu_{hp}   \\\\\n\\end{pmatrix} =  \\begin{pmatrix}\ngc_{ic} & gc_{oc}  \\\\\ngr_{ic} & gr_{oc}  \\\\\nec_{ic} & ec_{oc}  \\\\\ner_{ic} & er_{oc}  \\\\\nhp_{ic} & hp_{oc}  \\\\\n\\end{pmatrix} \\begin{pmatrix}\n\\color{blue}\\beta_{ic}   \\\\\n\\color{blue}\\beta_{oc}   \\\\\n\\end{pmatrix}  \\end{split}\n\\end{split}\n\\]\n\nNew Model Class: MNLogit with Wilkinson-style formula interface\nAlternative-Specific Attributes: Price, quality, brand effects\nIndividual Fixed Attributes: Income, demographics, preferences\nCausal Inference Ready: Built-in counterfactual analysis\n\n\n\nutility_formulas = [\n    \"gc ~ ic_gc + oc_gc \",\n    \"gr ~ ic_gr + oc_gr \",\n    \"ec ~ ic_ec + oc_ec \",\n    \"er ~ ic_er + oc_er \",\n    \"hp ~ ic_hp + oc_hp \",\n]\n\nmnl = MNLogit(df, utility_formulas, \"depvar\", covariates=[\"ic\", \"oc\"])\nmnl"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-red-busblue-bus-paradox",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-red-busblue-bus-paradox",
    "title": "The Art of Choice: Discrete Decision Models in PyMC-Marketing",
    "section": "The Red Bus/Blue Bus Paradox",
    "text": "The Red Bus/Blue Bus Paradox\nImagine a commuter’s choice between: - Car (probability 0.6) - Blue Bus (probability 0.4)\nNow introduce a Red Bus (identical to Blue Bus)…\nMNL Prediction: Each bus gets 0.2 probability, car stays 0.6 Reality: Car should get ~0.67, each bus ~0.165\nThe Adventure’s Crossroads\n\n“The path splits before you. To the left lies the familiar but flawed trail of independence. To the right, a more complex but realistic journey through nested structures…”"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-iia-problem---a-plot-twist",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#the-iia-problem---a-plot-twist",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "The IIA Problem - A Plot Twist",
    "text": "The IIA Problem - A Plot Twist\n\n\n\n\n\n50% Red Bus 50% Car\n\n\n\n\nWhat happens if we introduce a Blue Bus?\n\n\n\n\n33% Red Bus, 33% Blue Bus, 33% Car\n\n\n\nThe Multinomial Logit enforces the Indepdence of Irrelevant Alternatives property into preference calculations.\n\\[\\dfrac{P_{j}}{P_{i}}  = \\dfrac{ \\dfrac{e^{U_{j}}}{\\sum_{i}^{n}e^{U_{k}}}}{\\dfrac{e^{U_{i}}}{\\sum_{i}^{n}e^{U_{k}}}} = \\dfrac{e^{U_{j}}}{e^{U_{i}}} = e^{U_{j} - U_{k}}\\]\n\nKey Take-away: The Model Ignores Market Structure"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#counterfactual-plausibility-as-criteria-of-adequacy",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#counterfactual-plausibility-as-criteria-of-adequacy",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "Counterfactual Plausibility as Criteria of Adequacy",
    "text": "Counterfactual Plausibility as Criteria of Adequacy\nCounterfactual Choice Scenarios\nnew_policy_df = df.copy()\nnew_policy_df[[\"ic_ec\", \"ic_er\"]] = new_policy_df[[\"ic_ec\", \"ic_er\"]] * 1.5\n\n## Posterior Predictive Forecast under counterfactual setting\nidata_new_policy = mnl.apply_intervention(new_choice_df=new_policy_df)\n\n## Compare Old and New Policy Settings\nchange_df = mnl.calculate_share_change(mnl.idata, mnl.intervention_idata)\nchange_df"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#counterfactual-inference",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#counterfactual-inference",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "Counterfactual Inference",
    "text": "Counterfactual Inference\nCounterfactual Plausibility as Criteria of Adequacy\nnew_policy_df = df.copy()\nnew_policy_df[[\"ic_ec\", \"ic_er\"]] = new_policy_df[[\"ic_ec\", \"ic_er\"]] * 1.5\n\n## Posterior Predictive Forecast under counterfactual setting\nidata_new_policy = mnl.apply_intervention(new_choice_df=new_policy_df)\n\n## Compare Old and New Policy Settings\nchange_df = mnl.calculate_share_change(mnl.idata, mnl.intervention_idata)\nchange_df\n\n\n\n\n\nProportional Substitution Patterns\n\n\n\n\n\n\nProduct Switching follows prior Market share irrespective of product type"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#considered-choice-as-branching-decision-trees",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#considered-choice-as-branching-decision-trees",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "Considered Choice as Branching Decision Trees",
    "text": "Considered Choice as Branching Decision Trees"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#considered-choice-as-branching-probability-trees",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#considered-choice-as-branching-probability-trees",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "Considered Choice as Branching Probability Trees",
    "text": "Considered Choice as Branching Probability Trees"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#considered-choice",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#considered-choice",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "Considered Choice",
    "text": "Considered Choice\nBranching Probability Trees\n\n\n\n\\(U = Y + W\\)\n\n\n\n\\(P(i) \\text{ when } i \\in Alts\\)\n\\(P(\\text{choose nest B}) \\cdot P(\\text{choose i} | \\text{ i} \\in \\text{B})\\)\n\n\n\n\n\\(P(\\text{choose nest B}) = \\dfrac{e^{W + \\lambda_{k}I_{k}}}{\\sum_{l=1}^{K} e^{W + \\lambda_{l}I_{l}}}\\)\n\n\n\n\n\\(P(\\text{choose i} | \\text{ i} \\in \\text{B}) = \\dfrac{e^{Y_{i} / \\lambda_{k}}}{\\sum_{j \\in B_{k}} e^{Y_{j} / \\lambda_{k}}}\\)\n\n\n\n\n\\(I_{k} = ln \\sum_{j \\in B_{k}}  e^{Y_{j} / \\lambda_{k}} \\\\ \\text{ and } \\lambda_{k} \\sim Beta(1, 1)\\)\nThe log-sum component allows for the utility of any alternatives within a nest to “bubble up” and influence the attractiveness of the overall nest."
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#nesting-structure-in-pymc-marketing",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#nesting-structure-in-pymc-marketing",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "Nesting Structure in PyMC-Marketing",
    "text": "Nesting Structure in PyMC-Marketing\nutility_formulas = [\n    \"gc ~ ic_gc + oc_gc | income + rooms \",\n    \"ec ~ ic_ec + oc_ec | income + rooms \",\n    \"gr ~ ic_gr + oc_gr | income + rooms \",\n    \"er ~ ic_er + oc_er | income + rooms \",\n    \"hp ~ ic_hp + oc_hp | income + rooms \",\n]\n\nnesting_structure = {\"central\": [\"gc\", \"ec\"], \"room\": [\"hp\", \"gr\", \"er\"]}\n\nnstL_1 = NestedLogit(\n    df,\n    utility_formulas,\n    \"depvar\",\n    covariates=[\"ic\", \"oc\"],\n    nesting_structure=nesting_structure,\n    model_config={\n        \"alphas_\": Prior(\"Normal\", mu=0, sigma=5, dims=\"alts\"),\n        \"betas\": Prior(\"Normal\", mu=0, sigma=1, dims=\"alt_covariates\"),\n        \"betas_fixed_\": Prior(\"Normal\", mu=0, sigma=1, dims=\"fixed_covariates\"),\n        \"lambdas_nests\": Prior(\"Beta\", alpha=2, beta=2, dims=\"nests\"),\n    },\n)\nnstL_1"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#here-be-dragons",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#here-be-dragons",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "Here be Dragons",
    "text": "Here be Dragons"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#behaviourial-insight-in-product-preference",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#behaviourial-insight-in-product-preference",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "Behaviourial Insight in Product Preference",
    "text": "Behaviourial Insight in Product Preference\n\nThe relative importance of product attributes implied by our observed dataThe relative importance of installation costs versus operating costs might suggest where to impose a novel pricing strategy?"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#credible-counterfactuals",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#credible-counterfactuals",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "Credible Counterfactuals",
    "text": "Credible Counterfactuals\nNon-Proportional Substitution\nnew_policy_df = df.copy()\nnew_policy_df[[\"ic_ec\", \"ic_er\"]] = new_policy_df[[\"ic_ec\", \"ic_er\"]] * 1.5\n\nidata_new_policy_1 = nstL_1.apply_intervention(new_choice_df=new_policy_df)\nchange_df_1 = nstL_1.calculate_share_change(nstL_1.idata, nstL_1.intervention_idata)\nchange_df_1\n\nNested Logit allows for patterns of Non-Proportional Substitution under counterfactual settings"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#market-interventions-and-implied-worlds",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#market-interventions-and-implied-worlds",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "Market Interventions and Implied Worlds",
    "text": "Market Interventions and Implied Worlds\n\n\n\n\nwith pm.do(\n    model,\n    {\"X1\": np.ones(len(df)), \n    \"beta1\": 0.5},\n    prune_vars=True,\n) as counterfactual_model:\n    idata_trt = pm.sample_posterior_predictive(idata, \n    var_names=[\"like\", \"p\"])\nCausal Inference with the Do-Operator modifies world-state and data alike allowing for compelling intervention studies about consumer behaviour\n\n\\[ w = \\{ \\alpha, \\beta^{1}, \\beta_{2}, X^{1}, X^{2}  \\} \\\\\n\\Rightarrow w^{*} = \\{ \\alpha, \\beta^{*}, \\beta_{2}, X^{*}, X^{2}  \\} \\]"
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#conclusion-explore-the-branching-worlds",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#conclusion-explore-the-branching-worlds",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "Conclusion: Explore the Branching Worlds",
    "text": "Conclusion: Explore the Branching Worlds\n\n\n\n\n\n\nBayesian Models offer a data-informed tool for simulation experiments for ceteris-paribus inference.\n\n\n\n\nPlausible counterfactual inference depends on compelling world structure encoded in the model.\n\n\n\n\nNested Logit models in PyMC-Marketing allows us to encode market structures for compelling intervention studies.\n\n\n\n\nThey also reveal behaviorial insights of consumers unlocking new marketing strategies for your adventure!"
  },
  {
    "objectID": "oss/causalpy/paradox_propensity.html",
    "href": "oss/causalpy/paradox_propensity.html",
    "title": "Design and Analysis: Paradox of Bayesian Propensity Scores",
    "section": "",
    "text": "The Paradox of Bayesian Propensity\nIn this project I sought to outline the process of justifying the use of propensity scores in bayesian outcome regressions in CausalPy. We emphasised the role of propensity scores in Bayesian inference. The focus was on the manner in which the estimation of a causal treatment effects using propensity scores require a two-stage strategy to avoid biasing the propensity score distribution. The demonstration can be seen here or downloaded as a notebook here\n\n\n\nComparison with OLS\n\n\nPropensity scores are celebrated as a cornerstone of causal inference, offering elegant solutions to selection bias through inverse weighting and covariate balancing. Yet a provocative paradox emerges in Bayesian analysis: when you already condition on all relevant covariates, propensity scores should theoretically contain no additional information—they mathematically cancel out. So why do sophisticated Bayesian practitioners still rely on them?\nWe see a striking pattern: joint Bayesian models consistently misestimate treatment effects compared to two-stage approaches when using propensity score adjustments in the outcome model. The culprit is a violation of causal ordering: when you model propensity scores and outcomes simultaneously, information flows backwards in time, allowing observed outcomes to reshape your understanding of treatment assignment. By forcing a modular, two-stage approach, propensity scores can be retained and useful. The modular approach enforcse the temporal precedence of treatment assignment over outcomes reflecting the data generating process better."
  },
  {
    "objectID": "posts/post-with-code/PyTorch_Amortized_Bayes/amortized_bayes.html",
    "href": "posts/post-with-code/PyTorch_Amortized_Bayes/amortized_bayes.html",
    "title": "Heuristics in Latent Space: VAEs and Bayesian Inference",
    "section": "",
    "text": "TLDR: Transparency and Heuristics in Latent Space\n\n\n\nPrincipal Components Analysis is used to optimally reconstruct a complex multivariate data set from a lower level compressed dimensional space. Variational auto-encoders allow us to achieve yet more flexible reconstruction results in non-linear cases. Drawing a new sample from the posterior predictive distribution of Bayesian models similarly supplies us with insight in the variability of realised data. Both methods assume a latent model of the data generating process that aims to leverage a compressed representation of the data. These are different heuristics with different consequences for how we understand the variability in the world. Both encode different degrees of inductive bias through architectural specification, but of the two, only Bayesian methods do so transparently.\nWe find (a) a VAE architecture that can recover the latent correlation structure of a multivariate normal outcome if scaled up with enough data and (b) a theoretically superior architecture designed to handle missing data that breaks on the reconstruction task when trying to do so. Both are then compared to the simpler multivariate normal Bayesian model in PyMC which is able to recover the correlation structure much more efficiently. These methods are applied to an imputation problem for non-response in job-satisfaction surveys. We then draw some lessons about the value of transparent inductive biases in encoded in latent factor structures of Structural Equation Models."
  },
  {
    "objectID": "posts/post-with-code/PyTorch_Amortized_Bayes/amortized_bayes.html#reconstruction-error",
    "href": "posts/post-with-code/PyTorch_Amortized_Bayes/amortized_bayes.html#reconstruction-error",
    "title": "Heuristics in Latent Space: VAEs and Bayesian Inference",
    "section": "",
    "text": "In applied statistical modeling, the hardest problems are often those we can’t directly observe. Reconstructing latent structure from noisy, incomplete data forces us to ask: what do we believe about the world, and how do our models reflect those beliefs? Can they reproduce plausible patterns?\nThis notebook explores how different modeling choices—deep generative models versus structured Bayesian inference—handle reconstruction, imputation, and inductive bias. Along the way, we confront a central difficulty: in high-dimensional latent spaces, it’s easy to be wrong in convincing ways. Learning to “fill in the blanks” isn’t just a technical challenge—it’s a philosophical one.\nWhat we cover: - Marginal reconstruction with VAEs - How well do deep generative models recover observed variables when trained on complete data?\n\nMask-aware VAEs for missing data\n\nCan we explicitly teach VAEs to ignore the gaps—and does it help?\n\nMetric vs. structure recovery\n\nWhy good marginal fits can still hide deep structural errors.\n\nSimple strategies, surprising results\n\nWhen mean imputation and simple architectures outperform smarter models.\n\nBayesian inference as structured inductive bias\n\nHow priors over covariance structures yield robust estimates even with small samples.\n\nImplicit imputation via the posterior\n\nHow the Bayesian model naturally fills in missing data, respecting uncertainty.\n\n\nThroughout, we learn that reconstruction alone is not validation, and that model structure matters more than model complexity. Inductive bias isn’t a limitation—it’s our strongest tool for generalization.\n\nimport torch\nimport torchvision.datasets as dsets\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nimport pymc as pm \nimport arviz as az\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)"
  },
  {
    "objectID": "posts/post-with-code/PyTorch_Amortized_Bayes/amortized_bayes.html#job-satisfaction-data",
    "href": "posts/post-with-code/PyTorch_Amortized_Bayes/amortized_bayes.html#job-satisfaction-data",
    "title": "Heuristics in Latent Space: VAEs and Bayesian Inference",
    "section": "Job Satisfaction Data",
    "text": "Job Satisfaction Data\nJob satisfaction is not one thing. It’s a bundle: how we think, how we feel, how we work. In our simulation, we model this bundle through twelve indicators—grouped into job satisfaction (JW1–JW3), well-being (UF1, UF2, FOR), dysfunctional patterns (DA1–DA3), and constructive thought strategies (EBA, ST, MI). In particular, the focus is on the relationship of EBA (Evaluating beliefs and assumptions), ST (Self-talk) and MI (Mental imagery formation) have on the other constructs. This framing is adapted from the discussion in Vehkalahti and Everitt’s Multivariate Analysis for the Behavioral Sciences. The Python code generates data by first defining a realistic correlation matrix and scaling it by standard deviations to yield a structured covariance matrix. The parameterisation is drawn from the original study. From this, we draw synthetic observations using a multivariate normal distribution. This allows us to test estimators under controlled complexity.\n\n\nCode\n# Standard deviations\nstds = np.array([0.939, 1.017, 0.937, 0.562, 0.760, 0.524, \n                 0.585, 0.609, 0.731, 0.711, 1.124, 1.001])\n\nn = len(stds)\n\n# Lower triangular correlation values as a flat list\ncorr_values = [\n    1.000,\n    .668, 1.000,\n    .635, .599, 1.000,\n    .263, .261, .164, 1.000,\n    .290, .315, .247, .486, 1.000,\n    .207, .245, .231, .251, .449, 1.000,\n   -.206, -.182, -.195, -.309, -.266, -.142, 1.000,\n   -.280, -.241, -.238, -.344, -.305, -.230,  .753, 1.000,\n   -.258, -.244, -.185, -.255, -.255, -.215,  .554,  .587, 1.000,\n    .080,  .096,  .094, -.017,  .151,  .141, -.074, -.111,  .016, 1.000,\n    .061,  .028, -.035, -.058, -.051, -.003, -.040, -.040, -.018,  .284, 1.000,\n    .113,  .174,  .059,  .063,  .138,  .044, -.119, -.073, -.084,  .563,  .379, 1.000\n]\n\n# Fill correlation matrix\ncorr_matrix = np.zeros((n, n))\nidx = 0\nfor i in range(n):\n    for j in range(i+1):\n        corr_matrix[i, j] = corr_values[idx]\n        corr_matrix[j, i] = corr_values[idx]\n        idx += 1\n\n# Covariance matrix: Sigma = D * R * D\ncov_matrix = np.outer(stds, stds) * corr_matrix\n#cov_matrix_test = np.dot(np.dot(np.diag(stds), corr_matrix), np.diag(stds))\nFEATURE_COLUMNS=[\"JW1\",\"JW2\",\"JW3\", \"UF1\",\"UF2\",\"FOR\", \"DA1\",\"DA2\",\"DA3\", \"EBA\",\"ST\",\"MI\"]\ncorr_df = pd.DataFrame(corr_matrix, columns=FEATURE_COLUMNS)\n\ncov_df = pd.DataFrame(cov_matrix, columns=FEATURE_COLUMNS)\n\ndef make_sample(cov_matrix, size, columns, missing_frac=0.0, impute=False):\n    sample_df = pd.DataFrame(np.random.multivariate_normal([0]*12, cov_matrix, size=size), columns=FEATURE_COLUMNS)\n    if missing_frac &gt; 0.0: \n        total_values = sample_df.size\n        num_nans = int(total_values * missing_frac)\n\n        # Choose random flat indices\n        nan_indices = np.random.choice(total_values, num_nans, replace=False)\n\n        # Convert flat indices to (row, col)\n        rows, cols = np.unravel_index(nan_indices, sample_df.shape)\n\n        # Set the values to NaN\n        sample_df.values[rows, cols] = np.nan\n\n    if impute: \n        sample_df.fillna(sample_df.mean(axis=0), inplace=True)\n\n\n    return sample_df\n\nsample_df = make_sample(cov_matrix, 263, FEATURE_COLUMNS)\nsample_df.head(10)\n\n\n\n\n\n\n\n\n\nJW1\nJW2\nJW3\nUF1\nUF2\nFOR\nDA1\nDA2\nDA3\nEBA\nST\nMI\n\n\n\n\n0\n2.578412\n3.565673\n1.976356\n0.261307\n-0.162994\n0.425867\n-0.341104\n-0.663246\n-1.898728\n0.412019\n0.167105\n1.179567\n\n\n1\n0.147594\n0.316841\n1.244429\n-0.566278\n0.157724\n0.084468\n0.625595\n0.590472\n0.663543\n-0.392881\n0.434897\n-0.379236\n\n\n2\n0.799763\n0.277303\n-0.361947\n0.703534\n0.326682\n-0.503207\n0.558374\n0.313461\n0.005397\n0.373729\n-0.487022\n1.029725\n\n\n3\n-1.245506\n-1.338533\n-0.845951\n-0.700084\n-0.324149\n0.159880\n0.205925\n0.402143\n1.305458\n0.987969\n1.313665\n0.171655\n\n\n4\n0.066713\n0.958231\n0.697802\n0.071260\n-0.102972\n0.186527\n-0.015224\n0.147267\n0.674598\n-0.719964\n-0.845735\n-0.203553\n\n\n5\n-0.324724\n0.189098\n0.508612\n0.078070\n0.239252\n-0.556651\n0.560134\n0.180378\n-0.813426\n-0.731620\n-1.727260\n1.150983\n\n\n6\n-0.190670\n-0.169900\n-0.798084\n0.247005\n-0.475737\n0.325712\n0.892692\n-0.094400\n-0.648839\n-0.306447\n-0.484535\n0.358890\n\n\n7\n-1.042066\n-1.583407\n-2.318488\n-0.073663\n-0.476409\n0.235916\n0.038662\n-0.154278\n0.301485\n0.881227\n1.128888\n1.524227\n\n\n8\n-0.818276\n-0.596287\n-1.432412\n1.094289\n-0.194693\n-0.699425\n-0.652304\n-0.281707\n-0.135852\n-0.930961\n0.236916\n-0.257751\n\n\n9\n-1.167284\n-1.860362\n-0.947581\n-0.767971\n-1.306440\n-0.266554\n0.019500\n0.711873\n1.820465\n-1.407315\n-0.165145\n-2.395606\n\n\n\n\n\n\n\nA stringent test of any model for the job satisfaction process is whether it recovers the observed correlation structure. The code extracts pairwise correlations from the simulated data and visualizes them using a heatmap. Strong ties—positive or negative—appear as vivid blocks, offering a visual audit of relational intensity. This pattern matters. While it does not imply causation, it does offer a benchmark: if a model can’t replicate these patterns, it’s likely missing structure.\n\n\nCode\ndata = sample_df.corr()\n\ndef get_srmr(resids):\n    srmr = np.sqrt(np.sum(np.tril(resids).flatten()**2) / (12*(12 + 1))/2) \n    return srmr\n\ndef plot_heatmap(data, title=\"Correlation Matrix\",  vmin=-.2, vmax=.2, ax=None, figsize=(10, 6), colorbar=True, cmap='magma'):\n    data_matrix = data.values\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    im = ax.imshow(data, cmap=cmap, vmin=vmin, vmax=vmax)\n\n    for i in range(data_matrix.shape[0]):\n        for j in range(data_matrix.shape[1]):\n            text = ax.text(\n                j, i,                      # x, y coordinates\n                f\"{data_matrix[i, j]:.2f}\",       # text to display\n                ha=\"center\", va=\"center\",  # center alignment\n                color=\"white\" if data_matrix[i,j] &lt; 0.5 else \"black\"  # contrast color\n            )\n\n    ax.set_title(title)\n    ax.set_xticklabels(data.columns)  \n    ax.set_xticks(np.arange(data.shape[1]))\n    ax.set_yticklabels(data.index)  \n    ax.set_yticks(np.arange(data.shape[0]))\n    if colorbar:\n        plt.colorbar(im)\n\nplot_heatmap(data, vmin=-1, vmax=1, cmap='coolwarm')\n\n\n\n\n\n\n\n\n\n\nSingular Value Decomposition and Reconstruction Error\nMultivariate systems often lie on lower-dimensional structures. Principal Component Analysis (PCA), via Singular Value Decomposition (SVD), helps us explore these spaces. Any data matrix \\(X\\) can be decomposed as \\(X = U\\Sigma V^{T}\\) where \\(\\Sigma\\) contains singular values ordered by importance. Truncating this expansion at rank \\(k\\) reconstructs an approximation of the original. The code below shows that even with only 2 or 5 components, much of the structure is retained. As we increase \\(k\\) the reconstruction error falls—revealing the data’s latent geometry.\n\nX = make_sample(cov_matrix, 100, columns=FEATURE_COLUMNS)\nU, S, VT = np.linalg.svd(X, full_matrices=False)\n\nSVD decomposes a matrix into interpretable parts: directions of variation, scaled by importance. The reconstruction plots show that a handful of components (principal axes) capture most of the variation in job satisfaction data. This supports the idea of latent representations: the surface complexity of our variables arises from simpler underlying factors. Variational autoencoders (VAEs) and Bayesian posterior predictive sampling attempt similar compression and recovery strategies, but embed it within generative models. As we’ll see, their reconstruction quality also reflects the strength and focus of their latent encoding.\n\n\nCode\nranks = [2, 5, 12]\nreconstructions = []\nfor k in ranks:\n    X_k = U[:, :k] @ np.diag(S[:k]) @ VT[:k, :]\n    reconstructions.append(X_k)\n\n# Plot original and reconstructed matrices\nfig, axes = plt.subplots(1, len(ranks) + 1, figsize=(10,15))\naxes[0].imshow(X, cmap='viridis')\naxes[0].set_title(\"Original\")\naxes[0].axis(\"off\")\n\nfor ax, k, X_k in zip(axes[1:], ranks, reconstructions):\n    ax.imshow(X_k, cmap='viridis')\n    ax.set_title(f\"Rank {k}\")\n    ax.axis(\"off\")\n\nplt.suptitle(\"Reconstruction of Data Using SVD \\n various truncation options\",fontsize=12, x=.5, y=1.01)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nSVD shows us that high-dimensional data can often be compressed with little loss. But it’s linear and fixed: it doesn’t learn a generalisable representation. Variational autoencoders generalize this idea. Like PCA, they seek a latent representation, but they do so through non-linear mappings learned from data. An autoencoder has two parts: an encoder, which compresses data into a latent measure, and a decoder, which reconstructs it. Unlike PCA, this process is flexible and adapts to the data distribution, not just its linear axes. The variational auto-encoder (VAE) extends this idea by placing probability distributions over these latent codes, allowing us to sample and generate new data points."
  },
  {
    "objectID": "posts/post-with-code/PyTorch_Amortized_Bayes/amortized_bayes.html#variational-auto-encoders",
    "href": "posts/post-with-code/PyTorch_Amortized_Bayes/amortized_bayes.html#variational-auto-encoders",
    "title": "Heuristics in Latent Space: VAEs and Bayesian Inference",
    "section": "Variational Auto-Encoders",
    "text": "Variational Auto-Encoders\nThe NumericVAE class below captures this structure. The encoder maps observed data to the parameters of a distribution over latent variables \\(z\\) specifically a Gaussian with learned mean and variance: \\(q(z|x) = N(\\mu(x), \\sigma^{2}(x))\\) Using the reparameterization trick, we sample \\(z \\sim q(z| x)\\) and decode it into a predicted distribution over the observed variables \\(p(x |z)\\). This makes the model generative: not only can it reconstruct known data, it can synthesize new, plausible examples by sampling from the latent prior \\(p(z) \\sim N(0, I)\\) and decoding. The result is a flexible, probabilistic architecture that mimics and extends the SVD idea of structure through compression.\n\nclass NumericVAE(nn.Module):\n    def __init__(self, n_features, hidden_dim=64, latent_dim=8):\n        super().__init__()\n        \n        # ---------- ENCODER ----------\n        # First layer: compress input features into a hidden representation\n        self.fc1 = nn.Linear(n_features, hidden_dim)\n        \n        # Latent space parameters (q(z|x)): mean and log-variance\n        self.fc_mu = nn.Linear(hidden_dim, latent_dim)       # μ(x)\n        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)   # log(σ^2(x))\n        \n        # ---------- DECODER ----------\n        # First layer: map latent variable z back into hidden representation\n        self.fc2 = nn.Linear(latent_dim, hidden_dim)\n        \n        # Output distribution parameters for reconstruction p(x|z)\n        # For numeric data, we predict both mean and log-variance per feature\n        self.fc_out_mu = nn.Linear(hidden_dim, n_features)        # μ_x(z)\n        self.fc_out_logvar = nn.Linear(hidden_dim, n_features)    # log(σ^2_x(z))\n\n    # ENCODER forward pass: input x -&gt; latent mean, log-variance\n    def encode(self, x):\n        h = F.relu(self.fc1(x))       # Hidden layer with ReLU\n        mu = self.fc_mu(h)            # Latent mean vector\n        logvar = self.fc_logvar(h)    # Latent log-variance vector\n        return mu, logvar\n\n    # Reparameterization trick: sample z = μ + σ * ε  (ε ~ N(0,1))\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)   # σ = exp(0.5 * logvar)\n        eps = torch.randn_like(std)     # ε ~ N(0, I)\n        return mu + eps * std           # z = μ + σ * ε\n\n    # DECODER forward pass: latent z -&gt; reconstructed mean, log-variance\n    def decode(self, z):\n        h = F.relu(self.fc2(z))             # Hidden layer with ReLU\n        recon_mu = self.fc_out_mu(h)        # Mean of reconstructed features\n        recon_logvar = self.fc_out_logvar(h)# Log-variance of reconstructed features\n        return recon_mu, recon_logvar\n\n    # Full forward pass: input x -&gt; reconstructed (mean, logvar), latent params\n    def forward(self, x):\n        mu, logvar = self.encode(x)            # q(z|x)\n        z = self.reparameterize(mu, logvar)    # Sample z from q(z|x)\n        recon_mu, recon_logvar = self.decode(z)# p(x|z)\n        return (recon_mu, recon_logvar), mu, logvar\n\n    # Sample new synthetic data: z ~ N(0,I), decode to x\n    def generate(self, n_samples=100):\n        self.eval()\n        with torch.no_grad():\n            # Sample z from standard normal prior\n            z = torch.randn(n_samples, self.fc_mu.out_features)\n            \n            # Decode to get reconstruction distribution parameters\n            cont_mu, cont_logvar = self.decode(z)\n            \n            # Sample from reconstructed Gaussian: μ_x + σ_x * ε\n            return cont_mu + torch.exp(0.5 * cont_logvar) * torch.randn_like(cont_mu)\n\nThis generative ability connects VAEs to Bayesian modeling. In Bayesian inference, posterior predictive sampling draws new observations by integrating over uncertainty in parameters. VAEs achieve something similar by sampling from a learned latent distribution and propagating it through the decoder. Both approaches simulate data from a model conditioned on what has been learned. But where Bayesian models emphasize interpretability and prior structure, VAEs emphasize flexibility and scalability with “deep” latent structure. In the next section, we use this NumericVAE to see how well it reconstructs and generates data compared to posterior predictive draws from a Bayesian multivariate normal model.\nVariational autoencoders are trained by pairing their generative structure with a principled loss. The encoder maps input data to a distribution over latent variables; the decoder maps sampled latents back to data. This two-step process is trained end-to-end by minimizing a loss that balances two goals: accurate reconstruction and regularization of the latent space. The vae_loss function captures this trade-off. It computes a reconstruction loss using the negative log-likelihood of a Gaussian, plus a penalty—the Kullback-Leibler (KL) divergence—between the approximate posterior \\(q(z | x)\\) and the prior \\(p(x)\\)\n\ndef vae_loss(recon_mu, recon_logvar, x, mu, logvar):\n    # Reconstruction loss: Gaussian log likelihood\n    recon_var = torch.exp(recon_logvar)\n    recon_nll = 0.5 * (torch.log(2 * torch.pi * recon_var) + (x - recon_mu) ** 2 / recon_var)\n    recon_loss = recon_nll.sum(dim=1).mean()  # sum over features, mean over batch\n\n    # KL divergence: D_KL(q(z|x) || p(z)) where p(z)=N(0,I)\n    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n    kl_loss = kl_div.mean()\n\n    return recon_loss + kl_loss, recon_loss, kl_loss\n\nThe reconstruction loss assumes the decoder outputs a Gaussian distribution for each input dimension. The term recon_nll \\(=log(2\\pi\\sigma^{2}) + \\frac{(x-\\mu)^{2}}{\\sigma^{2}}\\) measures how surprising the observed data \\(x\\) would be under the predicted mean and variance. It penalizes poor fits and rewards precise reconstructions. This is summed across features and averaged across the batch. High-fidelity reconstruction requires the decoder to learn how each latent variable shapes the original space, making the VAE a powerful model for estimating high-dimensional latent representations that emit full probability distributions over inputs.\nThe KL divergence term acts as a regularizer. It measures how far the encoder’s learned distribution \\(q(z∣x)\\) is from the standard normal prior \\(p(z)\\). Without this term, the model could overfit—memorizing reconstructions without learning a useful latent structure. By keeping the latent codes close to \\(N(0,I)\\), the model remains generative: we can sample from the prior and decode into valid synthetic data. In short, KL divergence ensures generalization and interpretability, while reconstruction loss ensures fidelity. Their sum is what guides learning in the VAE.\n\nTraining VAE models\nAt the core of training deep learning models is backpropagation - an algorithm that computes gradients of a loss function with respect to each model parameter. PyTorch handles this automatically. We seperate the data into batches and compute the loss for each batch, Then calling loss.backward() computes these gradients via the chain rule. The optimizer then adjusts the weights with optimizer.step(), moving the model parameters in the direction that reduces the loss. Each training iteration refines the model, gradually improving its ability to encode and reconstruct the data. The train_vae function applies this cycle across epochs, using PyTorch’s autograd system to manage the entire computation graph.\nThe prep_data_vae function creates simulated data, adds optional missingness, and splits it into training and test sets. Each training set—of size 500, 1,000, or 10,000—is passed to a new VAE instance for fitting. More data provides better coverage of the latent space, allowing the model to generalize more confidently. With small data, the model risks memorizing noise. With large data, KL regularization helps the decoder learn smooth, generalizable mappings from latent codes to input reconstructions. The training runs below test this sensitivity empirically, keeping architecture fixed and varying only the sample size.\n\n\nCode\ndef prep_data_vae(sample_size=1000, missing_frac=0.0, impute=False):\n    sample_df = make_sample(cov_matrix=cov_matrix, size=sample_size, columns=FEATURE_COLUMNS, missing_frac=missing_frac, impute=impute)\n\n    X_train, X_test = train_test_split(sample_df.values, test_size=0.2, random_state=890)\n\n    X_train = torch.tensor(X_train, dtype=torch.float32)\n    X_test = torch.tensor(X_test, dtype=torch.float32)\n\n    train_loader = torch.utils.data.DataLoader(X_train, batch_size=32, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(X_test, batch_size=32)\n    return train_loader, test_loader\n\n\nTraining a variational autoencoder means adjusting a large array of weights to minimize reconstruction error and regularize the latent space. The train_vae function loops over batches from the training set, computes the loss, and updates the model via backpropagation. Each batch yields predicted distributions over features, from which we compute both the reconstruction loss and the KL divergence. These are averaged and logged for both training and held-out test data. Early stopping halts training when improvements plateau. Preventing overfitting while preserving the best model seen so far.\n\ndef train_vae(vae, optimizer, train, test, patience=10, wait=0, n_epochs=1000):\n    best_loss = float('inf')\n    losses = []\n\n    for epoch in range(n_epochs):\n        vae.train()\n        train_loss = 0.0\n        \n        for batch in train:\n            optimizer.zero_grad()\n\n            (recon_mu, recon_logvar), mu, logvar = vae(batch)\n            loss, recon_loss, kl_loss = vae_loss(recon_mu, recon_logvar, batch, mu, logvar)\n\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item() * batch.size(0)\n\n        avg_train_loss = train_loss / train.dataset.shape[0]\n\n        # --- Test Loop ---\n        vae.eval()\n        test_loss = 0.0\n        with torch.no_grad():\n            for batch in test:\n                (recon_mu, recon_logvar), mu, logvar = vae(batch)\n                loss, _, _ = vae_loss(recon_mu, recon_logvar, batch, mu, logvar)\n                test_loss += loss.item() * batch.size(0)\n        avg_test_loss = test_loss / test.dataset.shape[0]\n\n        print(f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f}\")\n\n        if  avg_test_loss &lt; best_loss - 1e-4:\n            best_loss, wait = avg_test_loss, 0\n            best_state = vae.state_dict()\n        else:\n            wait += 1\n            if wait &gt;= patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                vae.load_state_dict(best_state)  # restore best\n                break\n        losses.append([avg_train_loss, avg_test_loss, best_loss])\n\n    return vae, pd.DataFrame(losses, columns=['train_loss', 'test_loss', 'best_loss'])\n\nWe are now in a position to fit the same model architecture to different samples of the data. Training on 10,000 samples, for instance, challenges the model to encode the full covariance structure of our job satisfaction data while retaining generative flexibility. This training routine sets the stage for comparing VAEs to Bayesian models, where posterior predictive sampling plays a similar generative role—but with different assumptions and computational tradeoffs.\n\ntrain_500, test_500 = prep_data_vae(500)\nvae = NumericVAE(n_features=train_500.dataset.shape[1], hidden_dim=64)\noptimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\nvae_fit_500, losses_df_500 = train_vae(vae, optimizer, train_500, test_500)\n\ntrain_1000, test_1000 = prep_data_vae(1000)\nvae = NumericVAE(n_features=train_1000.dataset.shape[1], hidden_dim=64)\noptimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\nvae_fit_1000, losses_df_1000 = train_vae(vae, optimizer, train_1000, test_1000)\n\ntrain_10_000, test_10_000 = prep_data_vae(10_000)\nvae = NumericVAE(n_features=train_10_000.dataset.shape[1], hidden_dim=64)\noptimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\nvae_fit_10_000, losses_df_10_000 = train_vae(vae, optimizer, train_10_000, test_10_000)\n\nTraining and test losses tell us how well the model fits the data and how well it generalizes. The plots show these losses over training epochs for datasets of size 500, 1,000, and 10,000. A falling training loss indicates the model is learning to reconstruct the data. If test loss follows the same trend, generalization is improving too. If test loss flattens or rises while training loss keeps falling, the model may be overfitting—memorizing rather than learning.\n\n\nCode\ndef plot_metric_recovery(fit_vae, test_dataset):\n    recon_df = pd.DataFrame(fit_vae.generate(test_dataset.shape[0]), columns=FEATURE_COLUMNS)\n\n    test_df = pd.DataFrame(test_dataset, columns=FEATURE_COLUMNS)\n\n    fig, axs = plt.subplots(6, 2, figsize=(8, 20))\n    axs = axs.flatten()\n    for col, ax in zip(FEATURE_COLUMNS, axs):\n        ax.hist(recon_df[col], color='red', alpha=0.5, ec='grey', label=f'Reconstructed Metric {col}')\n        ax.hist(test_df[col], color='blue', alpha=0.5, ec='black', label=f'Test Metric {col}')\n        ax.legend()\n\nfig, axs = plt.subplots(1, 3, figsize=(8, 6))\naxs=axs.flatten()\nlosses_df_500[['train_loss', 'test_loss']].plot(ax=axs[0])\nlosses_df_1000[['train_loss', 'test_loss']].plot(ax=axs[1])\nlosses_df_10_000[['train_loss', 'test_loss']].plot(ax=axs[2])\n\naxs[0].set_title(\"Training and Test Losses \\n 500 observations\");\naxs[1].set_title(\"Training and Test Losses \\n 1000 observations\");\naxs[2].set_title(\"Training and Test Losses \\n 10_000 observations\");\n\n\n\n\n\n\n\n\n\nThese curves also validate our early stopping rule. When test loss plateaus, training halts—preserving the model that best balances fidelity and generalization. The plots confirm that this dynamic works across scales. In sum, training and test loss curves are not just diagnostics of performance—they’re windows into the model’s learning process and how data scale affects estimation.\nWe can also see that the models are able to re-construct the marginal metric distributions\n\nplot_metric_recovery(vae_fit_10_000, test_10_000.dataset)\n\n\n\n\n\n\n\n\nTo evaluate how well the VAE learns the joint structure of the data and not just feature-wise accuracy, we compare the correlation matrices of the reconstructed data. We know the ground truth and ask: does the VAE, when generating synthetic data, reproduce the correct dependency structure? For each bootstrap iteration, we sample new data from the trained VAE and compute its correlation matrix. We subtract this from the true correlation matrix, storing the residuals. Averaging across 1,000 samples gives us the expected discrepancy between true and learned structure.\nThis bootstrapping approach is key. Any one synthetic sample from the VAE might show idiosyncratic deviations due to randomness in the generative process. By repeating the process many times, we average out this noise and obtain a stable, expected residual. This makes the evaluation fair: it judges the model based on the distribution it has learned, not on any one lucky—or unlucky—draw. A good model will show consistently small residuals, especially along the strongest correlations of the test set.\n\ndef bootstrap_residuals(vae_fit, X_test, n_boot=1000):\n    recons = []\n    srmrs = []\n    resid_array = np.zeros((n_boot, 12, 12))\n    for i in range(n_boot):\n        recon_data = vae_fit.generate(n_samples=len(X_test))\n        reconstructed_df = pd.DataFrame(recon_data, columns=FEATURE_COLUMNS)\n        resid = pd.DataFrame(corr_matrix, columns=FEATURE_COLUMNS, index=FEATURE_COLUMNS) - reconstructed_df.corr()\n        srmr = get_srmr(resid)\n        srmrs.append(srmr)\n        resid_array[i] = resid.values\n        recons.append(reconstructed_df)\n\n    avg_resid = resid_array.mean(axis=0)\n    bootstrapped_resids = pd.DataFrame(avg_resid, columns=FEATURE_COLUMNS, index=FEATURE_COLUMNS)\n    return bootstrapped_resids, srmrs\n\nbootstrapped_resids_500, srmrs_500 = bootstrap_residuals(vae_fit_500, pd.DataFrame(test_500.dataset, columns=FEATURE_COLUMNS))\n\nbootstrapped_resids_1000, srmrs_1000 = bootstrap_residuals(vae_fit_1000, pd.DataFrame(test_1000.dataset, columns=FEATURE_COLUMNS))\n\nbootstrapped_resids_10_000, srmrs_10_000 = bootstrap_residuals(vae_fit_10_000, pd.DataFrame(test_10_000.dataset, columns=FEATURE_COLUMNS))\n\n\nfig, axs = plt.subplots(3, 1, figsize=(10, 20))\naxs = axs.flatten()\nplot_heatmap(bootstrapped_resids_500, title=f\"\"\"Expected Correlation Residuals for 500 observations \\n Under 1000 Bootstrapped Reconstructions. SRMR: {np.mean(srmrs_500).round(2)}\"\"\", ax=axs[0], colorbar=True, vmin=-.25, vmax=.25)\n\nplot_heatmap(bootstrapped_resids_1000, title=f\"\"\"Expected Correlation  Residuals  for 1000 observations \\n Under 1000 Bootstrapped Reconstructions. SRMR: {np.mean(srmrs_1000).round(2)}\"\"\", ax=axs[1], colorbar=True, vmin=-.25, vmax=.25)\n\nplot_heatmap(bootstrapped_resids_10_000, title=f\"\"\"Expected Correlation  Residuals  for 10,000 observations \\n Under 1000 Bootstrapped Reconstructions. SRMR: {np.mean(srmrs_10_000).round(2)}\"\"\", ax=axs[2], colorbar=True, vmin=-.25, vmax=.25)\n\n\n\n\n\n\n\n\nThe resulting heatmaps visualize the average residuals across correlation pairs. With 500 training samples, patterns are weaker and more erratic. At 1,000, the VAE begins to capture more of the joint structure. With 10,000, the residuals shrink further, especially near key variable clusters. This confirms that the VAE learns not only to reconstruct inputs but to replicate their internal relationships—an essential feature for any model claiming to approximate the data-generating process."
  },
  {
    "objectID": "posts/post-with-code/PyTorch_Amortized_Bayes/amortized_bayes.html#missing-data-and-inductive-bias",
    "href": "posts/post-with-code/PyTorch_Amortized_Bayes/amortized_bayes.html#missing-data-and-inductive-bias",
    "title": "Heuristics in Latent Space: VAEs and Bayesian Inference",
    "section": "Missing Data and Inductive Bias",
    "text": "Missing Data and Inductive Bias\nMissing data is a common challenge in survey research, especially in job satisfaction studies. Respondents often skip questions—sometimes at random, sometimes in patterns related to stress, disengagement, or privacy concerns. This creates non-response bias, where the absence of data is itself informative. Any imputation method must account not only for what’s missing, but why it’s missing. Ignoring this can distort correlation estimates and lead to misleading inferences.\n\nsample_df_missing = sample_df.copy()\n\n# Randomly pick 5% of the total elements\nmask_remove = np.random.rand(*sample_df_missing.shape) &lt; 0.05\n\n# Set those elements to NaN\nsample_df_missing[mask_remove] = np.nan\nsample_df_missing.head()\n\n\n\n\n\n\n\n\nJW1\nJW2\nJW3\nUF1\nUF2\nFOR\nDA1\nDA2\nDA3\nEBA\nST\nMI\n\n\n\n\n0\n2.578412\n3.565673\n1.976356\n0.261307\n-0.162994\n0.425867\n-0.341104\n-0.663246\n-1.898728\n0.412019\n0.167105\n1.179567\n\n\n1\n0.147594\n0.316841\n1.244429\n-0.566278\n0.157724\n0.084468\nNaN\n0.590472\n0.663543\n-0.392881\n0.434897\n-0.379236\n\n\n2\n0.799763\n0.277303\n-0.361947\n0.703534\n0.326682\n-0.503207\n0.558374\n0.313461\n0.005397\n0.373729\n-0.487022\n1.029725\n\n\n3\n-1.245506\n-1.338533\n-0.845951\n-0.700084\n-0.324149\n0.159880\n0.205925\n0.402143\n1.305458\n0.987969\n1.313665\n0.171655\n\n\n4\n0.066713\n0.958231\n0.697802\n0.071260\n-0.102972\n0.186527\n-0.015224\n0.147267\n0.674598\nNaN\n-0.845735\n-0.203553\n\n\n\n\n\n\n\nIn the code above, we simulate this problem by introducing 5% random missingness into the dataset. This models a common real-world scenario: partial responses from otherwise complete surveys. While this version assumes missing at random (MAR), in practice, missingness may depend on latent traits—precisely what our models aim to capture. A naive mean imputation might smooth over the problem, but risks losing structural information or introducing artificial correlations. We want to allow that our deep learning architecture can handle missing-data and we adapt our prep_data_vae_missing function to accomodate missing data.\n\n\nCode\nclass MissingDataDataset(Dataset):\n    def __init__(self, x, mask):\n        # x and mask are tensors of same shape\n        self.x = x\n        self.mask = mask\n        \n    def __len__(self):\n        return self.x.shape[0]\n    \n    def __getitem__(self, idx):\n        return self.x[idx], self.mask[idx]\n\ndef prep_data_vae_missing(sample_size=1000, batch_size=32, missing_frac=0.2, impute=False):\n    sample_df = make_sample(cov_matrix=cov_matrix, size=sample_size, columns=FEATURE_COLUMNS, missing_frac=missing_frac, impute=impute)\n\n    X_train, X_test = train_test_split(sample_df.values, test_size=0.2, random_state=890)\n\n    # Mask: 1=observed, 0=missing\n    mask_train = ~pd.DataFrame(X_train).isna()\n    mask_test = ~pd.DataFrame(X_test).isna()\n\n    # Tensors (keep NaNs for missing values)\n    x_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n    mask_train_tensor = torch.tensor(mask_train.values, dtype=torch.float32)\n\n    x_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n    mask_test_tensor = torch.tensor(mask_test.values, dtype=torch.float32)\n\n    train_dataset = MissingDataDataset(x_train_tensor, mask_train_tensor)\n    test_dataset = MissingDataDataset(x_test_tensor, mask_test_tensor)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    return train_loader, test_loader\n\n\nDeep learning models, including VAEs, come with inductive biases—assumptions about the structure of the data that guide learning in the face of ambiguity. These biases can be helpful or harmful. In the next section, we explore how our VAE behaves when trained and applied to incomplete data. This will show both the flexibility of generative models and the risks of overconfidence and poor generalisation.\nTo handle missing data, we extend the VAE by allowing it to learn how to impute. The NumericVAE_missing model introduces a vector of learnable parameters—self.missing_embeddings—one per feature. During training, missing values in the input \\(x\\) are replaced with these parameters using torch.where, giving the model freedom to discover plausible replacements. Unlike mean imputation, this strategy is dynamic: the imputation values are tuned as part of the model’s optimization, guided by the same loss function that trains the encoder and decoder.\n\nclass NumericVAE_missing(nn.Module):\n    def __init__(self, n_features, hidden_dim=64, latent_dim=24, dropout_rate=0.2):\n        super().__init__()\n        self.n_features = n_features\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n        # ---------- Learnable Imputation ----------\n        # One learnable parameter per feature for missing values\n        self.missing_embeddings = nn.Parameter(torch.zeros(n_features))\n\n        # ---------- ENCODER ----------\n        self.fc1_x = nn.Linear(n_features, hidden_dim)\n\n        # Stronger mask encoder: 2-layer MLP\n        self.fc1_mask = nn.Sequential(\n            nn.Linear(n_features, hidden_dim),\n            nn.SiLU(),\n            nn.Linear(hidden_dim, hidden_dim)\n        )\n\n        # Combine feature and mask embeddings\n        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n\n        # ---------- DECODER ----------\n        self.fc2 = nn.Linear(latent_dim, hidden_dim)\n        self.fc_out_mu = nn.Linear(hidden_dim, n_features)\n        self.fc_out_logvar = nn.Linear(hidden_dim, n_features)\n\n    def encode(self, x, mask):\n        # Impute missing values with learnable parameters\n        x_filled = torch.where(\n            torch.isnan(x),\n            self.missing_embeddings.expand_as(x),\n            x\n        )\n\n        # Encode features and mask separately\n        h_x = F.silu(self.fc1_x(x_filled))\n        h_x = self.dropout(h_x)\n        h_mask = self.fc1_mask(mask)\n\n        # Combine embeddings\n        h = h_x + h_mask\n\n        # Latent space\n        mu = self.fc_mu(h)\n        logvar = self.fc_logvar(h)\n        return mu, logvar\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def decode(self, z):\n        h = F.silu(self.fc2(z))\n        h = self.dropout(h)  # optional: decoder dropout\n        recon_mu = self.fc_out_mu(h)\n        recon_logvar = self.fc_out_logvar(h)\n        recon_logvar = torch.clamp(self.fc_out_logvar(h), -5, 5)\n        return recon_mu, recon_logvar\n\n    def forward(self, x, mask):\n        mu, logvar = self.encode(x, mask)\n        z = self.reparameterize(mu, logvar)\n        recon_mu, recon_logvar = self.decode(z)\n        return (recon_mu, recon_logvar), mu, logvar\n\n    def generate(self, n_samples=100):\n        self.eval()\n        with torch.no_grad():\n            z = torch.randn(n_samples, self.fc_mu.out_features)\n            recon_mu, recon_logvar = self.decode(z)\n            return recon_mu + torch.exp(0.5 * recon_logvar) * torch.randn_like(recon_mu)\n\nThe model also needs to know what was missing. For this, we pass a binary mask indicating which values are observed. This mask is processed through a two-layer MLP (self.fc1_mask), creating a learned representation of the missingness pattern. The encoded mask is then added to the encoded (and imputed) features. This design allows the encoder to distinguish between genuine low values and missing entries—and to condition the latent representation not just on the data, but on its absence.\nAside from this masking logic, the rest of the model mirrors a standard VAE: it uses a reparameterized latent space, decodes into means and variances for each feature, and generates new samples by drawing from the latent prior. Dropout layers add regularization. The overall effect is to give the model flexibility in the face of partial data. It can learn both how to fill in what’s missing and how to encode uncertainty about those guesses. This approach injects an inductive bias that missingness matters—and that learning should account for it directly.\n\ndef vae_loss_missing(recon_mu, recon_logvar, x, mu, logvar, mask):\n    # Clamp log-variance for numerical stability\n    recon_logvar = torch.clamp(recon_logvar, min=-5.0, max=5.0)\n    recon_var = torch.exp(recon_logvar)\n\n    # Only use observed entries\n    masked_x = torch.where(mask.bool(), x, torch.zeros_like(x))\n    masked_diff_sq = ((masked_x - recon_mu) ** 2) * mask\n    masked_logvar = recon_logvar * mask\n    masked_var = recon_var * mask\n\n    # Gaussian NLL: only computed on observed entries\n    recon_nll = 0.5 * (\n        torch.log(2 * torch.pi * masked_var + 1e-8) + masked_diff_sq / (masked_var + 1e-8)\n    )\n\n    # Reduce over features and average over batch\n    obs_counts = mask.sum(dim=1).clamp(min=1)\n    recon_loss = (recon_nll.sum(dim=1) / obs_counts).mean()\n\n    # KL divergence\n    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n    kl_loss = kl_div.mean()\n\n    return recon_loss, kl_loss\n\nTo adapt the VAE’s loss function for missing data, we restrict reconstruction error to observed entries only. The function vae_loss_missing computes a masked negative log-likelihood (NLL), treating missing values as undefined and excluding them from the loss. This is done by elementwise multiplying the squared error and log-variance terms by the binary mask. The mean squared error is thus scaled only over the available data, avoiding any implicit assumptions about the unobserved values. This approach lets the model learn from incomplete input without penalizing it for what it can’t see.\nThe KL divergence term remains unchanged, regularizing the latent distribution regardless of missingness. However, the reconstruction term is adjusted for each sample by dividing by the count of observed entries (obs_counts), avoiding bias due to varying amounts of missing data across rows. Numerical stability is enforced via clamping and small epsilon terms. Together, these changes ensure the objective function reflects uncertainty where it should and remains focused on the parts of the data the model can justifiably be judged on.\nThe training loop remains largely unchanged, but we fit two versions of the model architecture sampling 5000 and 25,000 data points respectively.\n\ndef fit_vae_missing(vae_missing, train_loader, test_loader, optimizer, patience=10, wait=0, n_epochs=1000):\n    best_loss = float('inf')\n    losses = []\n\n    for epoch in range(n_epochs):\n        vae_missing.train()\n        \n        train_loss = 0\n        for x_batch, mask_batch in train_loader:\n            optimizer.zero_grad()\n            (recon_mu, recon_logvar), mu, logvar = vae_missing(x_batch, mask_batch)\n            recon_loss, kl_loss = vae_loss_missing(recon_mu, recon_logvar, x_batch, mu, logvar, mask_batch)\n\n            loss = recon_loss + kl_loss\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * x_batch.size(0)\n\n        avg_train_loss = train_loss / len(train_loader.dataset)\n\n        # --- Validation ---\n        vae_missing.eval()\n        test_loss = 0.0\n        with torch.no_grad():\n            for x_batch, mask_batch in test_loader:\n                (recon_mu, recon_logvar), mu, logvar = vae_missing(x_batch, mask_batch)\n                recon_loss, kl_loss = vae_loss_missing(recon_mu, recon_logvar, x_batch, mu, logvar, mask_batch)\n                loss = recon_loss + kl_loss\n                test_loss += loss.item() * x_batch.size(0)\n        avg_test_loss = test_loss / len(test_loader.dataset)\n\n        print(f\"Epoch {epoch+1}/{n_epochs} | Train: {avg_train_loss:.4f} | Test: {avg_test_loss:.4f}\")\n\n        # Early stopping\n        if avg_test_loss &lt; best_loss - 1e-4:\n                best_loss, wait = avg_test_loss, 0\n                best_state = vae_missing.state_dict()\n        else:\n            wait += 1\n            if wait &gt;= patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                vae_missing.load_state_dict(best_state)  # restore best\n                break\n        losses.append([avg_train_loss, avg_test_loss, best_loss])\n    losses_df = pd.DataFrame(losses, columns=['train_loss', 'test_loss', 'best_loss'])    \n\n    return vae_missing, losses_df\n\n\ntrain_loader_5000, test_loader_5000 = prep_data_vae_missing(5000, batch_size=32)\nvae_missing_5000 = NumericVAE_missing(n_features=next(iter(train_loader_5000))[0].shape[1], latent_dim=12)\noptimizer = optim.Adam(vae_missing_5000.parameters(), lr=1e-4)\n\nfit_vae_missing_5000, losses_df_missing_5000 = fit_vae_missing(vae_missing_5000, train_loader_5000, test_loader_5000, optimizer)\n\n\ntrain_loader_25_000, test_loader_25_000 = prep_data_vae_missing(25_000, batch_size=32)\nvae_missing_25_000 = NumericVAE_missing(n_features=next(iter(train_loader_25_000))[0].shape[1], latent_dim=12)\noptimizer = optim.Adam(vae_missing_25_000.parameters(), lr=1e-4)\n\nfit_vae_missing_25_000, losses_df_missing_25_000 = fit_vae_missing(vae_missing_25_000, train_loader_25_000, test_loader_25_000, optimizer)\n\n\nPlot Metric Reconstruction\nThe histograms below show how well the VAE reconstructs the marginal distributions of individual survey metrics. For each variable, we overlay the reconstructed values (in red) with the test set values (in blue). Visually, there’s strong overlap across most dimensions, suggesting the model has learned to approximate the correct univariate distributions—even with missing data. This kind of marginal recovery is encouraging, especially in low-data regimes or when imputing incomplete responses.\n\n\nCode\nplot_metric_recovery(fit_vae_missing_5000, test_loader_5000.dataset.x)\n\n\n\n\n\n\n\n\n\nHowever, strong metric-wise recovery does not imply that the model has captured the joint structure of the data. Each histogram tells us how accurate the model is on a single variable, but ignores how variables co-vary—a crucial element in survey analysis where latent constructs often manifest as correlated responses. It’s entirely possible to match marginal distributions while failing to reproduce the inter-variable dependencies that define the data’s true geometry.\nFor this reason, histogram overlap is best viewed as a necessary but insufficient indicator of VAE reconstruction quality. It confirms that the decoder isn’t collapsing or hallucinating values, but it doesn’t validate the model’s deeper understanding of the data-generating process. In the next step, we’ll examine the reconstructed correlation structure—where, despite strong marginal performance, the VAE fails to capture the multivariate dependencies accurately.\n\n\nPlot Correlation Reconstruction\nThe heatmaps below visualize the residuals between the observed and reconstructed correlation matrices; a more stringent test of reconstruction fidelity than individual metric distributions. While the marginal histograms previously gave the impression of strong model performance, the correlation residuals tell a different story. There are consistent and structured deviations between the original and reconstructed relationships, particularly off-diagonal. These residuals are not random noise; they reveal systematic failure to capture the joint dependencies among survey metrics.\n\nbootstrapped_resids_5000, srmrs_5000 = bootstrap_residuals(fit_vae_missing_5000, pd.DataFrame(test_loader_5000.dataset.x, columns=FEATURE_COLUMNS))\n\nbootstrapped_resids_25_000, srmrs_25_000 = bootstrap_residuals(fit_vae_missing_25_000, pd.DataFrame(test_loader_25_000.dataset.x, columns=FEATURE_COLUMNS))\n\n\nfig, axs = plt.subplots(2, 1, figsize=(8, 15))\naxs = axs.flatten()\nplot_heatmap(bootstrapped_resids_5000, title=f\"\"\"Expected Correlation Residuals for 5000 observations \\n Under 1000 Bootstrapped Reconstructions. SRMR: {np.mean(srmrs_5000).round(2)}\"\"\", ax=axs[0], colorbar=True, vmin=-.25, vmax=.25)\n\nplot_heatmap(bootstrapped_resids_25_000, title=f\"\"\"Expected Correlation  Residuals  for 25,000 observations \\n Under 1000 Bootstrapped Reconstructions SRMR: {np.mean(srmrs_25_000).round(2)}\"\"\", ax=axs[1], colorbar=True, vmin=-.25, vmax=.25)\n\n\n\n\n\n\n\n\nNotably, the magnitude and pattern of these residuals remain remarkably stable across both 5,000 and 25,000 observations. If the problem were due to data scarcity, we would expect improvements with more data. Instead, this invariance points to the model architecture itself—specifically, to the inductive biases embedded in the structure of the VAE. Despite being trained on rich and complete data, the model consistently struggles to recover the true correlational geometry.\nThis reflects a broader challenge in deep learning. Complex architectures like VAEs encode implicit assumptions about data structure through their layers and non-linearities. These assumptions are rarely transparent. In this case, the interaction between the encoder’s masking strategy, the learnable imputations, and the decoder’s reconstruction logic appears to flatten or distort inter-variable relationships in ways that simple performance metrics can’t detect. It’s a cautionary example of how black-box models can fail silently—even when outputs look convincing on the surface."
  },
  {
    "objectID": "posts/post-with-code/PyTorch_Amortized_Bayes/amortized_bayes.html#simple-imputation-and-simple-variational-encoders",
    "href": "posts/post-with-code/PyTorch_Amortized_Bayes/amortized_bayes.html#simple-imputation-and-simple-variational-encoders",
    "title": "Heuristics in Latent Space: VAEs and Bayesian Inference",
    "section": "Simple Imputation and Simple Variational Encoders",
    "text": "Simple Imputation and Simple Variational Encoders\nDespite the sophistication of our masked VAE architecture, the earlier results showed it struggled to recover the true correlation structure of the data. In contrast, here we demonstrate that a simpler strategy—mean imputation followed by a standard VAE—performs substantially better.\nRather than explicitly modeling missingness, we use straightforward mean imputation, replacing each missing entry with the feature-wise mean from the training set. This naive approach ignores the potential structure in the missing data, but it has one key advantage: it preserves the full input dimensionality without introducing additional model components or parameters.\nWe then train a standard VAE on this imputed data. The correlation residual heatmap below shows that, unlike the masked VAE, this model accurately recovers the underlying correlation structure, even with 20% missing data. The off-diagonal residuals are smaller and less structured, indicating that the joint metric relationships are better preserved.\n\ntrain_loader_imputed, test_loader_imputed = prep_data_vae(sample_size=10_000, missing_frac=0.2, impute=True)\n\nvae = NumericVAE(n_features=train_loader_imputed.dataset.shape[1], hidden_dim=64)\noptimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\nvae_fit_10_000_imputed, losses_df_10_000_imputed = train_vae(vae, optimizer, train_loader_imputed, test_loader_imputed)\n\nbootstrapped_resids_10_000_imputed, srmrs_10_000_imputed = bootstrap_residuals(vae_fit_10_000_imputed, pd.DataFrame(test_loader_imputed.dataset, columns=FEATURE_COLUMNS))\n\n\n\nCode\nplot_heatmap(bootstrapped_resids_10_000_imputed, title=f\"\"\"Expected Correlation  Residuals  for 10,000 observations \\n Under 1000 Bootstrapped Reconstructions SRMR: {np.mean(srmrs_10_000_imputed).round(2)}\"\"\", colorbar=True, vmin=-.25, vmax=.25);\n\n\n\n\n\n\n\n\n\nThis contrast highlights a recurring lesson in applied modeling: simple can outperform complex - especially when the complexity introduces new sources of opaque inductive bias. The masked VAE encoded specific assumptions about missingness and reconstruction that, despite being theoretically appealing, failed to generalize. In contrast, the imputation-plus-VAE approach leverages a simpler inductive bias: that missing values can be smoothed over without disrupting the overall dependency structure. In this context, that assumption proved to be not only adequate but effective."
  },
  {
    "objectID": "posts/post-with-code/PyTorch_Amortized_Bayes/amortized_bayes.html#bayesian-inference",
    "href": "posts/post-with-code/PyTorch_Amortized_Bayes/amortized_bayes.html#bayesian-inference",
    "title": "Heuristics in Latent Space: VAEs and Bayesian Inference",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\nIn contrast to the VAE, where inductive bias emerges from the architecture and training dynamics, the Bayesian model defines its assumptions transparently through the prior. Here, we specify a multivariate normal likelihood with a mean vector and a covariance matrix drawn from an LKJ prior—a structured, interpretable distribution over valid correlation matrices. The model focuses directly on estimating the joint structure we care about, rather than reconstructing data through a bottleneck.\nThis PyMC model embodies the Bayesian workflow: first, we define a prior over parameters (here, the mean and correlation structure of features); second, we update this prior in light of observed data via Bayes’ rule, yielding the posterior; and finally, we generate samples from the posterior predictive distribution, which reflects the uncertainty in both parameter estimates and future data. These steps—prior → posterior → predictive sampling—form a disciplined approach to inference that naturally captures parameter uncertainty.\n\ndef make_pymc_model(sample_df):\n    coords = {'features': FEATURE_COLUMNS,\n            'features1': FEATURE_COLUMNS ,\n            'obs': range(len(sample_df))}\n\n    with pm.Model(coords=coords) as model:\n        # Priors\n        mus = pm.Normal(\"mus\", 0, 1, dims='features')\n        chol, _, _ = pm.LKJCholeskyCov(\"chol\", n=12, eta=1.0, sd_dist=pm.HalfNormal.dist(1))\n        cov = pm.Deterministic('cov', pm.math.dot(chol, chol.T), dims=('features', 'features1'))\n\n        pm.MvNormal('likelihood', mus, cov=cov, observed=sample_df.values, dims=('obs', 'features'))\n        \n        idata = pm.sample_prior_predictive()\n        idata.extend(pm.sample(random_seed=120))\n        pm.sample_posterior_predictive(idata, extend_inferencedata=True)\n\n    return idata, model \n\nsample_df = make_sample(cov_matrix, 500, FEATURE_COLUMNS)\n\nidata, model = make_pymc_model(sample_df)\n\nWhile the VAE also performs generative modeling, it does so with less targeted control over latent dependencies. Its inductive bias is entangled in neural architecture choices—nonlinearities, hidden dimensions, initialization schemes—making its behavior harder to interpret. The Bayesian model, by contrast, isolates the structure of interest in the prior and expresses uncertainty explicitly through the posterior. This cleaner separation of modeling goals is a strength: for structured estimation tasks like recovering correlation matrices, simpler Bayesian formulations often outperform complex, less interpretable alternatives.\n\npm.model_to_graphviz(model)\n\n\n\n\n\n\n\n\nAn additional advantage of the Bayesian approach is that the explicit inductive bias encoded in the prior helps stabilize estimation in low-data regimes. Because we constrain the model to estimate only correlation structures consistent with the LKJ prior—i.e., valid, regularized correlation matrices—it becomes possible to recover the essential structure of the data even with a small sample size. In this case, using just 500 observations, the model still reconstructs the underlying correlation matrix with notable accuracy. This is a powerful demonstration of the bias–variance trade-off in action: by introducing structured assumptions through the prior, we reduce variance in our estimates and avoid overfitting, which would be more likely in a more flexible, under-constrained model like the VAE.\n\nexpected_corr = pd.DataFrame(az.summary(idata, var_names=['chol_corr'])['mean'].values.reshape((12, 12)), columns=FEATURE_COLUMNS, index=FEATURE_COLUMNS)\n\nresids = pd.DataFrame(corr_matrix, columns=FEATURE_COLUMNS, index=FEATURE_COLUMNS) - expected_corr\n\nsrmr_bayes_500 = get_srmr(resids)\nplot_heatmap(resids, title=f\"Expected Correlation Residuals  \\n on the Posterior Correlation Estimate. SRMR: {np.mean(srmr_bayes_500).round(2)}\")\n\n\n\n\n\n\n\n\n\nMissing Data\nOne of the most elegant features of Bayesian modeling is its native ability to handle missing data through implicit imputation. In our probabilistic model, we never need to manually fill in missing values or mask them—PyMC automatically marginalizes over the missing entries in the observed data during sampling. The model treats these missing values as additional unknowns and estimates them jointly with the other parameters using the same prior and likelihood structure. This is particularly valuable in survey data settings, such as job satisfaction responses, where non-response bias is common and must be accounted for directly.\n\nsample_df_missing = make_sample(cov_matrix, 500, FEATURE_COLUMNS, missing_frac=0.1)\n\n\nidata_missing, model_missing = make_pymc_model(sample_df_missing)\n\nOur example shows that, even with 10% missingness in a modest-sized dataset of only 500 observations, the posterior over the correlation matrix remains well-calibrated, accurately recovering the underlying covariance structure. This demonstrates the strength of Bayesian inference: its inductive bias (in this case via the LKJ prior) not only makes estimation more robust but also provides principled uncertainty quantification for both model parameters and imputed values, all within a unified framework.\n\npm.model_to_graphviz(model_missing)\n\n\n\n\n\n\n\n\nThe model structure automatically parses out the observed from missing cells and imputes them as required automatically.\n\nexpected_corr = pd.DataFrame(az.summary(idata_missing, var_names=['chol_corr'])['mean'].values.reshape((12, 12)), columns=FEATURE_COLUMNS, index=FEATURE_COLUMNS)\n\nresids = pd.DataFrame(corr_matrix, columns=FEATURE_COLUMNS, index=FEATURE_COLUMNS) - expected_corr\n\nsrmr_bayes_500_missing = get_srmr(resids)\nplot_heatmap(resids, title=f\"Expected Correlation Residuals  \\n on the Posterior Correlation Estimate. SRMR: {np.mean(srmr_bayes_500_missing).round(2)}\")"
  },
  {
    "objectID": "posts/post-with-code/PyTorch_Amortized_Bayes/amortized_bayes.html#reconstruction-error-and-latent-representations",
    "href": "posts/post-with-code/PyTorch_Amortized_Bayes/amortized_bayes.html#reconstruction-error-and-latent-representations",
    "title": "Heuristics in Latent Space: VAEs and Bayesian Inference",
    "section": "Reconstruction Error and Latent Representations",
    "text": "Reconstruction Error and Latent Representations\nIn applied statistical modeling, some of the hardest problems are those we can’t observe directly. Inferring latent structure from incomplete data forces us to ask: what do we believe about the world, and how do our models reflect those beliefs? The answer lies in inductive bias—the assumptions built into a model’s architecture—that guide how it reconstructs and fills in the blanks.\nWe discuss the idea of reconstruction loss in the context of PCA/SVD before examining two main approaches to this challenge:\n\nVariational autoencoders (VAEs), both standard and mask-aware, which allow flexible non-linear reconstructions and can be adapted to handle missingness explicitly.\nBayesian multivariate models with explicit priors over covariance structures, which transparently encode inductive bias and naturally impute missing values through the posterior.\n\nEach method is applied to non-response in job satisfaction surveys. Our focus is not only on reconstruction accuracy, but on the recovery of latent correlation structures—because superficial similarity is not enough. Good marginal fits can still hide deep structural errors.\n\nimport torch\nimport torchvision.datasets as dsets\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nimport pymc as pm \nimport arviz as az\nimport pytensor.tensor as pt\n\nnp.random.seed(42324)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
  },
  {
    "objectID": "posts/post-with-code/PyTorch_Amortized_Bayes/amortized_bayes.html#conclusion-and-key-takeaways",
    "href": "posts/post-with-code/PyTorch_Amortized_Bayes/amortized_bayes.html#conclusion-and-key-takeaways",
    "title": "Heuristics in Latent Space: VAEs and Bayesian Inference",
    "section": "Conclusion and Key Takeaways",
    "text": "Conclusion and Key Takeaways\nMost complex phenomena have a complex data generating process. For the statistical model to retain plausible fidelty with the phenomena we should transparently encode as much complexity as required in our models. This, as we’ve seen, is not completely trivial.\n\nInductive Bias Shapes Learning: The assumptions built into models ,whether explicit priors or implicit architectures, directly influence what patterns are captured and how well they generalize.\nLatent Spaces Are Tricky: Heuristics in latent space can be elusive; interpretability and validation demand careful scrutiny beyond surface-level metrics.\nSimplicity Often Wins: Straightforward imputation paired with simpler models can outperform complex architectures when it comes to reconstructing key data structures like correlations.\nBayesian Frameworks Offer Clarity: Encoding domain knowledge through priors leads to cleaner, more targeted models that perform well even with limited data.\nImplicit Handling of Missing Data: Bayesian inference naturally accommodates uncertainty and missingness without complex architectural tweaks.\nIterative Model Development: Effective modeling is a continuous process of refining assumptions, balancing complexity, and aligning with the data’s underlying structure.\n\nOur exploration reveals a fundamental truth in statistical modeling: building models is not just about fitting data, but about embedding assumptions—inductive biases—that shape how we interpret and generalize from incomplete information. Complex models with rich latent spaces can mislead if their structure obscures what they truly learn. Simpler, well-targeted approaches, especially those grounded in principled Bayesian inference, often provide clearer, more reliable insight.\nUltimately, model development is an iterative art of balancing flexibility with structure, complexity with interpretability, and data with prior knowledge. There is a notorious empirical trial and test pattern to model development with deep learning models. This can seem opaque and frustrating, but I think this is more profitably seen along side contemporary Bayesian model development workflow as an exploration versus exploitation trade-off - where we systematically assess implications of model specification. Mastering this balance is key to robust inference, meaningful imputation, and trustworthy prediction in the face of uncertainty."
  },
  {
    "objectID": "posts/post-with-code/PyTorch_Amortized_Bayes/amortized_bayes.html#deep-latent-structures-with-sems",
    "href": "posts/post-with-code/PyTorch_Amortized_Bayes/amortized_bayes.html#deep-latent-structures-with-sems",
    "title": "Heuristics in Latent Space: VAEs and Bayesian Inference",
    "section": "Deep Latent Structures with SEMs",
    "text": "Deep Latent Structures with SEMs\nWe’ve seen how these kinds of models handle the estimation of latent correlation structure, but this is only to scratch the surface. Most work on job-satisfaction would suggest that there is a much richer causal structure between the variables in our data set than we recover with simple correlation analysis. Houghton and Jinkerson’s initial work on “Constructive Thought Strategies and Job Satisfaction: A Preliminary Examination” suggested that the latent causal structure was compellingly modeled with the following SEM structure\n\n\n\nHoughton and Jinkerson’s suggest SEM model of Job Satisfaction\n\n\nHere they express the idea that the effects of constructive thought strategies are doubly mediated by an individual’s dysfunctional thought processes and their sense of subjective well being. We can here develop the SEM model, but the general point is that complex causal processes require transparent and expressive models for their assessment. For background on SEM modelling with PyMC see here\n\n\nCode\nsample_df = make_sample(cov_matrix, 263, FEATURE_COLUMNS)\n\ncoords = {\n    \"obs\": list(range(len(sample_df))),\n    \"indicators\": FEATURE_COLUMNS,\n    \"indicators_1\": [\"JW1\", \"JW2\",  \"JW3\"], # job satisfaction\n    \"indicators_2\": [\"UF1\", \"UF2\", \"FOR\"], # well being\n    \"indicators_3\": [\"DA1\", \"DA2\", \"DA3\"], # dysfunction\n    \"indicators_4\": [\"EBA\", \"ST\", \"MI\"], # constructive thought strategies\n    \"latent\": [\"satisfaction\", \"well being\", \"dysfunctional\", \"constructive\"],\n    \"latent1\": [\"satisfaction\", \"well being\", \"dysfunctional\", \"constructive\"],\n    \"paths\": [\"dysfunctional ~ constructive\", \"well being ~ dysfunctional\", \"well being ~ constructive\", \"satisfaction ~ well being\", \"satisfaction ~ dysfunction\" ,  \"satisfaction ~ constructive\"], \n    \"sd_params\": [i + '_sd' for i in FEATURE_COLUMNS],\n    \"corr_params\": ['UF1 ~~ FOR']\n}\n\ndef make_lambda(indicators, name='lambdas1', priors=[1, 10]):\n    \"\"\" Takes an argument indicators which is a string in the coords dict\"\"\"\n    temp_name = name + '_'\n    lambdas_ = pm.Normal(temp_name, priors[0], priors[1], dims=(indicators))\n    # Force a fixed scale on the factor loadings for factor 1\n    lambdas_1 = pm.Deterministic(\n        name, pt.set_subtensor(lambdas_[0], 1), dims=(indicators)\n    )\n    return lambdas_1\n\ndef make_B(priors=[0, .5]):\n    coefs = pm.Normal('mu_betas', [0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1], dims='paths')\n\n    zeros = pt.zeros((4, 4))\n    ## dysfunctional ~ constructive\n    zeros = pt.set_subtensor(zeros[3, 2], coefs[0])\n    ## well being ~ dysfunctional\n    zeros = pt.set_subtensor(zeros[2, 1], coefs[1])\n    ## well being ~ constructive\n    zeros = pt.set_subtensor(zeros[3, 1], coefs[2])\n    ## satisfaction ~ well being\n    zeros = pt.set_subtensor(zeros[1, 0], coefs[3])\n    ## satisfaction ~ dysfunction\n    zeros = pt.set_subtensor(zeros[2, 0], coefs[4])\n    ## satisfaction ~ constructive\n    coefs_ = pt.set_subtensor(zeros[3, 0], coefs[5])\n    return coefs_\n\ndef make_Psi(indicators, name='Psi_cov'):\n    \"\"\" Takes an argument indicators which is a string in the coords dict\"\"\"\n    temp_name = name + '_'\n    n = len(coords[indicators])\n    cov_params = pm.InverseGamma(temp_name, 3, 4, dims='sd_params')\n    r = pt.zeros((n, n))\n    beta_params = pm.Beta(temp_name + 'beta', 1, 1, dims='corr_params')\n    for i in range(len(coords[indicators])):\n        r = pt.set_subtensor(r[i, i], 1)\n    # UF1 ~~ FOR\n    r = pt.set_subtensor(r[3, 5], beta_params[0])\n    s = pt.diag(cov_params)\n    cov = (s @ r) @ pt.transpose(s @ r)\n    r = pm.Deterministic('Psi_corr', r)\n    cov = pm.Deterministic('Psi_cov', cov)\n\n    return cov\n\n\n\nobs_idx = list(range(len(sample_df)))\nobserved_data = sample_df[coords['indicators']].values\nconditional = False\n\nwith pm.Model(coords=coords) as sem_model_clean:\n    \n    # --- Factor loadings ---\n    lambdas_1 = make_lambda('indicators_1', 'lambdas1', priors=[1, .5])\n    lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, .5])\n    lambdas_3 = make_lambda('indicators_3', 'lambdas3', priors=[1, .5])\n    lambdas_4 = make_lambda('indicators_4', 'lambdas4', priors=[1, .5])\n\n    Lambda = pt.zeros((12, 4))\n    Lambda = pt.set_subtensor(Lambda[0:3, 0], lambdas_1)\n    Lambda = pt.set_subtensor(Lambda[3:6, 1], lambdas_2)\n    Lambda = pt.set_subtensor(Lambda[6:9, 2], lambdas_3)\n    Lambda = pt.set_subtensor(Lambda[9:12, 3], lambdas_4)\n    Lambda = pm.Deterministic('Lambda', Lambda)\n    \n    # --- Structural coefficients matrix B ---\n    coefs_ = make_B()\n    coefs_ = pm.Deterministic('B', coefs_, dims=('latent', 'latent1'))\n    \n    # Identity matrix for latent_dim\n    latent_dim = len(coords['latent'])\n    I = pt.eye(latent_dim)\n    \n    # Structural errors covariance (Psi_zeta)\n    chol_latent, corr_latent, _ = pm.LKJCholeskyCov(\n        \"chol_latent\", n=latent_dim, eta=2, sd_dist=pm.HalfNormal.dist(1.0)\n    )\n    Psi_zeta = pm.Deterministic(\"Psi_zeta\", chol_latent.dot(chol_latent.T))\n    \n    # Structural errors for latent variables (exogenous residuals)\n    zeta = pm.MvNormal(\"zeta\", mu=pt.zeros(latent_dim), chol=chol_latent, shape=(len(sample_df), latent_dim))\n    \n    # Solve (I - B) * ksi.T = zeta.T  =&gt; ksi = ((I-B)^-1) * zeta\n    lhs = I - coefs_ + 1e-8 * pt.eye(latent_dim)  # (latent_dim, latent_dim)\n    lhs = pm.Deterministic('I-B', lhs)\n    ksi_T = pt.slinalg.solve(lhs, zeta.T)  # solve for ksi.T shape (latent_dim, n_obs)\n    ksi = pm.Deterministic(\"ksi\", ksi_T.T)  # transpose back (n_obs, latent_dim)\n    \n\n    # --- Residual covariance for observed variables ---\n    Psi = make_Psi('indicators')\n\n    if conditional:\n        mu = pt.dot(ksi, Lambda.T)  # (n_obs, n_indicators)\n\n        # --- Observed data likelihood ---\n        _ = pm.MvNormal(\"likelihood\", mu=mu, cov=Psi, observed=observed_data)\n\n    else: \n        inv_lhs = pt.slinalg.solve(lhs, pt.eye(latent_dim))\n        inv_lhs = pm.Deterministic('solve_I-B', inv_lhs, dims=('latent', 'latent1'))\n        Sigma_y = pm.Deterministic('Sigma_y', Lambda.dot(inv_lhs).dot(Psi_zeta).dot(inv_lhs.T).dot(Lambda.T) + Psi)\n        _ = pm.MvNormal(\"likelihood\", mu=0, cov=Sigma_y, observed=observed_data)\n\nThe model structure is now more complicated than the simple multivariate outcomes we’ve seen before - but this complexity is tied to an expressive structure and deliberately coded relations.\n\npm.model_to_graphviz(sem_model_clean)\n\n\n\n\n\n\n\n\nIn the Lavaan syntax we have something like:\n# measurement part\nCTS =~ EBA + ST + MI\nDTP =~ DA1 + DA2 + DA3\nSWB =~ UF1 + UF2 + FOR\nJS =~ JW1 + JW2 + JW3\n\n# error covariance\nUF1 ~~ FOR\n\n# structural part\nDTP ~ CTS\nSWB ~ CTS + DTP\nJS  ~ CTS + DTP + SWB \nwhere we have explicitly coded the indicator variables in our data set as loading onto one of four distinct factors. These factors in their turn are then placed into a regression relationship and we allow that there is a covariance structure among two of our indicator variables. We can now sample from this structure estimate the factor loadings and path coefficients at the same time.\n\nwith sem_model_clean: \n    idata = pm.sample_prior_predictive()\n    idata.extend(pm.sample(chains=4, tune=2000, draws=10_000, target_accept=.95, random_seed=232))\n\nWe plot the estimates parameters here. We never explicitly encoded the linear relationships in our simulation routine, but even so, it seems we can recover and expected positive impact of constructive thought strategies on job-satisfaction. These estimates align well with the original results reported in Vehkalahti and Everitt’s Multivariate Analysis for the Behavioral sciences.\n\nax = az.plot_forest(idata, figsize=(8, 8),\nvar_names=['lambdas1', 'lambdas2', 'lambdas3', 'lambdas4', 'mu_betas'], combined=True, r_hat=True);\nax[0].axvline(0, linestyle='--', color='k')\nax[0].set_title(\"Structural Parameter Estimates from SEM model\");\n\naz.summary(idata, var_names=['lambdas1', 'lambdas2', 'lambdas3', 'lambdas4', 'mu_betas'])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nlambdas1[JW1]\n1.000\n0.000\n1.000\n1.000\n0.000\nNaN\n40000.0\n40000.0\nNaN\n\n\nlambdas1[JW2]\n1.030\n0.093\n0.861\n1.209\n0.001\n0.000\n16702.0\n22989.0\n1.00\n\n\nlambdas1[JW3]\n0.941\n0.085\n0.783\n1.101\n0.001\n0.000\n19206.0\n25428.0\n1.00\n\n\nlambdas2[UF1]\n1.000\n0.000\n1.000\n1.000\n0.000\nNaN\n40000.0\n40000.0\nNaN\n\n\nlambdas2[UF2]\n2.070\n0.232\n1.644\n2.512\n0.002\n0.001\n10599.0\n19707.0\n1.00\n\n\nlambdas2[FOR]\n1.016\n0.146\n0.747\n1.292\n0.001\n0.001\n12431.0\n20468.0\n1.00\n\n\nlambdas3[DA1]\n1.000\n0.000\n1.000\n1.000\n0.000\nNaN\n40000.0\n40000.0\nNaN\n\n\nlambdas3[DA2]\n1.147\n0.077\n1.005\n1.294\n0.001\n0.000\n22948.0\n27973.0\n1.00\n\n\nlambdas3[DA3]\n1.013\n0.095\n0.841\n1.197\n0.000\n0.000\n36920.0\n31202.0\n1.00\n\n\nlambdas4[EBA]\n1.000\n0.000\n1.000\n1.000\n0.000\nNaN\n40000.0\n40000.0\nNaN\n\n\nlambdas4[ST]\n1.063\n0.136\n0.802\n1.316\n0.001\n0.001\n29345.0\n28238.0\n1.00\n\n\nlambdas4[MI]\n1.546\n0.155\n1.255\n1.835\n0.001\n0.001\n15054.0\n20925.0\n1.00\n\n\nmu_betas[dysfunctional ~ constructive]\n-0.044\n0.563\n-1.079\n1.043\n0.017\n0.011\n1051.0\n2277.0\n1.01\n\n\nmu_betas[well being ~ dysfunctional]\n-0.401\n0.631\n-1.593\n0.806\n0.017\n0.011\n1458.0\n2466.0\n1.00\n\n\nmu_betas[well being ~ constructive]\n0.306\n0.746\n-1.138\n1.678\n0.016\n0.008\n2171.0\n4796.0\n1.00\n\n\nmu_betas[satisfaction ~ well being]\n0.140\n0.202\n-0.255\n0.524\n0.008\n0.005\n605.0\n1215.0\n1.01\n\n\nmu_betas[satisfaction ~ dysfunction]\n-0.123\n0.321\n-0.765\n0.454\n0.011\n0.008\n800.0\n1517.0\n1.01\n\n\nmu_betas[satisfaction ~ constructive]\n0.063\n0.402\n-0.701\n0.824\n0.013\n0.009\n1000.0\n1741.0\n1.01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nand the chains look to have sampled the parameter space well.\n\naz.plot_trace(idata, var_names=['lambdas1', 'lambdas2', 'lambdas3', 'lambdas4', 'mu_betas'], figsize=(8, 10));\nplt.tight_layout();\n\n\n\n\n\n\n\n\nNow we can sample the posterior predictive distribution for the multivariate outcome.\n\nwith sem_model_clean:\n    idata.extend(pm.sample_posterior_predictive(idata))\n\nFinally, we plot the reconstruction of the correlation structure based on the posterior predictive distribution of the structural equation model. The results are pretty compelling. Recall now what we’ve done. We’ve encoded a causal DAG structure in our model specification and were able to recover a plausible reconstruction of the multivariate correlations while estimating structural parameters of interest.\n\ndef sample_ppc_calc_resids(idata, n_samples=300):\n    resid_array = np.zeros((n_samples, 12, 12))\n    for i in range(n_samples):\n        ppc_corr = pd.DataFrame(idata['posterior_predictive']['likelihood'].stack(sample=[\"chain\", \"draw\"])[:, :, i], columns=FEATURE_COLUMNS).corr()\n        resid = ppc_corr - pd.DataFrame(corr_matrix, columns=FEATURE_COLUMNS, index=FEATURE_COLUMNS)\n        resid_array[i] = resid.values\n\n    average_resids = pd.DataFrame(resid_array.mean(axis=0), columns=FEATURE_COLUMNS, index=FEATURE_COLUMNS)\n    return average_resids\n\naverage_resids = sample_ppc_calc_resids(idata)\nsrmr = get_srmr(average_resids)\nplot_heatmap(average_resids, title=f\"Reconstructed Correlation Structure from SEM model \\n SRMR: {srmr.round(2)}\")\n\n\n\n\n\n\n\n\nWe can additional pull out the direct, indirect and total effects of the mediated predictor relations.\n\n\nCode\nfig, axs = plt.subplots(1, 3, figsize=(9, 4))\naxs = axs.flatten()\n\ntotal_effect = az.summary(idata, var_names=['solve_I-B'])['mean'].values.reshape((4, 4))\n\ndirect_effect = az.summary(idata, var_names=['B'])['mean'].values.reshape((4, 4))\n\ni, j = np.triu_indices(total_effect.reshape((4, 4)).shape[0])\ntotal_effect[i, j] = np.nan\ndirect_effect[i, j] = np.nan\n\nindirect_effect = total_effect - direct_effect\n\nplot_heatmap(pd.DataFrame(direct_effect, columns=coords['latent'], index=coords['latent']), ax=axs[0], vmax=1, vmin=-1, colorbar=False, title=\"Direct Effects\", cmap='viridis')\nplt.tight_layout()\naxs[0].set_xticklabels(axs[0].get_xticklabels(), rotation=65)\naxs[0].set_xlabel(\"Target Variable\")\naxs[0].set_ylabel(\"Regression Predictor\")\n\n\nplot_heatmap(pd.DataFrame(indirect_effect, columns=coords['latent'], index=coords['latent']), ax=axs[1], vmax=1, vmin=-1, colorbar=False, title=\"In-Direct Effects\", cmap='viridis')\nplt.tight_layout()\naxs[1].set_yticklabels([])\naxs[1].set_xticklabels(axs[1].get_xticklabels(), rotation=65)\naxs[1].set_xlabel(\"Target Variable\")\n\nplot_heatmap(pd.DataFrame(total_effect.reshape((4, 4)), columns=coords['latent'], index=coords['latent']), ax=axs[2], vmax=1, vmin=-1, colorbar=False, title=\"Total Effects\", cmap='viridis')\nplt.tight_layout()\naxs[2].set_yticklabels([])\naxs[2].set_xticklabels(axs[2].get_xticklabels(), rotation=65)\naxs[2].set_xlabel(\"Target Variable\");\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRe-cap and Overview\n\n\n\nLet’s recall all we’ve just seen. We started with a specific covariance structure about the relationships between 12 indicator metrics that related to measures of job-satisfaction. We assessed two VAE architectures to determine if their latent structure was rich enough to recover correlation structure of those variables. We found one architecture which could recover the latent and another which broke in a subtle way. These models were compared first to a vanilla multivariate gaussian Bayesian with and without missing data. In both cases we saw that the Bayesian model specification was more efficient at recovering the latent structure. Finally, we asked whether we could further infer the deep structure of the latent correlations using a SEM architecture. This latent factor structure was able to recover the observed correlation structure and further decompose that matrix into a series of regression relations. This decomposition offers more insight into the additive effects of constructive thought processes on job satisfaction and well being i.e. that your own well being is the most decisive factor in predicting job-satisfaction, and dysfunctional thought strategies can have a negative effect on job-satisfaction."
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#preference-over-worlds",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#preference-over-worlds",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "Preference over Worlds",
    "text": "Preference over Worlds\n\n\n\n\nWorlds can be ranked in terms of:\n\nprobability\ndesirability\n\nLearning the drivers of desirability helps determine the probability of human choice and action.\nFixing the attributes of different alternatives allows us to estimate their desirability and course of probable choice."
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#from-observed-choice-to-latent-utility",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#from-observed-choice-to-latent-utility",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "From Observed Choice to Latent Utility",
    "text": "From Observed Choice to Latent Utility\nThe Mathematical Foundation of Decision\nThe utility function forms the cornerstone of choice modeling:\n\\[\\color{red}U_{ij} = \\color{blue}\\alpha_{ij} + \\color{blue}\\beta_{ij}^{1} \\color{black}\\cdot X_{ij}^{1} + \\color{blue} \\beta_{ij}^{2} \\color{black}\\cdot X_{ij}^{2} \\]\n\\[P_{ij} = \\frac{\\exp(\\color{red}U_{ij})}{\\sum_{k=1}^{J} \\exp(\\color{red}U_{ik})} \\Rightarrow Y_{ij} \\sim \\text{Categorical}(P_{ij}) \\]\nWhere:\n\n\\(U_{ij}\\) represents the systematic utility for individual \\(i\\) choosing alternative \\(j\\) at a particular world state \\(w = \\{ \\color{blue}\\alpha_{ij}, \\color{blue}\\beta_{ij}^{1}, \\color{blue}\\beta_{ij}^{2} X_{ij}^{1}, X_{ij}^{2} \\}\\)\n\\(X_{ij}\\) represents observed covariates (product attributes) and \\(\\color{blue}\\theta\\) structural parameters within the world \\(w\\).\nCollectively, the mathematical structure and world-state are combined to make a theory of the data generating process for the world \\(w\\)."
  },
  {
    "objectID": "talks/pydata_berlin_nested_logit/consumer_choice.html#from-latent-utility-to-observed-choice",
    "href": "talks/pydata_berlin_nested_logit/consumer_choice.html#from-latent-utility-to-observed-choice",
    "title": "Discrete Choice Models in PyMC-Marketing",
    "section": "From Latent Utility to Observed Choice",
    "text": "From Latent Utility to Observed Choice\nThe Mathematical Foundation of Decision\nThe utility function forms the cornerstone of choice modeling:\n\\[\\color{red}U_{ij} = \\color{blue}\\alpha_{ij} + \\color{blue}\\beta_{ij}^{1} \\color{black}\\cdot X_{ij}^{1} + \\color{blue} \\beta_{ij}^{2} \\color{black}\\cdot X_{ij}^{2} \\]\n\\[P_{ij} = \\frac{\\exp(\\color{red}U_{ij})}{\\sum_{k=1}^{J} \\exp(\\color{red}U_{ik})} \\Rightarrow Y_{ij} \\sim \\text{Categorical}(P_{ij}) \\]\nWhere:\n\n\\(U_{ij}\\) represents the systematic utility for individual \\(i\\) choosing alternative \\(j\\) at a particular world state \\(w = \\{ \\color{blue}\\alpha_{ij}, \\color{blue}\\beta_{ij}^{1}, \\color{blue}\\beta_{ij}^{2} X_{ij}^{1}, X_{ij}^{2} \\}\\)\n\\(X_{ij}\\) represents observed covariates (product attributes) and \\(\\color{blue}\\theta\\) structural parameters within the world \\(w\\).\nCollectively, the mathematical structure and world-state are combined to make a theory of the data generating process for the world \\(w\\)."
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#test",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#test",
    "title": "Bayesian Workflow with SEMs",
    "section": "Test",
    "text": "Test"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#preliminaries",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#preliminaries",
    "title": "Bayesian Workflow with SEMs",
    "section": "Preliminaries",
    "text": "Preliminaries\n\n\n\n\n\n\nWho am I?\n\n\n\nI’m a data scientist at Personio\n\nBayesian statistician,\nReformed philosopher and logician.\n\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\nThe worked examples used here can be found here\n\n\n\n\nMy Website"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-pitch-choice-matters",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-pitch-choice-matters",
    "title": "Bayesian Workflow with SEMs",
    "section": "The Pitch: Choice Matters",
    "text": "The Pitch: Choice Matters\nConsumer choice is everywhere — from cereal to cars to climate systems. Business success hinges on understanding these choices are driven by pricing, product design, market segmentation strategies.\nPyMC-Marketing allows you to simulate product interventions safely, and learn the expected impact of new product strategies"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-pitch-bayesian-workflow-inference-done-right",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-pitch-bayesian-workflow-inference-done-right",
    "title": "Bayesian Workflow with SEMs",
    "section": "The Pitch: Bayesian Workflow: Inference done Right",
    "text": "The Pitch: Bayesian Workflow: Inference done Right\nConsumer choice is everywhere — from cereal to cars to climate systems. Business success hinges on understanding these choices are driven by pricing, product design, market segmentation strategies.\nPyMC-Marketing allows you to simulate product interventions safely, and learn the expected impact of new product strategies"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-pitch-inference-done-right",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-pitch-inference-done-right",
    "title": "Bayesian Workflow with SEMs",
    "section": "The Pitch: Inference done Right",
    "text": "The Pitch: Inference done Right\nStructural Equation modelling is most compellingly conducted in a Bayesian Setting. Bayesian inference done right, adheres to transparent workflow.\nSEM modelling in PyMC enables us to automate much of this workflow of model evaluation, and parameter recovery even with complex hierarchical SEM models."
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#agenda",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#agenda",
    "title": "Bayesian Workflow with SEMs",
    "section": "Agenda",
    "text": "Agenda\n\nThe Idea of a Workflow:\n\nCraft versus Checklist\nJob Satisfaction\n\nBayesian Workflow with SEMs:\n\nConfirmatory Factor Structures\nAdding Structural Relations\nAdding Covariance Structure\n\nSensitivity Analysis:\n\nAdding Hierarchical Structure\nParameter Recovery and Model Validation\n\nConclusion\n\nCraft and Statistical Workflow"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#craft-in-modelling",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#craft-in-modelling",
    "title": "Bayesian Workflow with SEMs",
    "section": "Craft in Modelling",
    "text": "Craft in Modelling\n\n\n\nCraft embraces process, imperfection, and iteration.\nAims at the acquisition of scientific knowledge"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#metric-blindness",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#metric-blindness",
    "title": "Bayesian Workflow with SEMs",
    "section": "Metric Blindness",
    "text": "Metric Blindness\n\n\n\n\n\n::: }"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#craft-in-statistical-modelling",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#craft-in-statistical-modelling",
    "title": "Bayesian Workflow with SEMs",
    "section": "Craft in Statistical Modelling",
    "text": "Craft in Statistical Modelling\n\n\n\nEmbraces process, imperfection, and iteration.\n\n\n\nAims at the acquisition of scientific knowledge\n\n\n\n\nSupports generalisable findings and solutions\n\n\n\n\nRestore ownership: you shape, test, and refine — the model carries your imprint."
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#checklists-as-a-methodological-error",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#checklists-as-a-methodological-error",
    "title": "Bayesian Workflow with SEMs",
    "section": "Checklists as a Methodological Error",
    "text": "Checklists as a Methodological Error\n\n\n\nReduce inquiry to compliance: ticking boxes replaces genuine understanding.\n\n\n\nCreate the illusion of rigor while bypassing uncertainty and context.\n\n\n\n\nConfuse progress with throughput: more boxes checked ≠ better science.\n\n\n\n\nStrip away ownership: you don’t make something, you just complete a task."
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#job-satisfaction-data",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#job-satisfaction-data",
    "title": "Bayesian Workflow with SEMs",
    "section": "Job Satisfaction Data",
    "text": "Job Satisfaction Data\n\n\n\nConstructive Thought Strategies (CTS): Thought patterns that are positive or helpful, such as:\n\nSelf-Talk (positive internal dialogue): ST\nMental Imagery (visualizing successful performance or outcomes): MI\nEvaluating Beliefs & Assumptions (i.e. critically assessing one’s internal assumption: EBA\n\nDysfunctional Thought Processes: (DA1–DA3)\nSubjective Well Being: (UF1, UF2, FOR)\nJob Satisfaction: (JW1–JW3)"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#contemporary-bayesian-workflow",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#contemporary-bayesian-workflow",
    "title": "Bayesian Workflow with SEMs",
    "section": "Contemporary Bayesian Workflow",
    "text": "Contemporary Bayesian Workflow\n\n\n\nStart with Theory and Prior Knowledge\nIterate with Checks and Visual Diagnostics\nRefine Structure and Layer Complexity\n\nAssess consistency of Signal\n\nValidate through Sensitivty Analysis\nOwn the Process\n\n\n\n\n\n\n\n\nThe Big Red Book of Bayesian Modelling"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-confirmatory-factor-structure",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-confirmatory-factor-structure",
    "title": "Bayesian Workflow with SEMs",
    "section": "The Confirmatory Factor Structure",
    "text": "The Confirmatory Factor Structure"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-sem-regression",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-sem-regression",
    "title": "Bayesian Workflow with SEMs",
    "section": "The SEM Regression",
    "text": "The SEM Regression"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-sem-regression-with-mediation-effects",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-sem-regression-with-mediation-effects",
    "title": "Bayesian Workflow with SEMs",
    "section": "The SEM Regression with Mediation Effects",
    "text": "The SEM Regression with Mediation Effects"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-sem-regression-with-staggered-mediation-effects",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-sem-regression-with-staggered-mediation-effects",
    "title": "Bayesian Workflow with SEMs",
    "section": "The SEM Regression with Staggered Mediation Effects",
    "text": "The SEM Regression with Staggered Mediation Effects"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-sem-regression-with-residuals-covariance-structures",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-sem-regression-with-residuals-covariance-structures",
    "title": "Bayesian Workflow with SEMs",
    "section": "The SEM Regression with Residuals Covariance Structures",
    "text": "The SEM Regression with Residuals Covariance Structures"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-sem-regression-1",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-sem-regression-1",
    "title": "Bayesian Workflow with SEMs",
    "section": "The SEM Regression",
    "text": "The SEM Regression\nThe Structural Matrix\n\\[ \\mathbf{\\gamma} = \\begin{bmatrix} \\overbrace{0.5}^{sat} & \\overbrace{0.2}^{well being} & \\overbrace{-0.3}^{dys}  & \\overbrace{0.7}^{con} \\\\ ... & ... & ... & ..\n\\\\ ... & ... & ... & ..  \\\\ ... & ... \\\\ 0.4 & ...\\end{bmatrix},\\mathbf{B}  = \\begin{bmatrix} \\overbrace{0}^{sat} & 0 & 0 & \\overbrace{0}^{con}\n\\\\ \\beta_{21} & 0 & 0 & 0\n\\\\ \\beta_{31} & \\beta_{32} & 0 & 0\n\\\\ \\beta_{41} & \\beta_{42} & \\beta_{43} & 0\n\\end{bmatrix} \\]"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-sem-regression-2",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-sem-regression-2",
    "title": "Bayesian Workflow with SEMs",
    "section": "The SEM Regression",
    "text": "The SEM Regression\nThe Structural Matrix\n\\[ \\eta = solve(I - \\mathbf{B}, \\mathbf{\\gamma})\\]\n\\[ \\mathbf{\\eta} = \\begin{bmatrix} \\overbrace{3.2}^{sat} & \\overbrace{1.3}^{well being} & \\overbrace{2.1}^{dys}  & \\overbrace{4.9}^{con} \\\\ ... & ... & ... & ..\n\\\\ ... & ... & ... & ..  \\\\ ... & ... \\\\ 5.0 & ...\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-sem-regression-3",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-sem-regression-3",
    "title": "Bayesian Workflow with SEMs",
    "section": "The SEM Regression",
    "text": "The SEM Regression\nwith Mediation Effects"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-sem-workflow",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-sem-workflow",
    "title": "Bayesian Workflow with SEMs",
    "section": "The SEM workflow",
    "text": "The SEM workflow\n\n\nStart with Confirmatory Factor Analysis (CFA):\n\nValidate that our measurement model holds.\nEnsure latent constructs are reliably represented by observed indicators.\n\n\n\n\n\nLayer Structural Paths:\n\nAdd theoretically-motivated regressions between constructs.\nAssess whether hypothesized relationships improve model fit.\n\n\n\n\n\nRefine with Residual Covariances:\n\nAccount for specific shared variance not captured by factors.\nKeep structure transparent while improving realism.\n\n\n\n\n\nIterative Validation:\n\nEach step asks: Does this addition honor theory? Improve fit?\nWorkflow = constant negotiation between parsimony and fidelity."
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-sem-regression-4",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-sem-regression-4",
    "title": "Bayesian Workflow with SEMs",
    "section": "The SEM Regression",
    "text": "The SEM Regression\nwith Staggered Mediation Effects"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#bayesian-workflow-with-sems",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#bayesian-workflow-with-sems",
    "title": "Bayesian Workflow with SEMs",
    "section": "Bayesian Workflow with SEMs",
    "text": "Bayesian Workflow with SEMs\nCFA\n\n\nwith pm.Model(coords=coords) as cfa_model_v1:\n    \n    # --- Factor loadings ---\n    lambdas_1 = make_lambda('indicators_1', 'lambdas1', priors=[1, .5])\n    lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, .5])\n    lambdas_3 = make_lambda('indicators_3', 'lambdas3', priors=[1, .5])\n    lambdas_4 = make_lambda('indicators_4', 'lambdas4', priors=[1, .5])\n\n    Lambda = pt.zeros((12, 4))\n    Lambda = pt.set_subtensor(Lambda[0:3, 0], lambdas_1)\n    Lambda = pt.set_subtensor(Lambda[3:6, 1], lambdas_2)\n    Lambda = pt.set_subtensor(Lambda[6:9, 2], lambdas_3)\n    Lambda = pt.set_subtensor(Lambda[9:12, 3], lambdas_4)\n    Lambda = pm.Deterministic('Lambda', Lambda)\n\n    sd_dist = pm.Exponential.dist(1.0, shape=4)\n    chol, _, _ = pm.LKJCholeskyCov(\"chol_cov\", n=4, eta=2, sd_dist=sd_dist, compute_corr=True)\n    eta = pm.MvNormal(\"eta\", 0, chol=chol, dims=(\"obs\", \"latent\"))\n\n    # Construct Pseudo Observation matrix based on Factor Loadings\n    mu = pt.dot(eta, Lambda.T)  # (n_obs, n_indicators)\n\n    ## Error Terms\n    Psi = pm.InverseGamma(\"Psi\", 5, 10, dims=\"indicators\")\n    _ = pm.Normal('likelihood', mu=mu, sigma=Psi, observed=observed_data)\n\n\\[ \\eta \\sim MvN(0, \\Sigma)\\] \\[ \\mu = \\color{blue}{\\Lambda} \\eta\\] \\[\\mathbf{y} = N(\\mu, \\Psi)\\]"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#bayesian-workflow-with-sems-1",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#bayesian-workflow-with-sems-1",
    "title": "Bayesian Workflow with SEMs",
    "section": "Bayesian Workflow with SEMs",
    "text": "Bayesian Workflow with SEMs\nCFA Implications\n\n\n\n\n\nEstimated Factor Loadings are close to 1\nThe indicator(s) are strongly reflective of the latent factor.\nPosterior Predictive Residuals are close to 0\nLatent factors move together in intuitive ways.\nHigh Satisfaction ~~ High Well Being"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#bayesian-workflow-with-sems-2",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#bayesian-workflow-with-sems-2",
    "title": "Bayesian Workflow with SEMs",
    "section": "Bayesian Workflow with SEMs",
    "text": "Bayesian Workflow with SEMs\nSEM\n\n\nwith pm.Model(coords=coords) as sem_model_v3:\n    \n    # --- Factor loadings ---\n    lambdas_1 = make_lambda('indicators_1', 'lambdas1', priors=[1, .5])\n    lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, .5])\n    lambdas_3 = make_lambda('indicators_3', 'lambdas3', priors=[1, .5])\n    lambdas_4 = make_lambda('indicators_4', 'lambdas4', priors=[1, .5])\n\n    Lambda = pt.zeros((12, 4))\n    Lambda = pt.set_subtensor(Lambda[0:3, 0], lambdas_1)\n    Lambda = pt.set_subtensor(Lambda[3:6, 1], lambdas_2)\n    Lambda = pt.set_subtensor(Lambda[6:9, 2], lambdas_3)\n    Lambda = pt.set_subtensor(Lambda[9:12, 3], lambdas_4)\n    Lambda = pm.Deterministic('Lambda', Lambda)\n\n    latent_dim = len(coords['latent'])\n\n    sd_dist = pm.Exponential.dist(1.0, shape=latent_dim)\n    chol, _, _ = pm.LKJCholeskyCov(\"chol_cov\", n=latent_dim, eta=2, sd_dist=sd_dist, compute_corr=True)\n    gamma = pm.MvNormal(\"gamma\", 0, chol=chol, dims=(\"obs\", \"latent\"))\n\n    B = make_B()\n    I = pt.eye(latent_dim)\n    eta = pm.Deterministic(\"eta\", pt.slinalg.solve(I - B + 1e-8 * I, gamma.T).T)  \n\n    mu = pt.dot(eta, Lambda.T)\n\n    ## Error Terms\n    Psi = make_Psi('indicators')\n    _ = pm.MvNormal('likelihood', mu=mu, cov=Psi, observed=observed_data)\n\n\\[\\gamma \\sim MvN(0, \\Sigma_{\\zeta})\\] \\[\\eta = (I-\\color{blue}{B})^{-1} \\gamma\\] \\[\\mu = \\Lambda \\eta_i\\] \\[ \\mathbf{y} \\mid \\eta \\sim MvN(\\mu, \\Psi)\\]"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#bayesian-workflow-with-sems-3",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#bayesian-workflow-with-sems-3",
    "title": "Bayesian Workflow with SEMs",
    "section": "Bayesian Workflow with SEMs",
    "text": "Bayesian Workflow with SEMs\nSEM Implications\n\n\n\n\n\nThe Beta coefficients encode the directional effects of the latent constructs on one another\nHigh Dysfunction -&gt; Negative Impact on Satisfaction\nFactor loadings remain close to 1\nPosterior Predictive of the Residuals have Improved"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#bayesian-workflow-with-sems-4",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#bayesian-workflow-with-sems-4",
    "title": "Bayesian Workflow with SEMs",
    "section": "Bayesian Workflow with SEMs",
    "text": "Bayesian Workflow with SEMs\nMean Structures and Marginalising\n\n\nwith pm.Model(coords=coords) as sem_model_mean_structure:\n  # --- Factor loadings ---\n  lambdas_1 = make_lambda('indicators_1', 'lambdas1', priors=[1, .5])\n  lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, .5])\n  lambdas_3 = make_lambda('indicators_3', 'lambdas3', priors=[1, .5])\n  lambdas_4 = make_lambda('indicators_4', 'lambdas4', priors=[1, .5])\n\n  Lambda = pt.zeros((12, 4))\n  Lambda = pt.set_subtensor(Lambda[0:3, 0], lambdas_1)\n  Lambda = pt.set_subtensor(Lambda[3:6, 1], lambdas_2)\n  Lambda = pt.set_subtensor(Lambda[6:9, 2], lambdas_3)\n  Lambda = pt.set_subtensor(Lambda[9:12, 3], lambdas_4)\n  Lambda = pm.Deterministic('Lambda', Lambda)\n\n  sd_dist = pm.Exponential.dist(1.0, shape=4)\n  chol, _, _ = pm.LKJCholeskyCov(\"chol_cov\", n=4, eta=2, sd_dist=sd_dist, compute_corr=True)\n\n  Psi_zeta = pm.Deterministic(\"Psi_zeta\", chol.dot(chol.T))\n  Psi = make_Psi('indicators')\n\n  B = make_B()\n  latent_dim = len(coords['latent'])\n  I = pt.eye(latent_dim)\n  lhs = I - B + 1e-8 * pt.eye(latent_dim)  # (latent_dim, latent_dim)\n  inv_lhs = pm.Deterministic('solve_I-B', pt.slinalg.solve(lhs, pt.eye(latent_dim)), dims=('latent', 'latent1'))\n\n  # Mean Structure\n  tau = pm.Normal(\"tau\", mu=0, sigma=0.5, dims=\"indicators\")   # observed intercepts \n  alpha = pm.Normal(\"alpha\", mu=0, sigma=0.5, dims=\"latent\")       # latent means\n  mu_y = pm.Deterministic(\"mu_y\", tau + pt.dot(Lambda, pt.dot(inv_lhs, alpha)))\n\n  Sigma_y = pm.Deterministic('Sigma_y', Lambda.dot(inv_lhs).dot(Psi_zeta).dot(inv_lhs.T).dot(Lambda.T) + Psi)\n  M = Psi_zeta @ inv_lhs @ Lambda.T @ pm.math.matrix_inverse(Sigma_y)\n  eta_hat = pm.Deterministic('eta', alpha + (M @ (observed_data - mu_y).T).T, dims=('obs', 'latent')) \n  _ = pm.MvNormal(\"likelihood\", mu=mu_y, cov=Sigma_y, observed=observed_data)\n\n\n\\[\\mu = \\tau + \\Lambda(1 - B)^{-1}\\alpha\\] \\[\\color{blue}{\\Sigma_{\\mathcal{y}}} = \\Psi + \\\\ \\Lambda(I - B)^{-1}\\Psi_{\\gamma}(I - B)^{T}\\Lambda^{T} \\]\n\\[ \\mathcal{y} \\sim MvN(\\mu, \\Sigma_{y})\\]"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#mean-structures-and-marginalising",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#mean-structures-and-marginalising",
    "title": "Bayesian Workflow with SEMs",
    "section": "Mean Structures and Marginalising",
    "text": "Mean Structures and Marginalising\n\n\nwith pm.Model(coords=coords) as sem_model_mean_structure:\n  # --- Factor loadings ---\n  lambdas_1 = make_lambda('indicators_1', 'lambdas1', priors=[1, .5])\n  lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, .5])\n  lambdas_3 = make_lambda('indicators_3', 'lambdas3', priors=[1, .5])\n  lambdas_4 = make_lambda('indicators_4', 'lambdas4', priors=[1, .5])\n\n  Lambda = pt.zeros((12, 4))\n  Lambda = pt.set_subtensor(Lambda[0:3, 0], lambdas_1)\n  Lambda = pt.set_subtensor(Lambda[3:6, 1], lambdas_2)\n  Lambda = pt.set_subtensor(Lambda[6:9, 2], lambdas_3)\n  Lambda = pt.set_subtensor(Lambda[9:12, 3], lambdas_4)\n  Lambda = pm.Deterministic('Lambda', Lambda)\n\n  sd_dist = pm.Exponential.dist(1.0, shape=4)\n  chol, _, _ = pm.LKJCholeskyCov(\"chol_cov\", n=4, eta=2, sd_dist=sd_dist, compute_corr=True)\n\n  Psi_zeta = pm.Deterministic(\"Psi_zeta\", chol.dot(chol.T))\n  Psi = make_Psi('indicators')\n\n  B = make_B()\n  latent_dim = len(coords['latent'])\n  I = pt.eye(latent_dim)\n  lhs = I - B + 1e-8 * pt.eye(latent_dim)  # (latent_dim, latent_dim)\n  inv_lhs = pm.Deterministic('solve_I-B', pt.slinalg.solve(lhs, pt.eye(latent_dim)), dims=('latent', 'latent1'))\n\n  # Mean Structure\n  tau = pm.Normal(\"tau\", mu=0, sigma=0.5, dims=\"indicators\")   # observed intercepts \n  alpha = pm.Normal(\"alpha\", mu=0, sigma=0.5, dims=\"latent\")       # latent means\n  mu_y = pm.Deterministic(\"mu_y\", tau + pt.dot(Lambda, pt.dot(inv_lhs, alpha)))\n\n  Sigma_y = pm.Deterministic('Sigma_y', Lambda.dot(inv_lhs).dot(Psi_zeta).dot(inv_lhs.T).dot(Lambda.T) + Psi)\n  _ = pm.MvNormal(\"likelihood\", mu=mu_y, cov=Sigma_y, observed=mean_shift_data)"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#bayesian-workflow-with-sems-5",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#bayesian-workflow-with-sems-5",
    "title": "Bayesian Workflow with SEMs",
    "section": "Bayesian Workflow with SEMs",
    "text": "Bayesian Workflow with SEMs\nMean Structure Implications\n\n\n\n\n\nMean Structure Parameters \\(\\tau\\) show poor identification\nBeta coefficients and Factor Loading Estimates consistent with prior models\nPosterior Predictive check on Residuals provide evidence of a good fit."
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#bayesian-workflow-with-sems-6",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#bayesian-workflow-with-sems-6",
    "title": "Bayesian Workflow with SEMs",
    "section": "Bayesian Workflow with SEMs",
    "text": "Bayesian Workflow with SEMs\nComparing Model Estimates\n\n\n\n\n\nThe measurement model is stable: adding the structural paths did not disrupt how the latent variables are measured.\nSEM is not introducing distortions into the factor structure.\nThe hypothesized regression paths are consistent with the observed covariance patterns.\nConstructive thought processes: self-talk, visualisation and evaluation of beliefs improve job satisfaction.\nEffects are achieved directly and indirectly through overall well being."
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#ba",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#ba",
    "title": "Bayesian Workflow with SEMs",
    "section": "Ba",
    "text": "Ba"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#bayesian-workflow-with-sems-7",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#bayesian-workflow-with-sems-7",
    "title": "Bayesian Workflow with SEMs",
    "section": "Bayesian Workflow with SEMs",
    "text": "Bayesian Workflow with SEMs\nAssessing the Indirect Effects"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#job-satisfaction-data-1",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#job-satisfaction-data-1",
    "title": "Bayesian Workflow with SEMs",
    "section": "Job Satisfaction Data",
    "text": "Job Satisfaction Data\n\n\n\n\n\n\n\n \nJW1\nJW2\nJW3\nUF1\nUF2\nFOR\nDA1\nDA2\nDA3\nEBA\nST\nMI\n\n\n\n\n0\n-1.046719\n-1.472334\n-1.649844\n-0.740886\n-0.573890\n-0.992347\n1.269461\n1.805128\n1.230402\n-0.039732\n1.618562\n-0.169659\n\n\n1\n-1.649857\n-1.908889\n-1.841327\n0.120929\n-0.939917\n0.401440\n0.177058\n0.126041\n-0.004604\n-0.806541\n0.930899\n-0.438887\n\n\n2\n0.429099\n1.826533\n0.341107\n1.033988\n1.287623\n0.490457\n-0.627370\n-0.717461\n-0.246633\n0.261212\n0.913639\n0.496846\n\n\n3\n0.257582\n-0.315831\n1.258474\n0.241065\n-0.548987\n-0.247273\n0.858847\n0.964730\n1.233870\n-0.251100\n0.466743\n0.169622\n\n\n4\n-0.875969\n-0.263046\n-0.947966\n-0.231731\n-0.850588\n0.860900\n0.989963\n0.671778\n0.438236\n-0.129382\n2.266723\n-0.951899\n\n\n\n\n\nThe Data for SEM modelling is a multivariate data structure with natural theory-driven categories of variables which reflect some mis-measured latent factor."
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-sem-regression-5",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-sem-regression-5",
    "title": "Bayesian Workflow with SEMs",
    "section": "The SEM Regression",
    "text": "The SEM Regression\nwith Residuals Covariance Structures"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-do-operator-and-parameter-recovery",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-do-operator-and-parameter-recovery",
    "title": "Bayesian Workflow with SEMs",
    "section": "The Do-Operator and Parameter Recovery",
    "text": "The Do-Operator and Parameter Recovery\n\n# Generating data from model by fixing parameters\nfixed_parameters = {\n \"mu_betas_man\": [0.1, 0.5, 2.3, 0.9, 0.6, 0.8],\n \"mu_betas_woman\": [0.3, 0.2, 0.7, 0.8, 0.6, 1.2], \n \"tau\": [[-0.822,  1.917, -0.743, -0.585, -1.095,  2.207, -0.898, -0.99 ,\n        1.872, -0.044, -0.035, -0.085], [-0.882,  1.816, -0.828, -1.319,\n        0.202,  1.267, -1.784, -2.112,  3.705, -0.769,  2.048, -1.064]],\n \"lambdas1_\": [1, .8, .9],\n \"lambdas2_\": [1, .9, 1.2],\n \"lambdas3_\": [1, .95, .8],\n \"lambdas4_\": [1, 1.4, 1.1],\n \"alpha\": [[ 0.869,  0.242,  0.314, -0.175], [0.962,  1.036,  0.436,  0.166]], \n \"chol_cov\": [0.696, -0.096,  0.23 , -0.3  , -0.385,  0.398, -0.004,  0.043,\n       -0.037,  0.422], \n\"Psi_cov_\": [0.559, 0.603, 0.666, 0.483, 0.53 , 0.402, 0.35 , 0.28 , 0.498,\n       0.494, 0.976, 0.742], \n\"Psi_cov_beta\": [0.029]\n}\nwith pm.do(sem_model_hierarchical2, fixed_parameters) as synthetic_model:\n   idata = pm.sample_prior_predictive(random_seed=1000) # Sample from prior predictive distribution.\n   synthetic_y = idata['prior']['likelihood'].sel(draw=0, chain=0)\n\n# Infer parameters conditioned on observed data\nwith pm.observe(sem_model_hierarchical2, {\"likelihood\": synthetic_y}) as inference_model:\n   idata = pm.sample(random_seed=100, nuts_sampler='numpyro', chains=4, draws=500)"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#conclusion",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#conclusion",
    "title": "Bayesian Workflow with SEMs",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\n“Abandon the idea of predetermination, the shaping force of your intentions…rely less on the priority of your intentions and more on the immediacy of writing… You’ll see that some of your sentences are still conjectural… start noticing the thoughts and implications surrounding them.” - Verlyn Klinkenborg in Several Short Sentences about Writing”"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#craft-as-discovery",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#craft-as-discovery",
    "title": "Bayesian Workflow with SEMs",
    "section": "Craft as Discovery",
    "text": "Craft as Discovery\n\n\n\n“Abandon the idea of predetermination, the shaping force of your intentions…rely less on the priority of your intentions and more on the immediacy of writing… You’ll see that some of your sentences are still conjectural… start noticing the thoughts and implications surrounding them.” - Verlyn Klinkenborg in Several Short Sentences about Writing”\n\n\n\nModeling, like writing, is an act of exploration.\n\n\n\n\nExpect surprises and anomalies—they teach more than preconceptions.\n\n\n\n\nEmbrace uncertainty; allow the data to guide the process."
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#craft-as-discipline",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#craft-as-discipline",
    "title": "Bayesian Workflow with SEMs",
    "section": "Craft as Discipline",
    "text": "Craft as Discipline\n\n\n\n” [T]he goal is to represent the systematic relationships between the variables and between the variables and the parameters … Discrepancies between the model and data can be used to learn about the ways in which the model is inadequate for the scientific purposes at hand, and thus to motivate expansions and changes to the model … a model is a story of how the data could have been generated; the fitted model should therefore be able to generate synthetic data that look like the real data; failures to do so in important ways indicate faults in the model.” - Gelman & Shalizi in Philosophy and the practice of Bayesian statistics\n\n\n\n\n\nBuilding statistical models is inherently iterative and expansionary\n\n\n\n\nAssumptions are encoded transparently and their implications are assessed for cogency\n\n\n\n\nWhere our assumptions fail, they are revised or rejected. Building confidence and clarity.\n\n\n\n\nThe process yields compelling, justifiable conclusions worthy of your work."
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#checklists-in-statistical-modelling",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#checklists-in-statistical-modelling",
    "title": "Bayesian Workflow with SEMs",
    "section": "Checklists in Statistical Modelling",
    "text": "Checklists in Statistical Modelling\n\n\n\nReduce inquiry to compliance: ticking boxes replaces genuine understanding.\n\n\n\nCreate the illusion of rigor while bypassing uncertainty and context.\n\n\n\n\nConfuse progress with throughput: more boxes checked ≠ better science.\n\n\n\n\nPromotes shallow levels of engagement, infantalises the management class. Hinders effective decision making.\n\n\n\n\nStrips away ownership: you don’t make something, you just complete a task."
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-do-operator-and-parameter-recovery-1",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-do-operator-and-parameter-recovery-1",
    "title": "Bayesian Workflow with SEMs",
    "section": "The Do-Operator and Parameter Recovery",
    "text": "The Do-Operator and Parameter Recovery"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-do-operator-forward-simulation-and-backwards-inference",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-do-operator-forward-simulation-and-backwards-inference",
    "title": "Bayesian Workflow with SEMs",
    "section": "The Do-Operator, Forward Simulation and Backwards Inference",
    "text": "The Do-Operator, Forward Simulation and Backwards Inference\n\n# Generating data from model by fixing parameters\nfixed_parameters = {\n \"mu_betas_man\": [0.1, 0.5, 2.3, 0.9, 0.6, 0.8],\n \"mu_betas_woman\": [0.3, 0.2, 0.7, 0.8, 0.6, 1.2], \n \"tau\": [[-0.822,  1.917, -0.743, -0.585, -1.095,  2.207, -0.898, -0.99 ,\n        1.872, -0.044, -0.035, -0.085], [-0.882,  1.816, -0.828, -1.319,\n        0.202,  1.267, -1.784, -2.112,  3.705, -0.769,  2.048, -1.064]],\n \"lambdas1_\": [1, .8, .9],\n \"lambdas2_\": [1, .9, 1.2],\n \"lambdas3_\": [1, .95, .8],\n \"lambdas4_\": [1, 1.4, 1.1],\n \"alpha\": [[ 0.869,  0.242,  0.314, -0.175], [0.962,  1.036,  0.436,  0.166]], \n \"chol_cov\": [0.696, -0.096,  0.23 , -0.3  , -0.385,  0.398, -0.004,  0.043,\n       -0.037,  0.422], \n\"Psi_cov_\": [0.559, 0.603, 0.666, 0.483, 0.53 , 0.402, 0.35 , 0.28 , 0.498,\n       0.494, 0.976, 0.742], \n\"Psi_cov_beta\": [0.029]\n}\nwith pm.do(sem_model_hierarchical2, fixed_parameters) as synthetic_model:\n   idata = pm.sample_prior_predictive(random_seed=1000) # Sample from prior predictive distribution.\n   synthetic_y = idata['prior']['likelihood'].sel(draw=0, chain=0)\n\n# Infer parameters conditioned on observed data\nwith pm.observe(sem_model_hierarchical2, {\"likelihood\": synthetic_y}) as inference_model:\n   idata = pm.sample(random_seed=100, nuts_sampler='numpyro', chains=4, draws=500)"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#parameter-recovery",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#parameter-recovery",
    "title": "Bayesian Workflow with SEMs",
    "section": "Parameter Recovery",
    "text": "Parameter Recovery\n\nComplex models require proper validation methods. Parameter recovery methods are the best way to test the model’s ability to identify the correct effects."
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#conclusion-workflow-as-craft",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#conclusion-workflow-as-craft",
    "title": "Bayesian Workflow with SEMs",
    "section": "Conclusion: Workflow as Craft",
    "text": "Conclusion: Workflow as Craft\n\n\n\n“Here, in short, is what i want to tell you. Know what each sentence says, What it doesn’t say, And what it implies. Of these, the hardest is know what each sentence actually says” - V. Klinkenborg\n\n\n\n\n\nIn modelling, as in writing, clarity emerges through revision.\n\n\n\n\nThe Bayesian workflow with PyMC teaches us to listen to our models — to read them aloud through simulation, recovery, and critique.\n\n\n\n\nEach iteration reveals what the model truly says, what it hides, and what it implies.\n\n\n\n\n\nCraft lies in that attention — in resisting flattening automation, and choosing understanding over throughput.\n\n\n\n\n\nThrough this care, our models become not only more compelling, but more robust — resilient to noise, misfit, and misuse."
  },
  {
    "objectID": "oss/pymc/sem_workflow.html",
    "href": "oss/pymc/sem_workflow.html",
    "title": "Bayesian Workflow with Structural Equation Models",
    "section": "",
    "text": "Bayesian Workflow\nIn this project I demonstrated how to fit an escalating series of structural equation models. Emphasising the iterative and expansionary method of model fit and assessment as recommended in the contemporary Bayesian workflow.\nThe project culminated in a publication to the official PyMC documentation that can be found online here or downloaded as notebook here\n\n\n\nStructural Equation Models with Discrete Choice\n\n\nThe central argument posits that statistical modeling should be approached as a craft — a deliberate, iterative, and transparent process aimed at deep understanding — rather than a game of optimizing performance metrics. The Bayesian workflow, with its cycle of hypothesis, simulation, checking, and expansion, provides the ideal framework for this craft-based approach, a philosophy independently supported by the staggered model development strategies within the SEM literature.\nThe analysis demonstrates this synthesis through a job satisfaction model, progressively layering complexity from a basic Confirmatory Factor Analysis (CFA) to a full SEM. A key technical finding emerges from comparing different model formulations: the Marginal Formulation, which integrates out latent variables, proved more computationally robust and resolved sampler issues encountered with the more direct Conditional Formulation.\nFurthermore, the document highlights the expressive power of Bayesian SEMs through advanced extensions. A Hierarchical SEM is used to model how structural relationships differ across groups, enabling causal inference. An SEM with a Discrete Choice Component demonstrates how to link latent constructs like “job satisfaction” to tangible behavioral outcomes like employee attrition, providing a powerful method for “dual calibration” of expressed attitudes against observed choices. Throughout, the critical role of validation techniques, particularly parameter recovery simulations, is emphasized as an essential step for ensuring a model is well-specified and its parameters are identifiable. The preferred model, the Marginal SEM, is selected not just for its performance metrics but for its combination of plausible results, computational efficiency, and parsimonious structure, embodying the core principles of the craft-based workflow.\n\n\n\nPyCon Ireland Presentation"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-pitch-structuring-your-work-with-structural-equation-models",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-pitch-structuring-your-work-with-structural-equation-models",
    "title": "Bayesian Workflow with SEMs",
    "section": "The Pitch: Structuring Your Work with Structural Equation Models",
    "text": "The Pitch: Structuring Your Work with Structural Equation Models\n\nModels as workflow, not endpoints\nSEMs as language for theory\nBayesian inference as scaffold for iteration\nPyMC as tool for reproducibility"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-pitch",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#the-pitch",
    "title": "Bayesian Workflow with SEMs",
    "section": "The Pitch",
    "text": "The Pitch\nStructuring Your Work with Structural Equation Models\nProbabilistic programming languages (PPLs) are structures for articulating assumptions and relationships, and a scaffold for exploring uncertainty.\nStructural Equation models (SEMs) formalize scientific theory as a system of statistical relationships.\nThe Bayesian workflow binds these together into a compelling practice: an iterative conversation between theory and data disciplined by rigours of probabilistic programming"
  },
  {
    "objectID": "oss/causalpy/variable_selection_joint_models.html",
    "href": "oss/causalpy/variable_selection_joint_models.html",
    "title": "Structural Causal Models",
    "section": "",
    "text": "Structural Causal Models and Variable Selection Priors\nThis project outlines how CausalPy introduces Structural Causal Models (SCMs) as the foundation of modern causal inference. SCMs extend beyond correlational analysis by explicitly encoding causal mechanisms through equations and graphical structures. They provide a language to reason about interventions, confounding, and counterfactuals — the key ideas that unify Pearl’s do-calculus with practical modeling. You can download the notebook here\n\n\n\nCausal Graph\n\n\nCausalPy’s knowledge base emphasizes that SCMs combine two critical components:\n\nGraphical structure a Directed Acyclic Graph (DAG) showing which variables cause which.\nStructural equations explicit functional relationships with independent noise terms.\nCATE Estimation using BART models and interaction effects on linear structural models.\n\nTogether, these define how interventions propagate through a system and how causal effects differ from mere associations. The page links SCMs to related concepts like instrumental variables, and difference-in-differences, showing how all fit under the SCM umbrella. Additionally we show how to use variable selection priors to try and automate the discovery of instruments in the joint modelling. These techniques are applied to the NHEFs smoking cessation study."
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#knowing-what-your-model-implies",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#knowing-what-your-model-implies",
    "title": "Bayesian Workflow with SEMs",
    "section": "Knowing what your Model Implies",
    "text": "Knowing what your Model Implies"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#validating-model-implications",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#validating-model-implications",
    "title": "Bayesian Workflow with SEMs",
    "section": "Validating Model Implications",
    "text": "Validating Model Implications\nThe Do-Operator and Parameter Recovery\n\n# Generating data from model by fixing parameters\nfixed_parameters = {\n \"mu_betas_man\": [0.1, 0.5, 2.3, 0.9, 0.6, 0.8],\n \"mu_betas_woman\": [0.3, 0.2, 0.7, 0.8, 0.6, 1.2], \n \"tau\": [[-0.822,  1.917, -0.743, -0.585, -1.095,  2.207, -0.898, -0.99 ,\n        1.872, -0.044, -0.035, -0.085], [-0.882,  1.816, -0.828, -1.319,\n        0.202,  1.267, -1.784, -2.112,  3.705, -0.769,  2.048, -1.064]],\n \"lambdas1_\": [1, .8, .9],\n \"lambdas2_\": [1, .9, 1.2],\n \"lambdas3_\": [1, .95, .8],\n \"lambdas4_\": [1, 1.4, 1.1],\n \"alpha\": [[ 0.869,  0.242,  0.314, -0.175], [0.962,  1.036,  0.436,  0.166]], \n \"chol_cov\": [0.696, -0.096,  0.23 , -0.3  , -0.385,  0.398, -0.004,  0.043,\n       -0.037,  0.422], \n\"Psi_cov_\": [0.559, 0.603, 0.666, 0.483, 0.53 , 0.402, 0.35 , 0.28 , 0.498,\n       0.494, 0.976, 0.742], \n\"Psi_cov_beta\": [0.029]\n}\nwith pm.do(sem_model_hierarchical2, fixed_parameters) as synthetic_model:\n   idata = pm.sample_prior_predictive(random_seed=1000) # Sample from prior predictive distribution.\n   synthetic_y = idata['prior']['likelihood'].sel(draw=0, chain=0)\n\n# Infer parameters conditioned on observed data\nwith pm.observe(sem_model_hierarchical2, {\"likelihood\": synthetic_y}) as inference_model:\n   idata = pm.sample(random_seed=100, nuts_sampler='numpyro', chains=4, draws=500)"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#variations-on-the-theme",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#variations-on-the-theme",
    "title": "Bayesian Workflow with SEMs",
    "section": "Variations on the Theme",
    "text": "Variations on the Theme\nwith Hierarchical Structure\n\n\ncoords['group'] = ['man', 'woman']\nwith pm.Model(coords=coords) as sem_model_hierarchical2:\n    \n    # --- Factor loadings ---\n    lambdas_1 = make_lambda('indicators_1', 'lambdas1', priors=[1, .5])\n    lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, .5])\n    lambdas_3 = make_lambda('indicators_3', 'lambdas3', priors=[1, .5])\n    lambdas_4 = make_lambda('indicators_4', 'lambdas4', priors=[1, .5])\n\n    Lambda = pt.zeros((12, 4))\n    Lambda = pt.set_subtensor(Lambda[0:3, 0], lambdas_1)\n    Lambda = pt.set_subtensor(Lambda[3:6, 1], lambdas_2)\n    Lambda = pt.set_subtensor(Lambda[6:9, 2], lambdas_3)\n    Lambda = pt.set_subtensor(Lambda[9:12, 3], lambdas_4)\n    Lambda = pm.Deterministic('Lambda', Lambda)\n\n    sd_dist = pm.Exponential.dist(1.0, shape=4)\n    chol, _, _ = pm.LKJCholeskyCov(\"chol_cov\", n=4, eta=2, sd_dist=sd_dist, compute_corr=True)\n\n    Psi_zeta = pm.Deterministic(\"Psi_zeta\", chol.dot(chol.T))\n    Psi = make_Psi('indicators')\n\n    Bs = []\n    for g in coords[\"group\"]:\n        B_g = make_B(group_suffix=f\"_{g}\")  # give group-specific names\n        Bs.append(B_g)\n    B_ = pt.stack(Bs)\n\n    latent_dim = len(coords['latent'])\n    I = pt.eye(latent_dim)\n\n    # invert (I - B_g) for each group\n    inv_I_minus_B = pt.stack([\n        pt.slinalg.solve(I - B_[g] + 1e-8 * I, I)\n        for g in range(len(coords[\"group\"]))\n    ])\n\n    # Mean Structure\n    tau = pm.Normal(\"tau\", mu=0, sigma=0.5, dims=('group', 'indicators'))   # observed intercepts \n    alpha = pm.Normal(\"alpha\", mu=0, sigma=0.5, dims=('group', 'latent'))       # latent means\n    M = pt.matmul(Lambda, inv_I_minus_B)  # group, indicators latent\n    mu_latent = pt.matmul(alpha[:, None, :], M.transpose(0, 2, 1))[:,:,0] # shape handling dummy axis and transpose M -&gt; group, latent, indicators\n    mu_y = pm.Deterministic(\"mu_y\", tau + mu_latent) # size = group, indicators\n\n    Sigma_y = []\n    for g in range(len(coords['group'])):\n        inv_lhs = inv_I_minus_B[g]\n        Sigma_y_g = Lambda @ inv_lhs @ Psi_zeta @ inv_lhs.T @ Lambda.T + Psi\n        Sigma_y.append(Sigma_y_g)\n    Sigma_y = pt.stack(Sigma_y)\n    _ = pm.MvNormal(\"likelihood\", mu=mu_y[grp_idx], cov=Sigma_y[grp_idx])\n\n\n\\[ \\mu_g = \\tau_{g} + \\Lambda(1 - B_{g})^{-1}\\alpha_{g}\\] \\[\\Sigma_{\\mathcal{y}} = \\Psi + \\\\ \\Lambda(I - \\color{blue}{B_{g}})^{-1}\\Psi_{\\gamma}(I - \\color{blue}{B_{g}})^{T}\\Lambda^{T} \\]\n\\[ \\mathcal{y} \\sim MvN(\\mu_{g}, \\Sigma_{y}^{g})\\]"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#variations-on-theme",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#variations-on-theme",
    "title": "Bayesian Workflow with SEMs",
    "section": "Variations on theme",
    "text": "Variations on theme\nwith Hierarchical Structure"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#variations-on-theme-1",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#variations-on-theme-1",
    "title": "Bayesian Workflow with SEMs",
    "section": "Variations on theme",
    "text": "Variations on theme\nwith Hierarchical Structure\n\n\ncoords['group'] = ['man', 'woman']\nwith pm.Model(coords=coords) as sem_model_hierarchical2:\n    \n    # --- Factor loadings ---\n    lambdas_1 = make_lambda('indicators_1', 'lambdas1', priors=[1, .5])\n    lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, .5])\n    lambdas_3 = make_lambda('indicators_3', 'lambdas3', priors=[1, .5])\n    lambdas_4 = make_lambda('indicators_4', 'lambdas4', priors=[1, .5])\n\n    Lambda = pt.zeros((12, 4))\n    Lambda = pt.set_subtensor(Lambda[0:3, 0], lambdas_1)\n    Lambda = pt.set_subtensor(Lambda[3:6, 1], lambdas_2)\n    Lambda = pt.set_subtensor(Lambda[6:9, 2], lambdas_3)\n    Lambda = pt.set_subtensor(Lambda[9:12, 3], lambdas_4)\n    Lambda = pm.Deterministic('Lambda', Lambda)\n\n    sd_dist = pm.Exponential.dist(1.0, shape=4)\n    chol, _, _ = pm.LKJCholeskyCov(\"chol_cov\", n=4, eta=2, sd_dist=sd_dist, compute_corr=True)\n\n    Psi_zeta = pm.Deterministic(\"Psi_zeta\", chol.dot(chol.T))\n    Psi = make_Psi('indicators')\n\n    Bs = []\n    for g in coords[\"group\"]:\n        B_g = make_B(group_suffix=f\"_{g}\")  # give group-specific names\n        Bs.append(B_g)\n    B_ = pt.stack(Bs)\n\n    latent_dim = len(coords['latent'])\n    I = pt.eye(latent_dim)\n\n    # invert (I - B_g) for each group\n    inv_I_minus_B = pt.stack([\n        pt.slinalg.solve(I - B_[g] + 1e-8 * I, I)\n        for g in range(len(coords[\"group\"]))\n    ])\n\n    # Mean Structure\n    tau = pm.Normal(\"tau\", mu=0, sigma=0.5, dims=('group', 'indicators'))   # observed intercepts \n    alpha = pm.Normal(\"alpha\", mu=0, sigma=0.5, dims=('group', 'latent'))       # latent means\n    M = pt.matmul(Lambda, inv_I_minus_B)  # group, indicators latent\n    mu_latent = pt.matmul(alpha[:, None, :], M.transpose(0, 2, 1))[:,:,0] # shape handling dummy axis and transpose M -&gt; group, latent, indicators\n    mu_y = pm.Deterministic(\"mu_y\", tau + mu_latent) # size = group, indicators\n\n    Sigma_y = []\n    for g in range(len(coords['group'])):\n        inv_lhs = inv_I_minus_B[g]\n        Sigma_y_g = Lambda @ inv_lhs @ Psi_zeta @ inv_lhs.T @ Lambda.T + Psi\n        Sigma_y.append(Sigma_y_g)\n    Sigma_y = pt.stack(Sigma_y)\n    _ = pm.MvNormal(\"likelihood\", mu=mu_y[grp_idx], cov=Sigma_y[grp_idx])\n\n\n\\[ \\mu_g = \\tau_{g} + \\Lambda(1 - B_{g})^{-1}\\alpha_{g}\\] \\[\\Sigma_{\\mathcal{y}} = \\Psi + \\\\ \\Lambda(I - \\color{blue}{B_{g}})^{-1}\\Psi_{\\gamma}(I - \\color{blue}{B_{g}})^{T}\\Lambda^{T} \\]\n\\[ \\mathcal{y} \\sim MvN(\\mu_{g}, \\Sigma_{y}^{g})\\]"
  },
  {
    "objectID": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#hierarchies-in-code",
    "href": "talks/pycon_ireland_2025/bayesian_workflow_sem.html#hierarchies-in-code",
    "title": "Bayesian Workflow with SEMs",
    "section": "Hierarchies in Code",
    "text": "Hierarchies in Code\nComplexity and Rich Parameterisation\n\n\ncoords['group'] = ['man', 'woman']\nwith pm.Model(coords=coords) as sem_model_hierarchical2:\n    \n    # --- Factor loadings ---\n    lambdas_1 = make_lambda('indicators_1', 'lambdas1', priors=[1, .5])\n    lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, .5])\n    lambdas_3 = make_lambda('indicators_3', 'lambdas3', priors=[1, .5])\n    lambdas_4 = make_lambda('indicators_4', 'lambdas4', priors=[1, .5])\n\n    Lambda = pt.zeros((12, 4))\n    Lambda = pt.set_subtensor(Lambda[0:3, 0], lambdas_1)\n    Lambda = pt.set_subtensor(Lambda[3:6, 1], lambdas_2)\n    Lambda = pt.set_subtensor(Lambda[6:9, 2], lambdas_3)\n    Lambda = pt.set_subtensor(Lambda[9:12, 3], lambdas_4)\n    Lambda = pm.Deterministic('Lambda', Lambda)\n\n    sd_dist = pm.Exponential.dist(1.0, shape=4)\n    chol, _, _ = pm.LKJCholeskyCov(\"chol_cov\", n=4, eta=2, sd_dist=sd_dist, compute_corr=True)\n\n    Psi_zeta = pm.Deterministic(\"Psi_zeta\", chol.dot(chol.T))\n    Psi = make_Psi('indicators')\n\n    Bs = []\n    for g in coords[\"group\"]:\n        B_g = make_B(group_suffix=f\"_{g}\")  # give group-specific names\n        Bs.append(B_g)\n    B_ = pt.stack(Bs)\n\n    latent_dim = len(coords['latent'])\n    I = pt.eye(latent_dim)\n\n    # invert (I - B_g) for each group\n    inv_I_minus_B = pt.stack([\n        pt.slinalg.solve(I - B_[g] + 1e-8 * I, I)\n        for g in range(len(coords[\"group\"]))\n    ])\n\n    # Mean Structure\n    tau = pm.Normal(\"tau\", mu=0, sigma=0.5, dims=('group', 'indicators'))   # observed intercepts \n    alpha = pm.Normal(\"alpha\", mu=0, sigma=0.5, dims=('group', 'latent'))       # latent means\n    M = pt.matmul(Lambda, inv_I_minus_B)  # group, indicators latent\n    mu_latent = pt.matmul(alpha[:, None, :], M.transpose(0, 2, 1))[:,:,0] # shape handling dummy axis and transpose M -&gt; group, latent, indicators\n    mu_y = pm.Deterministic(\"mu_y\", tau + mu_latent) # size = group, indicators\n\n    Sigma_y = []\n    for g in range(len(coords['group'])):\n        inv_lhs = inv_I_minus_B[g]\n        Sigma_y_g = Lambda @ inv_lhs @ Psi_zeta @ inv_lhs.T @ Lambda.T + Psi\n        Sigma_y.append(Sigma_y_g)\n    Sigma_y = pt.stack(Sigma_y)\n    _ = pm.MvNormal(\"likelihood\", mu=mu_y[grp_idx], cov=Sigma_y[grp_idx])\n\n\n\\[ \\mu_g = \\tau_{g} + \\Lambda(1 - B_{g})^{-1}\\alpha_{g}\\] \\[\\Sigma_{\\mathcal{y}} = \\Psi + \\\\ \\Lambda(I - \\color{blue}{B_{g}})^{-1}\\Psi_{\\gamma}(I - \\color{blue}{B_{g}})^{T}\\Lambda^{T} \\]\n\\[ \\mathcal{y} \\sim MvN(\\mu_{g}, \\Sigma_{y}^{g})\\]"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-pet-shop-analogy",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-pet-shop-analogy",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Pet Shop Analogy",
    "text": "The Pet Shop Analogy\n\n\nThe Question: “Does this new fish survive?”\nThe Traditional View: Focus on the intrinsic properties of the fish (the treatment).\nThe Structural View: The fish’s survival depends on the tank—the pH balance, the predators, the temperature.\nThe Insight: When we ask about a causal effect, we are not asking about an isolated variable. We are asking: What world are we operating in?\n\n\n\nImagine a causal analyst as a pet-shop owner introducing a new fish to one of their many aquariums.\nThe new fish’s survival and behavior depend less on its intrinsic properties than on how it fits within this complex, interconnected system.\nIn causal inference, we must ask: In which tank will the new fish thrive? To answer this, we must model the tank, not just the fish."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#two-schools-of-thought",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#two-schools-of-thought",
    "title": "Bayesian Structural Causal Inference",
    "section": "Two Schools of Thought",
    "text": "Two Schools of Thought\n\n\nMinimalism\n(Design-Based)\n\nGoal: Isolate the effect by minimizing assumptions.\nTools: RCTs, Instrumental Variables, Diff-in-Diff.\nThe Vibe: “Don’t model the tank; just find a clean experiment within it.”\n\n\nMaximalism\n(Structural)\n\nGoal: Explicitly parameterize the system.\nTools: Bayesian Structural Models (SCMs).\nThe Vibe: “Model the physics of the tank to understand how the fish survives.”"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-bayesian-move-backwards-forwards",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-bayesian-move-backwards-forwards",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Bayesian Move: Backwards & Forwards",
    "text": "The Bayesian Move: Backwards & Forwards\nStep 1: Backwards (Inference) We infer the most plausible state of the world (\\(w\\)) conditioned on observable data. \\[P(w | X, T, O)\\]\nStep 2: Forwards (Prediction) With this world in place, we simulate counterfactuals. \\[P(Y^* | w, do(T))\\]\n\nThis is a two-step move in the Bayesian paradigm.\nFirst, we infer “backwards”—what is the physics of this aquarium? What are the relations between the variables?\nOnce the “world” is defined by our posterior distribution, we move “forwards.” We play out scenarios. We assess the probabilistic predictive distribution of treatment and outcome in plausible counterfactual worlds."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#visualizing-the-structure",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#visualizing-the-structure",
    "title": "Bayesian Structural Causal Inference",
    "section": "Visualizing the Structure",
    "text": "Visualizing the Structure\n%%| fig-align: center graph LR U((UnobservedConfounders)) –&gt; T[Treatment] U –&gt; Y[Outcome] X[Covariates] –&gt; T X –&gt; Y T –&gt; Y\nstyle U fill:#f9f,stroke:#333,stroke-width:2px,stroke-dasharray: 5 5\nstyle T fill:#bbf,stroke:#333,stroke-width:2px\nstyle Y fill:#bbf,stroke:#333,stroke-width:2px"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-role-of-priors",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-role-of-priors",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Role of Priors",
    "text": "The Role of Priors\nIdentification by Structure\nIn the presence of confounding (e.g., \\(\\rho = 0.6\\)), standard models fail.\n\nThe Problem: Endogeneity inflates estimates. The model attributes outcome variation from unobserved factors to the treatment.\nThe Solution: Tight Priors or Shrinkage (Horseshoe, Spike-and-Slab).\nThe Result: We “regularize” the latent correlation, effectively limiting how much endogeneity can distort inference.\n\n\nWithout structural constraints or informative priors, the model attributes part of the outcome variation caused by unobserved factors to the treatment itself.\nModels that introduce structure through priors perform noticeably better. This helps isolate valid “instrument-like” components of variation. We are using our prior knowledge of the “tank” to constrain our estimates of the “fish.”"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#non-parametric-causal-inference",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#non-parametric-causal-inference",
    "title": "Bayesian Structural Causal Inference",
    "section": "Non-Parametric Causal Inference",
    "text": "Non-Parametric Causal Inference\nSometimes, the shape of the “tank” is too complex for linear equations.\nEnter BART (Bayesian Additive Regression Trees)\n\nFit a flexible model for \\(E[Y | X, T]\\).\nImpute \\(Y(1)\\): Set everyone to treated, predict outcomes.\nImpute \\(Y(0)\\): Set everyone to control, predict outcomes.\nCompute the difference.\n\nWe replace rigid parameters with flexible function approximation.\n\nWe might worry that parametric approaches hide the real lesson.\nNon-parametric functions like BART allows us to learn the correct expected value function without forcing a linear shape.\nEven if the structural parameter \\(\\alpha\\) is uninterpretable, if the model has learned the correct conditional expectation function, counterfactual imputation recovers the true causal effect."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#epistemic-modesty",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#epistemic-modesty",
    "title": "Bayesian Structural Causal Inference",
    "section": "Epistemic Modesty",
    "text": "Epistemic Modesty\n\n“Every causal model, like every fish tank, is a ‘small world’ whose regularities we can nurture but never universalize.”\n\nBayesian structural causal inference unites epistemic modesty with computational rigor.\nEach model is not a final map of the world. It is a provisional machine for generating causal understanding."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-pet-shop-problem",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-pet-shop-problem",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Pet Shop Problem",
    "text": "The Pet Shop Problem\n\n\nA causal analyst is like a pet-shop owner introducing a new fish to an aquarium.\nThe Question: “Does this fish survive?”\nWrong Focus: The fish’s intrinsic properties (the “Treatment” in isolation).\nRight Focus: The physics of the tank—pH, predators, currents (the “Structural” environment).\n\n\n\n\n\n\n\nThe Insight\n\n\n“What is the causal effect?” really asks:\n“In which world are we operating?”"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-systemic-view",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-systemic-view",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Systemic View",
    "text": "The Systemic View\nBut the fish’s survival depends less on its intrinsic properties than on the tank.\n\n\nThe pH balance.\nThe predators.\nThe temperature.\nThe hidden currents.\n\n\n The Insight: When we ask “What is the causal effect?”, we are not asking about an isolated variable. We are asking: “In which world are we operating?”"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-great-trade-off",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-great-trade-off",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Great Trade-Off",
    "text": "The Great Trade-Off\nWhy choose Structural Maximalism?\nThe Cost: Risk of Misspecification. If you get the physics of the tank wrong, your answers will be wrong.\nThe Reward: Transparency. Every assumption becomes an explicit, testable component rather than an implicit background condition. We trade robustness for a map of the mechanisms."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#step-1-inference-backwards",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#step-1-inference-backwards",
    "title": "Bayesian Structural Causal Inference",
    "section": "Step 1: Inference (Backwards)",
    "text": "Step 1: Inference (Backwards)\nFirst, we must understand the “physics” of the aquarium.\nWe infer the most plausible state of the world (\\(w\\)) conditioned on the observable data (\\(X, T, O\\)).\n\\[P(w | X, T, O)\\]\nWe are asking: Given the data we see, what must the causal graph look like?"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#step-2-prediction-forwards",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#step-2-prediction-forwards",
    "title": "Bayesian Structural Causal Inference",
    "section": "Step 2: Prediction (Forwards)",
    "text": "Step 2: Prediction (Forwards)\nOnce the “world” is defined by our posterior distribution, we move forwards.\nWe simulate Counterfactual Worlds.\n\\[P(Y^* | w, do(T))\\]\nWe are asking: In a world defined by these physics, what happens if we intervene?"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-problem-of-endogeneity",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-problem-of-endogeneity",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Problem of Endogeneity",
    "text": "The Problem of Endogeneity\nIn the real world, the propensity to take a treatment is often predicted by the same factors that determine the outcome.\n\nThe Bias: The model attributes outcome variation from unobserved factors (\\(U\\)) to the treatment (\\(T\\)).\nThe Result: We confuse the “currents” of the water with the “movement” of the fish."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#priors-as-structure",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#priors-as-structure",
    "title": "Bayesian Structural Causal Inference",
    "section": "Priors as Structure",
    "text": "Priors as Structure\nHow do we solve this without a Randomized Controlled Trial?\nWe use Information to constrain the Structure.\nBy placing tight priors on the correlation parameters (e.g., \\(\\rho\\)), we “regularize” the latent correlation. We effectively limit how much endogeneity is allowed to distort the inference.\nWe use our prior knowledge of the tank to constrain our estimates of the fish."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#beyond-prediction",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#beyond-prediction",
    "title": "Bayesian Structural Causal Inference",
    "section": "Beyond Prediction",
    "text": "Beyond Prediction\nIn causal inference, variable selection isn’t just about pruning noise.\nIt’s about discovering causal architecture.\n\nThe Question: Which variables are…\n\nConfounders (affect both T and Y)?\n\nInstruments (affect only T)?\n\nPredictors (affect only Y)?\n\n\n\nSparsity priors let the model learn exclusion restrictions from data."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#causal-discovery-via-shrinkage",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#causal-discovery-via-shrinkage",
    "title": "Bayesian Structural Causal Inference",
    "section": "Causal Discovery via Shrinkage",
    "text": "Causal Discovery via Shrinkage\nWhen we apply “sparsity priors” (like Horseshoe or Spike-and-Slab) to a system of equations, the model can discriminate between:\n\nInstruments: Variables that drive Treatment (\\(T\\)) but not Outcome (\\(Y\\)).\nConfounders: Variables that drive both.\n\nThe model effectively “learns” exclusion restrictions. It separates the levers that move the treatment from the confounders that muddy the waters."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#when-parameters-fail",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#when-parameters-fail",
    "title": "Bayesian Structural Causal Inference",
    "section": "When Parameters Fail",
    "text": "When Parameters Fail\nSometimes, the shape of the “tank” is too complex for linear equations.\nIf we force a linear line on a non-linear world, our structural parameter (\\(\\alpha\\)) collapses.\nThe Solution: Bayesian Additive Regression Trees (BART)."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#flexible-imputation",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#flexible-imputation",
    "title": "Bayesian Structural Causal Inference",
    "section": "Flexible Imputation",
    "text": "Flexible Imputation\nWith BART, we replace rigid parameters with flexible function approximation.\n\nFit a flexible model for \\(E[Y | X, T]\\).\nImpute \\(Y(1)\\): Set everyone to treated, predict outcomes.\nImpute \\(Y(0)\\): Set everyone to control, predict outcomes.\n\nEven if the “coefficient” is uninterpretable, if the model learns the shape of the tank, the imputed difference recovers the causal effect."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#final-thought",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#final-thought",
    "title": "Bayesian Structural Causal Inference",
    "section": "Final Thought",
    "text": "Final Thought\n\n“Every causal model, like every fish tank, is a ‘small world’ whose regularities we can nurture but never universalize.”\n\n\nOur task is not to master the ocean.\nOur task is to eliminate impossible tanks, explore plausible ones, and know when to change the water."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#design-vs.-structure",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#design-vs.-structure",
    "title": "Bayesian Structural Causal Inference",
    "section": "Design vs. Structure",
    "text": "Design vs. Structure\n\n\nMinimalism\nPhilosophy: Don’t model the tank—find clean experiments within it.\nGains: Robust to misspecification\nLoses: Mechanistic transparency\n\nMaximalism\nPhilosophy: Model the tank’s physics explicitly.\nGains: Transparent mechanisms\nLoses: Risk of misspecification\n\n\n\nThe Bayesian Move: Make misspecification explicit and tunable through prior specification."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#confounding-as-latent-correlation",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#confounding-as-latent-correlation",
    "title": "Bayesian Structural Causal Inference",
    "section": "Confounding as Latent Correlation",
    "text": "Confounding as Latent Correlation\nIn observational data, treatment propensity and outcomes share hidden causes.\n\\[\n\\begin{pmatrix} U \\\\ V \\end{pmatrix} \\sim \\mathcal{N}\\left(0, \\begin{bmatrix} \\sigma_U^2 & \\rho\\sigma_U\\sigma_V \\\\ \\rho\\sigma_U\\sigma_V & \\sigma_V^2 \\end{bmatrix}\\right)\n\\]\nwhere:\n\n\\(U\\) = outcome error\n\n\\(V\\) = treatment error\n\n\\(\\rho\\) = the correlation we don’t observe\n\n\nTraditional regression assumes \\(\\rho = 0\\). But what if it’s not?"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-bias-under-confounding",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-bias-under-confounding",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Bias Under Confounding",
    "text": "The Bias Under Confounding\n# True treatment effect: α = 3\nfor rho in np.linspace(-1, 1, 10):\n    data = simulate_data(alpha_true=3, rho=rho)\n    model = ols('Y ~ T + controls', data).fit()\n    estimated_alpha = model.params['T']\n\nWhen ρ = 0: OLS recovers truth (α̂ ≈ 3)\nWhen ρ ≠ 0: Estimates drift wildly\nEven controlling for all observed variables, the unobserved correlation creates bias."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#model-ρ-explicitly",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#model-ρ-explicitly",
    "title": "Bayesian Structural Causal Inference",
    "section": "Model ρ Explicitly",
    "text": "Model ρ Explicitly\nDon’t assume confounding away—model it as a parameter.\n# Joint model for treatment and outcome\nrho = pm.TruncatedNormal(\"rho\", mu=0, sigma=0.5, \n                         lower=-0.99, upper=0.99)\n\n# Correlated errors\ncov = build_covariance_matrix(sigma_U, sigma_V, rho)\nerrors = pm.MvNormal(\"errors\", mu=0, cov=cov)\n\n# Structural equations\nT = X @ beta_T + errors[1]\nY = alpha * T + X @ beta_Y + errors[0]\n\nNow \\(\\rho\\) is estimated from data, not assumed to be zero."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#ρ-as-a-sensitivity-dial",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#ρ-as-a-sensitivity-dial",
    "title": "Bayesian Structural Causal Inference",
    "section": "ρ as a Sensitivity Dial",
    "text": "ρ as a Sensitivity Dial\nBy varying priors on \\(\\rho\\), we stress test our conclusions:\n# No confounding (traditional assumption)\npriors = {\"rho\": [0.0, 0.001]}\n\n# Moderate confounding (weakly informative)\npriors = {\"rho\": [0.0, 0.5]}\n\n# Strong confounding (conservative)\npriors = {\"rho\": [0.4, 0.99]}\n\nThe Workflow:\nFit models under different \\(\\rho\\) priors.\n\nIf α is stable → conclusions are robust\n\nIf α varies → that variation is real uncertainty"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#spike-and-slab-for-discovery",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#spike-and-slab-for-discovery",
    "title": "Bayesian Structural Causal Inference",
    "section": "Spike-and-Slab for Discovery",
    "text": "Spike-and-Slab for Discovery\nA mixture of point mass (spike) and diffuse normal (slab):\n\\[\\beta_j = \\gamma_j \\cdot \\beta_j^{\\text{raw}}, \\quad \\gamma_j \\in [0,1]\\]\n\npi = pm.Beta(\"pi\", alpha=2, beta=2)\nbeta_raw = pm.Normal(\"beta_raw\", 0, 2, dims=\"features\")\ngamma = relaxed_bernoulli(\"gamma\", pi, temperature=0.1)\nbeta = pm.Deterministic(\"beta\", gamma * beta_raw)\n\n\nEffect: Irrelevant variables shrink to zero.\nGain: Model discovers which features are true confounders vs. noise."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#horseshoe-for-shrinkage",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#horseshoe-for-shrinkage",
    "title": "Bayesian Structural Causal Inference",
    "section": "Horseshoe for Shrinkage",
    "text": "Horseshoe for Shrinkage\nContinuous shrinkage with heavy tails:\n\\[\\beta_j = \\tau \\cdot \\tilde{\\lambda}_j \\cdot \\beta_j^{\\text{raw}}\\]\n\n\n\\(\\tau\\) = global shrinkage\n\n\\(\\tilde{\\lambda}_j\\) = local (per-coefficient) shrinkage\n\n\n\nEffect: Weak signals dampened, strong signals preserved.\nGain: Smoother than spike-and-slab, but less aggressive."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#coupling-ρ-with-variable-selection",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#coupling-ρ-with-variable-selection",
    "title": "Bayesian Structural Causal Inference",
    "section": "Coupling ρ with Variable Selection",
    "text": "Coupling ρ with Variable Selection\nThe Power Move: Combine confounding correction with variable selection.\nmodel = make_joint_model(\n    X, Y, T,\n    priors_type=\"spike_and_slab\",\n    priors={\"rho\": [0.0, 0.5]}\n)\n\nWhat happens?\n\nVariable selection discovers instruments (variables that predict T but not Y)\nStructural modeling uses those instruments to identify α\nThe \\(\\rho\\) parameter absorbs residual confounding\n\n\n\n\nData-driven instrumental variables through Bayesian regularization."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-bart-pitfall",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-bart-pitfall",
    "title": "Bayesian Structural Causal Inference",
    "section": "The BART Pitfall",
    "text": "The BART Pitfall\nBART (Bayesian Additive Regression Trees) = extremely flexible function approximation.\n\nIntuition: If the world is non-linear, use BART instead of linear models.\n\n\nThe Problem: Where you place BART matters enormously."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#bart-in-the-outcome-equation",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#bart-in-the-outcome-equation",
    "title": "Bayesian Structural Causal Inference",
    "section": "BART in the Outcome Equation",
    "text": "BART in the Outcome Equation\nmu_outcome = pmb.BART(\"mu_outcome\", X=X, Y=Y) + U\nY ~ Normal(alpha * T + mu_outcome, sigma)\n\nResult: Treatment effect collapses to zero (α̂ ≈ 0.1 when truth is 3.0).\n\n\nWhy? BART learns the total association X → Y.\nIf X predicts T, and T affects Y, BART absorbs the causal pathway.\nThe structural parameter α has nothing left to explain."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#bart-in-the-treatment-equation",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#bart-in-the-treatment-equation",
    "title": "Bayesian Structural Causal Inference",
    "section": "BART in the Treatment Equation",
    "text": "BART in the Treatment Equation\nmu_treatment = pmb.BART(\"mu_treatment\", X=X, Y=T) + V\nT ~ Bernoulli(logit(mu_treatment))\n\nResult: Treatment effect recovers correctly (α̂ ≈ 3.0).\n\n\nWhy? BART flexibly models selection into treatment.\nThe structural parameter α still captures treatment effect on outcome."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-general-principle",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-general-principle",
    "title": "Bayesian Structural Causal Inference",
    "section": "The General Principle",
    "text": "The General Principle\nSafe: Flexibility in the treatment equation\nDangerous: Flexibility in the outcome equation\n\nAny sufficiently flexible method faces this challenge:\n\nNeural networks\n\nGaussian processes\n\nSplines\n\nBART\n\n\n\n\nFlexibility absorbs causal variation we’re trying to attribute."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#does-quitting-smoking-cause-weight-gain",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#does-quitting-smoking-cause-weight-gain",
    "title": "Bayesian Structural Causal Inference",
    "section": "Does Quitting Smoking Cause Weight Gain?",
    "text": "Does Quitting Smoking Cause Weight Gain?\nResearch Question: NHEFS study—does quitting smoking cause weight gain?\nChallenge: Selection effects—who quits is not random.\n\nApproach:\n\nValidate model through parameter recovery\nFit multiple specifications with different ρ priors\nCompare conclusions across specifications"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#results-sensitivity-analysis",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#results-sensitivity-analysis",
    "title": "Bayesian Structural Causal Inference",
    "section": "Results: Sensitivity Analysis",
    "text": "Results: Sensitivity Analysis\n\n\n\nModel\nα̂ (Effect)\nρ̂ (Confounding)\n\n\n\n\nOLS\n3.3\n(not modeled)\n\n\nρ = 0\n5.1\n0.0 (fixed)\n\n\nLinear\n6.2\n-0.35\n\n\nSpike-Slab\n5.8\n-0.28\n\n\n\n\nKey Finding: Allowing \\(\\rho \\neq 0\\) increases the estimated effect.\nNegative \\(\\rho\\) suggests: factors that increase quitting propensity are associated with factors that reduce weight gain.\nBy modeling this correlation, α no longer absorbs it—yielding a larger treatment effect."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#three-questions-before-fitting",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#three-questions-before-fitting",
    "title": "Bayesian Structural Causal Inference",
    "section": "Three Questions Before Fitting",
    "text": "Three Questions Before Fitting\n1. Can I defend my causal structure theoretically?\nWrite down your DAG before your priors.\nData-driven selection can’t substitute for domain knowledge.\n\n2. How sensitive are my conclusions to structural assumptions?\nVary \\(\\rho\\) priors. Vary sparsity assumptions.\nIf conclusions shift dramatically, report that as uncertainty.\n\n\n3. Where have I placed flexibility?\nFlexibility in treatment equation: safe.\nFlexibility in outcome equation: dangerous."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-bayesian-contribution",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-bayesian-contribution",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Bayesian Contribution",
    "text": "The Bayesian Contribution\nWhat makes this uniquely Bayesian?\n\n1. Joint Structural Modeling\nTreatment and outcome modeled together, capturing covariance.\n\n\n2. ρ as Tunable Sensitivity\nConfounding made explicit and testable through priors.\n\n\n3. Variable Selection as Causal Discovery\nSparsity priors discover instruments and confounders.\n\n\n4. Uncertainty Throughout\nNo false precision—full posteriors for every quantity."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#structural-causal-inference",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#structural-causal-inference",
    "title": "Bayesian Structural Causal Inference",
    "section": "Structural Causal Inference",
    "text": "Structural Causal Inference\n\n\nDesign-Based Minimalism\nPhilosophy: Find clean experiments within the tank.\nStrength: Robust to misspecification\nLimitation: Mechanisms remain implicit\n\nStructural Maximalism\nPhilosophy: Model the tank’s physics explicitly.\nStrength: Transparent mechanisms\nLimitation: Risk of getting physics wrong\n\n\n\nThe Bayesian Commitment: Make assumptions explicit, then test how much your conclusions depend on them."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-invisible-threat",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-invisible-threat",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Invisible Threat",
    "text": "The Invisible Threat\n\n\n\n\n\n\nA Real Example\n\n\nYou controlled for age, education, income, health history—12 variables.\nYou ran your regression. You got p &lt; 0.05.\nAnd you’re still wrong.\nBecause motivation—the thing you can’t measure—drives both treatment and outcome.\n\n\n\n\nThe confounding isn’t in what we measure—it’s in what we don’t."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#confounding-as-hidden-correlation",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#confounding-as-hidden-correlation",
    "title": "Bayesian Structural Causal Inference",
    "section": "Confounding as Hidden Correlation",
    "text": "Confounding as Hidden Correlation\nTreatment and outcome share unobserved causes:\n\\[\n\\begin{pmatrix} U \\\\ V \\end{pmatrix} \\sim \\mathcal{N}\\left(0, \\begin{bmatrix} \\sigma_U^2 & \\rho\\sigma_U\\sigma_V \\\\ \\rho\\sigma_U\\sigma_V & \\sigma_V^2 \\end{bmatrix}\\right)\n\\]\nwhere:\n\n\\(U\\) = unobserved outcome drivers\n\n\\(V\\) = unobserved treatment drivers\n\n\\(\\rho\\) = the correlation we cannot see\n\n\nTraditional methods assume \\(\\rho = 0\\).\nBut confounders don’t respect our assumptions."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-bias-is-real",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-bias-is-real",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Bias is Real",
    "text": "The Bias is Real\n\nOLS estimates drift as ρ varies, even with full covariate controlEven controlling for all observed variables, estimates drift with hidden correlation.\n\n\nThe confounder lurks in the covariance structure."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#make-the-invisible-visible",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#make-the-invisible-visible",
    "title": "Bayesian Structural Causal Inference",
    "section": "Make the Invisible Visible",
    "text": "Make the Invisible Visible\nThe Bayesian move: Don’t assume confounding away—model it explicitly.\n# ρ becomes a parameter we estimate\nrho = pm.TruncatedNormal(\"rho\", mu=0, sigma=0.5, \n                         lower=-0.99, upper=0.99)\n\n# Joint model captures correlation\ncov = build_covariance_matrix(sigma_U, sigma_V, rho)\n\nNow \\(\\rho\\) has a posterior distribution.\nWe learn how much confounding exists from the data."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#discovering-causal-architecture",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#discovering-causal-architecture",
    "title": "Bayesian Structural Causal Inference",
    "section": "Discovering Causal Architecture",
    "text": "Discovering Causal Architecture\nBut which variables are the confounders? Which are safe to control for?\n\nVariable selection priors help the model discover structure:\n\nConfounders: Affect both treatment and outcome\nInstruments: Affect treatment only\n\nPredictors: Affect outcome only\n\n\n\n# Spike-and-slab: mixture of zero and non-zero\nbeta = gamma * beta_raw  # gamma ∈ [0,1] learned from data\n\n# Horseshoe: continuous shrinkage\nbeta = tau * lambda_tilde * beta_raw"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#structure-discovery-in-action",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#structure-discovery-in-action",
    "title": "Bayesian Structural Causal Inference",
    "section": "Structure Discovery in Action",
    "text": "Structure Discovery in Action\nmodel = make_joint_model(\n    X, Y, T,\n    priors_type=\"spike_and_slab\",\n    priors={\"rho\": [0.0, 0.5]}\n)\n\nWhat the model learns:\n\nWhich features predict treatment vs outcome (exclusion restrictions)\nHow much residual correlation remains (ρ)\nThe causal effect purged of confounding (α)\n\n\n\n\nData-driven discovery of causal architecture—grounded in theory through priors."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#a-cautionary-tale",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#a-cautionary-tale",
    "title": "Bayesian Structural Causal Inference",
    "section": "A Cautionary Tale",
    "text": "A Cautionary Tale\nFlexibility can destroy identification.\n\nUsing BART (highly flexible) in the outcome equation:\nmu_outcome = pmb.BART(\"mu_outcome\", X=X, Y=Y) + U\nY ~ Normal(alpha * T + mu_outcome, sigma)\nResult: Treatment effect collapses (α̂ ≈ 0.1 when truth is 3.0)\n\n\nWhy? BART learns total association. If covariates predict treatment, BART absorbs the causal pathway.\n\n\nThe lesson: Discovering structure requires constraints. Complete flexibility = no identification."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#stress-testing-with-priors",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#stress-testing-with-priors",
    "title": "Bayesian Structural Causal Inference",
    "section": "Stress Testing with Priors",
    "text": "Stress Testing with Priors\nOnce we’ve discovered structure, test its robustness.\nThe question: How much do conclusions depend on assumptions vs data?\n\nThe method: Vary priors on \\(\\rho\\) and observe how estimates shift.\n# Optimistic: minimal confounding\npriors_min = {\"rho\": [0.0, 0.2]}\n\n# Realistic: moderate confounding  \npriors_mod = {\"rho\": [0.0, 0.5]}\n\n# Conservative: strong confounding\npriors_max = {\"rho\": [0.4, 0.8]}"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-sensitivity-workflow",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-sensitivity-workflow",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Sensitivity Workflow",
    "text": "The Sensitivity Workflow\n\nPosterior distributions for α under different ρ priorsIf α is stable: Conclusions robust to confounding assumptions.\nIf α shifts substantially: Conclusion depends on beliefs about confounding."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#application-smoking-and-weight",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#application-smoking-and-weight",
    "title": "Bayesian Structural Causal Inference",
    "section": "Application: Smoking and Weight",
    "text": "Application: Smoking and Weight\nResearch Question: Does quitting smoking cause weight gain?\nChallenge: Who quits is not random.\n\nApproach:\nTest sensitivity to confounding assumptions."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#results-the-structure-matters",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#results-the-structure-matters",
    "title": "Bayesian Structural Causal Inference",
    "section": "Results: The Structure Matters",
    "text": "Results: The Structure Matters\n\n\n\nSpecification\nα̂ (Effect)\nρ̂ (Confounding)\n\n\n\n\nOLS (ignores ρ)\n3.3\n—\n\n\nFixed ρ = 0\n5.1\n0.0\n\n\nPrior: ρ ~ [0, 0.5]\n6.2\n-0.35\n\n\n+ Variable Selection\n5.8\n-0.28\n\n\n\n\nFinding: Allowing \\(\\rho \\neq 0\\) increases the estimated effect.\nInterpretation: Factors that increase quitting propensity (motivation, health-consciousness) are negatively correlated with weight gain drivers.\nBy modeling this correlation explicitly, α captures the true intervention effect."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#what-the-sensitivity-tells-us",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#what-the-sensitivity-tells-us",
    "title": "Bayesian Structural Causal Inference",
    "section": "What the Sensitivity Tells Us",
    "text": "What the Sensitivity Tells Us\nThe range of estimates (5.1 to 6.2) reflects genuine uncertainty about unobserved confounding.\n\nTraditional approach: Pick one model, report one number (3.3).\nBayesian approach: Report the sensitivity range—conclusions hold across reasonable confounding assumptions.\n\n\n\n\n\n\n\n\nScientific Integrity\n\n\nThe width of our uncertainty is not a weakness.\nIt’s an honest characterization of what the data can and cannot tell us."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#two-movements",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#two-movements",
    "title": "Bayesian Structural Causal Inference",
    "section": "Two Movements",
    "text": "Two Movements\n\nMovement 1: Pruning the Multiverse\n\nSpecify joint model for treatment and outcome\nUse variable selection to eliminate impossible worlds\nEstimate ρ within the remaining plausible worlds\n\nThis collapses infinite possibilities to tractable dimensions."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#three-questions-before-you-fit",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#three-questions-before-you-fit",
    "title": "Bayesian Structural Causal Inference",
    "section": "Three Questions Before You Fit",
    "text": "Three Questions Before You Fit\n1. What causal structure do I believe exists?\nDraw your DAG first. This defines which worlds are theoretically possible. Variable selection then eliminates those inconsistent with data.\n\n2. What range of confounding is plausible?\nUse domain knowledge to bound which universes are realistic. You’re defining the boundaries of the multiverse to explore.\n\n\n3. Where have I placed flexibility?\n\nFlexibility in treatment equation: safe (explores selection without expanding worlds)\nFlexibility in outcome equation: dangerous (explodes the multiverse, prevents elimination)"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-bayesian-discipline",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-bayesian-discipline",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Bayesian Discipline",
    "text": "The Bayesian Discipline\nWhat makes this distinctively Bayesian?\n\nNot: That we use priors (frequentists can too)\nNot: That we quantify uncertainty (bootstraps exist)\n\n\nBut: That we treat causal inference as progressive elimination across a multiverse.\nEach assumption, each prior, each variable selection choice eliminates worlds.\nWe explore what survives."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-virtue-of-transparency",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-virtue-of-transparency",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Virtue of Transparency",
    "text": "The Virtue of Transparency\nTraditional causal inference often hides assumptions:\n\nParallel trends “just hold”\nInstruments “are valid”\n\nConfounding “is controlled for”\n\n\nBayesian structural modeling makes every assumption an elimination criterion:\n\nVariable selection eliminates causal structures inconsistent with data\nPriors define boundaries of the explorable multiverse\nSensitivity analysis maps which worlds support which conclusions"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#epistemic-humility",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#epistemic-humility",
    "title": "Bayesian Structural Causal Inference",
    "section": "Epistemic Humility",
    "text": "Epistemic Humility\n\n“Every causal model, like every fish tank, is a ‘small world’ whose regularities we can nurture but never universalize.”\n\n\nThe confounder lurks in some corner of the multiverse.\nOur task is not to assume we’ve found it—it’s to systematically eliminate the worlds where it can’t be.\n\n\nVariable selection eliminates impossible causal structures.\nSensitivity analysis maps remaining possibilities given data."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#confounding-under-the-hood",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#confounding-under-the-hood",
    "title": "Bayesian Structural Causal Inference",
    "section": "Confounding Under the Hood",
    "text": "Confounding Under the Hood\n\nTwo-step inference:\n\nBackwards: Infer world state \\(w\\) given data\nForwards: Simulate counterfactuals given \\(w\\)"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-correlation-we-cant-see",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-correlation-we-cant-see",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Correlation We Can’t See",
    "text": "The Correlation We Can’t See\nTreatment and outcome share unobserved causes:\n\\[\n\\begin{pmatrix} U \\\\ V \\end{pmatrix} \\sim \\mathcal{N}\\left(0, \\begin{bmatrix} \\sigma_U^2 & \\rho\\sigma_U\\sigma_V \\\\ \\rho\\sigma_U\\sigma_V & \\sigma_V^2 \\end{bmatrix}\\right)\n\\]\nwhere:\n\n\\(U\\) = unobserved outcome drivers\n\n\\(V\\) = unobserved treatment drivers\n\n\\(\\rho\\) = the correlation we cannot see\n\n\nTraditional methods assume \\(\\rho = 0\\).\nBut confounders don’t respect our assumptions."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-causal-architecture",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-causal-architecture",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Causal Architecture",
    "text": "The Causal Architecture\n\nEach causal structure represents a different universe:\n\nUniverse A: X0, X1 are confounders; X2, X3 are instruments\nUniverse B: X0, X1, X2 are all confounders; X3 is noise\nUniverse C: Everything is a confounder\n\n\nThe question: Which universe do we inhabit?\nThe answer: Variable selection eliminates the implausible ones."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#discovering-structure-with-priors",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#discovering-structure-with-priors",
    "title": "Bayesian Structural Causal Inference",
    "section": "Discovering Structure with Priors",
    "text": "Discovering Structure with Priors\nVariable selection priors eliminate impossible worlds:\n\n\nSpike-and-Slab\n# Mixture: zero or non-zero\nbeta = gamma * beta_raw  \n# gamma ∈ [0,1] learned\nEliminates worlds where irrelevant variables matter.\n\nHorseshoe\n# Continuous shrinkage\nbeta = tau * lambda * beta_raw\nProgressively eliminates worlds as evidence accumulates against weak predictors.\n\n\n\nEach coefficient shrunk to zero eliminates an entire class of causal universes."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#when-flexibility-destroys-identification",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#when-flexibility-destroys-identification",
    "title": "Bayesian Structural Causal Inference",
    "section": "When Flexibility Destroys Identification",
    "text": "When Flexibility Destroys Identification\n\n\n\n\n\n\nThe BART Pitfall\n\n\nUsing “Infinite Flexibility” (BART/GPs) in the Outcome equation is dangerous.\n\n\n\nWhy? If the model can explain \\(Y\\) through any arbitrary “wiggle,” it will “soak up” the causal signal.\n\nOutcome Flexibility: The model hides the effect of \\(T\\) inside the flexibility of the curve. \\(\\alpha\\) becomes unidentifiable.\nTreatment Flexibility: Safe. It helps us model the “selection” process more accurately without obscuring the causal link."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#before-and-after",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#before-and-after",
    "title": "Bayesian Structural Causal Inference",
    "section": "Before and After",
    "text": "Before and After\n\n\nTraditional Reporting\n“Treatment effect: 3.3 (SE: 0.4)\np &lt; 0.001”\nSingle point estimate.\nConfounding assumed away.\n\nBayesian Sensitivity\n“Treatment effect ranges from 5.1 to 6.2 depending on confounding (ρ ∈ [0, 0.5]).\nConclusions robust to moderate unobserved confounding.”\n\n\nSame data. Different honesty."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#parameter-recovery-validation",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#parameter-recovery-validation",
    "title": "Bayesian Structural Causal Inference",
    "section": "Parameter Recovery Validation",
    "text": "Parameter Recovery Validation\n\nRecovered vs true parameters from synthetic dataModel accurately recovers known parameters—validates the inference machinery."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#results-sensitivity-reveals-truth",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#results-sensitivity-reveals-truth",
    "title": "Bayesian Structural Causal Inference",
    "section": "Results: Sensitivity Reveals Truth",
    "text": "Results: Sensitivity Reveals Truth\n\n\n\nSpecification\nα̂ (Effect)\nρ̂ (Confounding)\n\n\n\n\nOLS (ignores ρ)\n3.3\n—\n\n\nFixed ρ = 0\n5.1\n0.0\n\n\nPrior: ρ ~ [0, 0.5]\n6.2\n-0.35\n\n\n+ Variable Selection\n5.8\n-0.28\n\n\n\n\nKey Finding: Allowing \\(\\rho \\neq 0\\) increases the estimated effect.\nInterpretation: Unobserved factors that increase quitting propensity (motivation) are negatively correlated with weight gain. By modeling this, α captures the true intervention effect."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#what-sensitivity-tells-us",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#what-sensitivity-tells-us",
    "title": "Bayesian Structural Causal Inference",
    "section": "What Sensitivity Tells Us",
    "text": "What Sensitivity Tells Us\n\nThe joint posterior of (α, ρ) shows the trade-off:\n\nStronger negative ρ → larger treatment effect\nModel learns this structure from data\nRange (5.1 to 6.2) reflects genuine uncertainty\n\n\n\nThis variation is not weakness—it’s honest characterization of what data can tell us."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#two-movements-cont.",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#two-movements-cont.",
    "title": "Bayesian Structural Causal Inference",
    "section": "Two Movements (cont.)",
    "text": "Two Movements (cont.)\nMovement 2: Exploring What Remains\n\nVary priors on ρ to navigate surviving counterfactual universes\nObserve how treatment effects shift across plausible worlds\nIdentify which conclusions hold everywhere vs depend on confounding\n\nThis maps the territory we haven’t eliminated.\n\n\n\n\n\n\n\nThe Core Insight\n\n\nElimination + Exploration = Robust Causal Inference\nVariable selection prunes impossible worlds.\nSensitivity analysis navigates the possible ones."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#in-practice",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#in-practice",
    "title": "Bayesian Structural Causal Inference",
    "section": "In Practice",
    "text": "In Practice\nWhen you write your next causal analysis:\n\n1. Show what you’ve eliminated\nReport which causal structures were ruled out by variable selection.\n\n\n2. Map what remains\nShow treatment effects across surviving counterfactual worlds—not just one universe.\n\n\n3. Make elimination criteria explicit\nState which worlds you ruled out through theory vs through data.\n\n\n\nScience progresses by elimination. Show what you’ve ruled out, not just what you believe."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-call-to-action",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-call-to-action",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Call to Action",
    "text": "The Call to Action\nThe next time you see a causal analysis with a confident point estimate and no sensitivity analysis, ask:\n\n“Which worlds did you eliminate? Which did you explore?”\n\n\nIf they can’t answer, they’re not doing causal inference.\nThey’re picking one universe and hoping it’s theirs."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#pruning-the-multiverse",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#pruning-the-multiverse",
    "title": "Bayesian Structural Causal Inference",
    "section": "Pruning the Multiverse",
    "text": "Pruning the Multiverse\nBefore variable selection: Infinite possible causal structures\n\nDoes X₂ affect Y? (2 worlds: yes/no)\nDoes X₃ affect Y? (2 worlds: yes/no)\nDoes X₄ affect T? (2 worlds: yes/no)\n…\nWith p variables: 2^p possible worlds\n\n\nAfter variable selection: Sparse set of plausible structures\nThe model eliminates worlds inconsistent with data, leaving only those where the causal architecture is well-identified."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#structure-discovery-as-world-elimination",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#structure-discovery-as-world-elimination",
    "title": "Bayesian Structural Causal Inference",
    "section": "Structure Discovery as World Elimination",
    "text": "Structure Discovery as World Elimination\nmodel = make_joint_model(\n    X, Y, T,\n    priors_type=\"spike_and_slab\",\n    priors={\"rho\": [0.0, 0.5]}\n)\n\nWhat the model does:\n\nEliminates worlds where irrelevant variables are confounders (spike component)\nPreserves worlds where true confounders/instruments exist (slab component)\nMaps remaining worlds across different ρ values\n\n\n\n\n\n\n\n\n\nThe Power of Elimination\n\n\nVariable selection doesn’t just reduce overfitting.\nIt collapses the multiverse to navigable dimensions."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#from-one-world-to-many",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#from-one-world-to-many",
    "title": "Bayesian Structural Causal Inference",
    "section": "From One World to Many",
    "text": "From One World to Many\nTraditional causal inference asks: “What is THE causal effect?”\n\nBayesian structural inference asks: “What is the causal effect in EACH plausible world?”\n\n\nEach value of \\(\\rho\\) defines a different universe:\n\n\\(\\rho = 0\\): A world with no hidden confounding\n\\(\\rho = 0.3\\): A world with moderate correlation between unobservables\n\n\\(\\rho = 0.6\\): A world with strong confounding\n\n\n\n\nWe don’t know which world we’re in. So we explore them all."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#navigating-the-multiverse",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#navigating-the-multiverse",
    "title": "Bayesian Structural Causal Inference",
    "section": "Navigating the Multiverse",
    "text": "Navigating the Multiverse\nThe method: Treat \\(\\rho\\) as a coordinate system across possible worlds.\n# Map the universe of possible confounding\nworlds = {\n    \"optimistic\": {\"rho\": [0.0, 0.2]},   # minimal confounding\n    \"realistic\":  {\"rho\": [0.0, 0.5]},   # moderate confounding  \n    \"pessimistic\": {\"rho\": [0.4, 0.8]}   # strong confounding\n}\n\nfor world_name, priors in worlds.items():\n    model = make_joint_model(X, Y, T, priors=priors)\n    idata[world_name] = pm.sample(model)\n\nEach fit explores a different region of the counterfactual universe."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-multiverse-map",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-multiverse-map",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Multiverse Map",
    "text": "The Multiverse Map\n\nPosterior distributions for α across counterfactual worldsEach posterior is a world characterized by different confounding assumptions.\n\nIf worlds agree (α stable): Effect is robust across the universe.\nIf worlds diverge (α shifts): Effect depends on which world we inhabit."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#same-data-different-universes",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#same-data-different-universes",
    "title": "Bayesian Structural Causal Inference",
    "section": "Same Data, Different Universes",
    "text": "Same Data, Different Universes\n\n\nSingle-World View\n“Treatment effect: 3.3 (SE: 0.4)\np &lt; 0.001”\nAssumes we know which universe we’re in.\n(Spoiler: we don’t)\n\nMultiverse View\n“Treatment effect ranges from 5.1 to 6.2 across plausible worlds (ρ ∈ [0, 0.5]).\nEffect is robust to moderate confounding universes.”\n\n\nSame data. Different metaphysics."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#results-exploring-the-universe",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#results-exploring-the-universe",
    "title": "Bayesian Structural Causal Inference",
    "section": "Results: Exploring the Universe",
    "text": "Results: Exploring the Universe\n\n\n\nWorld\nα̂ (Effect)\nρ̂ (Confounding)\nUniverse Type\n\n\n\n\nOLS (ignores ρ)\n3.3\n—\nSingle world (assumed)\n\n\nρ = 0\n5.1\n0.0\nExogenous universe\n\n\nρ ~ [0, 0.5]\n6.2\n-0.35\nRealistic universe\n\n\n+ Selection\n5.8\n-0.28\nDiscovered universe\n\n\n\n\nInsight: Each specification explores a different counterfactual world.\nThe OLS result implicitly assumes an exogenous universe.\nThe Bayesian results map the actual territory."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#mapping-the-multiverse",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#mapping-the-multiverse",
    "title": "Bayesian Structural Causal Inference",
    "section": "Mapping the Multiverse",
    "text": "Mapping the Multiverse\nWe don’t estimate a point; we map a territory.\n\n\n\n\n# We explore different 'Universes' of confounding\npriors = {\n  \"Exogenous\": pm.DiracDelta(0),\n  \"Moderate\": pm.Normal(0, 0.2),\n  \"Deep Selection\": pm.TruncatedNormal(0.5, 0.1, lower=0)\n  ..."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#what-makes-this-bayesian",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#what-makes-this-bayesian",
    "title": "Bayesian Structural Causal Inference",
    "section": "What Makes This Bayesian",
    "text": "What Makes This Bayesian\nNot: That we use priors (frequentists can too)\nNot: That we quantify uncertainty (bootstraps exist)\n\nBut: That we treat causal inference as progressive elimination across a multiverse.\n\n\nEach assumption, each prior, each variable selection choice eliminates worlds.\nWe explore what survives."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#two-philosophies-of-the-tank",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#two-philosophies-of-the-tank",
    "title": "Bayesian Structural Causal Inference",
    "section": "Two Philosophies of the Tank",
    "text": "Two Philosophies of the Tank\n\n\nDesign-Based Minimalism\nPhilosophy: Find a “clean” corner of the tank (Natural Experiments).\nStrength: Robust to local errors.\nLimitation: If the whole tank is cloudy, you’re blind.\n\nStructural Maximalism\nPhilosophy: Model the pumps, filters, and chemistry explicitly.\nStrength: Explains why the fish survives.\nLimitation: If the physics model is wrong, the prediction fails.\n\n\n\nThe Bayesian Commitment: We don’t know the exact physics. We use data to eliminate impossible tank setups and explore the plausible ones."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-two-movements-of-inference",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-two-movements-of-inference",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Two Movements of Inference",
    "text": "The Two Movements of Inference\n\n\nCausal inference in a structural framework is a two-step dance:\n\nBackwards (Inference): Use observed Treatment (\\(T\\)) and Outcome (\\(Y\\)) to infer the hidden state of the world (\\(w\\)).\nForwards (Counterfactual): Use that world state to simulate what would happen if we changed \\(T\\)."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-invisible-threat-hidden-correlation",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-invisible-threat-hidden-correlation",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Invisible Threat: Hidden Correlation",
    "text": "The Invisible Threat: Hidden Correlation\nThe confounding isn’t just in what we measure—it’s in the covariance of the errors:\n\\[\n\\begin{pmatrix} U \\\\ V \\end{pmatrix} \\sim N\\left(0, \\begin{bmatrix} \\sigma_U^2 & \\rho\\sigma_U\\sigma_V \\\\ \\rho\\sigma_U\\sigma_V & \\sigma_V^2 \\end{bmatrix}\\right)\n\\]\n\n\n\\(\\rho = 0\\): The OLS assumption. \\(T\\) is “as-if” random.\n\\(\\rho \\neq 0\\): The Structural reality. Selection bias is modeled as a parameter."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-bias-is-real-ols-drift",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-bias-is-real-ols-drift",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Bias is Real: OLS Drift",
    "text": "The Bias is Real: OLS Drift\n\nOLS estimates drift as ρ variesIf we ignore \\(\\rho\\), our estimate of the effect (\\(\\alpha\\)) is a ghost. It’s not just “noisy”—it’s fundamentally misplaced because we’ve ignored the unobserved “currents” in the tank."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#discovering-structure-as-world-elimination",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#discovering-structure-as-world-elimination",
    "title": "Bayesian Structural Causal Inference",
    "section": "Discovering Structure as World Elimination",
    "text": "Discovering Structure as World Elimination\nHow do we find the “right” tank physics? We use Variable Selection Priors to collapse the multiverse.\n\n\nSpike-and-Slab\nThe Sieve: It forces coefficients to be exactly zero or significantly non-zero.\nResult: Eliminates worlds where irrelevant variables act as noise.\n\nHorseshoe\nThe Funnel: It provides global and local shrinkage.\nResult: Progressively prunes weak “paths” in the causal graph until only the strongest structural links remain."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#conclusion",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#conclusion",
    "title": "Bayesian Structural Causal Inference",
    "section": "Conclusion",
    "text": "Conclusion\nThe Bayesian Workflow:\n\nBackwards Movement: Use the joint model to infer the likely world state (\\(w, \\rho\\))\n.Elimination: Use Spike-and-Slab to prune impossible causal architectures.\nExploration: Map treatment effects across the “Multiverse” of plausible \\(\\rho\\) values. Three Questions for Any Analyst:\n“Which worlds did you eliminate?” (Variable selection/DAG pruning)- “Which worlds did you explore?” (Sensitivity analysis across \\(\\rho\\))-“Where did you put the flexibility?” (Did you accidentally bury \\(\\alpha\\) in a BART model?)"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#case-study-smoking-and-weight",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#case-study-smoking-and-weight",
    "title": "Bayesian Structural Causal Inference",
    "section": "Case Study: Smoking and Weight",
    "text": "Case Study: Smoking and Weight\n\n\n\n\n\n\n\n\nWorld\n\\(\\hat{\\alpha}\\) (Effect)\nInterpretation\n\n\n\n\nOLS (Naive)\n3.3\nIgnores the physics of selection.\n\n\nBayesian (\\(\\rho=0\\))\n5.1\nModels structural links, assumes no hidden bias.\n\n\nBayesian (\\(\\rho \\approx -0.3\\))\n6.2\nAccounts for the “unobserved” drivers of quitting.\n\n\n\n\n\n\n\n\n\n\nThe “Bias Gap”\n\n\nThe difference between 3.3 and 6.2 is the “Cost of Ignorance.” The OLS benchmark assumes conditional ignorability. The various joint models allow for latent confounding i.e. that the covariate profile used in the study is not sufficiently exhaustive to block all confounding."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-bayesian-workflow",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-bayesian-workflow",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Bayesian Workflow",
    "text": "The Bayesian Workflow\n\nBackwards Movement: Use the joint model to infer the likely world state (\\(w, \\rho\\)).\nElimination: Use Spike-and-Slab to prune impossible causal architectures.\nExploration: Map treatment effects across the “Multiverse” of plausible \\(\\rho\\) values.\n\n\n\n“Which worlds did you eliminate?” (Variable selection/DAG pruning)\n“Which worlds did you explore?” (Sensitivity analysis across \\(\\rho\\))\n“Where did you put the flexibility?” (Did you accidentally bury \\(\\alpha\\) in a BART model?)"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#three-questions-for-any-analyst",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#three-questions-for-any-analyst",
    "title": "Bayesian Structural Causal Inference",
    "section": "Three Questions for Any Analyst",
    "text": "Three Questions for Any Analyst\n\n“Which worlds did you eliminate?” (Variable selection/DAG pruning)\n“Which worlds did you explore?” (Sensitivity analysis across \\(\\rho\\))\n“Where did you put the flexibility?” (Did you accidentally bury \\(\\alpha\\) in a BART model?)"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#bayesian-inference",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#bayesian-inference",
    "title": "Bayesian Structural Causal Inference",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#bayesian-inference-and-probable-worlds",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#bayesian-inference-and-probable-worlds",
    "title": "Bayesian Structural Causal Inference",
    "section": "Bayesian Inference and Probable Worlds",
    "text": "Bayesian Inference and Probable Worlds"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#joint-structural-modelling",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#joint-structural-modelling",
    "title": "Bayesian Structural Causal Inference",
    "section": "Joint Structural Modelling",
    "text": "Joint Structural Modelling"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#sensitivity-parameters-for-unobserved-confound",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#sensitivity-parameters-for-unobserved-confound",
    "title": "Bayesian Structural Causal Inference",
    "section": "Sensitivity Parameters for Unobserved Confound",
    "text": "Sensitivity Parameters for Unobserved Confound"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#variable-selection-priors-and-instrument-discovery",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#variable-selection-priors-and-instrument-discovery",
    "title": "Bayesian Structural Causal Inference",
    "section": "Variable Selection Priors and Instrument Discovery",
    "text": "Variable Selection Priors and Instrument Discovery"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#identification-denied",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#identification-denied",
    "title": "Bayesian Structural Causal Inference",
    "section": "Identification Denied",
    "text": "Identification Denied\n\nBART outcome model absorbs the structural distinction between treatment and other covariates, rendering the treatment effect surplus to purpose. The signal is swallowed by the noise-handler."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#wrong-turns-in-the-multiverse",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#wrong-turns-in-the-multiverse",
    "title": "Bayesian Structural Causal Inference",
    "section": "Wrong turns in the Multiverse",
    "text": "Wrong turns in the Multiverse\n\n\n\n\n\n\nThe BART Pitfall\n\n\nUsing “Infinite Flexibility” (BART/GPs) in the Outcome equation is dangerous.\n\n\n\nWhy? If the model can explain \\(Y\\) through any arbitrary “wiggle,” it will “soak up” the causal signal.\n\nOutcome Flexibility: The model hides the effect of \\(T\\) inside the flexibility of the curve. \\(\\alpha\\) becomes unidentifiable.\nTreatment Flexibility: Safe. It helps us model the “selection” process more accurately without obscuring the causal link."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-bayesian-causal-workflow",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-bayesian-causal-workflow",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Bayesian Causal Workflow",
    "text": "The Bayesian Causal Workflow\n\nBackwards Movement: Use the joint model to infer the likely world state (\\(w, \\rho\\)).\nElimination: Use Spike-and-Slab/Horsehoe to prune implausible causal architectures.\nExploration: Map treatment effects across the “Multiverse” of plausible \\(\\rho\\) values.\n\n\n\n“Which worlds did you eliminate?” (Variable selection/DAG pruning)\n“Which worlds did you explore?” (Sensitivity analysis across \\(\\rho\\))\n“Where did you put the flexibility?” (Did you ensure identifiability?)"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#causal-inference-and-counterfactual-worlds",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#causal-inference-and-counterfactual-worlds",
    "title": "Bayesian Structural Causal Inference",
    "section": "Causal Inference and Counterfactual Worlds",
    "text": "Causal Inference and Counterfactual Worlds"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#testing-robustness",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#testing-robustness",
    "title": "Bayesian Structural Causal Inference",
    "section": "Testing Robustness",
    "text": "Testing Robustness\n\nWe assess the range of treatment effects under a variety of model specifications including a BART treatment equation. Results suggest some unmeasured confounding distorts the OLS estimate.\n\n\n\n\n\n\nThe Hard Truth\n\n\nWe can’t prove \\(\\alpha &gt; 3.3\\) But we can: (1) Show it’s plausible given domain knowledge (2) Map the range of effects across ρ values (3) Make robust decisions that work across scenarios"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#preliminaries",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#preliminaries",
    "title": "Bayesian Structural Causal Inference",
    "section": "Preliminaries",
    "text": "Preliminaries\n\n\n\n\n\n\nWho am I?\n\n\n\nI’m a data scientist at Personio\n\nBayesian statistician,\nReformed philosopher and logician.\n\nWebsite: https://nathanielf.github.io/\n\n\n\n\n\n\n\n\n\n\nCode or it didn’t Happen\n\n\nThe worked examples used here can be found here\n\n\n\n\nMy Website"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-pitch-world-building-matters",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-pitch-world-building-matters",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Pitch: World-Building Matters",
    "text": "The Pitch: World-Building Matters\nScientific success hinges on understanding that outcomes are driven by hidden structures, unobserved correlations, and selection mechanisms.\nStructural causal modelling allows you to simulate system interventions safely, mapping the multiverse of potential outcomes to find the one world where your strategy actually works, and quantify where it might."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#agenda",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#agenda",
    "title": "Bayesian Structural Causal Inference",
    "section": "Agenda",
    "text": "Agenda\n\nThe Metaphor: The Physics of the Fish Tank\nThe Invisible Threat: Hidden Confounding and \\(\\rho\\)\nThe Pruning: Variable Selection as World Elimination\nThe Pitfalls: Why Total Flexibility Destroys Identification\nThe Navigation: Mapping the Multiverse of Results"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#parameter-recovery",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#parameter-recovery",
    "title": "Bayesian Structural Causal Inference",
    "section": "Parameter Recovery",
    "text": "Parameter Recovery\n\n\n\n\nfixed_parameters = {\n    \"rho\": 0.6,\n    \"alpha\": 3,\n    \"beta_O\": [0, 1, 0.4, 0.3, 0.1, 0.8, 0, 0, 0, 0, 0, 0, 3, 0],\n    \"beta_T\": [1, 1.3, 0.5, 0.3, 0.7, 1.6, 0, 0.4, 0, 0, 0, 0, 0, 0],\n}\nwith pm.do(nhefs_binary_model, fixed_parameters) as synthetic_model:\n    idata = pm.sample_prior_predictive(\n        random_seed=1000\n    )  # Sample from prior predictive distribution.\n    synthetic_y = idata[\"prior\"][\"likelihood_outcome\"].sel(draw=0, chain=0)\n    synthetic_t = idata[\"prior\"][\"likelihood_treatment\"].sel(draw=0, chain=0)\n# Infer parameters conditioned on observed data\nwith pm.observe(\n    nhefs_binary_model,\n    {\"likelihood_outcome\": synthetic_y, \"likelihood_treatment\": synthetic_t},\n) as inference_model:\n    idata_sim = pm.sample_prior_predictive()\n    idata_sim.extend(pm.sample(random_seed=100, chains=4, tune=2000, draws=500))"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#identification-achieved",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#identification-achieved",
    "title": "Bayesian Structural Causal Inference",
    "section": "Identification Achieved",
    "text": "Identification Achieved\n\n\n\n\n\n\n\nCore Principle\n\n\nFlexibility (capturing complex patterns) and Identification (isolating causal effects) are in tension. Structure the role flexible models well in causal inference."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#who-should-care-about-this",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#who-should-care-about-this",
    "title": "Bayesian Structural Causal Inference",
    "section": "Who Should Care About This?",
    "text": "Who Should Care About This?\n\n\nYou’re in the right place if you’re asking:\n\n“Did our intervention actually work, or did selection bias fool us?”\n“How sensitive are our conclusions to unobserved confounding?”\n“Can we simulate policy changes before deploying them?”\n“Which variables truly matter for our causal story?”\n\n\nYou’ll need familiarity with:\n\nBasic causal inference (treatment effects, confounding)\nRegression modeling (OLS, interpreting coefficients)\nBayesian thinking (priors, posteriors, uncertainty)\nSome Python/PyMC (though concepts transfer)\n\n\n\n\n\n\n\n\nThe Promise\n\n\nBy the end, you’ll understand how to systematically explore causal uncertainty rather than just hoping your assumptions hold."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-core-structural-model",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-core-structural-model",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Core Structural Model",
    "text": "The Core Structural Model\nInstead of treating treatment as exogenous, we model the joint data-generating process:\n\n\nOutcome Equation:\n\\[Y_i = \\alpha T_i + \\beta' X_i + U_i\\]\n\n\\(\\alpha\\) = causal effect (what we want)\n\\(X_i\\) = observed covariates\n\\(U_i\\) = unobserved factors affecting outcome\n\nTreatment Equation:\n\\[T_i = \\gamma' Z_i + V_i\\]\n\n\\(Z_i\\) = variables predicting treatment selection\n\\(V_i\\) = unobserved factors affecting treatment choice\n\n\nThe Key Question:\nAre \\(U_i\\) and \\(V_i\\) correlated?\n\\[\\text{Cov}(U_i, V_i) = \\rho \\sigma_U \\sigma_V\\]\n\nIf \\(\\rho = 0\\): Traditional methods work fine\nIf \\(\\rho \\neq 0\\): We have hidden confounding\n\n\n\n\n\n\n\nImportant\n\n\nThis is what OLS assumes away. We’re going to estimate it instead."
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#the-identification-tightrope",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#the-identification-tightrope",
    "title": "Bayesian Structural Causal Inference",
    "section": "The Identification Tightrope",
    "text": "The Identification Tightrope\n\n\n\n\n\n\nCore Principle\n\n\nFlexibility (capturing complex patterns) and Identification (isolating causal effects) are in tension.\n\n\n\n\n\nWhere Flexibility Helps:\nTreatment Equation: ✅\nT = f(Z) + V  # f can be complex!\n\nCaptures selection mechanisms accurately\nImproves inference about who selects in\nDoesn’t threaten α identification\n\nExample: BART, GAMs, splines for treatment prediction\n\nWhere Flexibility Destroys:\nOutcome Equation: ⚠️\nY = α·T + g(X) + U  # If g is too flexible...\n\nCan “explain away” treatment effect\nα becomes unidentified\nModel says: “Effect is just non-linearity in X”\n\nExample: BART for outcome can absorb causal signal into the g(X) term\n\nNext: We’ll see this failure mode empirically…"
  },
  {
    "objectID": "talks/scm_with_vs_priors/scm_causalpy.html#what-does-the-bias-gap-tell-us",
    "href": "talks/scm_with_vs_priors/scm_causalpy.html#what-does-the-bias-gap-tell-us",
    "title": "Bayesian Structural Causal Inference",
    "section": "What Does the “Bias Gap” Tell Us?",
    "text": "What Does the “Bias Gap” Tell Us?\n\n\n\n\n\n\n\n\n\nApproach\n\\(\\hat{\\alpha}\\)\nImplicit Assumption\nWhat It Means\n\n\n\n\nOLS\n3.3\n\\(\\rho = 0\\) exactly\n“Covariates perfectly block confounding”\n\n\nBayesian (ρ~Uniform)\n5.1\n\\(\\rho\\) could be anything\n“Agnostic about hidden confounding”\n\n\nBayesian (ρ~N(-0.3,0.1))\n6.2\nLikely negative confounding\n“Easier quitters have faster metabolism”\n\n\n\n\n\nThe 3.3 → 6.2 Gap Represents:\n\nUnmeasured health-consciousness that predicts both quitting ease and weight regulation\nGenetic factors affecting both nicotine addiction and metabolism\nSocioeconomic variables we didn’t capture perfectly\n\nPolicy Implication: If we designed smoking cessation programs based on 3.3, we’d underestimate benefits by 88%.\n\n\n\n\n\n\n\nThe Hard Truth\n\n\nWe can’t prove ρ = -0.3. But we can: 1. Show it’s plausible given domain knowledge 2. Map the range of effects across ρ values 3. Make robust decisions that work across scenarios"
  },
  {
    "objectID": "posts/post-with-code/AalenDynamicPaths/aalen_dynamic_paths.html",
    "href": "posts/post-with-code/AalenDynamicPaths/aalen_dynamic_paths.html",
    "title": "The Journey Is the Model: Dynamic Path Analysis in PyMC",
    "section": "",
    "text": "import pymc as pm\nimport numpy as np \nimport pandas as pd\nimport arviz as az\nimport pytensor.tensor as pt\nfrom scipy.interpolate import BSpline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\n\n# Suppress all FutureWarnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nIf you look to Odysseus on the morning the gates of Troy fell, he is well set up for a happy journey home. He is the architect of victory, his ships are loaded with spoils, and the wind is at his back. Yet, an odyssey can’t be completed in a single day and conclusions drawn on the outset rarely survive journey’s end.\nWhen we rely on static snapshots, like a single blood draw or a particular sales campaign, we are watching Odysseus board his ship and guessing how the story ends. We ignore the consequences emerging in time. This is an apt observation with which to begin the year. Will your new year’s resolutions, survive the January? This is the kind of question we’ll assess here."
  },
  {
    "objectID": "posts/post-with-code/AalenDynamicPaths/aalen_dynamic_paths.html#interventions-and-attenuated-effects-the-theory",
    "href": "posts/post-with-code/AalenDynamicPaths/aalen_dynamic_paths.html#interventions-and-attenuated-effects-the-theory",
    "title": "The Journey Is the Model: Dynamic Path Analysis in PyMC",
    "section": "Interventions and Attenuated Effects: The Theory",
    "text": "Interventions and Attenuated Effects: The Theory\nTo track how intention precedes and predicts evolving outcomes we need a statistical framework that doesn’t just record the departure but tracks the entire voyage. Enter Aalen’s Additive Model, formulated as a Dynamic Path Model. The question is when will we achieve our goals? When will Odysseus get home? How does the risk of attaining our goal vary over time?\nWhile traditional models, like the Cox Proportional Hazards or Accelerated Failure time models, often assume that an intervention’s effect is a constant “multiplier” throughout the study period, Aalen’s approach treats effects as a living process. It allows the impact of a policy or treatment to wax, wane, or even reverse as the narrative unfolds.\n\nThe Machinery of Change\nIn a dynamic path system, we decompose the total risk into a series of additive “layers.” If we are interested in how an intervention (\\(X\\)) works through a mediator (\\(M\\)), we model the hazard \\(\\lambda(t)\\) as:\n\\[\\lambda(t | X, M) = \\alpha_0(t) + \\alpha_1(t)X + \\alpha_2(t)M\\]\nWhere:\n\n\\(\\alpha_0(t)\\) is the Baseline Hazard, representing the background “tension” of the story.\n\\(\\alpha_1(t)\\) is the Time-Varying Direct Effect, showing how the intervention influences risk at every specific moment.\n\\(\\alpha_2(t)\\) is the Time-Varying Mediator Effect, capturing how the intermediate variable (the “storm” or the “detour”) contributes to the outcome.\n\nAll three components are modelled as time-varying functions that distil the effects over time.\n\n\nFrom Static to Dynamic: A Sequence of DAGs\nStandard causal inference often relies on a static Directed Acyclic Graph (DAG) to represent the “rules” of the system. But in a survival context, the DAG itself is dynamic. We can think of the model as a sequence of DAGs—one for each “scene” in our odyssey—where the causal arrows between \\(X\\), \\(M\\), and the Hazard (\\(\\lambda\\)) strengthen or weaken over time.\nWe can visualize this “filmstrip” of causality using networkx. We have a series of dynamic causal DAGs representing our assumptions of the relationships:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport networkx as nx\n\ndef plot_temporal_dag(stages=[0.1, 0.5, 0.9], labels=[\"Act I\", \"Act II\", \"Act III\"]):\n    fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n    \n    # Define nodes: Exposure, Mediator, Hazard\n    nodes = {'X': (0, 1), 'M': (1, 2), 'H': (2, 1)}\n    \n    for i, (stage, label) in enumerate(zip(stages, labels)):\n        G = nx.DiGraph()\n        G.add_nodes_from(nodes.keys())\n        \n        # We vary the weights to represent alpha_1(t) and alpha_2(t)\n        # Act I: Direct effect strong, Mediator weak\n        # Act III: Mediator dominates or effects attenuate\n        direct_w = 4 * (1 - stage) \n        indirect_w = 6 * stage\n        \n        edges = [('X', 'H', direct_w), ('X', 'M', 3), ('M', 'H', indirect_w)]\n        \n        for u, v, w in edges:\n            G.add_edge(u, v, weight=w)\n            \n        pos = nodes\n        nx.draw_networkx_nodes(G, pos, ax=axs[i], node_color='maroon', node_size=2000)\n        nx.draw_networkx_labels(G, pos, ax=axs[i], font_color='white', font_weight='bold')\n        \n        # Draw edges with widths corresponding to causal strength\n        weights = [G[u][v]['weight'] for u, v in G.edges()]\n        nx.draw_networkx_edges(G, pos, ax=axs[i], width=weights, \n                               edge_color='gray', arrowsize=30, connectionstyle=\"arc3,rad=0.1\")\n        \n        axs[i].set_title(f\"{label}\\n(Time-varying Causal Strengths)\", fontsize=14)\n        axs[i].axis('off')\n\n    plt.tight_layout()\n    return fig\n\nfig = plot_temporal_dag()\nfig.savefig(\"evolving_dag.png\")\n\n\n\nSeeing it this way makes it clearer that we are estimating a system of equations over time. To truly capture the journey, we cannot look at these variables in isolation. In our Odyssey, the “storm” (\\(M\\)) is not an independent accident; it is a consequence of the path Odysseus (\\(X\\)) chose to take. To model this, we must treat the intervention and the outcome as a system of simultaneous equations.\n\n\n\n\n\n\nDynamic Path Analysis and Causal Interpretation\n\n\n\nDynamic path analysis admits a causal interpretation only under sequential unconfoundedness, implied by the assumed temporal DAG. At each time point, conditional on the observed history, there are no unmeasured common causes of the treatment, the mediator process, and the event intensity.\nUnder this assumption, the time-varying coefficients in the additive hazard model can be interpreted causally: effects of treatment on the mediator, effects of the mediator on the hazard, and their product as a dynamic indirect effect. The path decomposition follows directly from the graph.\nIf relevant confounders are omitted, or if feedback from the event process into the mediator is not properly lagged, the decomposition remains algebraically valid but loses its causal meaning. The model estimates paths; the DAG and sequential unconfoundedness justify interpreting them as causal.\n\n\nBy solving these equations in tandem, we ensure that the mediator is not just another covariate, but a character with its own backstory, influenced by the intervention even as it influences the final risk.\n\nThe Mediator Equation (The Will of Poseidon)\nBefore we can calculate the risk of shipwreck, we must determine how the intervention has altered the environment. We model the mediator as a function of the exposure:\n\\[M_i = \\beta_0 + \\beta_{1}(t)X_i + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\]\nWhere \\(\\beta_{1}(t)\\) tells us how effectively the intervention “recruits” or “triggers” the mediator.\n\n\n2. The Hazard Equation (The Risk of Shipwreck)\nThe risk at time \\(t\\) is then a combination of the background noise, the direct path from \\(X\\), and the indirect path from the now-defined \\(M\\):\n\\[\\lambda(t | X, M) = g\\left( \\alpha_0(t) + \\alpha_1(t)X_i + \\alpha_2(t)M_i \\right)\\]\nwhere \\(g\\) is a linking function to translate the covariates into a hazard scale. Taken together these observations should make it clear why this modelling strategy is known as dynamic path analysis. We essentially have a structural equation model (SEM) with specific path coefficients that trace out the influence between variables. This means we are no longer ‘controlling for M’ but explicitly modeling how X creates M and how M creates risk. So we also require that we capture the time evolution of these relationships. It’s not enough to say of Odysseus that he had good intentions, it all ended well. We lose something if we don’t understand the journey.\n\n\n\nCausal Inference and Additive Effects\nAalen’s main concern was how to represent mechanisms that unfold in time without forcing them into a proportional straitjacket as in Cox style regressions. In many applied settings, an intervention does not exert a constant relative effect. Instead we act, with effects accumulating, and sometimes attenuated or disappearing over time.\nBy modeling covariate effects as increments to risk, the coefficients themselves become causal estimands. A time-varying coefficient \\(\\alpha_{1}(t)\\) answers a concrete question: what is the level of risk attributable to the exposure at time \\(t\\)? Once effects are additive, they can be decomposed and recombined. Direct, indirect, and total effects become sums of paths traced through time. The model reads naturally as a time-indexed causal graph with estimable path strengths.\n\n“We have a sequence of dynamic path models, one for each time t when we collect information. The estimation of each dynamic path model is done by recursive least squares regression as usual in path analysis” - Fosen et al in “Dynamic path analysis – a new approach to analyzing time-dependent covariates”\n\nThe central objects in dynamic path analysis are the time-varying regression functions that link treatment, mediator, and survival. In the canonical dpasurv formulation, these are the functions \\(\\beta_1(t), \\alpha_2(t)\\) and \\(\\alpha_1(t)\\), which parameterize the direct path from treatment to the mediator, the effect of mediator on the hazard, and the effect of the treatment on the hazard, respectively.\n\nCumulative Effects in dpasurv\nDynamic path analysis is concerned not with isolated coefficients, but with how these functions evolve over time. They form the building blocks from which direct, indirect, and total effects are constructed.\nBecause effects act continuously, interpretation is naturally expressed in cumulative terms. A cumulative coefficient is like a running total of a covariate’s effect on risk — it sums up, over time, how much that treatment or mediator has nudged the chance of an event happening. The cumulative direct effect up to time \\(t\\) is defined as\n\\[\\text{cumdir}(t) = \\int_0^t \\beta_1(s)\\, ds,\\]\nrepresenting the accumulated contribution of the treatment along the direct path to the hazard. In an additive hazards framework like Aalen’s model, we are directly estimating the cumulative coefficient function — the total accumulated effect up to time t, rather than first estimating the instaneous effect \\(\\beta_{1}(t_{i})\\) and then summing or integrating it over the index. Similarly, the cumulative indirect effect is defined as\n\\[\\text{cumind}(t) = \\int_0^t \\alpha_1(s)\\, \\beta_2(s)\\, ds,\\]\nwhich aggregates the mediated influence of treatment on survival. At each instant, the indirect effect is obtained by multiplying the strength of the treatment–mediator link with the strength of the mediator–hazard link, mirroring the logic of path analysis. A defining feature of dynamic path analysis is that these quantities satisfy an exact analytical decomposition:\n\\[\\text{cumtot}(t) = \\text{cumdir}(t) + \\text{cumind}(t).\\]\nThis identity holds because the model is additive. The dpasurv package demonstrates these decomposition with the following data.\n\ndf = pd.read_csv(\"aalen_simdata.csv\")\ndf = df[['subject', 'x', 'dose', 'M', 'start', 'stop', 'event']]\ndf.head()\n\n\n\n\n\n\n\n\nsubject\nx\ndose\nM\nstart\nstop\nevent\n\n\n\n\n0\n1\n0\nctrl\n6.74\n0\n4.00\n0\n\n\n1\n1\n0\nctrl\n6.91\n4\n8.00\n0\n\n\n2\n1\n0\nctrl\n6.90\n8\n12.00\n0\n\n\n3\n1\n0\nctrl\n6.71\n12\n26.00\n0\n\n\n4\n1\n0\nctrl\n6.45\n26\n46.85\n1\n\n\n\n\n\n\n\nThe data is an “long format” which is multiple rows per individual subject with running values for the state of the treatment indicator x and the mediator M, and crucially the even flag which denotes whether or not the terminal event occured. This is simulated data from the dpasurv r package, but the abstract names serve to illustrate the ubiquity of the mediator relationship! Even if you’re not Odysseus, it’s common knowledge that the Gods laugh when men make plans. Mediator structures are everywhere, and we will show how to handle them with dynamic path analysis.\n\n\n\nReplicating the dpasurv Benchmark\nThe figure below shows the cumulative direct, indirect, and total effects estimated by dpasurv from our simulated dataset. The direct effect (left panel) traces the immediate influence of the treatment on the outcome, while the indirect effect (middle panel) captures the pathway mediated through \\(M\\). The total effect (right panel) is simply the sum of the two.\n\nNotice the jumpy, step-like patterns in all three panels. This is characteristic of the dpasurv estimator, which produces cumulative coefficients by summing contributions at discrete event time intervals. Each step corresponds to a unique event time, with the height of the step reflecting the combined contribution of all events that occurred at that time. The gray lines indicate the approximate bootstrap confidence intervals, which widen as events become sparse, reflecting increasing uncertainty over time.\nThe main pattern we’re seeing here is that the direct effect of x in the first panel. The curve begins to dip below zero almost immediately and maintains a consistent downward slope, meaning the negative cumulative effect indicates that the intervention itself is actively reducing the hazard.. This is good, but contrast it with slight emerging effect in central panel. For the first 100 days, the mediator is a bystander. It is either not being triggered by the intervention or has no impact on survival. Around Day 100, the curve “breaks” and spikes upward. This represents a positive contribution to the hazard. The combination of the total effects reflects this too. The total effect become less negative (closer to zero) after Day 100. This tells us that while the intervention is still helpful overall, its net benefit is being attenuated by the mediator.\nThe dpasurv data is simulated to capture precisely this delayed impact, and these the patterns we will try and replicate in our Bayesian dynamic path model. Having established the theoretical architecture above, we now face the practical challenge of implementation. The elegance of Aalen’s additive model conceals three technical hurdles: discretizing continuous time, parameterizing smooth effects, and ensuring positive hazards. We’ll tackle each in turn"
  },
  {
    "objectID": "posts/post-with-code/AalenDynamicPaths/aalen_dynamic_paths.html#exploring-the-data",
    "href": "posts/post-with-code/AalenDynamicPaths/aalen_dynamic_paths.html#exploring-the-data",
    "title": "The Journey Is the Model: Dynamic Path Analysis in PyMC",
    "section": "Exploring the Data:",
    "text": "Exploring the Data:\nLet’s begin by looking at a simple cross-cut.\n\ndf.groupby(['x', 'dose'])[['event', 'M']].agg(['mean', 'sum'])\n\n\n\n\n\n\n\n\n\nevent\nM\n\n\n\n\nmean\nsum\nmean\nsum\n\n\nx\ndose\n\n\n\n\n\n\n\n\n0\nctrl\n0.164179\n66\n6.996915\n2812.76\n\n\n1\nhigh\n0.119205\n54\n8.081589\n3660.96\n\n\nlow\n0.139037\n52\n7.302620\n2731.18\n\n\n\n\n\n\n\nwhich shows a higher incidence of the event in the control group than the treatment group, but even within the treatment it seems a higher dosage is linked to lower incident rate. The size of the groups for each treatment level are approximately equal, but ignoring the dosage we see more people in the treatment group than the control.\n\nThe plot shows the timelines and the occurence of incidents for each treatment group. In most cases we see the terminal event occurs for each subject. Next we’ll try and pull out the differences due to dosage.\n\n\nCode\nmean_levels = df.groupby('subject')[['M', 'event', 'x', 'dose']].agg({'event': 'sum', 'M': 'mean', 'x': 'count', 'dose': 'first'}).rename({'M':'Mean_Subject_Mediator', 'x': 'Observation Periods', 'event': 'Terminal Event Y/N'}, axis=1)\nmean_levels\n\n# Assuming your dataframe is named 'df'\nplt.figure(figsize=(15, 6))\n\n\ng = sns.FacetGrid(mean_levels, col=\"dose\", hue=\"Terminal Event Y/N\", palette='viridis', height=5)\ng.map(sns.scatterplot, \"Observation Periods\", \"Mean_Subject_Mediator\", alpha=0.7)\ng.add_legend()\n\nplt.xlabel('Observation Duration (Periods)')\nplt.ylabel('Mean Subject Mediator')\nplt.legend(title='Terminal Event (1=Yes, 0=No)')\nplt.grid(True, linestyle='--', alpha=0.6)\nfig = plt.gcf()\nfig.savefig(\"Crosscut_MeanMediator.png\")\n\n\n\nThis plot shows in a crude way how time is the primary driver of the outcome with most terminal events clustering to the right longer treatment durations. There is also some visual evidence that higher mediator values have more terminal outcomes on the right hand side of the plot. But also, that higher dosage appears to have more surviving cases. But to really partial out these effects we need a more disciplined modelling approach.\nWith these exploratory insights in hand, we now turn to the technical implementation.\n\nData Preparation: Discretising time\nThis function restructures the original start–stop survival data into a finite set of time bins, where each bin corresponds to a unique (start, stop) interval observed in the data representation. By assigning every subject–interval to a discrete time index, we obtain a representation in which all time-varying quantities are constant within bins but allowed to vary across bins.\nThis discretisation step is central to our approach. Rather than treating regression coefficients as fully continuous functions of time, we work with piecewise constant approximations indexed by these bins. Conceptually, this mirrors the classical Aalen estimator, where coefficient paths only change at observed event times and remain flat in between.\n\ndef prepare_aalen_dpa_data(\n    df,\n    subject_col=\"subject\",\n    start_col=\"start\",\n    stop_col=\"stop\",\n    event_col=\"event\",\n    x_col=\"x\",\n    m_col=\"M\",\n):\n    \"\"\"\n    Prepare Andersen–Gill / Aalen dynamic path data for PyMC.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Long-format start–stop survival data\n    subject_col : str\n        Subject identifier\n    start_col, stop_col : str\n        Interval boundaries\n    event_col : str\n        Event indicator (0/1)\n    x_col : str\n        Exposure / treatment\n    m_col : str\n        Mediator measured at interval start\n\n    Returns\n    -------\n    dict\n        Dictionary of numpy arrays ready for PyMC\n    \"\"\"\n\n    df = df.copy()\n\n    # -------------------------------------------------\n    # 1. Basic quantities\n    # -------------------------------------------------\n    df[\"dt\"] = df[stop_col] - df[start_col]\n\n    if (df[\"dt\"] &lt;= 0).any():\n        raise ValueError(\"Non-positive interval lengths detected.\")\n\n    N = df[event_col].astype(int).values\n    Y = np.ones(len(df), dtype=int)  # Andersen–Gill at-risk indicator\n\n    # -------------------------------------------------\n    # 2. Time-bin indexing (piecewise-constant effects)\n    # -------------------------------------------------\n    bins = (\n        df[[start_col, stop_col]]\n        .drop_duplicates()\n        .sort_values([start_col, stop_col])\n        .reset_index(drop=True)\n    )\n    bins[\"bin_idx\"] = np.arange(len(bins))\n\n    df = df.merge(\n        bins,\n        on=[start_col, stop_col],\n        how=\"left\",\n        validate=\"many_to_one\"\n    )\n\n    bin_idx = df[\"bin_idx\"].values\n    n_bins = bins.shape[0]\n\n    # -------------------------------------------------\n    # 3. Center covariates (important for Aalen models)\n    # -------------------------------------------------\n    df[\"x_c\"] = df[x_col]\n    df[\"m_c\"] = df[m_col] - df[m_col].mean()\n\n    x = df[\"x_c\"].values\n    m = df[\"m_c\"].values\n\n    # -------------------------------------------------\n    # 4. Predictable mediator (lag within subject)\n    # -------------------------------------------------\n    df = df.sort_values([subject_col, start_col])\n\n    df[\"m_lag\"] = (\n        df.groupby(subject_col)[\"m_c\"]\n          .shift(1)\n          .fillna(0.0)\n    )\n\n    m_lag = df[\"m_lag\"].values\n\n    df[\"I_low\"]  = (df[\"dose\"] == \"low\").astype(int)\n    df[\"I_high\"] = (df[\"dose\"] == \"high\").astype(int)\n\n    # -------------------------------------------------\n    # 5. Assemble output\n    # -------------------------------------------------\n    data = {\n        \"bins\": bins,     # required discretisation\n        \"df_long\": df     # feature set for time-varying features\n    }\n\n    return data\n\nIn addition to defining the time bins, the function constructs predictable covariates—most notably a lagged mediator within subject—and performs the centering required for additive hazard models.\n\ndata = prepare_aalen_dpa_data(df)\ndf_long = data['df_long']\ndf_long[['subject', 'x', 'dose', 'm_c', 'm_lag', 'event', 'dt', 'bin_idx']].head(14)\n\n\n\n\n\n\n\n\nsubject\nx\ndose\nm_c\nm_lag\nevent\ndt\nbin_idx\n\n\n\n\n0\n1\n0\nctrl\n-0.749748\n0.000000\n0\n4.00\n7\n\n\n1\n1\n0\nctrl\n-0.579748\n-0.749748\n0\n4.00\n13\n\n\n2\n1\n0\nctrl\n-0.589748\n-0.579748\n0\n4.00\n23\n\n\n3\n1\n0\nctrl\n-0.779748\n-0.589748\n0\n14.00\n53\n\n\n4\n1\n0\nctrl\n-1.039748\n-0.779748\n1\n20.85\n81\n\n\n5\n2\n1\nhigh\n-1.379748\n0.000000\n0\n4.00\n7\n\n\n6\n2\n1\nhigh\n-1.209748\n-1.379748\n0\n4.00\n13\n\n\n7\n2\n1\nhigh\n-0.449748\n-1.209748\n0\n4.00\n23\n\n\n8\n2\n1\nhigh\n-0.559748\n-0.449748\n0\n14.00\n53\n\n\n9\n2\n1\nhigh\n0.370252\n-0.559748\n0\n26.00\n89\n\n\n10\n2\n1\nhigh\n0.980252\n0.370252\n0\n26.00\n115\n\n\n11\n2\n1\nhigh\n1.420252\n0.980252\n0\n26.00\n137\n\n\n12\n2\n1\nhigh\n1.500252\n1.420252\n0\n52.00\n162\n\n\n13\n2\n1\nhigh\n1.870252\n1.500252\n0\n104.00\n188\n\n\n\n\n\n\n\nOnce time has been discretised into bins, the remaining challenge is how to allow effects to evolve smoothly across those bins. Rather than treating each interval as completely unrelated to its neighbours, we introduce a spline-based representation that ties adjacent time bins together.\n\n\nSpline Functions in Time\nThis code lets us approximate continuously varying coefficient functions using a flexible but structured set of basis functions defined over the discretised time axis.\n\ndef create_bspline_basis(n_bins, n_knots=10, degree=3):\n    \"\"\"\n    Create B-spline basis functions for smooth time-varying effects.\n    \n    Parameters\n    ----------\n    n_bins : int\n        Number of time bins\n    n_knots : int\n        Number of internal knots (fewer = smoother)\n    degree : int\n        Degree of spline (3 = cubic, recommended)\n    \n    Returns\n    -------\n    basis : np.ndarray\n        Matrix of shape (n_bins, n_basis) with basis function values\n    \"\"\"\n    # Create knot sequence\n    # Internal knots equally spaced across time range\n    internal_knots = np.linspace(0, n_bins-1, n_knots)\n    \n    # Add boundary knots (repeated degree+1 times for clamped spline)\n    knots = np.concatenate([\n        np.repeat(internal_knots[0], degree),\n        internal_knots,\n        np.repeat(internal_knots[-1], degree)\n    ])\n    \n    # Number of basis functions\n    n_basis = len(knots) - degree - 1\n    \n    # Evaluate each basis function at each time point\n    t = np.arange(n_bins, dtype=float)\n    basis = np.zeros((n_bins, n_basis))\n    \n    for i in range(n_basis):\n        # Create coefficient vector (indicator for basis i)\n        coef = np.zeros(n_basis)\n        coef[i] = 1.0\n        \n        # Evaluate B-spline\n        spline = BSpline(knots, coef, degree, extrapolate=False)\n        basis[:, i] = spline(t)\n    \n    return basis\n\nn_knots = 10\nn_bins = data['bins'].shape[0]\nbasis = create_bspline_basis(n_bins, n_knots=n_knots, degree=3)\nn_cols = basis.shape[1]\nbasis_df = pd.DataFrame(basis, columns=[f'feature_{i}' for i in range(n_cols)])\nbasis_df.head(10)\n\n\n\n\n\n\n\n\nfeature_0\nfeature_1\nfeature_2\nfeature_3\nfeature_4\nfeature_5\nfeature_6\nfeature_7\nfeature_8\nfeature_9\nfeature_10\nfeature_11\n\n\n\n\n0\n1.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n0.863149\n0.133496\n0.003337\n0.000018\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n0.739389\n0.247518\n0.012946\n0.000146\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n0.628064\n0.343219\n0.028223\n0.000494\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n0.528515\n0.421749\n0.048566\n0.001170\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n5\n0.440083\n0.484261\n0.073370\n0.002286\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n6\n0.362110\n0.531908\n0.102032\n0.003950\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n7\n0.293939\n0.565840\n0.133949\n0.006272\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n8\n0.234909\n0.587211\n0.168518\n0.009362\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n9\n0.184365\n0.597171\n0.205134\n0.013330\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\nIn Aalen’s additive model, the primary objects of interest are cumulative regression coefficients: time-indexed functions that accumulate the effect of a covariate on the hazard as time progresses. Empirically, these functions are only identified at a discrete set of time points and are typically visualised as step functions that jump at event times.\nOur discretisation of time into bins provides a natural scaffold for working with these cumulative effects. Rather than estimating an unconstrained step height for every bin, we approximate each cumulative coefficient path as a smooth function of the bin index, represented as a linear combination of B-spline basis functions.\n\nfig, ax = plt.subplots()\nax.set_title(\"Random Spline Functions over Time\")\nfor i in range(5):\n    betas_spline = np.random.normal(0, 3, 12)\n    np.dot(basis_df, betas_spline)\n    ax.plot(np.dot(basis_df, betas_spline))\n\nfig.savefig(\"random_splines.png\")\n\n\nThe figure above illustrates this idea. Each curve corresponds to a different random draw of spline coefficients, combined with the same underlying basis. Although these curves are generated without reference to the data, they all resemble plausible cumulative coefficient paths: continuous, gradually evolving over time, and free of the abrupt jumps that arise when coefficients are estimated independently at each event time.\nThis perspective is important conceptually. Rather than viewing cumulative coefficients as the result of summing many instantaneous effects, we treat them as primary objects to be estimated directly, with smoothness acting as a regularising assumption. In the next section, we will show how this spline-based representation is embedded into a full probabilistic model."
  },
  {
    "objectID": "posts/post-with-code/AalenDynamicPaths/aalen_dynamic_paths.html#survival-models-and-their-likelihood",
    "href": "posts/post-with-code/AalenDynamicPaths/aalen_dynamic_paths.html#survival-models-and-their-likelihood",
    "title": "The Journey Is the Model: Dynamic Path Analysis in PyMC",
    "section": "Survival Models and their Likelihood",
    "text": "Survival Models and their Likelihood\nIn contemporary survival analysis, the adoption of the Poisson bridge is a strategic move to resolve an incompatibility between Bayesian inference and the “pseudo-likelihoods” that are often used in survival modelling.\n\nExplicit Parameterization via the Discrete Index\nThe most significant advantage of this approach is that it replaces the “nuisance” of the baseline hazard with an explicit parameterization of time. By discretizing the timeline into a finite index of bins, we transform a nebulous, continuous survival function into a structured set of parameters that a Bayesian sampler can navigate.\nIn our discrete-time framework, the baseline hazard is no longer hidden background noise. Instead, it is parameterized via the discrete index \\(t\\), allowing the model to “see” exactly how the background risk changes from the start of the study to its conclusion. This clear mapping provides a well-defined probability distribution for every observed data point:\n\\[d_{it} \\sim \\text{Poisson}(\\lambda(t))\\] \\[\\lambda(t) = Y_{it} \\cdot \\Delta t \\cdot \\lambda_{it}\\]\nWhere:\n\n\\(d_{it}\\) is the event indicator (0 or 1).\n\\(Y_{it}\\) is the “at-risk” indicator.\n\\(\\Delta t\\) is the duration of the time bin.\n\\(\\lambda_{it}\\) is the cumulative hazard.\n\nIt may seem odd to approximate binary events with a “counting” distribution, but viewed at a distance binary events are just counts capped at 1.\n\nFrom Ordering to Intensities\nPseudo-likelihoods primarily care about the ordering of events rather than the absolute scale of time. They effectively discard the actual duration of time between events in favour of focusing on who failed before whom. By parameterizing time via a discrete index, we move from mere rankings to intensities. We model the actual “velocity” of risk in every interval, which allows us to place Bayesian priors on the baseline hazard itself (such as Random Walks or Splines). This enforces a “temporal smoothness” that prevents the background risk from jumping erratically. This is the requirement for narrative coherence in Odysseus’ journey.\nIt is common in survival analysis to estimate covariate effects using a partial likelihood, which identifies regression coefficients without specifying the baseline hazard. In its canonical form, this likelihood depends only on the ordering of event times,\n\\[\nL_p(\\beta) = \\prod_{i:d_i=1}\n\\frac{\\exp(X_i \\beta)}{\\sum_{j \\in R(t_i)} \\exp(X_j \\beta)},\n\\]\nand does not define a joint probability model for the observed survival times themselves. As a result, the hazard function and cumulative risk must be reconstructed after estimation rather than arising from a generative model of time-to-event data.\nFor the purposes of dynamic path analysis, this is limiting. The approach developed here requires a likelihood that generates event times, accommodates time-varying effects, and allows direct simulation of hazards and counterfactual trajectories. We therefore adopt a full likelihood representation, which makes these quantities explicit and enables coherent Bayesian inference.\nThe Poisson likelihood specification solves this by reframing survival as a counting process over discretized time bins. By mapping the survival hazard to the mean of a Poisson distribution, we transition from an incomplete pseudo-likelihood to a full, generative likelihood:\n\\[d_{it} \\sim \\text{Poisson}(\\lambda(t)), \\quad \\text{where} \\quad \\lambda(t) = Y_{it} \\cdot \\Delta t \\cdot \\lambda_{it}\\]\nThis transformation turns the baseline hazard into an explicit, time-indexed parameter rather than a nuisance to be factored out. This structure is perfectly suited for Bayesian modeling; it provides a differentiable likelihood that allows the baseline and time-varying effects to be estimated simultaneously via MCMC, while enabling posterior predictive simulations that the partial likelihood approach cannot natively support.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Setup data\ntime_bins = np.arange(1, 11)\nbaseline_hazard = 0.1 * np.exp(0.2 * time_bins) + 0.5 * np.sin(0.8 * time_bins)\nevent_times = [2.2, 4.8, 8.5]\nevent_ranks = [1, 2, 3]\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\nplt.subplots_adjust(hspace=0.3)\n\n# --- Panel 1: Partial Likelihood (Ordering) ---\nax1.set_title(\"Partial Likelihood: Event Ordering\", fontsize=14, fontweight='bold')\nax1.hlines([1, 2, 3], 0, 10, colors='gray', linestyles='--', alpha=0.3)\nfor i, et in enumerate(event_times):\n    ax1.annotate('', xy=(et, i+1), xytext=(et, 3.5),\n                 arrowprops=dict(facecolor='red', shrink=0.05, width=2))\n    ax1.text(et, i+0.7, f\"Rank {event_ranks[i]}\", color='red', ha='center', fontweight='bold')\n\nax1.set_ylabel(\"Subject Index\")\nax1.set_ylim(0.5, 4)\nax1.set_yticklabels([])\nax1.text(5, 3.5, \"Focus: Who fails before whom?\\nAbsolute time and hazard shape are 'nuisance'.\", \n         bbox=dict(facecolor='white', alpha=0.8), ha='center')\n\n# --- Panel 2: Poisson Intensity (Explicit Parameterization) ---\nax2.set_title(\"Poisson Likelihood: Intensity Estimation (Discrete Bins)\", fontsize=14, fontweight='bold')\n\n# Draw the bins\nfor b in time_bins:\n    ax2.axvline(b, color='black', alpha=0.1, linestyle='-')\n    ax2.bar(b-0.5, baseline_hazard[b-1], width=0.9, alpha=0.3, color='blue', label='Bin Intensity' if b==1 else \"\")\n\n# Smooth hazard curve (the \"Explicit Parameter\")\nx_smooth = np.linspace(0.5, 9.5, 100)\ny_smooth = 0.1 * np.exp(0.2 * x_smooth) + 0.5 * np.sin(0.8 * x_smooth)\nax2.plot(x_smooth, y_smooth, color='blue', linewidth=3, label=r'Estimated $\\lambda(t)$')\n\nax2.set_xlabel(\"Time (Discrete Index)\")\nax2.set_ylabel(r\"Hazard Intensity $\\lambda$\")\nax2.legend()\nax2.text(5, 1.5, \"Focus: Risk intensity in each bin.\\nTime is an explicit parameter (t).\", \n         bbox=dict(facecolor='white', alpha=0.8), ha='center')\n\nplt.tight_layout()\n\nfig.savefig(\"poisson_approximation.png\")\n\n\n\nThe Poisson bridge treats time as an explicit covariate rather than a nuisance to be conditioned away. By mapping the hazard to a discrete index, we gain a “universal currency” of probability. This allows the baseline risk, the treatment effects, and the mediated paths to all live in the same mathematical space, estimated simultaneously through a single likelihood formulation. The Poisson formulation matters causally because it lets us specify a full data-generating process consistent with the DAG.\n\ndef make_model(data, basis, sample=True, observed=True): \n    df_long = data['df_long'].copy()\n    n_basis = basis.shape[1]\n    n_obs = data['df_long'].shape[0]\n    time_bins = data['bins']['bin_idx'].values\n    b = df_long['bin_idx']\n\n    observed_mediator = df_long[\"m_c\"].values\n    observed_events = df_long['event'].astype(int).values\n    observed_treatment = df_long['x'].astype(int).values\n    observed_mediator_lag = df_long['m_lag'].values\n\n    coords = {'tv': ['intercept', 'direct', 'mediator'], \n            'splines': ['spline_f_{i}' for i in range(n_basis)], \n            'obs': range(n_obs), \n            'time_bins': time_bins}\n\n    with pm.Model(coords=coords) as aalen_dpa_model:\n\n        trt = pm.Data(\"trt\", observed_treatment, dims=\"obs\")\n        med = pm.Data(\"mediator\", observed_mediator, dims=\"obs\")\n        med_lag = pm.Data(\"mediator_lag\", observed_mediator_lag, dims=\"obs\")\n        events = pm.Data(\"events\", observed_events, dims=\"obs\")\n        I_low  = pm.Data(\"I_low\",  df_long[\"I_low\"].values,  dims=\"obs\")\n        I_high = pm.Data(\"I_high\", df_long[\"I_high\"].values, dims=\"obs\")\n        dt = pm.Data(\"duration\", df_long['dt'].values, dims='obs')\n        ## because our long data format has a cell per obs\n        at_risk = pm.Data(\"at_risk\", np.ones(len(observed_events)), dims=\"obs\")\n        basis_ = pm.Data(\"basis\", basis, dims=('time_bins', 'splines') )\n\n        # -------------------------------------------------\n        # 1. B-spline coefficients for HAZARD model\n        # -------------------------------------------------\n        # Prior on spline coefficients\n        # Smaller sigma = less wiggliness\n        # Random Walk 1 (RW1) Prior for coefficients\n        # This is the Bayesian version of the smoothing penalty in R's 'mgcv' or 'timereg'\n        sigma_smooth = pm.Exponential(\"sigma_smooth\", [1, 1, 1], dims='tv')\n        beta_raw = pm.Normal(\"beta_raw\", 0, 1, dims=('splines', 'tv'))\n\n        # Cumulative sum makes it a Random Walk\n        # This ensures coefficients evolve smoothly over time\n        coef_alpha = pm.Deterministic(\"coef_alpha\", pt.cumsum(beta_raw * sigma_smooth, axis=0), dims=('splines', 'tv'))\n\n        # Construct smooth time-varying functions\n        alpha_0_t = pt.dot(basis_, coef_alpha[:, 0])\n        alpha_1_t = pt.dot(basis_, coef_alpha[:, 1])\n        alpha_2_t = pt.dot(basis_, coef_alpha[:, 2])\n        \n        # -------------------------------------------------\n        # 2. B-spline coefficients for MEDIATOR model\n        # -------------------------------------------------\n        sigma_beta_smooth = pm.Exponential(\"sigma_beta_smooth\", 0.1)\n        beta_raw = pm.Normal(\"beta_raw_m\", 0, 1, dims=('splines'))\n        coef_beta = pt.cumsum(beta_raw * sigma_beta_smooth)\n        \n        beta_t = pt.dot(basis_, coef_beta)\n\n        # -------------------------------------------------\n        # 3. Mediator model (A path: x → M)\n        # -------------------------------------------------\n        sigma_m = pm.HalfNormal(\"sigma_m\", 1.0)\n        \n        # Autoregressive component\n        rho = pm.Beta(\"rho\", 2, 2)\n        \n        mu_m = beta_t[b] * trt + rho * med_lag\n\n        pm.Normal(\n            \"obs_m\",\n            mu=mu_m,\n            sigma=sigma_m,\n            observed=med,\n            dims='obs'\n        )\n\n        # -------------------------------------------------\n        # 4. Hazard model (direct + B path)\n        # -------------------------------------------------\n        beta_low  = pm.Normal(\"beta_low\",  0, 0.1)\n        beta_high = pm.Normal(\"beta_high\", 0, 0.1)\n        # Log-additive hazard\n        log_lambda_t = (alpha_0_t[b] \n                        + alpha_1_t[b] * trt # direct effect\n                        + alpha_2_t[b] * med  # mediator effect\n                        + beta_low  * I_low\n                        + beta_high * I_high\n        )\n        \n        # Expected number of events\n        time_at_risk = at_risk * dt\n        Lambda = time_at_risk * pm.math.log1pexp(log_lambda_t)\n\n        if observed:\n            pm.Poisson(\n                \"obs_event\",\n                mu=Lambda,\n                observed=events, \n                dims='obs'\n            )\n        else: \n            pm.Poisson(\n                \"obs_event\",\n                mu=Lambda,\n                dims='obs'\n            )\n\n        # -------------------------------------------------\n        # 5. Causal path effects\n        # -------------------------------------------------\n        # Store time-varying coefficients\n        pm.Deterministic(\"alpha_0_t\", alpha_0_t, dims='time_bins')\n        pm.Deterministic(\"alpha_1_t\", alpha_1_t, dims='time_bins')  # direct effect\n        pm.Deterministic(\"alpha_2_t\", alpha_2_t, dims='time_bins')  # B path\n        pm.Deterministic(\"beta_t\", beta_t, dims='time_bins')        # A path\n        \n        # Cumulative direct effect\n        cum_de = pm.Deterministic(\n            \"tv_direct_effect\",\n            alpha_1_t, \n            dims='time_bins'\n        )\n\n        # Cumulative indirect effect (product of paths)\n        cum_ie = pm.Deterministic(\n            \"tv_indirect_effect\",\n            beta_t * alpha_2_t, \n            dims='time_bins'\n        )\n\n        # Total effect\n        cum_te = pm.Deterministic(\n            \"tv_total_effect\",\n            cum_de + cum_ie,\n            dims='time_bins'\n        )\n\n        pm.Deterministic('tv_baseline_hazard', pm.math.log1pexp(alpha_0_t), \n            dims='time_bins')\n\n        pm.Deterministic('tv_hazard_with_exposure', pm.math.log1pexp(alpha_0_t + alpha_1_t), \n            dims='time_bins')\n\n        pm.Deterministic(\n        \"tv_RR\",\n        pm.math.log1pexp(alpha_0_t + alpha_1_t) /\n        pm.math.log1pexp(alpha_0_t),\n        dims=\"time_bins\"\n        )\n\n        # -------------------------------------------------\n        # 6. Sample\n        # -------------------------------------------------\n        if sample:\n            idata = pm.sample_prior_predictive()\n            idata.extend(pm.sample(\n                draws=2000,\n                tune=2000,\n                target_accept=0.95,\n                chains=4,\n                nuts_sampler=\"numpyro\",\n                random_seed=42,\n                init=\"adapt_diag\", \n                idata_kwargs={\"log_likelihood\": True}\n            ))\n            idata.extend(pm.sample_posterior_predictive(idata))\n    \n    return aalen_dpa_model, idata\n\nbasis = create_bspline_basis(n_bins, n_knots=12, degree=3)\naalen_dpa_model, idata_aalen =  make_model(data, basis)\n\n\n\n\nThe Softplus Link: Preserving Aalen’s Additivity\nIn our PyMC implementation, the core challenge of an Additive Hazards model is ensuring that the hazard intensity \\(\\lambda(t)\\) remains strictly positive, as a negative risk is physically impossible. While Aalen’s original formulation often struggled with this constraint (sometimes requiring hard clipping at zero), our model utilizes the Softplus link (pm.math.log1pexp) to provide a robust, differentiable solution.\n\n\n\n\n\n\nThe Softplus Link\n\n\n\nThe Softplus function, implemented via pm.math.log1pexp(x), is defined as:\n\\[f(x) = \\ln(1 + \\exp(x))\\]\nIn the context of this survival model, it serves as a smooth link function that transforms the linear combination of effects into a strictly positive hazard intensity. Unlike a hard truncation at zero, which creates non-differentiable “corners” that hinder NUTS sampling, the Softplus function provides a continuous manifold. It allows the additive components—the baseline \\(\\alpha_0(t)\\), the direct effect \\(\\alpha_1(t)\\), and the mediator effect \\(\\alpha_2(t)\\)—to fluctuate freely while ensuring the resulting Poisson mean \\(\\lambda(t)\\) is always physically plausible (\\(&gt; 0\\)).\n\n\n\n\nImplementation Summary\nWe used our splines and random walk parameterisation to represent time-varying hazard intensities and then preserved the additive structure by using the softplus link. Strictly speaking, this is no longer an identity-link Aalen model; it is a smooth, monotone transformation chosen for computational stability. The causal interpretation relies on monotonicity, not exact linearity. By defining Lambda = time_at_risk * pm.math.log1pexp(log_lambda_t), we have created a generative model where the total hazard is the sum of time-varying intensities. This allows for a clean decomposition of causal paths:\n\nDirect Intensity: \\(\\alpha_{1}(t)\\) (the time-varying coefficient for treatment)\nIndirect Intensity: \\(\\beta_{1}(t) \\cdot \\alpha_{2}(t)\\) (the product of the \\(A\\) and \\(B\\) paths)\n\nThis structure is perfectly suited for Bayesian modeling, providing a “well-behaved” likelihood that handles the Aalen additivity while remaining amenable to MCMC. We can see the model structure illustrated in the PyMC graph.\n\npm.model_to_graphviz(aalen_dpa_model)"
  },
  {
    "objectID": "posts/post-with-code/AalenDynamicPaths/aalen_dynamic_paths.html#evaluating-the-dynamic-paths-analysis",
    "href": "posts/post-with-code/AalenDynamicPaths/aalen_dynamic_paths.html#evaluating-the-dynamic-paths-analysis",
    "title": "The Journey Is the Model: Dynamic Path Analysis in PyMC",
    "section": "Evaluating the Dynamic Paths Analysis",
    "text": "Evaluating the Dynamic Paths Analysis\nThe use of B-splines to model time-varying effects introduces a critical hyperparameter: the number of knots. The knots determine the “wiggliness” or flexibility of the recovered hazard paths. To ensure the causal conclusions are not artifacts of an underfit or overfit baseline, we perform a sensitivity analysis by varying the knot density and comparing the models using Leave-One-Out Cross-Validation (LOO).\n\nmodels = {}\nidatas = {}\nfor i in range(4, 15, 2):\n    basis = create_bspline_basis(n_bins, n_knots=i, degree=3)\n    aalen_dpa_model, idata = make_model(data, basis)\n    models[i] = aalen_dpa_model\n    idatas[f\"splines_{i}\"] = idata\n\ncompare_df = az.compare(idatas, var_name='obs_event')\naz.plot_compare(compare_df, figsize=(8, 6), plot_ic_diff=True)\n\nWhy Spline Sensitivity Matters Evaluating the model across a range of spline dimensions (from 4 to 14 knots) is essential for several reasons:\n\nBias-Variance Trade-off: Too few knots (e.g., 4) may over-smooth the hazard, potentially masking sudden changes in treatment effects or surges in the baseline risk. Conversely, too many knots (e.g., 14) can lead to “overfitting the noise,” where the model interprets random fluctuations in the Poisson counting process as meaningful temporal trends.\nStructural Stability of Causal Paths: The goal is to identify a “plateau” where adding more knots no longer significantly improves the expected log pointwise predictive density (ELPD). If the decomposition of direct and indirect effects remains consistent across this plateau, we gain high confidence that the dynamic mediation we are observing is a robust feature of the data.\nPredictive Performance: Using az.compare, we can formally rank the models. The LOO comparison identifies the optimal degree of complexity that maximizes predictive accuracy for out-of-sample data, ensuring that our “Poisson Bridge” provides a reliable generative representation of the survival process.\n\n\nIn the above plot we seen the model fits ranked by the number of splines. Suggesting that 12 is the optimal of the tested splines. The stability of the implied direct effects can be seen in the below plot.\n\nax = az.plot_forest([idatas[k] for k in idatas.keys()], combined=True, var_names=['tv_direct_effect'], model_names=idatas.keys(), coords={'time_bins': [180, 182, 182, 183, 184, 185, 186, 187, 188]}, \nfigsize=(12, 10),  r_hat=True)\nax[0].set_title(\"Time Vary Direct Effects \\n Comparing Models on Final Time Intervals\", fontsize=15)\nax[0].set_ylabel(\"Nth Time Interval\", fontsize=15)\nfig = ax[0].figure\nfig.savefig('forest_plot_comparing_tv_direct.png')\n\n\nThe stability of the direct effects across the 10-14 knot range confirms our causal conclusions aren’t artifacts of oversmoothing or overfitting.”\nMore diagnostically, we can see how the sampler behaves well for our model too. The trace plots show that each chain properly explores the parameter space.\n\naz.plot_trace(idata_aalen, var_names=['tv_direct_effect', 'tv_indirect_effect', 'tv_total_effect', 'beta_high', 'beta_low'], divergences=False);\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nPosterior Predictive Checks (PPC)\nThe PPC is the model’s primary diagnostic tool for assessing “goodness-of-fit.” By simulating new data from the posterior and comparing it to the actual observations, we ensure the model has accurately captured the data-generating process.\nThe first plot on the left shows the Mediator Outcome. The good fit validates the autoregressive B-spline component (\\(X \\to M\\)). A strong overlap indicates that the Gaussian path is correctly specified and the B-splines are capturing the mediator’s temporal trend.\nThe second plot on the right panel confirms the “Poisson Bridge” is functioning. It checks if the simulated event counts per time bin match the observed frequency. A tight fit here confirms that your hazard intensity is neither over-smoothed nor tracking random noise.\n\nax = az.plot_ppc(idata_aalen);\nax[0].set_title(\"Posterior Predictive Checks \\n Mediator Outcome\")\nax[1].set_title(\"Posterior Predictive Checks \\n Event Outcome\")\nfig = ax[0].figure\nfig.savefig(\"Posterior_Predictive_checks.png\")\n\n\nThe model checks appear healthy, but have we actually replicated the dpasurv analysis?\n\n\nTime-Varying Causal Effects (Log-Additive Scale)\nThe next plots aim to recreate the dpasurv analysis. They represent the additive decomposition of the treatment’s impact within the linear predictor. Because these are plotted on the scale of the \\(\\alpha\\) coefficients, they show the “raw” causal pressure before the Softplus transformation.\n\nTime-Varying Direct Effect (\\(\\alpha_{1,t}\\)): This path shows a steady downward trend into negative territory, indicating that the treatment has a progressively protective direct effect on the hazard. If analyzed in isolation, this would suggest the treatment is successfully reducing risk over time.\nTime-Varying Indirect Effect (\\(\\beta_t \\cdot \\alpha_{2,t}\\)): In contrast, the indirect path shows a stable, materializing harmful effect mediated through \\(M\\). As the study progresses, the mediator offsets the treatment’s benefits, introducing a source of risk that grows in certainty (narrowing HDI).\nTime-Varying Total Effect: The sum of these two opposing forces. Because the harmful indirect effect partially “cancels out” the protective direct effect, the total effect appears significantly more modest—or even neutral. This reveals a masked harmful effect: without this decomposition, the treatment might be judged as ineffective, when in reality, it is highly effective but being undermined by the specific biological or behavioral pathway of the mediator.\n\n\n\nCode\ndef plot_effects(idata, vars_to_plot, labels, scale=\"Log Hazard Ratio Scale\"):\n    fig, axs = plt.subplots(1, 3, figsize=(20, 10))\n    color='teal'\n    if scale != \"Log Hazard Ratio Scale\":\n        color='darkred'\n\n    for i, var in enumerate(vars_to_plot):\n        # 1. Extract the posterior samples for this variable\n        # Shape will be (chain * draw, time)\n        post_samples = az.extract(idata, var_names=[var]).values.T\n        \n        # 2. Calculate the mean and the 94% HDI across the chains/draws\n        mean_val = post_samples.mean(axis=0)\n        hdi_val = az.hdi(post_samples, hdi_prob=0.94) # Returns [time, 2] array\n        \n        # 3. Plot the Mean line\n        x_axis = np.arange(len(mean_val))\n        axs[i].plot(x_axis, mean_val, label=labels[i], color=color, lw=2)\n        \n        # 4. Plot the Shaded HDI region\n        axs[i].fill_between(x_axis, hdi_val[:, 0], hdi_val[:, 1], color=color, alpha=0.2, label='94% HDI')\n        \n        # Formatting\n        axs[i].set_title(labels[i])\n        axs[i].legend()\n        axs[i].grid(alpha=0.3)\n        axs[i].set_ylabel(scale)\n    plt.tight_layout()\n    return fig\n\n\n\nvars_to_plot = ['tv_direct_effect', 'tv_indirect_effect', 'tv_total_effect']\nlabels = ['Time varying Direct Effect', 'Time varying Indirect Effect', 'Time varying Total Effect']\n\nplot_effects(idata_aalen, vars_to_plot, labels);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelationship to Aalen’s Additive dpasurv\n\n\n\nThe structure of the recovered effects in our Bayesian model closely replicates the trends seen in the frequentist dpasurv implementation. However, there is a notable difference in the scale of the y-axis and the magnitude of the trajectories.\nThis divergence is expected and stems from our use of the Softplus link function (\\(f(x) = \\ln(1 + \\exp(x))\\)). While dpasurv typically operates on a strictly linear identity scale (which can result in negative hazards if not carefully constrained), our Bayesian model operates on a transformed scale to ensure sampling stability and physical plausibility. The Softplus scaling “squashes” values as they approach zero, meaning that while the directional causal story (protective direct effect vs. harmful indirect effect) remains identical, the absolute units of the cumulative effect are scaled differently to accommodate the non-linear map to the Poisson intensity.\n\n\nGiven that the previous causal decomposition occurs on the transformed scale of the linear predictor, interpreting the raw magnitudes of the \\(\\alpha\\) coefficients can be challenging when comparing against traditional identity-scale models like dpasurv. To move from these internal model dynamics to a more universally interpretable metric, we calculate the Time-Varying Relative Risk (RR). By taking the ratio of the hazard with exposure and compare it with the baseline hazard.\n\nvars_to_plot = ['tv_baseline_hazard', 'tv_hazard_with_exposure', 'tv_RR']\nlabels = ['Time varying Baseline Hazard', 'Time varying Hazard + Exposure', 'Time varying RR']\nplot_effects(idata_aalen, vars_to_plot, labels, scale='Hazard Scale');\n\n\n\n\n\n\n\n\nThis effectively “normalizes” the protective direct effects and harmful indirect effects, allowing us to visualize the net impact on the risk profile in a format familiar to survival analysis, regardless of the underlying link function or scaling required for MCMC sampling. A horizontal line at \\(1\\) would imply a neutral treatment with no more or less protective effects. Instead, the slope indicates that the treatment becomes more protective over time. The risk for the exposed group drops by roughly \\(30\\%\\) (\\(RR \\approx 0.7\\)) by the end of the observation period."
  },
  {
    "objectID": "posts/post-with-code/AalenDynamicPaths/aalen_dynamic_paths.html#conclusion-the-journey-is-the-model",
    "href": "posts/post-with-code/AalenDynamicPaths/aalen_dynamic_paths.html#conclusion-the-journey-is-the-model",
    "title": "The Journey Is the Model: Dynamic Path Analysis in PyMC",
    "section": "Conclusion: The Journey Is the Model",
    "text": "Conclusion: The Journey Is the Model\nWhen Odysseus finally reached Ithaca, his journey home had taken ten years—not because the distance was far, but because the voyage itself transformed him and his circumstances. This is the fundamental insight of dynamic path analysis: outcomes are not destinations; they are cumulative consequences of time-varying processes.\nIn this post, we’ve built a complete Bayesian implementation of Aalen’s Dynamic Path Model that moves beyond the proportional hazards paradigm. Rather than assuming a treatment has a constant multiplicative effect throughout a study, we’ve allowed direct, indirect, and total effects to evolve as smooth functions of time using B-spline basis expansions. This shift from multiplicative to additive models is more than technical—it’s conceptual. Effects become increments to risk rather than multipliers, making them naturally interpretable as causal estimands. Time-varying coefficients directly answer: “What is the cumulative risk attributable to this exposure right now?” And crucially, direct and indirect effects become sums of paths traced through time.\nBy adopting the “Poisson Bridge,” we’ve established a fully generative model where both baseline hazards and time-varying effects are explicit parameters. This additivity, combined with the Softplus link function (log1pexp), gives us a model that is both theoretically grounded in causal inference and practically stable for MCMC sampling.\nThe payoff is substantial: the analysis revealed that while the treatment has a strong protective direct effect (reducing risk by ~30%), this benefit is partially offset by a harmful indirect pathway through the mediator. Without temporal decomposition, the treatment might appear only modestly effective—or even neutral. Our Bayesian model successfully replicates the directional trends from the dpasurv R package, while providing richer uncertainty quantification through full posterior distributions rather than bootstrap confidence intervals. Dynamic path analysis is particularly valuable when effects attenuate or strengthen over time—drug effectiveness that fades, policies with delayed impacts, interventions with “honeymoon periods”—or when the mechanism through which treatment works changes as the study progresses. When proportional hazards are implausible or you need full Bayesian uncertainty quantification, this approach provides a principled framework.\nOdysseus’s journey reminds us that understanding how we arrive at outcomes is often more valuable than simply knowing whether we arrived. In causal inference, this means moving beyond static treatment effects to models that capture the full temporal dynamics of intervention, mediation, and risk. The tools presented here—B-splines for smooth time-varying effects, the Poisson Bridge for Bayesian inference, and dynamic path decomposition for causal interpretation—provide a principled way to track these journeys. Whether you’re evaluating a New Year’s resolution, a clinical trial, or a policy intervention, this framework lets you see not just the destination, but the evolving forces that shaped the voyage.\n\nFurther Reading\nThis implementation builds on Fosen et al.’s 2006 paper “Dynamic path analysis—a new approach to analyzing time-dependent covariates” (Lifetime Data Analysis) and Aalen’s foundational 1989 work on additive regression models for survival data (Statistics in Medicine) and the discussion of causal inference in Brostrom’s Event History Analysis with R. We aim to replicate, as close as feasible, the results reported in dpasurv"
  },
  {
    "objectID": "posts/post-with-code/AalenDynamicPaths/aalen_dynamic_paths.html#from-time-varying-to-marginal-effects-g-computation",
    "href": "posts/post-with-code/AalenDynamicPaths/aalen_dynamic_paths.html#from-time-varying-to-marginal-effects-g-computation",
    "title": "The Journey Is the Model: Dynamic Path Analysis in PyMC",
    "section": "From Time-Varying to Marginal Effects: G-Computation",
    "text": "From Time-Varying to Marginal Effects: G-Computation\nThe time-varying plots reveal how treatment effects evolve, but applied work often requires a single summary: what is the average treatment effect? Using g-computation, we simulate two counterfactual worlds—one where everyone is treated, one where no one is—and compare outcomes. We’re exploring different journey trajectories.\n\ntrt_values = np.ones(len(df_long)).astype(int)\nwith aalen_dpa_model: \n    pm.set_data({'trt': trt_values})\n    idata_trt1 = pm.sample_posterior_predictive(idata_aalen, return_inferencedata=True)\n\ntrt_values = np.zeros(len(df_long)).astype(int)\nwith aalen_dpa_model: \n    pm.set_data({'trt': trt_values})\n    idata_trt0 = pm.sample_posterior_predictive(idata_aalen, return_inferencedata=True)\n\nWe summarise the imputed posterior distribution by describing range of probable RR scores. We clip the maximim posterior predictive distribution at 1 as scores above 1 are an artefact the Poisson likelihood, not a meaningful contribution to the rate.\n\n## Clipping at one in case Poisson prediction exceeds 1. Event is binary.\nrr_imputed = idata_trt1['posterior_predictive']['obs_event'].clip(max=1).mean(dim='obs') / idata_trt0['posterior_predictive']['obs_event'].clip(max=1).mean(dim='obs')\n\nrr_mean = rr_imputed.mean()\nrr_hdi = az.hdi(rr_imputed, hdi_prob=.5)\n\nprint(f\"Imputed Mean Relative Risk {np.round(rr_mean.item(), 3)} and 50% HDI {[rr_hdi['obs_event'][0].item(), rr_hdi['obs_event'][1].item()]}\")\nprint(f\"Imputed Percent Risk Reduction of {1- np.round(rr_mean.item(), 3)}\")\n\nImputed Mean Relative Risk 0.8 and 50% HDI [0.6666666666666665, 0.8676470588235293]\nImputed Percent Risk Reduction of 0.19999999999999996\n\n\nThe marginal risk ratio tells us treatment reduces events by ~20% on average across all time periods. This single number, however, conceals the rich temporal story: the treatment is consistently protective (direct effect), but partially undermined by a harmful mediated pathway emerging around Day 100. The dynamic path model gives you both the summary and the mechanistic decomposition over time. Yet, the imputation mechanism is not merely for bookkeeping and statistical summaries. It can be extended to compute alternative causal estimands under different counterfactual dosages. The imputaton tooling is an invitation to explore possible worlds and trace the consequences of counterfactual decisions."
  }
]