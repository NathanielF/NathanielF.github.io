<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Nathaniel Forde">
<meta name="dcterms.date" content="2021-03-22">

<title>Examined Algorithms - Bayesian Decisions &amp; Model Comparison</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Examined Algorithms</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../opensource.html">
 <span class="menu-text">Open Source Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../talks.html">
 <span class="menu-text">Talks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../resume/Nathaniel_Forde_CV.pdf">
 <span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-reader-toggle nav-link" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Bayesian Decisions &amp; Model Comparison</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title d-none d-lg-block">Bayesian Decisions &amp; Model Comparison</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">model_evaluation</div>
                <div class="quarto-category">likelihood_ratio_tests</div>
                <div class="quarto-category">loss_ratios</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Nathaniel Forde </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 22, 2021</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">Logic</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notes/certain_things/Public/Logic/Introduction - Logic Topics.html" class="sidebar-item-text sidebar-link">Introduction - Logic Topics</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">Philosophy</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notes/certain_things/Public/Philosophy/Introduction - Philosophy Topics.html" class="sidebar-item-text sidebar-link">Introduction - Philosophy Topics</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notes/certain_things/Public/Philosophy/Thinking about Statistics.html" class="sidebar-item-text sidebar-link">Statistical Models and the Problem of Induction</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notes/certain_things/Public/Philosophy/Sorites Paradox and Survival Analysis.html" class="sidebar-item-text sidebar-link">The Sorites Paradox</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">Statistics</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notes/certain_things/Public/Statistics/Introduction - Statistics Topics.html" class="sidebar-item-text sidebar-link">Introduction - Statistics Topics</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../notes/certain_things/Public/Statistics/Analogies between Missing Data and Causal Inference.html" class="sidebar-item-text sidebar-link">Taxonomies of Missing-ness</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#choice-comparison-and-probability" id="toc-choice-comparison-and-probability" class="nav-link active" data-scroll-target="#choice-comparison-and-probability">Choice, Comparison and Probability</a></li>
  <li><a href="#loss-functions-and-ratio-tests" id="toc-loss-functions-and-ratio-tests" class="nav-link" data-scroll-target="#loss-functions-and-ratio-tests">Loss functions and Ratio Tests</a>
  <ul class="collapse">
  <li><a href="#optimising-success-and-and-failure-rates" id="toc-optimising-success-and-and-failure-rates" class="nav-link" data-scroll-target="#optimising-success-and-and-failure-rates">Optimising Success and and Failure Rates</a></li>
  </ul></li>
  <li><a href="#example-expected-loss-curves" id="toc-example-expected-loss-curves" class="nav-link" data-scroll-target="#example-expected-loss-curves">Example: Expected Loss Curves</a>
  <ul class="collapse">
  <li><a href="#model-comparison" id="toc-model-comparison" class="nav-link" data-scroll-target="#model-comparison">Model Comparison</a></li>
  </ul></li>
  <li><a href="#conclusion-confidence-in-point-estimates-are-unearned." id="toc-conclusion-confidence-in-point-estimates-are-unearned." class="nav-link" data-scroll-target="#conclusion-confidence-in-point-estimates-are-unearned.">Conclusion: Confidence in Point Estimates are Unearned.</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Most days we face an infinite horizon of future decisions without being overwhelmed. A sequence of such decisions can compound, or reduce that certainty. But we tend to focus on the exaggerated drama of the momentous choice. Even when change is gradual and cumulative, history tells of a tale of pivot points, epic junctures and transformative events. Two paths diverge in a woods and we choose one. An array of possibilities suddenly open before us, or gets collapsed into a <em>cul de sac</em>. We weigh the options differently according to our needs and desires and pursue strategies to maximise our goals.</p>
<p>Incorporating uncertainty Leonard J. Savage famously showed that the optimal weights on each desire are proportional to their probabilities if we seek to maximise our expected gains. In particular, he showed that if we reason with probabilities our outcomes will the dominate all other methods for mitigating uncertainty. This alone should serve to motivate probabilisitic modelling, but we want to show concretely how this translates to making decisions under uncertainty. We’ll first show how such considerations underwrite a slew of familiar decision rules, and then how these methods can be applied to problem of reserving enough premium to cover costs accruing against insurance contracts. In both cases a concern for accuracy is implict.</p>
<section id="choice-comparison-and-probability" class="level3">
<h3 class="anchored" data-anchor-id="choice-comparison-and-probability">Choice, Comparison and Probability</h3>
<p>Not all problems can be expressed with a generative probability model. More pertinently, even if the problem is expressable, it is a fine art to aptly approximate the real process in the lexicon of familiar probability distributions. But there are benefits to the effort. A probability model allows direct comparisons of theories about the data. If we postulate some explanatory theory which is parameterised with the vector <span class="math inline">\(\mathbf{T\_{\beta}}\)</span>, where <span class="math inline">\(\beta\)</span> is governed by an explicit probability model, then we are able to state the ratio of likelihoods for any observed data comparing theories <span class="math inline">\(\mathbf{T\_{\beta}}, \mathbf{T\_{\beta^{1}}}\)</span>.</p>
<p><span class="math display">\[ \frac{p( D | \mathbf{T}_{\beta^{1}})}{p(D | \mathbf{T}_{\beta})} \]</span></p>
<p>This style of comparison underwrites most decision rules as we set bounds on acceptable divergences. In particular, we see how the special case of the Neyman Pearson lemma (which underwrites most simple hypothesis testing) is an instance of a just such a liklihood ratio comparison. We follow the presentation of Moritz Hardt and Benjamin Recht in their <em>Patterns, Prediction and Actions</em>.</p>
</section>
<section id="loss-functions-and-ratio-tests" class="level2">
<h2 class="anchored" data-anchor-id="loss-functions-and-ratio-tests">Loss functions and Ratio Tests</h2>
<p>Imagine we frame the cost or loss of incorrectly choosing amongst our theories under uncertainty.Then the question becomes one of expected loss. Define <span class="math inline">\(l(T, T\_1)\)</span> as the loss incurred when choosing <span class="math inline">\(T\)</span> when <span class="math inline">\(T\_1\)</span> is true. Then the expected loss in light of the data is calculated as:</p>
<p><span class="math display">\[ (1):  E(l(T_{\beta} , T) | D) =
l(T_{\beta} , T_{\beta})p(T_{\beta} | D) + l(T_{\beta} , T_{\beta^{1}})p(T_{\beta^{1}} | D) \]</span></p>
<p><span class="math display">\[ (2):  E(l(T_{\beta^{1}} , T) | D) =
l(T_{\beta^{1}} , T_{\beta})p(T_{\beta} | D) + l(T_{\beta^{1}} , T_{\beta^{1}})p(T_{\beta^{1}} | D) \]</span></p>
<p>We want to choose the theory with least expected loss. Assume (1) <span class="math inline">\(\geq\)</span> (2), then</p>
<p><span class="math display">\[ l(T_{\beta} , T_{\beta})p(T_{\beta} | D) + l(T_{\beta} , T_{\beta^{1}})p(T_{\beta^{1}} | D) \geq l(T_{\beta^{1}} , T_{\beta})p(T_{\beta} | D) + l(T_{\beta^{1}} , T_{\beta^{1}})p(T_{\beta^{1}} | D) \]</span></p>
<p>manipulating:</p>
<p><span class="math display">\[ l(T_{\beta} , T_{\beta^{1}})p(T_{\beta^{1}} | D) \geq l(T_{\beta^{1}} , T_{\beta})p(T_{\beta} | D) -  l(T_{\beta} , T_{\beta})p(T_{\beta} | D) + l(T_{\beta^{1}} , T_{\beta^{1}})p(T_{\beta^{1}} | D) \]</span></p>
<p>grouping:</p>
<p><span class="math display">\[ l(T_{\beta} , T_{\beta^{1}})p(T_{\beta^{1}} | D) \geq p(T_{\beta} | D)(l(T_{\beta^{1}} , T_{\beta}) -  l(T_{\beta} , T_{\beta})) + l(T_{\beta^{1}} , T_{\beta^{1}})p(T_{\beta^{1}} | D) \]</span></p>
<p>rearraging:</p>
<p><span class="math display">\[ (l(T_{\beta} , T_{\beta^{1}}) - l(T_{\beta^{1}} , T_{\beta^{1}}))p(T_{\beta^{1}} | D)  \geq p(T_{\beta} | D)(l(T_{\beta^{1}} , T_{\beta}) -  l(T_{\beta} , T_{\beta})) \]</span></p>
<p><span class="math display">\[ p(T_{\beta^{1}} | D)  \geq p(T_{\beta} | D)\frac{(l(T_{\beta^{1}} , T_{\beta}) -  l(T_{\beta} , T_{\beta}))}{(l(T_{\beta} , T_{\beta^{1}}) - l(T_{\beta^{1}} , T_{\beta^{1}}))} \]</span></p>
<p><span class="math display">\[ \frac{p(T_{\beta^{1}} | D)}{p(T_{\beta} | D)}  \geq \frac{(l(T_{\beta^{1}} , T_{\beta}) -  l(T_{\beta} , T_{\beta}))}{(l(T_{\beta} , T_{\beta^{1}}) - l(T_{\beta^{1}} , T_{\beta^{1}}))} \]</span></p>
<p>which by Bayes Rule:</p>
<p><span class="math display">\[ \frac{\frac{p(D | T_{\beta^{1}})p(T_{\beta^{1}})}{p(D)}}{\frac{p(D | T_{\beta})p(T_{\beta})}{p(D)}} \geq \frac{(l(T_{\beta^{1}} , T_{\beta}) -  l(T_{\beta} , T_{\beta}))}{(l(T_{\beta} , T_{\beta^{1}}) - l(T_{\beta^{1}} , T_{\beta^{1}}))} \]</span></p>
<p>which cancels to become:</p>
<p><span class="math display">\[ \frac{p(D | T_{\beta^{1}})p(T_{\beta^{1}})}{p(D | T_{\beta})p(T_{\beta})} \geq \frac{(l(T_{\beta^{1}} , T_{\beta}) -  l(T_{\beta} , T_{\beta}))}{(l(T_{\beta} , T_{\beta^{1}}) - l(T_{\beta^{1}} , T_{\beta^{1}}))} \]</span></p>
<p><span class="math display">\[ \Rightarrow \frac{p(D | T_{\beta^{1}})}{p(D | T_{\beta})} \geq \frac{p(T_{\beta})(l(T_{\beta^{1}} , T_{\beta}) -  l(T_{\beta} , T_{\beta}))}{p(T_{\beta^{1}})(l(T_{\beta} , T_{\beta^{1}}) - l(T_{\beta^{1}} , T_{\beta^{1}}))} \]</span></p>
<p>which shows how each expected loss calculation between two theories reduces to a likelihood ratio test, where we have a specific threshold for accepted preference. The preferred theory dominates the other just when the ratio of their likelihood exceeds that threshold. Therefore our estimate of the correct theory <span class="math inline">\(\hat{\mathbf{T}}\)</span> is characterised by this comparison.</p>
<p><span class="math display">\[ \hat{\mathbf{T}} = \mathbb{1} \Bigg[  \frac{p( D | \mathbf{T}_{\beta^{1}})}{p(D | \mathbf{T}_{\beta})} \geq \eta \Bigg] \]</span></p>
<p>Hardt and Recht show a number of the very familiar procedures reduce to likelihood ratio tests with more or less constraints on threshold priors or loss functions. For instance, the (MAP) maximum a posteriori decision rule emerges naturally as a likelihood ratio test when we set the <span class="math inline">\(l(T, T) = l(\neg T, \neg T) = 0\)</span> and <span class="math inline">\(l(T, \neg T) = l(\neg T, T) = 1\)</span> This results in the following decision rule:</p>
<p><span class="math display">\[ \hat{\mathbf{T}} = \mathbb{1} \Bigg[  \frac{p( D | \mathbf{T}_{\beta^{1}})}{p(D | \mathbf{T}_{\beta})} \geq \frac{p(\mathbf{T_{\beta}})}{p(\mathbf{T_{\beta^{1}}})} \Bigg] \]</span></p>
<p>This is a particularly natural decision rule since it relies in how much we have learned about the likely theories (after conditioning on the data) relative to our prior commitments.</p>
<section id="optimising-success-and-and-failure-rates" class="level3">
<h3 class="anchored" data-anchor-id="optimising-success-and-and-failure-rates">Optimising Success and and Failure Rates</h3>
<p>In the binary classification case we can compare two hypotheses directly while aiming to maximise some measure of accuracy with a high probability. This requires that we express our decision rule as some selective mapping over our data points in light of the theories.</p>
<p><span class="math display">\[ \hat{\mathbf{d}}: \{ \mathbf{T_{\beta}}, \mathbf{T\_{\beta^{1}}} \} \times D  \mapsto \{ False, True \} \]</span></p>
<p>With this set up the Neyman Pearson lemma states that any attempt to optimise our rule so that we maximise the True positive rate (TPR) while we minimise our False positive rate (FPR) subject to a constraint is equivalent to a likelihood ratio test.</p>
<p><span class="math display">\[ \text{ maximise TPR: } p(\hat{\mathbf{d}} \text{ is True} | \mathbf{T_\beta}) \]</span> <span class="math display">\[ \text{ subject to FPR: } p(\hat{\mathbf{d}} \text{ is True} | \mathbf{T_\beta^{1}}) \leq \alpha \]</span></p>
<p>This follows because the optimisation task does not require specific information about the prior probabilities. The TPR and FPR terms likelihood terms, so we can construct a loss function such that the decision rule is equivalent to a simple likelihood ratio test.</p>
<p>Lemma 1. <strong>Neyman-Pearson Lemma</strong> <em>The optimal probabilistic decision rule that maximizes TPR with an upper bound on FPR is a deterministic likelihood ratio test.</em></p>
<p>The proof is short and elaborated in <em>Patterns, Predictions and Actions</em>, but the key point is that a likelihood ratio test can be constructed to mirror this optimisation constraint with appropriate modifications to our priors over both theories.</p>
<div class="proof" data-text="Neyman-Pearson">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(R\)</span> be a decision rule such that</p>
<p><span class="math display">\[ \Bigg[  \frac{p( \hat{\mathbf{d}} | \mathbf{T}_{\beta^{1}})}{p(\hat{\mathbf{d}} | \mathbf{T}_{\beta})} \geq \eta \Bigg] \text{ where FPR = } \alpha \]</span> Then let <span class="math inline">\(\tau\)</span> be the TPR of R. Keeping the likelihood terms untouched we’re free to manipulate the priors so that</p>
<p><span class="math display">\[ \frac{p(\mathbf{T_{\beta}})}{p(\mathbf{T_{\beta^{1}}})} = \frac{1}{1 + \eta} / \frac{\eta}{1 + \eta} = \frac{1+\eta}{(1+\eta)\eta} = \eta\]</span></p>
<p>which is just the likelihood ratio test MAP rule above. Then let <span class="math inline">\(tpr\)</span> and <span class="math inline">\(fpr\)</span> be the true postive rate and false postive rate of an alternative rule <span class="math inline">\(R^{1}\)</span> with <span class="math inline">\(fpr \leq \alpha\)</span> implying $ p(<em>{}) fpr p(</em>{}) $. Adding to both sides:</p>
<p><span class="math display">\[ (1) \text{    } p(\mathbf{T}_{\beta}) fpr + p(\mathbf{T}_{\beta^{1}})(1- tpr) \leq p(\mathbf{T}_{\beta}) \alpha  +  p(\mathbf{T}_{\beta^{1}})(1- tpr)  \]</span></p>
<p>Assume <span class="math inline">\(fpr &lt; \alpha\)</span> then unpacking a bit we see by complementarity that:</p>
<p><span class="math display">\[ \frac{fp}{neg}  &lt;  \frac{FP}{neg} \text{ and since } (1-tpr) = \frac{fn}{pos} , (1-\tau) = \frac{FN}{pos} \text{ implies } (1-\tau) &lt; (1 - tpr) \]</span></p>
<p><span class="math display">\[ \Rightarrow  (2) \text{   } p(\mathbf{T}_{\beta}) \alpha  +  p(\mathbf{T}_{\beta^{1}})(1- \tau) \leq p(\mathbf{T}_{\beta})fpr + p(\mathbf{T}_{\beta^{1}})(1- tpr) \]</span></p>
<p>Alternatively <span class="math inline">\(fpr = \alpha\)</span> and (2) folllows quickly. In either case (1) &amp; (2) imply:</p>
<p><span class="math display">\[ (3) p(\mathbf{T}_{\beta}) \alpha  +  p(\mathbf{T}_{\beta^{1}})(1- \tau) \leq \text{    } p(\mathbf{T}_{\beta}) fpr + p(\mathbf{T}_{\beta^{1}})(1- tpr) \leq p(\mathbf{T}_{\beta}) \alpha  +  p(\mathbf{T}_{\beta^{1}})(1- tpr)  \]</span></p>
<p>which implies that <span class="math inline">\(tpr \leq \tau\)</span> in all cases.</p>
<p>Which is enough to complete the proof since <span class="math inline">\(R^{1}\)</span> was arbitrary, we have shown that <span class="math inline">\(R\)</span> maximises TPR with a fixed FPR.</p>
</div>
<p>So, in practice, optimisation amounts to the modification of these priors. Our priors are adjusted as hyper-parameters as we tune them to achieve optimal classification performance and generalisability.</p>
</section>
</section>
<section id="example-expected-loss-curves" class="level2">
<h2 class="anchored" data-anchor-id="example-expected-loss-curves">Example: Expected Loss Curves</h2>
<p>Imagine an insurer seeks to reserve enough of their premium to cover their expected losses. Then month on month we need to estimate the approriate loss ratio and project those losses over the ensuing months. This problem has a natural expression in the Bayesian setting. As can be seen in Stan language case study <a href="https://mc-stan.org/users/documentation/case-studies/losscurves_casestudy.html">here</a></p>
<p>We’ll write up the model in python’s PYMC3 framework and walk through the code. The main point to see here is that we are trying to estimate the losses accruing the a given cohort of insurance policies. So we are estimating in turn both the loss ratio and growth curves. We’ll canvas two options for the growth curves, one based on the logistic curve and another based on the Weibull function. The empirical loss curves show a particular growth curve which rises and plateaus as we step through time.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="loss_ratios.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Loss Ratios</figcaption><p></p>
</figure>
</div>
<p>The modelling challenge is to determine both the probable loss ratios for a given year’s premium and the manner in which the most probable curvature for the accruing losses. The range of probable realisations are, in the Bayesian setting, constrained by the distributions used for the prior probabilities. In our model we need specifications for the priors governing the loss ratios for each year, and the growth factors for each subsequent year. These are combined to calculate the ultimate losses as a growing proportion of the annual premium. We use lognormal priors for to loosely constrain our prior distributions above zero without a fixed upper bound. The growth factors are modeled using either a logistic function over theta and omega at each subsequent year <span class="math inline">\(t\)</span>.</p>
<p><span class="math display">\[t^{\omega} /  (t^{\omega} + \theta^{\omega})\]</span></p>
<p>or</p>
<p><span class="math display">\[ 1 - e^{-(t/ \theta)^{\omega}} \]</span></p>
<p>We set priors for these time-based parameters too, so that by conditioning on the observed data we may learn and update our beliefs about the likely curvature of the growth as we step through time.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="model_structure.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Model Structure</figcaption><p></p>
</figure>
</div>
<p>As we can see here the model has a hierarchical structure, where we have specified a series of priors for each of the features we wish to model. The main call outs are that we model observed cumulative losses ‘loss’ as the likelihood based on the prior parameters of an expected loss ratio ‘LR’, against an input dollar premium, and a latent growth factor ‘gf’ which increases over time. These are combined in the final loss calculation:</p>
<p><span class="math display">\[ (premium * LR_{j})*gf_{i} \]</span></p>
<p>The benefits of using a hierarchical structure is that we can capture the variance of the loss ratios across the years, and this uncertainty propagates through our estimates of the growth parameters fleshing out the range of plausible loss curves. The code to put this altogether is below.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {<span class="st">'mu_LR'</span>: [<span class="dv">0</span>, <span class="fl">0.5</span>], <span class="st">'sd_LR'</span>: [<span class="dv">0</span>, <span class="fl">0.5</span>], <span class="st">'loss_sd'</span>: [<span class="dv">0</span>, <span class="fl">.7</span>], <span class="st">'omega'</span>: [<span class="dv">0</span>, <span class="fl">.5</span>], <span class="st">'theta'</span>: [<span class="dv">0</span>, <span class="fl">.5</span>], </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>         <span class="st">'tune'</span>: <span class="dv">2000</span>, <span class="st">'target_accept'</span>:<span class="fl">.9</span>}</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_model(model_data, params,  growth_function <span class="op">=</span><span class="st">'logistic'</span>):   </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> pm.Model(coords<span class="op">=</span>coords) <span class="im">as</span> basic_model:</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Priors for unknown model parameters</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        mu_LR <span class="op">=</span> pm.Normal(<span class="st">'mu_LR'</span>, params[<span class="st">'mu_LR'</span>][<span class="dv">0</span>],  params[<span class="st">'mu_LR'</span>][<span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        sd_LR <span class="op">=</span> pm.Lognormal(<span class="st">'sd_LR'</span>, params[<span class="st">'sd_LR'</span>][<span class="dv">0</span>], params[<span class="st">'sd_LR'</span>][<span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        LR <span class="op">=</span> pm.Lognormal(<span class="st">'LR'</span>, mu_LR, sd_LR, dims<span class="op">=</span><span class="st">'cohort'</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        loss_sd <span class="op">=</span> pm.Lognormal(<span class="st">'loss_sd'</span>, params[<span class="st">'loss_sd'</span>][<span class="dv">0</span>], params[<span class="st">'loss_sd'</span>][<span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Parameters for the growth factor</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        omega <span class="op">=</span> pm.Lognormal(<span class="st">'omega'</span>, params[<span class="st">'omega'</span>][<span class="dv">0</span>], params[<span class="st">'omega'</span>][<span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> pm.Lognormal(<span class="st">'theta'</span>, params[<span class="st">'theta'</span>][<span class="dv">0</span>], params[<span class="st">'theta'</span>][<span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> pm.Data(<span class="st">"t"</span>, t_values, dims<span class="op">=</span><span class="st">'t_values'</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> growth_function <span class="op">==</span> <span class="st">'logistic'</span>:</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>            gf <span class="op">=</span> pm.Deterministic(<span class="st">'gf'</span>, (t<span class="op">**</span>omega <span class="op">/</span>  (t<span class="op">**</span>omega <span class="op">+</span> theta<span class="op">**</span>omega)), dims<span class="op">=</span><span class="st">'t_values'</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>            gf <span class="op">=</span> pm.Deterministic(<span class="st">'gf'</span>, <span class="dv">1</span><span class="op">-</span>(pm.math.exp(<span class="op">-</span>(t<span class="op">/</span>theta)<span class="op">**</span>omega)), dims<span class="op">=</span><span class="st">'t_values'</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Premium</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        prem <span class="op">=</span> pm.Data(<span class="st">"premium"</span>, premium, dims<span class="op">=</span><span class="st">'cohort'</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        t_indx <span class="op">=</span> pm.Data(<span class="st">"t_idx"</span>, t_idx, dims<span class="op">=</span><span class="st">'obs'</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        cohort_idx <span class="op">=</span> pm.Data(<span class="st">'c_idx'</span>, cohort_id, dims<span class="op">=</span><span class="st">'obs'</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        lm <span class="op">=</span> pm.Deterministic(<span class="st">'lm'</span>, LR[cohort_idx] <span class="op">*</span> prem[cohort_idx] <span class="op">*</span>gf[t_indx], dims<span class="op">=</span>(<span class="st">'obs'</span>))</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Likelihood (sampling distribution) of observations</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> pm.Normal(<span class="st">'loss'</span>, lm, (loss_sd <span class="op">*</span> prem[cohort_idx]), observed<span class="op">=</span>loss_real, dims<span class="op">=</span><span class="st">'obs'</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        prior_checks <span class="op">=</span> pm.sample_prior_predictive(samples<span class="op">=</span><span class="dv">100</span>, random_seed<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        idata <span class="op">=</span> az.from_pymc3(prior<span class="op">=</span>prior_checks)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        trace <span class="op">=</span> pm.sample(return_inferencedata<span class="op">=</span><span class="va">True</span>, tune<span class="op">=</span>params[<span class="st">'tune'</span>], init<span class="op">=</span><span class="st">"adapt_diag"</span>, </span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>                          target_accept<span class="op">=</span>params[<span class="st">'target_accept'</span>])</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        idata.extend(trace)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        ppc <span class="op">=</span> pm.sample_posterior_predictive(</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>            trace, var_names<span class="op">=</span>[<span class="st">"loss"</span>, <span class="st">"LR"</span>, <span class="st">"lm"</span>], random_seed<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        ppc <span class="op">=</span> az.from_pymc3(posterior_predictive<span class="op">=</span>ppc)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        idata.extend(ppc)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> basic_model, idata</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Running these models generates enough sample data to plot some posterior predictive checks and test that our model fits well with the observed data. But crucially we also see a range of plausible curves. These are the probable range of alternatives which can be used to hedge against a range of losses rather than merely expected losses.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ppc_plot.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Posterior Predictive Checks</figcaption><p></p>
</figure>
</div>
<p>Fundamentally we want to model the shape of the curve and our question remains which candidate model is better? Which should we choose to reserve enough premium against future costs?</p>
<section id="model-comparison" class="level3">
<h3 class="anchored" data-anchor-id="model-comparison">Model Comparison</h3>
<p>We’ve seen above how likelihood ratio tests may be used to assess how each model fits the observed data. But it is quite another to determine how the model generalises outside of sample. There are a number complexities to computing model comparisons for Bayesian models, but the state of the art relies on information theoretic measures such as AIC, BIC and WAIC or Leave one out cross-validation methods. Information Criteria mostly rely on some comparison of likelihood as described above, but with some additional controls for the complexity of each model. The cross validation methods asseses predictive fit by successively partitioning the data into training and test sets where the accuracy of the fit is assessed by predicting on the test set. The details of the computation are a little complex but Aki Vehtari has a nice tutorial <a href="https://avehtari.github.io/modelselection/modelselection_tutorial_slides.pdf">here</a>. The output of the LOO test ranks each model according to their relative accuracy.</p>

<table class="dataframe table table-sm table-striped">
<thead>
<tr>
<th>
</th>
<th>
rank
</th>
<th>
loo
</th>
<th>
p_loo
</th>
<th>
d_loo
</th>
<th>
weight
</th>
<th>
se
</th>
<th>
dse
</th>
<th>
warning
</th>
<th>
loo_scale
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
Weibull Growth Model
</th>
<td>
0
</td>
<td>
-387.915
</td>
<td>
10.6734
</td>
<td>
0
</td>
<td>
0.811983
</td>
<td>
11.1849
</td>
<td>
0
</td>
<td>
True
</td>
<td>
log
</td>
</tr>
<tr>
<th>
Logistic Growth Model
</th>
<td>
1
</td>
<td>
-391.599
</td>
<td>
10.2344
</td>
<td>
3.68346
</td>
<td>
0.188017
</td>
<td>
11.874
</td>
<td>
3.81681
</td>
<td>
True
</td>
<td>
log
</td>
</tr>
</tbody>

</table>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="model_compare_plot.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Model Comparison</figcaption><p></p>
</figure>
</div>
<p>In our case we can see that the Weibull growth model is deemed slightly more accurate than the logistic model. Allowing us to lean on criteria of predictive accuracy in choosing our model specification.</p>
</section>
</section>
<section id="conclusion-confidence-in-point-estimates-are-unearned." class="level2">
<h2 class="anchored" data-anchor-id="conclusion-confidence-in-point-estimates-are-unearned.">Conclusion: Confidence in Point Estimates are Unearned.</h2>
<p>This discussion serves to show the wide applicability of Bayesian decision theory and its implicit role in many of the typical frequentist decision rules and model comparison tests. The role and flexibility of the priors in those decision rules suggests that it’s better to be aware of the specification of the prior rather than assume their use is innocent. More positively the Bayesian approach to decision making benefits from our ability to sample directly from the posterior and evaluate probable loss across an entire range of the probability distribution. The gains of this perspective are a clarity and confidence that cannot come from decisions made on the comparison of simple point estimates.</p>
<p>More concretely, we’ve seen an explicit model of loss-curve generation where we’ve specified the priors and constraints as appropriate and varied the parameters to test and evaluate competing theories in an transparent probabilisitc framework. This is not a black box model or an obscure algorithm, but a chain of reasoning embedded in code. Any decision worth making is worth the effort required to trace it’s dimensions thouroughly. The value of the exercise stems as much from complexities and pitfalls discovered in the modelling process as the ultimate the decision it enables. There is then something cheap about an alternative model which fails to capture the richness of the data generating process and settles for a simple point estimate comparison. Perhaps we’re guilty of a sunk-costs fallacy - valuing the model for the effort it took to make rather than it’s genuine usefulness, but at least Bayesian models enforce awareness of choices made and assumptions accepted. The tangled network-graph of influence and dependencies traces out a series of discoveries. Our conclusions rest on our best understanding of the process and our decisions are justified in light of the full range of each candidate probability distribution and our explanations can be traced back through the web of these beliefs.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>