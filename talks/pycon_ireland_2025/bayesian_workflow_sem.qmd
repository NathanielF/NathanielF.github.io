---
title: "Bayesian Workflow with SEMs"
subtitle: "PyCon Ireland 2025"
author: 
    - name: Nathaniel Forde
      affiliations: 
        - Data Science @ Personio
        - and Open Source Contributor @ PyMC
format: 
    revealjs:
         theme: [default, clean.scss]
         logo: images/dublinSkyline.png
         footer: "Bayesian Workflow for SEMs with PyMC"
         scroll: true
         fig-width: 6
         fig-height: 4
categories: [bayesian, discrete choice]
image: "images/dublinSkyline.png"
date: "2025-11-16"
---

## Preliminaries {.smaller}

:::{.callout-warning}
## Who am I?
- I'm a __data scientist__ at __[Personio]{.blue}__ 
    - Bayesian statistician, 
    - Reformed philosopher and logician. 
- Website: [https://nathanielf.github.io/](https://nathanielf.github.io/)
:::
 
::: {.callout-tip}
## Code or it didn't Happen
The worked examples used here can be found [here](https://www.pymc-marketing.io/en/latest/notebooks/customer_choice/nested_logit.html)
:::

![My Website](images/QR_CODE.png)


## **The Pitch**: _Inference done Right_

Structural Equation modelling is [__most compellingly__]{.blue} conducted in a Bayesian Setting. Bayesian inference done right, adheres to transparent workflow. 

SEM modelling in **PyMC** enables us to automate much of this workflow of model evaluation, and parameter recovery even with complex hierarchical SEM models.  

## Agenda

- __The Idea of a Workflow__: Craft versus Checklist
- __Structural Equation Models__: Expressive Candidate Structure(s)
- __Bayesian Workflow with SEMs__: 
  - Confirmatory Factor Structures
  - Adding Structural Relations
  - Adding Covariance Structure
  - Adding Hierarchical Structure
  - Parameter Recovery and Model Validation
- __Conclusion__

# The __Idea__ of a _Workflow_

## Craft in Statistical Modelling {.smaller}

:::: {.columns}
::: {.column}
- Embraces process, imperfection, and iteration.

::: {.fragment .fade-in}
- Aims at the acquisition of scientific knowledge
:::

::: {.fragment .fade-in}
- Supports generalisable findings and solutions
:::

::: {.fragment .fade-in}
- Restore ownership: you shape, test, and refine — the model carries your imprint.
:::

:::
::: {.column}
![](images/craft_v_corporate.png)
:::
:::

## Checklists as a Methodological Error {.smaller}

::::  {.columns}
::: {.column}
- Reduce inquiry to compliance: ticking boxes replaces genuine understanding.

::: {.fragment .fade-in}
- Create the illusion of rigor while bypassing uncertainty and context.
:::

::: {.fragment .fade-in}
- Confuse progress with throughput: more boxes checked ≠ better science.
:::

::: {.fragment .fade-in}
- Strip away ownership: you don’t make something, you just complete a task.
:::

:::
::: {.column}
![](images/metric_blindness.png)
:::
::: 

## Job Satisfaction Data {.smaller}

:::: {.rows}
::: {.row}
- Constructive Thought Strategies (CTS): Thought patterns that are positive or helpful, such as:
  - __Self-Talk__ (positive internal dialogue): `ST`
  - __Mental Imagery__ (visualizing successful performance or outcomes): `MI` 
  - __Evaluating Beliefs & Assumptions__ (i.e. critically assessing one’s internal assumption: `EBA`
- Dysfunctional Thought Processes: (`DA1`–`DA3`)
- Subjective Well Being: (`UF1`, `UF2`, `FOR`)
- Job Satisfaction: (`JW1`–`JW3`)
:::
::: {.row}

![](images/SEM_structure.png)
:::
:::

## Contemporary Bayesian Workflow {.smaller}

:::: {.columns}
::: {.column}
- Start with Theory and Prior Knowledge
- Iterate with Checks and Visual Diagnostics
- Refine Structure and Layer Complexity
  - Assess consistency of Signal
- Validate through Sensitivty Analysis
- Own the Process

::: {.fragment .fade-in}
![](images/arviz.png)
:::

:::
::: {.column}
![The Big Red Book of Bayesian Modelling](images/bda_image.jpg)

::: {.fragment .fade-in}
![](images/pymc.jpg)
:::

:::
::: 


# Structural Equation Models
Richly Parameterised Regressions with Expressive Encodings of Measurement Error and Latent Constructs

## The Confirmatory Factor Structure


<img src="images/cfa_excalidraw.png"  width="80%" height="80%">

## The SEM Regression

<img src="images/sem1_excalidraw.png"  width="80%" height="80%">

## The SEM Regression 
### with Mediation Effects

<img src="images/sem2_excalidraw.png"  width="90%" height="90%">

## The SEM Regression 
### with Staggered Mediation Effects

<img src="images/sem3_excalidraw.png"  width="90%" height="90%">

## The SEM Regression 
### with Residuals Covariance Structures

<img src="images/final_sem_excalidraw.png"  width="90%" height="90%">

## The SEM Regression 
### with Hierarchical Structure

<img src="images/hierarchical_excalidraw.png"  width="90%" height="90%">


## The SEM workflow {.smaller}

- Start with CFA:
  - Validate that our measurement model holds.
  - Ensure latent constructs are reliably represented by observed indicators.

- Layer Structural Paths:
  - Add theoretically-motivated regressions between constructs.
  - Test whether hypothesized relationships improve model fit.

- Refine with Residual Covariances:
  - Account for specific shared variance not captured by factors.
  - Keep structure transparent while improving realism.

- Iterative Validation:
  - Each step asks: Does this addition honor theory? Improve fit?
  - Workflow = constant negotiation between parsimony and fidelity.


## Bayesian Workflow with SEMs
### CFA

:::: {.columns}
::: {.column width="70%"}

```{.python code-line-numbers="|9-14|18|21|25"}
with pm.Model(coords=coords) as cfa_model_v1:
    
    # --- Factor loadings ---
    lambdas_1 = make_lambda('indicators_1', 'lambdas1', priors=[1, .5])
    lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, .5])
    lambdas_3 = make_lambda('indicators_3', 'lambdas3', priors=[1, .5])
    lambdas_4 = make_lambda('indicators_4', 'lambdas4', priors=[1, .5])

    Lambda = pt.zeros((12, 4))
    Lambda = pt.set_subtensor(Lambda[0:3, 0], lambdas_1)
    Lambda = pt.set_subtensor(Lambda[3:6, 1], lambdas_2)
    Lambda = pt.set_subtensor(Lambda[6:9, 2], lambdas_3)
    Lambda = pt.set_subtensor(Lambda[9:12, 3], lambdas_4)
    Lambda = pm.Deterministic('Lambda', Lambda)

    sd_dist = pm.Exponential.dist(1.0, shape=4)
    chol, _, _ = pm.LKJCholeskyCov("chol_cov", n=4, eta=2, sd_dist=sd_dist, compute_corr=True)
    eta = pm.MvNormal("eta", 0, chol=chol, dims=("obs", "latent"))

    # Construct Pseudo Observation matrix based on Factor Loadings
    mu = pt.dot(eta, Lambda.T)  # (n_obs, n_indicators)

    ## Error Terms
    Psi = pm.InverseGamma("Psi", 5, 10, dims="indicators")
    _ = pm.Normal('likelihood', mu=mu, sigma=Psi, observed=observed_data)

```

:::
::: {.column width="30%"}
$$ \eta \sim MvN(0, \Sigma)$$
$$ \mu = \color{blue}{\Lambda} \eta$$
$$\mathbf{x} = N(\mu, \Psi)$$

:::
::: 

## Bayesian Workflow with SEMs {.smaller}
### CFA Implications

:::: {.columns}

::: {.column width="70%"}
<img src="images/cfa_model_summary.png"  width="90%" height="90%">
:::

::: {.column width="30%"}
- Estimated Factor Loadings are close to 1
- The indicator(s) are strongly reflective of the latent factor.
- Posterior Predictive Residuals are close to 0
- Latent factors move together in intuitive ways.
- High Satisfaction ~~ High Well Being
:::
::: 



## Bayesian Workflow with SEMs
### SEM

:::: {.columns}
::: {.column width="70%"}

```{.python code-line-numbers="|21-22|24|30"}
with pm.Model(coords=coords) as sem_model_v3:
    
    # --- Factor loadings ---
    lambdas_1 = make_lambda('indicators_1', 'lambdas1', priors=[1, .5])
    lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, .5])
    lambdas_3 = make_lambda('indicators_3', 'lambdas3', priors=[1, .5])
    lambdas_4 = make_lambda('indicators_4', 'lambdas4', priors=[1, .5])

    Lambda = pt.zeros((12, 4))
    Lambda = pt.set_subtensor(Lambda[0:3, 0], lambdas_1)
    Lambda = pt.set_subtensor(Lambda[3:6, 1], lambdas_2)
    Lambda = pt.set_subtensor(Lambda[6:9, 2], lambdas_3)
    Lambda = pt.set_subtensor(Lambda[9:12, 3], lambdas_4)
    Lambda = pm.Deterministic('Lambda', Lambda)

    latent_dim = len(coords['latent'])

    sd_dist = pm.Exponential.dist(1.0, shape=latent_dim)
    chol, _, _ = pm.LKJCholeskyCov("chol_cov", n=latent_dim, eta=2, sd_dist=sd_dist, compute_corr=True)
    gamma = pm.MvNormal("gamma", 0, chol=chol, dims=("obs", "latent"))

    B = make_B()
    I = pt.eye(latent_dim)
    eta = pm.Deterministic("eta", pt.slinalg.solve(I - B + 1e-8 * I, gamma.T).T)  

    mu = pt.dot(eta, Lambda.T)

    ## Error Terms
    Psi = make_Psi('indicators')
    _ = pm.MvNormal('likelihood', mu=mu, cov=Psi, observed=observed_data)

```

:::
::: {.column width="30%"}
$$\zeta \sim MvN(0, \Sigma_{\zeta})$$
$$\eta = (I-\color{blue}{B})^{-1} \zeta$$
$$\mu = \Lambda \eta_i$$
$$ \mathbf{x} \mid \eta \sim MvN(\mu, \Psi)$$
:::
::: 

## Bayesian Workflow with SEMs {.smaller}
### SEM Implications

:::: {.columns}

::: {.column width="70%"}
![](images/sem_model_summary.png)
:::
::: {.column width="30%"}
- The Beta coefficients encode the directional effects of the latent constructs on one another
- High Dysfunction -> Negative Impact on Satisfaction
- Factor loadings remain close to 1
- Posterior Predictive of the Residuals have Improved
:::
:::



## Bayesian Workflow with SEMs
### Mean Structures and Marginalising

:::: {.columns}
::: {.column width="70%"}

```{.python code-line-numbers="|28-31|32-34"}
with pm.Model(coords=coords) as sem_model_mean_structure:
  # --- Factor loadings ---
  lambdas_1 = make_lambda('indicators_1', 'lambdas1', priors=[1, .5])
  lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, .5])
  lambdas_3 = make_lambda('indicators_3', 'lambdas3', priors=[1, .5])
  lambdas_4 = make_lambda('indicators_4', 'lambdas4', priors=[1, .5])

  Lambda = pt.zeros((12, 4))
  Lambda = pt.set_subtensor(Lambda[0:3, 0], lambdas_1)
  Lambda = pt.set_subtensor(Lambda[3:6, 1], lambdas_2)
  Lambda = pt.set_subtensor(Lambda[6:9, 2], lambdas_3)
  Lambda = pt.set_subtensor(Lambda[9:12, 3], lambdas_4)
  Lambda = pm.Deterministic('Lambda', Lambda)

  sd_dist = pm.Exponential.dist(1.0, shape=4)
  chol, _, _ = pm.LKJCholeskyCov("chol_cov", n=4, eta=2, sd_dist=sd_dist, compute_corr=True)

  Psi_zeta = pm.Deterministic("Psi_zeta", chol.dot(chol.T))
  Psi = make_Psi('indicators')

  B = make_B()
  latent_dim = len(coords['latent'])
  I = pt.eye(latent_dim)
  lhs = I - B + 1e-8 * pt.eye(latent_dim)  # (latent_dim, latent_dim)
  inv_lhs = pm.Deterministic('solve_I-B', pt.slinalg.solve(lhs, pt.eye(latent_dim)), dims=('latent', 'latent1'))

  # Mean Structure
  tau = pm.Normal("tau", mu=0, sigma=0.5, dims="indicators")   # observed intercepts 
  alpha = pm.Normal("alpha", mu=0, sigma=0.5, dims="latent")       # latent means
  mu_y = pm.Deterministic("mu_y", tau + pt.dot(Lambda, pt.dot(inv_lhs, alpha)))

  Sigma_y = pm.Deterministic('Sigma_y', Lambda.dot(inv_lhs).dot(Psi_zeta).dot(inv_lhs.T).dot(Lambda.T) + Psi)
  M = Psi_zeta @ inv_lhs @ Lambda.T @ pm.math.matrix_inverse(Sigma_y)
  eta_hat = pm.Deterministic('eta', alpha + (M @ (observed_data - mu_y).T).T, dims=('obs', 'latent')) 
  _ = pm.MvNormal("likelihood", mu=mu_y, cov=Sigma_y, observed=observed_data)


```
:::
::: {.column width="30%"}
$$\color{blue}{\Sigma_{\mathcal{x}}} = \Psi + \\ \Lambda(I - B)^{-1}\Psi_{\gamma}(I - B)^{T}\Lambda^{T} $$

$$ \mathcal{x} \sim MvN(\tau, \Sigma_{x})$$
:::
:::

## Bayesian Workflow with SEMs {.smaller}
### Mean Structure Implications

:::: {.columns}
::: {.column width="70%"}
![](images/sem_model_marginal_summary.png)
:::
::: {.column width="30%"}
- Mean Structure Parameters $\tau$ show poor identification
- Beta coefficients and Factor Loading Estimates consistent with prior models
- Posterior Predictive check on Residuals provide evidence of a good fit.



:::
::: 


## Bayesian Workflow with SEMs {.smaller}
### Comparing Model Estimates

:::: {.columns}
::: {.column width="70%"}
![](images/compare_model_parameters.png)
:::
::: {.column width="30%"}
- The measurement model is stable: adding the structural paths did not disrupt how the latent variables are measured.
- SEM is not introducing distortions into the factor structure.
- The hypothesized regression paths are consistent with the observed covariance patterns.
:::
::: 


## Bayesian Workflow with SEMs 
### Comparing Measures of Global Fit

![](images/comparison_table.png)
