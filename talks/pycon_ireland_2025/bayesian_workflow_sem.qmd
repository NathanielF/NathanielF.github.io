---
title: "Bayesian Workflow with SEMs"
subtitle: "PyCon Ireland 2025"
author: 
    - name: Nathaniel Forde
      affiliations: 
        - Data Science @ Personio
        - and Open Source Contributor @ PyMC
format: 
    revealjs:
         theme: [default, clean.scss]
         logo: images/dublinSkyline.png
         footer: "Bayesian Workflow for SEMs with PyMC"
         scroll: true
         fig-width: 6
         fig-height: 4
categories: [bayesian, discrete choice]
image: "images/dublinSkyline.png"
date: "2025-11-16"
---

## Preliminaries {.smaller}

:::{.callout-warning}
## Who am I?
- I'm a __data scientist__ at __[Personio]{.blue}__ 
    - Bayesian statistician, 
    - Reformed philosopher and logician. 
- Website: [https://nathanielf.github.io/](https://nathanielf.github.io/)
:::
 
::: {.callout-tip}
## Code or it didn't Happen
The worked examples used here can be found [here](https://www.pymc.io/projects/examples/en/latest/case_studies/bayesian_sem_workflow.html#bayesian-workflow-with-sems)
:::

![My Website](images/QR_CODE.png)


## **The Pitch**

[__Structuring Your Work with Structural Equation Models__]{.blue}

Probabilistic programming languages (PPLs) are structures for articulating assumptions and relationships, and a scaffold for exploring uncertainty.

Structural Equation models (SEMs) formalize scientific theory as a system of statistical relationships.

The Bayesian workflow __binds these together into a compelling practice__: an iterative conversation between theory and data disciplined by rigours of probabilistic programming


## Agenda {.smaller}

- __The Idea of a Workflow__: 
  - Craft versus Checklist
  - Job Satisfaction
- __Bayesian Workflow with SEMs__: 
  - Confirmatory Factor Structures
  - Adding Structural Relations
  - Adding Covariance Structure
- __Sensitivity Analysis__:
  - Adding Hierarchical Structure
  - Parameter Recovery and Model Validation
- __Conclusion__
  - Craft and Statistical Workflow


# The __Idea__ of a _Workflow_

## Craft in Statistical Modelling {.smaller}

:::: {.columns}
::: {.column}
- Embraces process, imperfection, and iteration.

::: {.fragment .fade-in}
- Aims at the acquisition of scientific knowledge
:::

::: {.fragment .fade-in}
- Supports generalisable findings and solutions
:::

::: {.fragment .fade-in}
- Restore ownership: you shape, test, and refine — the model carries your imprint.
:::

:::
::: {.column}
![](images/craft_v_corporate.png)
:::
:::

## Checklists in Statistical Modelling {.smaller}

::::  {.columns}
::: {.column}
- Reduce inquiry to compliance: ticking boxes replaces genuine understanding.

::: {.fragment .fade-in}
- Create the illusion of rigor while bypassing uncertainty and context.
:::

::: {.fragment .fade-in}
- Confuse progress with throughput: more boxes checked ≠ better science.
:::

::: {.fragment .fade-in}
- Promotes shallow levels of engagement, infantalises the management class. Hinders effective decision making.
:::

::: {.fragment .fade-in}
- Strips away ownership: you don’t make something, you just complete a task.
:::

:::
::: {.column}
![](images/metric_blindness.png)
:::
::: 

## Job Satisfaction Data {.smaller}

:::: {.rows}
::: {.row}
- Constructive Thought Strategies (CTS): Thought patterns that are positive or helpful, such as:
  - __Self-Talk__ (positive internal dialogue): `ST`
  - __Mental Imagery__ (visualizing successful performance or outcomes): `MI` 
  - __Evaluating Beliefs & Assumptions__ (i.e. critically assessing one’s internal assumption: `EBA`
- Dysfunctional Thought Processes: (`DA1`–`DA3`)
- Subjective Well Being: (`UF1`, `UF2`, `FOR`)
- Job Satisfaction: (`JW1`–`JW3`)
:::
::: {.row}

![](images/SEM_structure.png)
:::
:::

## Contemporary Bayesian Workflow {.smaller}

:::: {.columns}
::: {.column}
- Start with Theory and Prior Knowledge
- Iterate with Checks and Visual Diagnostics
- Refine Structure and Layer Complexity
  - Assess consistency of Signal
- Validate through Sensitivty Analysis
- [Own the Process](https://arxiv.org/abs/2011.01808)

::: {.fragment .fade-in}
![](images/arviz.png)
:::

:::
::: {.column}
![The Big Red Book of Bayesian Modelling](images/bda_image.jpg)

::: {.fragment .fade-in}
![](images/pymc.jpg)
:::

:::
::: 


# Structural Equation Models
Richly Parameterised Regressions with Expressive Encodings of Measurement Error and Latent Constructs

## The SEM workflow {.smaller}

- Start with Confirmatory Factor Analysis (CFA):
  - Validate that our measurement model holds.
  - Ensure latent constructs are reliably represented by observed indicators.

- Layer Structural Paths:
  - Add theoretically-motivated regressions between constructs.
  - Assess whether hypothesized relationships improve model fit.

- Refine with Residual Covariances:
  - Account for specific shared variance not captured by factors.
  - Keep structure transparent while improving realism.

- Iterative Validation:
  - Each step asks: Does this addition honor theory? Improve fit?
  - Workflow = constant negotiation between parsimony and fidelity.

## Job Satisfaction Data {.smaller}

```{python}

#| echo: false
import numpy as np
import pandas as pd

# Standard deviations
stds = np.array([0.939, 1.017, 0.937, 0.562, 0.760, 0.524, 
                 0.585, 0.609, 0.731, 0.711, 1.124, 1.001])

n = len(stds)

# Lower triangular correlation values as a flat list
corr_values = [
    1.000,
    .668, 1.000,
    .635, .599, 1.000,
    .263, .261, .164, 1.000,
    .290, .315, .247, .486, 1.000,
    .207, .245, .231, .251, .449, 1.000,
   -.206, -.182, -.195, -.309, -.266, -.142, 1.000,
   -.280, -.241, -.238, -.344, -.305, -.230,  .753, 1.000,
   -.258, -.244, -.185, -.255, -.255, -.215,  .554,  .587, 1.000,
    .080,  .096,  .094, -.017,  .151,  .141, -.074, -.111,  .016, 1.000,
    .061,  .028, -.035, -.058, -.051, -.003, -.040, -.040, -.018,  .284, 1.000,
    .113,  .174,  .059,  .063,  .138,  .044, -.119, -.073, -.084,  .563,  .379, 1.000
]

# Fill correlation matrix
corr_matrix = np.zeros((n, n))
idx = 0
for i in range(n):
    for j in range(i+1):
        corr_matrix[i, j] = corr_values[idx]
        corr_matrix[j, i] = corr_values[idx]
        idx += 1

# Covariance matrix: Sigma = D * R * D
cov_matrix = np.outer(stds, stds) * corr_matrix
#cov_matrix_test = np.dot(np.dot(np.diag(stds), corr_matrix), np.diag(stds))
FEATURE_COLUMNS=["JW1","JW2","JW3", "UF1","UF2","FOR", "DA1","DA2","DA3", "EBA","ST","MI"]
corr_df = pd.DataFrame(corr_matrix, columns=FEATURE_COLUMNS)

cov_df = pd.DataFrame(cov_matrix, columns=FEATURE_COLUMNS)

def make_sample(cov_matrix, size, columns, missing_frac=0.0, impute=False):
    sample_df = pd.DataFrame(np.random.multivariate_normal([0]*12, cov_matrix, size=size), columns=FEATURE_COLUMNS)
    if missing_frac > 0.0: 
        total_values = sample_df.size
        num_nans = int(total_values * missing_frac)

        # Choose random flat indices
        nan_indices = np.random.choice(total_values, num_nans, replace=False)

        # Convert flat indices to (row, col)
        rows, cols = np.unravel_index(nan_indices, sample_df.shape)

        # Set the values to NaN
        sample_df.values[rows, cols] = np.nan

    if impute: 
        sample_df.fillna(sample_df.mean(axis=0), inplace=True)


    return sample_df

sample_df = make_sample(cov_matrix, 263, FEATURE_COLUMNS)
```



```{python}
#| echo: false
def header_style():
    return [
        "color: red; font-weight: bold;",
        "color: blue; font-weight: bold;",
        "color: green; font-weight: bold;"
    ]


sample_df.head().style.set_properties(**{'background-color': 'skyblue'}, subset=['JW1',	'JW2',	'JW3']).set_properties(**{'background-color': 'moccasin'}, subset=['DA1',	'DA2',	'DA3']).set_properties(**{'background-color': 'lightcoral'}, subset=['EBA',	'ST',	'MI'])


```

The Data for SEM modelling is a multivariate data structure with natural theor-driven categories of variables which reflect some mis-measured latent factor. 

## The Confirmatory Factor Structure

<img src="images/cfa_excalidraw.png"  width="80%" height="80%">

## Bayesian Workflow with SEMs
### CFA

:::: {.columns}
::: {.column width="70%"}

```{.python code-line-numbers="|9-14|18|21|25"}
with pm.Model(coords=coords) as cfa_model_v1:
    
    # --- Factor loadings ---
    lambdas_1 = make_lambda('indicators_1', 'lambdas1', priors=[1, .5])
    lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, .5])
    lambdas_3 = make_lambda('indicators_3', 'lambdas3', priors=[1, .5])
    lambdas_4 = make_lambda('indicators_4', 'lambdas4', priors=[1, .5])

    Lambda = pt.zeros((12, 4))
    Lambda = pt.set_subtensor(Lambda[0:3, 0], lambdas_1)
    Lambda = pt.set_subtensor(Lambda[3:6, 1], lambdas_2)
    Lambda = pt.set_subtensor(Lambda[6:9, 2], lambdas_3)
    Lambda = pt.set_subtensor(Lambda[9:12, 3], lambdas_4)
    Lambda = pm.Deterministic('Lambda', Lambda)

    sd_dist = pm.Exponential.dist(1.0, shape=4)
    chol, _, _ = pm.LKJCholeskyCov("chol_cov", n=4, eta=2, sd_dist=sd_dist, compute_corr=True)
    eta = pm.MvNormal("eta", 0, chol=chol, dims=("obs", "latent"))

    # Construct Pseudo Observation matrix based on Factor Loadings
    mu = pt.dot(eta, Lambda.T)  # (n_obs, n_indicators)

    ## Error Terms
    Psi = pm.InverseGamma("Psi", 5, 10, dims="indicators")
    _ = pm.Normal('likelihood', mu=mu, sigma=Psi, observed=observed_data)

```

:::
::: {.column width="30%"}
$$ \eta \sim MvN(0, \Sigma)$$
$$ \mu = \color{blue}{\Lambda} \eta$$
$$\mathbf{y} = N(\mu, \Psi)$$

:::
::: 

## Bayesian Workflow with SEMs {.smaller}
### CFA Implications

:::: {.columns}

::: {.column width="70%"}
<img src="images/cfa_model_summary.png"  width="90%" height="90%">
:::

::: {.column width="30%"}
- Estimated Factor Loadings are close to 1
- The indicator(s) are strongly reflective of the latent factor.
- Posterior Predictive Residuals are close to 0
- Latent factors move together in intuitive ways.
- High Satisfaction ~~ High Well Being
:::
::: 


## The SEM Regression

<img src="images/sem1_excalidraw.png"  width="80%" height="80%">


## The SEM Regression 
### with Mediation Effects

<img src="images/sem2_excalidraw.png"  width="90%" height="90%">

## The SEM Regression 
### with Staggered Mediation Effects

<img src="images/sem3_excalidraw.png"  width="90%" height="90%">

## The SEM Regression 
### with Residuals Covariance Structures

<img src="images/final_sem_excalidraw.png"  width="90%" height="90%">

## Bayesian Workflow with SEMs
### SEM

:::: {.columns}
::: {.column width="70%"}

```{.python code-line-numbers="|21-22|24|30"}
with pm.Model(coords=coords) as sem_model_v3:
    
    # --- Factor loadings ---
    lambdas_1 = make_lambda('indicators_1', 'lambdas1', priors=[1, .5])
    lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, .5])
    lambdas_3 = make_lambda('indicators_3', 'lambdas3', priors=[1, .5])
    lambdas_4 = make_lambda('indicators_4', 'lambdas4', priors=[1, .5])

    Lambda = pt.zeros((12, 4))
    Lambda = pt.set_subtensor(Lambda[0:3, 0], lambdas_1)
    Lambda = pt.set_subtensor(Lambda[3:6, 1], lambdas_2)
    Lambda = pt.set_subtensor(Lambda[6:9, 2], lambdas_3)
    Lambda = pt.set_subtensor(Lambda[9:12, 3], lambdas_4)
    Lambda = pm.Deterministic('Lambda', Lambda)

    latent_dim = len(coords['latent'])

    sd_dist = pm.Exponential.dist(1.0, shape=latent_dim)
    chol, _, _ = pm.LKJCholeskyCov("chol_cov", n=latent_dim, eta=2, sd_dist=sd_dist, compute_corr=True)
    gamma = pm.MvNormal("gamma", 0, chol=chol, dims=("obs", "latent"))

    B = make_B()
    I = pt.eye(latent_dim)
    eta = pm.Deterministic("eta", pt.slinalg.solve(I - B + 1e-8 * I, gamma.T).T)  

    mu = pt.dot(eta, Lambda.T)

    ## Error Terms
    Psi = make_Psi('indicators')
    _ = pm.MvNormal('likelihood', mu=mu, cov=Psi, observed=observed_data)

```

:::
::: {.column width="30%"}
$$\zeta \sim MvN(0, \Sigma_{\zeta})$$
$$\eta = (I-\color{blue}{B})^{-1} \zeta$$
$$\mu = \Lambda \eta_i$$
$$ \mathbf{y} \mid \eta \sim MvN(\mu, \Psi)$$
:::
::: 

## Bayesian Workflow with SEMs {.smaller}
### SEM Implications

:::: {.columns}

::: {.column width="70%"}
![](images/sem_model_summary.png)
:::
::: {.column width="30%"}
- The Beta coefficients encode the directional effects of the latent constructs on one another
- High Dysfunction -> Negative Impact on Satisfaction
- Factor loadings remain close to 1
- Posterior Predictive of the Residuals have Improved
:::
:::



## Bayesian Workflow with SEMs
### Mean Structures and Marginalising

:::: {.columns}
::: {.column width="70%"}

```{.python code-line-numbers="|28-31|32|33-34"}
with pm.Model(coords=coords) as sem_model_mean_structure:
  # --- Factor loadings ---
  lambdas_1 = make_lambda('indicators_1', 'lambdas1', priors=[1, .5])
  lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, .5])
  lambdas_3 = make_lambda('indicators_3', 'lambdas3', priors=[1, .5])
  lambdas_4 = make_lambda('indicators_4', 'lambdas4', priors=[1, .5])

  Lambda = pt.zeros((12, 4))
  Lambda = pt.set_subtensor(Lambda[0:3, 0], lambdas_1)
  Lambda = pt.set_subtensor(Lambda[3:6, 1], lambdas_2)
  Lambda = pt.set_subtensor(Lambda[6:9, 2], lambdas_3)
  Lambda = pt.set_subtensor(Lambda[9:12, 3], lambdas_4)
  Lambda = pm.Deterministic('Lambda', Lambda)

  sd_dist = pm.Exponential.dist(1.0, shape=4)
  chol, _, _ = pm.LKJCholeskyCov("chol_cov", n=4, eta=2, sd_dist=sd_dist, compute_corr=True)

  Psi_zeta = pm.Deterministic("Psi_zeta", chol.dot(chol.T))
  Psi = make_Psi('indicators')

  B = make_B()
  latent_dim = len(coords['latent'])
  I = pt.eye(latent_dim)
  lhs = I - B + 1e-8 * pt.eye(latent_dim)  # (latent_dim, latent_dim)
  inv_lhs = pm.Deterministic('solve_I-B', pt.slinalg.solve(lhs, pt.eye(latent_dim)), dims=('latent', 'latent1'))

  # Mean Structure
  tau = pm.Normal("tau", mu=0, sigma=0.5, dims="indicators")   # observed intercepts 
  alpha = pm.Normal("alpha", mu=0, sigma=0.5, dims="latent")       # latent means
  mu_y = pm.Deterministic("mu_y", tau + pt.dot(Lambda, pt.dot(inv_lhs, alpha)))

  Sigma_y = pm.Deterministic('Sigma_y', Lambda.dot(inv_lhs).dot(Psi_zeta).dot(inv_lhs.T).dot(Lambda.T) + Psi)
  M = Psi_zeta @ inv_lhs @ Lambda.T @ pm.math.matrix_inverse(Sigma_y)
  eta_hat = pm.Deterministic('eta', alpha + (M @ (observed_data - mu_y).T).T, dims=('obs', 'latent')) 
  _ = pm.MvNormal("likelihood", mu=mu_y, cov=Sigma_y, observed=observed_data)


```
:::
::: {.column width="30%"}
$$\mu = \tau + \Lambda(1 - B)^{-1}\alpha$$
$$\color{blue}{\Sigma_{\mathcal{y}}} = \Psi + \\ \Lambda(I - B)^{-1}\Psi_{\gamma}(I - B)^{T}\Lambda^{T} $$

$$ \mathcal{y} \sim MvN(\mu, \Sigma_{y})$$
:::
:::

## Bayesian Workflow with SEMs {.smaller}
### Mean Structure Implications

:::: {.columns}
::: {.column width="70%"}
![](images/sem_model_marginal_summary.png)
:::
::: {.column width="30%"}
- Mean Structure Parameters $\tau$ show poor identification
- Beta coefficients and Factor Loading Estimates consistent with prior models
- Posterior Predictive check on Residuals provide evidence of a good fit.



:::
::: 

# _Sensitivity_ Analysis
__Knowing what your Model Implies__

## Bayesian Workflow with SEMs
### Assessing the Indirect Effects

![](images/direct_indirect.png)


## Bayesian Workflow with SEMs {.smaller}
### Comparing Model Estimates

:::: {.columns}
::: {.column width="60%"}
![](images/compare_model_parameters.png)
:::
::: {.column width="40%"}
- The measurement model is stable: adding the structural paths did not disrupt how the latent variables are measured.
- SEM is not introducing distortions into the factor structure.
- The hypothesized regression paths are consistent with the observed covariance patterns.
- __Constructive thought processes: [self-talk, visualisation and evaluation of beliefs]{.blue} improve job satisfaction.__
- Effects are achieved directly and indirectly through overall well being.
:::
::: 


## Variations on theme
### with Hierarchical Structure

<img src="images/hierarchical_excalidraw.png"  width="90%" height="90%">

## Hierarchies in Code
### Complexity and Rich Parameterisation

:::: {.columns}
::: {.column width="70%"}

```{.python code-line-numbers="|24-27|33-36|41-43|45-50"}
coords['group'] = ['man', 'woman']
with pm.Model(coords=coords) as sem_model_hierarchical2:
    
    # --- Factor loadings ---
    lambdas_1 = make_lambda('indicators_1', 'lambdas1', priors=[1, .5])
    lambdas_2 = make_lambda('indicators_2', 'lambdas2', priors=[1, .5])
    lambdas_3 = make_lambda('indicators_3', 'lambdas3', priors=[1, .5])
    lambdas_4 = make_lambda('indicators_4', 'lambdas4', priors=[1, .5])

    Lambda = pt.zeros((12, 4))
    Lambda = pt.set_subtensor(Lambda[0:3, 0], lambdas_1)
    Lambda = pt.set_subtensor(Lambda[3:6, 1], lambdas_2)
    Lambda = pt.set_subtensor(Lambda[6:9, 2], lambdas_3)
    Lambda = pt.set_subtensor(Lambda[9:12, 3], lambdas_4)
    Lambda = pm.Deterministic('Lambda', Lambda)

    sd_dist = pm.Exponential.dist(1.0, shape=4)
    chol, _, _ = pm.LKJCholeskyCov("chol_cov", n=4, eta=2, sd_dist=sd_dist, compute_corr=True)

    Psi_zeta = pm.Deterministic("Psi_zeta", chol.dot(chol.T))
    Psi = make_Psi('indicators')

    Bs = []
    for g in coords["group"]:
        B_g = make_B(group_suffix=f"_{g}")  # give group-specific names
        Bs.append(B_g)
    B_ = pt.stack(Bs)

    latent_dim = len(coords['latent'])
    I = pt.eye(latent_dim)

    # invert (I - B_g) for each group
    inv_I_minus_B = pt.stack([
        pt.slinalg.solve(I - B_[g] + 1e-8 * I, I)
        for g in range(len(coords["group"]))
    ])

    # Mean Structure
    tau = pm.Normal("tau", mu=0, sigma=0.5, dims=('group', 'indicators'))   # observed intercepts 
    alpha = pm.Normal("alpha", mu=0, sigma=0.5, dims=('group', 'latent'))       # latent means
    M = pt.matmul(Lambda, inv_I_minus_B)  # group, indicators latent
    mu_latent = pt.matmul(alpha[:, None, :], M.transpose(0, 2, 1))[:,:,0] # shape handling dummy axis and transpose M -> group, latent, indicators
    mu_y = pm.Deterministic("mu_y", tau + mu_latent) # size = group, indicators

    Sigma_y = []
    for g in range(len(coords['group'])):
        inv_lhs = inv_I_minus_B[g]
        Sigma_y_g = Lambda @ inv_lhs @ Psi_zeta @ inv_lhs.T @ Lambda.T + Psi
        Sigma_y.append(Sigma_y_g)
    Sigma_y = pt.stack(Sigma_y)
    _ = pm.MvNormal("likelihood", mu=mu_y[grp_idx], cov=Sigma_y[grp_idx])


```

:::
::: {.column width="30%"}
$$ \mu_g = \tau_{g} + \Lambda(1 - B_{g})^{-1}\alpha_{g}$$
$$\Sigma_{\mathcal{y}} = \Psi + \\ \Lambda(I - \color{blue}{B_{g}})^{-1}\Psi_{\gamma}(I - \color{blue}{B_{g}})^{T}\Lambda^{T} $$

$$ \mathcal{y} \sim MvN(\mu_{g}, \Sigma_{y}^{g})$$
:::
:::


## Validating Model Implications
### The Do-Operator and Parameter Recovery

```{.python code-line-numbers="|20-23|24-26"}

# Generating data from model by fixing parameters
fixed_parameters = {
 "mu_betas_man": [0.1, 0.5, 2.3, 0.9, 0.6, 0.8],
 "mu_betas_woman": [0.3, 0.2, 0.7, 0.8, 0.6, 1.2], 
 "tau": [[-0.822,  1.917, -0.743, -0.585, -1.095,  2.207, -0.898, -0.99 ,
        1.872, -0.044, -0.035, -0.085], [-0.882,  1.816, -0.828, -1.319,
        0.202,  1.267, -1.784, -2.112,  3.705, -0.769,  2.048, -1.064]],
 "lambdas1_": [1, .8, .9],
 "lambdas2_": [1, .9, 1.2],
 "lambdas3_": [1, .95, .8],
 "lambdas4_": [1, 1.4, 1.1],
 "alpha": [[ 0.869,  0.242,  0.314, -0.175], [0.962,  1.036,  0.436,  0.166]], 
 "chol_cov": [0.696, -0.096,  0.23 , -0.3  , -0.385,  0.398, -0.004,  0.043,
       -0.037,  0.422], 
"Psi_cov_": [0.559, 0.603, 0.666, 0.483, 0.53 , 0.402, 0.35 , 0.28 , 0.498,
       0.494, 0.976, 0.742], 
"Psi_cov_beta": [0.029]
}
with pm.do(sem_model_hierarchical2, fixed_parameters) as synthetic_model:
   idata = pm.sample_prior_predictive(random_seed=1000) # Sample from prior predictive distribution.
   synthetic_y = idata['prior']['likelihood'].sel(draw=0, chain=0)

# Infer parameters conditioned on observed data
with pm.observe(sem_model_hierarchical2, {"likelihood": synthetic_y}) as inference_model:
   idata = pm.sample(random_seed=100, nuts_sampler='numpyro', chains=4, draws=500)


```

## Parameter Recovery

![](images/parameter_recovery_hierarchical.png)

Complex models require proper validation methods. Parameter recovery methods are the best way to test the model's ability to identify the correct effects.


# Workflow and Craft

## Craft as Discovery {.smaller}

:::: {.columns}

::: {.column}
> "Abandon the idea of predetermination, the shaping force of your intentions...__rely less on the priority of your intentions and more on the immediacy of writing__... You'll see that some of your sentences are still conjectural... start noticing the thoughts and implications surrounding them." - Verlyn Klinkenborg in _Several Short Sentences about Writing_"

::: {.fragment .fade-in}
- Modeling, like writing, is an act of exploration.
:::

::: {.fragment .fade-in}
- Expect surprises and anomalies—they teach more than preconceptions.
:::

::: {.fragment .fade-in}
- Embrace uncertainty; allow the data to guide the process.
:::

:::
::: {.column}

<img src="images/several_short.png"  width="90%" height="60%">
:::
::: 

## Craft as Discipline {.smaller}

:::: {.columns}
::: {.column}
> " [T]he goal is to represent the systematic relationships between the variables and between the variables and the parameters ... Discrepancies between the model and data can be used __to learn about the ways in which the model is inadequate for the scientific purposes at hand, and thus to motivate expansions and changes to the model__ ... __[a model is a story of how the data could have been generated]{.blue}__; the fitted model should therefore be able to generate synthetic data that look like the real data; failures to do so in important ways indicate faults in the model." - Gelman & Shalizi in _Philosophy and the practice of Bayesian statistics_
:::
::: {.column}

<img src="images/sisyphus.png"  width="30%" height="20%">

::: {.fragment .fade-in}
- Building statistical models is inherently iterative and expansionary
:::
::: {.fragment .fade-in}
- Assumptions are encoded transparently and their implications are assessed for cogency
:::
::: {.fragment .fade-in}
- Where our assumptions fail, they are revised or rejected. Building confidence and clarity.
:::
::: {.fragment .fade-in}
- The process yields compelling, justifiable conclusions __worthy of your work.__
:::

:::
::: 

## Conclusion: Workflow as Craft {.smaller}

:::: {.columns}
::: {.column}
> "Here, in short, is what i want to tell you. __Know what each sentence says, What it doesn't say, And what it implies.__ Of these, the hardest is know what each sentence actually says" - V. Klinkenborg

![](images/marx_manuscript.jpg)
:::
::: {.column}

::: {.fragment .fade-in}
- In modelling, as in writing, clarity emerges through revision. 
:::

::: {.fragment .fade-in}
- The Bayesian workflow with PyMC teaches us to listen to our models — to *read them aloud* through simulation, recovery, and critique. 
:::

::: {.fragment .fade-in}
- Each iteration reveals what the model truly says, what it hides, and what it implies.  
:::

::: {.fragment .fade-in}
- Craft lies in that attention — in resisting a flattening automation, and choosing understanding over throughput.  
:::

::: {.fragment .fade-in}
- Through this care, our models become not only more compelling, but more **robust** — resilient to noise, misfit, and misuse. 
:::

:::
:::



