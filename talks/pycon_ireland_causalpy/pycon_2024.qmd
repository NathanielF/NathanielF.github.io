---
title: "Uncertainty and Causal Inference in Python"
subtitle: "CausalPy and Quasi-Experimental Designs"
author: 
    - name: Nathaniel Forde
      affiliations: 
        - Data Science @ Personio
        - and Open Source Contributor @ PyMC
format: 
    revealjs:
         theme: [default, clean.scss]
         footer: "Causal Inference in Python"
categories: [bayesian, missing data, causal inference]
image: "images/causalpy.webp"
date: last-modified
draft: False
---

# The Pitch

Causal inference in practice needs to be credible. Decision makers will demand it! Your inferential strategies need to be transparent and defensible. 

This is where you should leverage `CausalPy` and Bayesian causal modelling for the quantification of uncertainty

## Agenda

- Being a Data Scientist in Industry
    - The "Good enough" rule and other varieties of Satisficing
- Causal Inference, Defensive coding and Credibility
- Bayesian Causal Models and Design Patterns
- IV designs and Instruments
- PS designs and Weighting Schemes
- Conclusion: 
    - Uncertainty in the Model
    - Uncertainty about the Model


## Data Science in Industry
### and other varieties of Satisficing


![Where do we get to if everyone does the minimum? \n
 An eye for an eye will leave everyone blind.](images/satisficing.png)

## Directionally Correct ... {.smaller}
### Maybe in a ZIRP world

:::: {.columns}

::: {.column width="60%"}
![Directionally Correct, Magnitude Poor](images/magnitude-of-a-vector.png)
:::


::: {.column width="40%"}
- Important investment decisions require a view of magnitude and direction. 
- Most business decisions reflect some kind of investment
    - But time and effort are harder to place on a balance sheet
- Only sound causal inference supports generalisable decision rules
- __"Directionally Correct" is a Zero-interest-rate-phenomena.__
:::

::::

## Experiment Recipes, Rules and Responsibility{.smaller}

:::: {.columns}

::: {.column width="30%"}
![One Rule to rule them all?](images/p-value.jpg)
![Valid](images/valid.jpg)
:::


::: {.column width="70%"}
::: {.fragment .fade-in}
- P-value based decision criteria on policy change questions are based on the null model of an asymptotic univariate distribution. 
:::
::: {.fragment .fade-in}
- Most aggregate data (e.g. Total Revenue) we see in industry result from a complex array of mixture distributions and any long-run aggregates take time to converge. 
:::
::: {.fragment .fade-in}
- Management often doesn't want to spend the time to validate the long-run characteristics of a phenomena that we would observe in a well powered A/B test.
:::
::: {.fragment .fade-in}
- Risks underpowered experiments through a __[HIPPO-like decision rules]{.red}__ and costly mistakes, ungeneralisable effects. 
:::
::: {.fragment .fade-in}
- __Challenge__: How to improve decision quality in a resource constrained/time-bound environment?
:::
:::
::::

## Causal Inference, Crediblility {.smaller}
### ... and Quasi-Experimental Design


:::: {.columns}

::: {.column width="60%"}
![](images/dataspeak.png)

:::


::: {.column width="40%"}

::: {.fragment .fade-in}
- If the data can speak for itself, the answer is ussually __[blindingly obvious]{.red}__ or has taken an inordinate amount of time to accumulate
:::

::: {.fragment .fade-in}
- __Conversely__, if we've modeled the data generating process we can answer an array of subtle questions about __[cause]{.blue}__ and __[effect]{.blue}__ that support effective decision-making.
:::
::: {.fragment .fade-in}
- Build __[credibility]{.green}__ through linking theoretical estimand and empirical data while __[partnering]{.green}__ across the business with subject matter experts.
:::
:::

::::

## CausalPy and Bayesian Inference {.smaller}

:::: {.columns}

::: {.column width="60%"}
![](images/causalpy.webp)
:::

::: {.column width="40%"}

- A __[python package](https://causalpy.readthedocs.io/en/latest/index.html)__ for Bayesian Models and Causal Inference methods
- Developed and maintained by @PyMC Labs 
- Broad Coverage of quasi-experimental designs.

![](images/star_hist.png)
:::

::::


## Causal Methods and Models

:::: {.columns}

::: {.column width="40%"}

![](images/causalpy_toc.png)

:::

::: {.column width="60%"}

![](images/framework.png)
Causal question(s) of import can be interrogated just when we can pair a research design with an __appropriate__ statistical model. 

:::
::::

## Canonical DAGs and Methods

:::: {.columns}

::: {.column width="50%"}

![Random Control Trial](images/rct_dag.webp)
![Instrumental Variable Design](images/iv_dag.webp)

:::

::: {.column width="50%"}

The treatment effect can be estimated cleanly
$$ y \sim 1 + Z $$


The treatment effect has to be estimated so as to avoid the bias due to X
$$ y \sim 1 + \widehat{Z} $$
$$ \hat{Z} \sim 1 + IV $$ 

:::
::::


## IV Regression in CausalPy
```{.python code-line-numbers="|1-7|18-27"}
N = 100
e1 = np.random.normal(0, 3, N)
e2 = np.random.normal(0, 1, N)
Z = np.random.uniform(0, 1, N)
## Ensure the endogeneity of the the treatment variable
X = -1 + 4 * Z + e2 + 2 * e1
y = 2 + 3 * X + 3 * e1

test_data = pd.DataFrame({"y": y, "X": X, "Z": Z})

sample_kwargs = {
    "tune": 1000,
    "draws": 2000,
    "chains": 4,
    "cores": 4,
    "target_accept": 0.99,
}
instruments_formula = "X  ~ 1 + Z"
formula = "y ~  1 + X"
instruments_data = test_data[["X", "Z"]]
data = test_data[["y", "X"]]
iv = InstrumentalVariable(
    instruments_data=instruments_data,
    data=data,
    instruments_formula=instruments_formula,
    formula=formula,
    model=InstrumentalVariableRegression(sample_kwargs=sample_kwargs),
)

```


## Bayesian Structural Model {.smaller}
### And Instrument Strength

$$\begin{align*}
\left(
\begin{array}{cc}
Y \\
Z
\end{array}
\right)
& \sim
\text{MultiNormal}(\color{green} \mu, \color{purple} \Sigma) \\
\color{green} \mu & = \left(
\begin{array}{cc}
\mu_{y} \\
\mu_{z}
\end{array}
\right)
=
\left(
\begin{array}{cc}
\beta_{00} + \color{blue} \beta_{01}Z ... \\
\beta_{10} + \beta_{11}IV ...
\end{array}
\right)
\end{align*} 
$$

The treatment effect $\color{blue}\beta_{01}$ of is the primary quantity of interest

::: {.fragment .fade-in}
$$ \color{purple} \Sigma  = \begin{bmatrix}
1 & \color{blue} \sigma \\
\color{blue} \sigma & 1
\end{bmatrix} 
$$

The Bayesian estimation strategy incorporates two structural equations and the success of the IV model relies of the correlation between terms. 
:::

## Returns to Schooling: Concrete Example

:::: {.columns}

::: {.column width="50%"}
![](images/ability_edu.png)

:::


::: {.column width="50%"}

__Recipe of Assumptions__:

- Exclusion Restriction
- Independence
- Relevance
:::

::::


## Returns to Schooling: Instrument Relevance

:::: {.columns}

::: {.column width="60%"}
![Visual check of relevance for different instrument variables on the outcome](images/nearness.webp)
:::


::: {.column width="40%"}
We want to argue for: 

- the relevance of our instrument i.e. that it has a non-trivial impact on the outcome of interest

- that it influences the result only via the treatment condition. 

- evaluating multiple instrument candidates


:::

:::: 

## Returns to Schooling: Quantifying Confounding {.smaller}

:::: {.columns}

::: {.column width="40%"}
![Comparison between IV and OLS model parameter estimates](images/compare_ols.webp)
:::


::: {.column width="60%"}
The natural comparison with OLS shows:

- evidence of genuine confounding in the estimates of treatment effect

- Crucially it highlights the false precision in the OLS estimate.

- Model comparison is at the heart of understanding confounding in causal infernece.

- IV models may be compared by structure but also by instruments used. 

:::
:::: 


## Returns to Schooling: Justifying Instruments {.smaller}

:::: {.columns}

::: {.column width="40%"}
![Samples from LKJ prior on Covariance structure with different prior settings](images/priors_on_corr.webp)
:::


::: {.column width="60%"}
The strength of an instrument is determined by the correlation structure between instrument and outcome via the treatment solely:

- F-tests can be used to assess how the instrument relates to the outcome.

- In the Bayesian setting we can directly estimate the correlation structure and apply sensitivity tests.

- Stronger priors on correlation strength influence the outcomes and can be evaluated against the data through posterior predictive checks

:::
:::: 


## Returns to Schooling: Sensitivity Analysis and Model Evaluation 

:::: {.columns}

::: {.column width="40%"}
![Parameter estimates across different model formulations](images/model_comparison.webp)
:::


::: {.column width="60%"}
![Posterior Predictive checks and Marginal Effects on our best IV model](images/final_iv_model.webp)


:::
:::: 


## Canonical DAGs and Methods

:::: {.columns}

::: {.column width="50%"}

![Random Control Trial](images/rct_dag.webp)

![Propensity Score Adjustment](images/prop_score.png)

:::

::: {.column width="50%"}

The treatment effect can be estimated cleanly
$$ y \sim 1 + Z + X_1 ... X_N $$
$$ p(Z) \sim X_1 ... X_N $$


Propensity score adjustments reweight our outcome by the probability of treatment
$$ y \sim  f(p(Z), Z)$$

:::
::::

## Propensity Scores in CausalPy

```{.python code-line-numbers="|1-16|18-33"}
df1 = pd.DataFrame(
    np.random.multivariate_normal([0.5, 1], [[2, 1], [1, 1]], size=10000),
    columns=["x1", "x2"],
)
df1["trt"] = np.where(
    -0.5 + 0.25 * df1["x1"] + 0.75 * df1["x2"] + np.random.normal(0, 1, size=10000) > 0,
    1,
    0,
)
TREATMENT_EFFECT = 2
df1["outcome"] = (
    TREATMENT_EFFECT * df1["trt"]
    + df1["x1"]
    + df1["x2"]
    + np.random.normal(0, 1, size=10000)
)

result1 = cp.InversePropensityWeighting(
    df1,
    formula="trt ~ 1 + x1 + x2",
    outcome_variable="outcome",
    weighting_scheme="robust",
    model=cp.pymc_models.PropensityScore(
        sample_kwargs={
            "draws": 1000,
            "target_accept": 0.95,
            "random_seed": seed,
            "progressbar": False,
        },
    ),
)

result1

```

## Propensity Score Models

![Overfitting Propensity Score models to the sample data confounds causal inference](images/prop_models.png)

## Validating Balancing Effects

![Balancing Covariate Distributions across Treatment Groups](images/balancing.png)

Balanced covariate distributions is testable implication the propensity score design.


## Models and Weighting Methods

:::: {.columns}

::: {.column width="50%"}

![](images/PS_WEIGHT.png)

:::

::: {.column width="50%"}

![](images/overlap_weight.png)

:::
::::
Propensity Weighting Adjusts Outcome Distribution to improve estimation of treatment effects

## Conclusion {.smaller}

:::: {.columns}

::: {.column width="50%"}

- Uncertainty in the Model
    - Causal inference wrestles with the uncertain consequence(s) of assumed data generating processes.
    - Bayesian Causal Inference takes uncertainty quantification seriously
    - Prior sensitivity Analysis further establishes the robustness of our estimates under uncertainty
- Uncertainty about the Model
    - The combinatorial complexity of the possible models necessitates informed opinion to zero-in on plausible models


:::

::: {.column width="50%"}

![](images/probable_graphs.png)

::: {.fragment .fade-in}
__[CausalPy](https://causalpy.readthedocs.io/en/latest/)__ streamlines the deployment and analysis of these causal inference models with proper tools to asses their uncertainty, and communicate impact of policy changes.

Faciliates model evaluation, comparison and combination - to build credible patterns of inference and justified approaches to critical decisions. 

:::


:::
::::
