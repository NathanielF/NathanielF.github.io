{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32006c6b",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Aalen's Dynamic Path Model\"\n",
    "subtitle: \"Causal Inference with Time Varying Effects in PyMC\"\n",
    "categories: [\"path-models\", \"sem\", \"causal inference\"]\n",
    "keep-ipynb: true\n",
    "self-contained: true\n",
    "draft: true\n",
    "toc: true\n",
    "execute: \n",
    "  freeze: auto \n",
    "  execute: true\n",
    "  eval: false\n",
    "jupyter: applied-bayesian-regression-modeling-env\n",
    "image: 'forking_paths.jpg'\n",
    "author:\n",
    "    - url: https://nathanielf.github.io/\n",
    "    - affiliation: PyMC dev\n",
    "citation: true\n",
    "---\n",
    "\n",
    "\n",
    "hello world\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5f04580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import arviz as az\n",
    "import pytensor.tensor as pt\n",
    "from scipy.interpolate import BSpline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e64ba40",
   "metadata": {},
   "source": [
    "If you look to Odysseus on the morning the gates of Troy fell, he is well set up for a happy journey home. He is the architect of victory, his ships are loaded with spoils, and the wind is at his back. Yet, an odyssey can't be completed in a single day and conclusions drawn on the outset rarely survive journey's end. \n",
    "\n",
    "When we rely on static snapshots, like a single blood draw or a particular sales campaign, we are essentially watching Odysseus board his ship and guessing how the story ends. We ignore the __consequences emerging in time.__ This is an apt observation with which to begin the year. Will your new year's resolutions, survive the January? This is the kind of question we'll assess here. \n",
    "\n",
    "## Interventions and Attenuated Effects\n",
    "\n",
    "To track how intention precedes and predicts evolving outcomes we need a statistical framework that doesn't just record the departure but tracks the entire voyage. Enter **Aalen’s Additive Model**, formulated as a **Dynamic Path Model**. The question is when will we achieve our goals? When will Odysseus get home? How does the risk of attaining our goal vary over time?\n",
    "\n",
    "While traditional models, like the Cox Proportional Hazards or Accelerated Failure time models, often assume that an intervention’s effect is a constant \"multiplier\" throughout the study period, Aalen’s approach treats effects as a living process. It allows the impact of a policy or treatment to wax, wane, or even reverse as the narrative unfolds. \n",
    "\n",
    "### The Machinery of Change\n",
    "\n",
    "In a dynamic path system, we decompose the total risk into a series of additive \"layers.\" If we are interested in how an intervention ($X$) works through a mediator ($M$), we model the instantaneous hazard $\\lambda(t)$ as:\n",
    "\n",
    "$$\\lambda(t | X, M) = \\alpha_0(t) + \\alpha_1(t)X + \\alpha_2(t)M$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\alpha_0(t)$** is the **Baseline Hazard**, representing the background \"tension\" of the story.\n",
    "- **$\\alpha_1(t)$** is the **Time-Varying Direct Effect**, showing how the intervention influences risk at every specific moment.\n",
    "- **$\\alpha_2(t)$** is the **Time-Varying Mediator Effect**, capturing how the intermediate variable (the \"storm\" or the \"detour\") contributes to the outcome.\n",
    "\n",
    "All three components are modelled as time-varying functions that distil the effects over time. \n",
    "\n",
    "### Visualizing the Evolving DAG\n",
    "\n",
    "Standard causal inference often relies on a static Directed Acyclic Graph (DAG) to represent the \"rules\" of the system. But in a survival context, the DAG itself is dynamic. We can think of the model as a sequence of DAGs—one for each \"scene\" in our odyssey—where the causal arrows between $X$, $M$, and the Hazard ($\\lambda$) strengthen or weaken over time.\n",
    "\n",
    "\n",
    "We can visualize this \"filmstrip\" of causality using `networkx`. We have a series of dynamic causal DAGs representing our assumptions of the relationships:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc03c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: true\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "def plot_temporal_dag(stages=[0.1, 0.5, 0.9], labels=[\"Act I\", \"Act II\", \"Act III\"]):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Define nodes: Exposure, Mediator, Hazard\n",
    "    nodes = {'X': (0, 1), 'M': (1, 2), 'H': (2, 1)}\n",
    "    \n",
    "    for i, (stage, label) in enumerate(zip(stages, labels)):\n",
    "        G = nx.DiGraph()\n",
    "        G.add_nodes_from(nodes.keys())\n",
    "        \n",
    "        # We vary the weights to represent alpha_1(t) and alpha_2(t)\n",
    "        # Act I: Direct effect strong, Mediator weak\n",
    "        # Act III: Mediator dominates or effects attenuate\n",
    "        direct_w = 4 * (1 - stage) \n",
    "        indirect_w = 6 * stage\n",
    "        \n",
    "        edges = [('X', 'H', direct_w), ('X', 'M', 3), ('M', 'H', indirect_w)]\n",
    "        \n",
    "        for u, v, w in edges:\n",
    "            G.add_edge(u, v, weight=w)\n",
    "            \n",
    "        pos = nodes\n",
    "        nx.draw_networkx_nodes(G, pos, ax=axs[i], node_color='maroon', node_size=2000)\n",
    "        nx.draw_networkx_labels(G, pos, ax=axs[i], font_color='white', font_weight='bold')\n",
    "        \n",
    "        # Draw edges with widths corresponding to causal strength\n",
    "        weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "        nx.draw_networkx_edges(G, pos, ax=axs[i], width=weights, \n",
    "                               edge_color='gray', arrowsize=30, connectionstyle=\"arc3,rad=0.1\")\n",
    "        \n",
    "        axs[i].set_title(f\"{label}\\n(Time-varying Causal Strengths)\", fontsize=14)\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "fig = plot_temporal_dag()\n",
    "fig.savefig(\"evolving_dag.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a3906",
   "metadata": {},
   "source": [
    "![](evolving_dag.png)\n",
    "\n",
    "Seeing it this way makes it clearer that we are estimating a system of equations over time. To truly capture the journey, we cannot look at these variables in isolation. In our Odyssey, the \"storm\" ($M$) is not an independent accident; it is a consequence of the path Odysseus ($X$) chose to take. To model this, we must treat the intervention and the outcome as a **system of simultaneous equations**.\n",
    "\n",
    "By solving these equations in tandem, we ensure that the mediator is not just another covariate, but a character with its own backstory, influenced by the intervention even as it influences the final risk.\n",
    "\n",
    "#### 1. The Mediator Equation (The Will of Poseidon)\n",
    "Before we can calculate the risk of shipwreck, we must determine how the intervention has altered the environment. We model the mediator as a function of the exposure:\n",
    "\n",
    "$$M_i = \\beta_0 + \\beta_{1}(t)X_i + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "Where $\\beta_{1}(t)$ tells us how effectively the intervention \"recruits\" or \"triggers\" the mediator.\n",
    "\n",
    "#### 2. The Hazard Equation (The Risk of Shipwreck)\n",
    "The instantaneous risk at time $t$ is then a combination of the background noise, the direct path from $X$, and the indirect path from the now-defined $M$:\n",
    "\n",
    "$$\\lambda(t | X, M) = g\\left( \\alpha_0(t) + \\alpha_1(t)X_i + \\alpha_2(t)M_i \\right)$$\n",
    "\n",
    "where $g$ is a linking function to translate the covariates into a hazard scale. Taken together these observations should make it clear why this modelling strategy is known as _dynamic path analysis_. We essentially have a structural equation model (SEM) with specific path coefficients that trace out the influence between variables. But we also require that we capture the time evolution of these relationships. It's not enough to say of Odysseus that he had good intentions, it all ended well. We lose something if we don't understand the journey.\n",
    "\n",
    "### Causal Inference and Additive Effects\n",
    "\n",
    "Aalen's main concern was how to represent mechanisms that unfold in time without forcing them into a proportional straitjacket as in Cox style regressions. In many applied settings, an intervention does not exert a constant relative effect. Instead we act, with effects accumulating, and sometimes attenuated or disappearing over time.\n",
    "\n",
    "By modeling covariate effects as _increments to risk_, the coefficients themselves become causal estimands. A time-varying coefficient $\\alpha_{1}(t)$ answers a concrete question: what is the instantaneous risk attributable to the exposure at time $t$? Once effects are additive, they can be decomposed and recombined. Direct, indirect, and total effects become sums of paths traced through time. The model reads naturally as a time-indexed causal graph with estimable path strengths.\n",
    "\n",
    "> \"We have a sequence of dynamic path models, one for each time t when we collect information. The estimation of each dynamic path model is done by recursive least squares regression as usual in path analysis\" - Fosen et al in _\"Dynamic path analysis – a new approach to\n",
    "analyzing time-dependent covariates\"_\n",
    "\n",
    "The central objects in dynamic path analysis are the time-varying regression functions that link treatment, mediator, and survival. In the canonical `dpasurv` formulation, these are the functions $\\beta_1(t), \\alpha_2(t)$ and $\\alpha_1(t)$, which parameterize the direct path from treatment to the mediator, the effect of mediator on the hazard, and the effect of the treatment on the hazard, respectively.\n",
    "\n",
    "#### Cumulative Effects in `dpasurv`\n",
    "\n",
    "Dynamic path analysis is concerned not with isolated coefficients, but with how these functions evolve over time. They form the building blocks from which direct, indirect, and total effects are constructed.\n",
    "\n",
    "Because effects act continuously, interpretation is naturally expressed in cumulative terms. A cumulative coefficient is like a running total of a covariate’s effect on risk — it sums up, over time, how much that treatment or mediator has nudged the chance of an event happening. The **cumulative direct effect** up to time $t$ is defined as\n",
    "\n",
    "$$\\text{cumdir}(t) = \\int_0^t \\beta_1(s)\\, ds,$$\n",
    "\n",
    "representing the accumulated contribution of the treatment along the direct path to the hazard. In an additive hazards framework like Aalen’s model, we are directly estimating the cumulative coefficient function — the total accumulated effect up to time t, rather than first estimating the instantaneous effect $\\beta_{1}(t)$ and then summing or integrating it. Similarly, the **cumulative indirect effect** is defined as\n",
    "\n",
    "$$\\text{cumind}(t) = \\int_0^t \\alpha_1(s)\\, \\beta_2(s)\\, ds,$$\n",
    "\n",
    "which aggregates the mediated influence of treatment on survival. At each instant, the indirect effect is obtained by multiplying the strength of the treatment–mediator link with the strength of the mediator–hazard link, mirroring the logic of path analysis. A defining feature of dynamic path analysis is that these quantities satisfy an exact analytical decomposition:\n",
    "\n",
    "$$\\text{cumtot}(t) = \\text{cumdir}(t) + \\text{cumind}(t).$$\n",
    "\n",
    "This identity holds because the model is additive. The `dpasurv` package demonstrates these decomposition with the following data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe1b66c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"aalen_simdata.csv\")\n",
    "df = df[['subject', 'x', 'dose', 'M', 'start', 'stop', 'event']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c102b50",
   "metadata": {},
   "source": [
    "The data is an \"long format\" which is multiple rows per individual `subject` with running values for the state of the treatment indicator `x` and the mediator `M`, and crucially the `even` flag which denotes whether or not the terminal event occured. This is simulated data from the `dpasurv` r package, but the abstract names serve to illustrate the ubiquity of the mediator relationship! \n",
    "\n",
    "### Replicating the `dpasurv` Benchmark\n",
    "\n",
    "The figure belwo shows the cumulative direct, indirect, and total effects estimated by dpasurv from our simulated dataset. The direct effect (left panel) traces the immediate influence of the treatment on the outcome, while the indirect effect (middle panel) captures the pathway mediated through $M$. The total effect (right panel) is simply the sum of the two.\n",
    "\n",
    "![](dpasurv_benchmark.png)\n",
    "\n",
    "Notice the jumpy, step-like patterns in all three panels. This is characteristic of the `dpasurv` estimator, which produces cumulative coefficients by summing contributions at discrete event time intervals. Each step corresponds to a unique event time, with the height of the step reflecting the combined contribution of all events that occurred at that time. The gray lines indicate the approximate bootstrap confidence intervals, which widen as events become sparse, reflecting increasing uncertainty over time.\n",
    "\n",
    "The main pattern we're seeing here is that the direct effect of `x` in the first panel. The curve begins to dip below zero almost immediately and maintains a consistent downward slope, meaning the negative cumulative effect indicates that the intervention itself is actively __reducing the hazard.__. This is good, but contrast it with slight emerging effect in central panel.For the first 100 days, the mediator is a bystander. It is either not being triggered by the intervention or has no impact on survival. Around Day 100, the curve \"breaks\" and spikes upward. This represents a __positive contribution to the hazard.__ The combination of the total effects reflects this too. The total effect become less negative (closer to zero) after Day 100. This tells us that while the intervention is still helpful overall, its net benefit is being attenuated by the mediator.\n",
    "\n",
    "\n",
    "These are the patterns we will try and replicate in our Bayesian dynamic path model. But first we'll look a bit more into the data. \n",
    "\n",
    "## Exploring the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b882728",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['x', 'dose'])[['event', 'M']].agg(['mean', 'sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c959f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: true\n",
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# -----------------------\n",
    "# Color palette (muted & clean)\n",
    "# -----------------------\n",
    "COLOR_X0 = \"#6B7280\"    # muted slate gray\n",
    "COLOR_X1 = \"#2563EB\"    # soft rich blue\n",
    "EVENT_COLOR = \"#991B1B\" # deep muted red\n",
    "\n",
    "# -----------------------\n",
    "# Split subject ordering by treatment group\n",
    "# -----------------------\n",
    "subject_info = (\n",
    "    df.groupby('subject')\n",
    "      .agg(\n",
    "          x=('x', 'first'),\n",
    "          max_stop=('stop', 'max')\n",
    "      )\n",
    ")\n",
    "\n",
    "subject_info_0 = (\n",
    "    subject_info[subject_info['x'] == 0]\n",
    "    .sort_values('max_stop')\n",
    ")\n",
    "\n",
    "subject_info_1 = (\n",
    "    subject_info[subject_info['x'] == 1]\n",
    "    .sort_values('max_stop')\n",
    ")\n",
    "\n",
    "subjects_0 = subject_info_0.index.tolist()\n",
    "subjects_1 = subject_info_1.index.tolist()\n",
    "\n",
    "subject_to_y_0 = {s: i for i, s in enumerate(subjects_0)}\n",
    "subject_to_y_1 = {s: i for i, s in enumerate(subjects_1)}\n",
    "\n",
    "# -----------------------\n",
    "# Create side-by-side plots\n",
    "# -----------------------\n",
    "fig, axes = plt.subplots(\n",
    "    ncols=2,\n",
    "    figsize=(12, 0.1 * max(len(subjects_0), len(subjects_1))),\n",
    "    sharex=True\n",
    ")\n",
    "\n",
    "ax0, ax1 = axes\n",
    "\n",
    "# -----------------------\n",
    "# Plot x = 0 group\n",
    "# -----------------------\n",
    "for _, row in df[df['x'] == 0].iterrows():\n",
    "    y = subject_to_y_0[row['subject']]\n",
    "\n",
    "    ax0.hlines(\n",
    "        y=y,\n",
    "        xmin=row['start'],\n",
    "        xmax=row['stop'],\n",
    "        color=COLOR_X0,\n",
    "        linewidth=2.5\n",
    "    )\n",
    "\n",
    "    if row['event'] == 1:\n",
    "        ax0.plot(\n",
    "            row['stop'],\n",
    "            y,\n",
    "            marker='o',\n",
    "            color=EVENT_COLOR,\n",
    "            markersize=6,\n",
    "            zorder=3\n",
    "        )\n",
    "\n",
    "ax0.set_yticks(range(len(subjects_0)))\n",
    "ax0.set_yticklabels(subjects_0)\n",
    "ax0.set_title(\"x = 0\")\n",
    "ax0.set_ylabel(\"Subject\")\n",
    "\n",
    "# -----------------------\n",
    "# Plot x = 1 group\n",
    "# -----------------------\n",
    "for _, row in df[df['x'] == 1].iterrows():\n",
    "    y = subject_to_y_1[row['subject']]\n",
    "\n",
    "    ax1.hlines(\n",
    "        y=y,\n",
    "        xmin=row['start'],\n",
    "        xmax=row['stop'],\n",
    "        color=COLOR_X1,\n",
    "        linewidth=2.5\n",
    "    )\n",
    "\n",
    "    if row['event'] == 1:\n",
    "        ax1.plot(\n",
    "            row['stop'],\n",
    "            y,\n",
    "            marker='o',\n",
    "            color=EVENT_COLOR,\n",
    "            markersize=6,\n",
    "            zorder=3\n",
    "        )\n",
    "\n",
    "ax1.set_yticks(range(len(subjects_1)))\n",
    "ax1.set_yticklabels(subjects_1)\n",
    "ax1.set_title(\"x = 1\")\n",
    "\n",
    "# -----------------------\n",
    "# Shared formatting & polish\n",
    "# -----------------------\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.grid(False)\n",
    "\n",
    "# -----------------------\n",
    "# Legend\n",
    "# -----------------------\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color=COLOR_X1, lw=2.5, label='x = 1'),\n",
    "    Line2D([0], [0], color=COLOR_X0, lw=2.5, label='x = 0'),\n",
    "    Line2D([0], [0], marker='o', color=EVENT_COLOR,\n",
    "           lw=0, label='Event', markersize=6),\n",
    "]\n",
    "\n",
    "ax0.tick_params(axis='y', labelsize=8)\n",
    "ax1.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "fig.legend(\n",
    "    handles=legend_elements,\n",
    "    loc='upper center',\n",
    "    ncol=3,\n",
    "    frameon=False,\n",
    "    bbox_to_anchor=(0.5, 0.98)\n",
    ")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Subject Timelines by Treatment Group\",\n",
    "    y=1.08\n",
    ")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "fig.savefig(\"timelines_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be18bb5",
   "metadata": {},
   "source": [
    "![](timelines_plot.png)\n",
    "\n",
    "### Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b29c5dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_aalen_dpa_data(\n",
    "    df,\n",
    "    subject_col=\"subject\",\n",
    "    start_col=\"start\",\n",
    "    stop_col=\"stop\",\n",
    "    event_col=\"event\",\n",
    "    x_col=\"x\",\n",
    "    m_col=\"M\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare Andersen–Gill / Aalen dynamic path data for PyMC.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Long-format start–stop survival data\n",
    "    subject_col : str\n",
    "        Subject identifier\n",
    "    start_col, stop_col : str\n",
    "        Interval boundaries\n",
    "    event_col : str\n",
    "        Event indicator (0/1)\n",
    "    x_col : str\n",
    "        Exposure / treatment\n",
    "    m_col : str\n",
    "        Mediator measured at interval start\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of numpy arrays ready for PyMC\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 1. Basic quantities\n",
    "    # -------------------------------------------------\n",
    "    df[\"dt\"] = df[stop_col] - df[start_col]\n",
    "\n",
    "    if (df[\"dt\"] <= 0).any():\n",
    "        raise ValueError(\"Non-positive interval lengths detected.\")\n",
    "\n",
    "    N = df[event_col].astype(int).values\n",
    "    Y = np.ones(len(df), dtype=int)  # Andersen–Gill at-risk indicator\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 2. Time-bin indexing (piecewise-constant effects)\n",
    "    # -------------------------------------------------\n",
    "    bins = (\n",
    "        df[[start_col, stop_col]]\n",
    "        .drop_duplicates()\n",
    "        .sort_values([start_col, stop_col])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    bins[\"bin_idx\"] = np.arange(len(bins))\n",
    "\n",
    "    df = df.merge(\n",
    "        bins,\n",
    "        on=[start_col, stop_col],\n",
    "        how=\"left\",\n",
    "        validate=\"many_to_one\"\n",
    "    )\n",
    "\n",
    "    bin_idx = df[\"bin_idx\"].values\n",
    "    n_bins = bins.shape[0]\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 3. Center covariates (important for Aalen models)\n",
    "    # -------------------------------------------------\n",
    "    df[\"x_c\"] = df[x_col]\n",
    "    df[\"m_c\"] = df[m_col] - df[m_col].mean()\n",
    "\n",
    "    x = df[\"x_c\"].values\n",
    "    m = df[\"m_c\"].values\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 4. Predictable mediator (lag within subject)\n",
    "    # -------------------------------------------------\n",
    "    df = df.sort_values([subject_col, start_col])\n",
    "\n",
    "    df[\"m_lag\"] = (\n",
    "        df.groupby(subject_col)[\"m_c\"]\n",
    "          .shift(1)\n",
    "          .fillna(0.0)\n",
    "    )\n",
    "\n",
    "    m_lag = df[\"m_lag\"].values\n",
    "\n",
    "    df[\"I_low\"]  = (df[\"dose\"] == \"low\").astype(int)\n",
    "    df[\"I_high\"] = (df[\"dose\"] == \"high\").astype(int)\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 5. Assemble output\n",
    "    # -------------------------------------------------\n",
    "    data = {\n",
    "        \"bins\": bins,     # useful for plotting\n",
    "        \"df_long\": df     # optional: debugging / inspection\n",
    "    }\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37805c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_aalen_dpa_data(df)\n",
    "df_long = data['df_long']\n",
    "df_long[['subject', 'x', 'dose', 'M', 'event', 'dt', 'bin_idx']].head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "163b7247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bspline_basis(n_bins, n_knots=10, degree=3):\n",
    "    \"\"\"\n",
    "    Create B-spline basis functions for smooth time-varying effects.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_bins : int\n",
    "        Number of time bins\n",
    "    n_knots : int\n",
    "        Number of internal knots (fewer = smoother)\n",
    "    degree : int\n",
    "        Degree of spline (3 = cubic, recommended)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    basis : np.ndarray\n",
    "        Matrix of shape (n_bins, n_basis) with basis function values\n",
    "    \"\"\"\n",
    "    # Create knot sequence\n",
    "    # Internal knots equally spaced across time range\n",
    "    internal_knots = np.linspace(0, n_bins-1, n_knots)\n",
    "    \n",
    "    # Add boundary knots (repeated degree+1 times for clamped spline)\n",
    "    knots = np.concatenate([\n",
    "        np.repeat(internal_knots[0], degree),\n",
    "        internal_knots,\n",
    "        np.repeat(internal_knots[-1], degree)\n",
    "    ])\n",
    "    \n",
    "    # Number of basis functions\n",
    "    n_basis = len(knots) - degree - 1\n",
    "    \n",
    "    # Evaluate each basis function at each time point\n",
    "    t = np.arange(n_bins, dtype=float)\n",
    "    basis = np.zeros((n_bins, n_basis))\n",
    "    \n",
    "    for i in range(n_basis):\n",
    "        # Create coefficient vector (indicator for basis i)\n",
    "        coef = np.zeros(n_basis)\n",
    "        coef[i] = 1.0\n",
    "        \n",
    "        # Evaluate B-spline\n",
    "        spline = BSpline(knots, coef, degree, extrapolate=False)\n",
    "        basis[:, i] = spline(t)\n",
    "    \n",
    "    return basis\n",
    "\n",
    "n_knots = 10\n",
    "n_bins = data['bins'].shape[0]\n",
    "basis = create_bspline_basis(n_bins, n_knots=n_knots, degree=3)\n",
    "n_cols = basis.shape[1]\n",
    "basis_df = pd.DataFrame(basis, columns=[f'feature_{i}' for i in range(n_cols)])\n",
    "basis_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a4c88e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "\n",
    "def make_model(data, basis, sample=True, observed=True): \n",
    "    df_long = data['df_long'].copy()\n",
    "    n_basis = basis.shape[1]\n",
    "    n_obs = data['df_long'].shape[0]\n",
    "    time_bins = data['bins']['bin_idx'].values\n",
    "    b = df_long['bin_idx']\n",
    "\n",
    "    observed_mediator = df_long[\"m_c\"].values\n",
    "    observed_events = df_long['event'].astype(int).values\n",
    "    observed_treatment = df_long['x'].astype(int).values\n",
    "    observed_mediator_lag = df_long['m_lag'].values\n",
    "\n",
    "    coords = {'tv': ['intercept', 'direct', 'mediator'], \n",
    "            'splines': ['spline_f_{i}' for i in range(n_basis)], \n",
    "            'obs': range(n_obs), \n",
    "            'time_bins': time_bins}\n",
    "\n",
    "    with pm.Model(coords=coords) as aalen_dpa_model:\n",
    "\n",
    "        trt = pm.Data(\"trt\", observed_treatment, dims=\"obs\")\n",
    "        med = pm.Data(\"mediator\", observed_mediator, dims=\"obs\")\n",
    "        med_lag = pm.Data(\"mediator_lag\", observed_mediator_lag, dims=\"obs\")\n",
    "        events = pm.Data(\"events\", observed_events, dims=\"obs\")\n",
    "        I_low  = pm.Data(\"I_low\",  df_long[\"I_low\"].values,  dims=\"obs\")\n",
    "        I_high = pm.Data(\"I_high\", df_long[\"I_high\"].values, dims=\"obs\")\n",
    "        dt = pm.Data(\"duration\", df_long['dt'].values, dims='obs')\n",
    "        ## because our long data format has a cell per obs\n",
    "        at_risk = pm.Data(\"at_risk\", np.ones(len(observed_events)), dims=\"obs\")\n",
    "        basis_ = pm.Data(\"basis\", basis, dims=('time_bins', 'splines') )\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # 1. B-spline coefficients for HAZARD model\n",
    "        # -------------------------------------------------\n",
    "        # Prior on spline coefficients\n",
    "        # Smaller sigma = less wiggliness\n",
    "        # Random Walk 1 (RW1) Prior for coefficients\n",
    "        # This is the Bayesian version of the smoothing penalty in R's 'mgcv' or 'timereg'\n",
    "        sigma_smooth = pm.Exponential(\"sigma_smooth\", [1, 1, 1], dims='tv')\n",
    "        beta_raw = pm.Normal(\"beta_raw\", 0, 1, dims=('splines', 'tv'))\n",
    "\n",
    "        # Cumulative sum makes it a Random Walk\n",
    "        # This ensures coefficients evolve smoothly over time\n",
    "        coef_alpha = pm.Deterministic(\"coef_alpha\", pt.cumsum(beta_raw * sigma_smooth, axis=0), dims=('splines', 'tv'))\n",
    "\n",
    "        # Construct smooth time-varying functions\n",
    "        alpha_0_t = pt.dot(basis_, coef_alpha[:, 0])\n",
    "        alpha_1_t = pt.dot(basis_, coef_alpha[:, 1])\n",
    "        alpha_2_t = pt.dot(basis_, coef_alpha[:, 2])\n",
    "        \n",
    "        # -------------------------------------------------\n",
    "        # 2. B-spline coefficients for MEDIATOR model\n",
    "        # -------------------------------------------------\n",
    "        sigma_beta_smooth = pm.Exponential(\"sigma_beta_smooth\", 0.1)\n",
    "        beta_raw = pm.Normal(\"beta_raw_m\", 0, 1, dims=('splines'))\n",
    "        coef_beta = pt.cumsum(beta_raw * sigma_beta_smooth)\n",
    "        \n",
    "        beta_t = pt.dot(basis_, coef_beta)\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # 3. Mediator model (A path: x → M)\n",
    "        # -------------------------------------------------\n",
    "        sigma_m = pm.HalfNormal(\"sigma_m\", 1.0)\n",
    "        \n",
    "        # Autoregressive component\n",
    "        rho = pm.Beta(\"rho\", 2, 2)\n",
    "        \n",
    "        mu_m = beta_t[b] * trt + rho * med_lag\n",
    "\n",
    "        pm.Normal(\n",
    "            \"obs_m\",\n",
    "            mu=mu_m,\n",
    "            sigma=sigma_m,\n",
    "            observed=med,\n",
    "            dims='obs'\n",
    "        )\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # 4. Hazard model (direct + B path)\n",
    "        # -------------------------------------------------\n",
    "        beta_low  = pm.Normal(\"beta_low\",  0, 0.1)\n",
    "        beta_high = pm.Normal(\"beta_high\", 0, 0.1)\n",
    "        # Log-additive hazard\n",
    "        log_lambda_t = (alpha_0_t[b] \n",
    "                        + alpha_1_t[b] * trt # direct effect\n",
    "                        + alpha_2_t[b] * med  # mediator effect\n",
    "                        + beta_low  * I_low\n",
    "                        + beta_high * I_high\n",
    "        )\n",
    "        \n",
    "        # Expected number of events\n",
    "        time_at_risk = at_risk * dt\n",
    "        Lambda = time_at_risk * pm.math.log1pexp(log_lambda_t)\n",
    "\n",
    "        if observed:\n",
    "            pm.Poisson(\n",
    "                \"obs_event\",\n",
    "                mu=Lambda,\n",
    "                observed=events, \n",
    "                dims='obs'\n",
    "            )\n",
    "        else: \n",
    "            pm.Poisson(\n",
    "                \"obs_event\",\n",
    "                mu=Lambda,\n",
    "                dims='obs'\n",
    "            )\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # 5. Causal path effects\n",
    "        # -------------------------------------------------\n",
    "        # Store time-varying coefficients\n",
    "        pm.Deterministic(\"alpha_0_t\", alpha_0_t, dims='time_bins')\n",
    "        pm.Deterministic(\"alpha_1_t\", alpha_1_t, dims='time_bins')  # direct effect\n",
    "        pm.Deterministic(\"alpha_2_t\", alpha_2_t, dims='time_bins')  # B path\n",
    "        pm.Deterministic(\"beta_t\", beta_t, dims='time_bins')        # A path\n",
    "        \n",
    "        # Cumulative direct effect\n",
    "        cum_de = pm.Deterministic(\n",
    "            \"tv_direct_effect\",\n",
    "            alpha_1_t, \n",
    "            dims='time_bins'\n",
    "        )\n",
    "\n",
    "        # Cumulative indirect effect (product of paths)\n",
    "        cum_ie = pm.Deterministic(\n",
    "            \"tv_indirect_effect\",\n",
    "            beta_t * alpha_2_t, \n",
    "            dims='time_bins'\n",
    "        )\n",
    "\n",
    "        # Total effect\n",
    "        cum_te = pm.Deterministic(\n",
    "            \"tv_total_effect\",\n",
    "            cum_de + cum_ie,\n",
    "            dims='time_bins'\n",
    "        )\n",
    "\n",
    "        pm.Deterministic('tv_baseline_hazard', pm.math.log1pexp(alpha_0_t), \n",
    "            dims='time_bins')\n",
    "\n",
    "        pm.Deterministic('tv_hazard_with_exposure', pm.math.log1pexp(alpha_0_t + alpha_1_t), \n",
    "            dims='time_bins')\n",
    "\n",
    "        pm.Deterministic(\n",
    "        \"tv_RR\",\n",
    "        pm.math.log1pexp(alpha_0_t + alpha_1_t) /\n",
    "        pm.math.log1pexp(alpha_0_t),\n",
    "        dims=\"time_bins\"\n",
    "        )\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # 6. Sample\n",
    "        # -------------------------------------------------\n",
    "        if sample:\n",
    "            idata = pm.sample_prior_predictive()\n",
    "            idata.extend(pm.sample(\n",
    "                draws=2000,\n",
    "                tune=2000,\n",
    "                target_accept=0.95,\n",
    "                chains=4,\n",
    "                nuts_sampler=\"numpyro\",\n",
    "                random_seed=42,\n",
    "                init=\"adapt_diag\", \n",
    "                idata_kwargs={\"log_likelihood\": True}\n",
    "            ))\n",
    "            idata.extend(pm.sample_posterior_predictive(idata))\n",
    "    \n",
    "    return aalen_dpa_model, idata\n",
    "\n",
    "basis = create_bspline_basis(n_bins, n_knots=12, degree=3)\n",
    "aalen_dpa_model, idata_aalen =  make_model(data, basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fddb44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(aalen_dpa_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8618dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "models = {}\n",
    "idatas = {}\n",
    "for i in range(4, 15, 2):\n",
    "    basis = create_bspline_basis(n_bins, n_knots=i, degree=3)\n",
    "    aalen_dpa_model, idata = make_model(data, basis)\n",
    "    models[i] = aalen_dpa_model\n",
    "    idatas[f\"splines_{i}\"] = idata\n",
    "\n",
    "compare_df = az.compare(idatas, var_name='obs_event')\n",
    "az.plot_compare(compare_df, figsize=(8, 6), plot_ic_diff=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84b8ae0",
   "metadata": {},
   "source": [
    "![](spline_loo_comparison.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2e741bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "ax = az.plot_forest([idatas[k] for k in idatas.keys()], combined=True, var_names=['tv_direct_effect'], model_names=idatas.keys(), coords={'time_bins': [180, 182, 182, 183, 184, 185, 186, 187, 188]}, \n",
    "figsize=(12, 10),  r_hat=True)\n",
    "ax[0].set_title(\"Time Vary Direct Effects \\n Comparing Models on Final Time Intervals\", fontsize=15)\n",
    "ax[0].set_ylabel(\"Nth Time Interval\", fontsize=15)\n",
    "fig = ax[0].figure\n",
    "fig.savefig('forest_plot_comparing_tv_direct.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050c8169",
   "metadata": {},
   "source": [
    "![](forest_plot_comparing_tv_direct.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d71d4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idata_aalen, var_names=['tv_direct_effect', 'tv_indirect_effect', 'tv_total_effect', 'beta_high', 'beta_low'], divergences=False);\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ffc9058",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_plot = ['tv_direct_effect', 'tv_indirect_effect', 'tv_total_effect']\n",
    "labels = ['Time varying Direct Effect', 'Time varying Indirect Effect', 'Time varying Total Effect']\n",
    "\n",
    "def plot_effects(idata, vars_to_plot, labels, scale=\"Log Hazard Ratio Scale\"):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 10))\n",
    "    color='teal'\n",
    "    if scale != \"Log Hazard Ratio Scale\":\n",
    "        color='darkred'\n",
    "\n",
    "    for i, var in enumerate(vars_to_plot):\n",
    "        # 1. Extract the posterior samples for this variable\n",
    "        # Shape will be (chain * draw, time)\n",
    "        post_samples = az.extract(idata, var_names=[var]).values.T\n",
    "        \n",
    "        # 2. Calculate the mean and the 94% HDI across the chains/draws\n",
    "        mean_val = post_samples.mean(axis=0)\n",
    "        hdi_val = az.hdi(post_samples, hdi_prob=0.94) # Returns [time, 2] array\n",
    "        \n",
    "        # 3. Plot the Mean line\n",
    "        x_axis = np.arange(len(mean_val))\n",
    "        axs[i].plot(x_axis, mean_val, label=labels[i], color=color, lw=2)\n",
    "        \n",
    "        # 4. Plot the Shaded HDI region\n",
    "        axs[i].fill_between(x_axis, hdi_val[:, 0], hdi_val[:, 1], color=color, alpha=0.2, label='94% HDI')\n",
    "        \n",
    "        # Formatting\n",
    "        axs[i].set_title(labels[i])\n",
    "        axs[i].legend()\n",
    "        axs[i].grid(alpha=0.3)\n",
    "        axs[i].set_ylabel(scale)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "plot_effects(idata_aalen, vars_to_plot, labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c75ad960",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_plot = ['tv_baseline_hazard', 'tv_hazard_with_exposure', 'tv_RR']\n",
    "labels = ['Time varying Baseline Hazard', 'Time varying Hazard + Exposure', 'Time varying RR']\n",
    "plot_effects(idata_aalen, vars_to_plot, labels, scale='Hazard Scale');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applied-bayesian-regression-modeling-env",
   "language": "python",
   "name": "applied-bayesian-regression-modeling-env",
   "path": "/Users/nathanielforde/Library/Jupyter/kernels/applied-bayesian-regression-modeling-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
