{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: GAMs and GPs\n",
        "author: Nathaniel Forde\n",
        "date: '2024-02-15'\n",
        "categories:\n",
        "  - probability\n",
        "  - generalised additive models\n",
        "  - gaussian processes\n",
        "keep-ipynb: true\n",
        "self-contained: true\n",
        "draft: false\n",
        "execute:\n",
        "  enabled: true\n",
        "image: Spline.png\n",
        "---"
      ],
      "id": "33722bec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import bambi as bmb\n",
        "import seaborn as sns\n",
        "from pygam.datasets import mcycle\n",
        "from pygam import LinearGAM, s, f, GAM, l, utils\n",
        "import numpy as np\n",
        "import arviz as az\n",
        "import pymc as pm\n",
        "\n",
        "\n",
        "random_seed = 100\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.simplefilter('ignore')"
      ],
      "id": "78d11815",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Flexibility and Calibration\n",
        "\n",
        "In this blog post we're going to dive into modelling of non-linear functions and explore some of the tooling available in the python eco-system. We'll start by looking into Generalised Additive Models with splines in `pyGAM` before preceding to look Bayesian versions of spline modelling comparing the splines to Gaussian processes in `Bambi` and `PyMC` \n",
        "\n",
        "Our interest in these models stems from their flexibility to approximate functions of arbitrary complexity. We'll see how the methods work in the case of relatively straightforward toy example and then we'll apply each of the methods to deriving insights into the functional form of insurance loss curves. In this application we adapt a data set discussed in Mick Cooney's `Stan` [case study](https://mc-stan.org/users/documentation/case-studies/losscurves_casestudy.html) to demonstrate the power of hierarchical spline models. Throughout we'll draw on the discussion of these methods in Osvaldo Martin's _\"Bayesian Analysis with Python\"_ for practical details implementing these models. \n",
        "\n",
        "All of these methods need to be assessed with respect to their in-sample model fit and their out of sample performance. How can we best calibrate the model fits to perform reasonably well out of sample?\n",
        "\n",
        "## Generalised Additive models\n",
        "\n",
        "The canonical reference for GAMs is Simon Wood's _\"Generalised Additive Models: An Introduction with R\"_ which outlines in some detail the theoretical background of splines and univariate smoothers. The book stresses the trade-offs between the flexibility of splines and the need for cross-validation and penalised estimation methods for spline based modelling. \n",
        "\n",
        "In R these penalised models fits can be achieved in `mgcv` which incorporates a wilkinson like formula syntax for model specification: `y ~ s(x) + s(x1)`. \n",
        "The closest implementation in python is available in `PyGam` and we will adopt this package to illustrate the application of smoothing terms and the penalities. \n",
        "\n",
        "### PyGAM and Penalised Fits\n",
        "In the next code block we load in an example data set on which to demonstrate univariate smoothing patterns using penalised splines. These models can be optimised by fitting differing strength penalities over a varying number of splines"
      ],
      "id": "e043f931"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X, y = mcycle(return_X_y=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.scatter(X, y)\n",
        "ax.set_ylabel(\"Acceleration\")\n",
        "ax.set_xlabel(\"Time Step\")\n",
        "ax.set_title(\"Crash Test Dummy Acceleration \\n Simulated Motorcycle Crash\", fontsize=20)"
      ],
      "id": "69d37350",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we fit a number of different models to account for the herky-jerky of the data generating processs. We vary the parameterisations to see how the numbers of splines and strength of the penalty help account for the variation in $y$ over the support of $X$. \n"
      ],
      "id": "220db19d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gam1 = LinearGAM(s(0, n_splines=5)).fit(X, y)\n",
        "gam2 = LinearGAM(s(0, n_splines=7)).fit(X, y)\n",
        "gam3 = LinearGAM(s(0, n_splines=10)).fit(X, y)\n",
        "gam4 = LinearGAM(s(0, n_splines=15)).fit(X, y)\n",
        "gam5 = LinearGAM(s(0, lam=.1)).fit(X, y)\n",
        "gam6 = LinearGAM(s(0, lam=.5)).fit(X, y)\n",
        "gam7 = LinearGAM(s(0, lam=5)).fit(X, y)\n",
        "gam8 = LinearGAM(s(0, lam=15)).fit(X, y)\n",
        "\n",
        "\n",
        "def plot_fit(gam, X, y, ax, t, c1='b', c2='r'):\n",
        "    XX = gam.generate_X_grid(term=0, n=500)\n",
        "\n",
        "    ax.plot(XX, gam.predict(XX), color=c2, linestyle='--')\n",
        "    ax.plot(XX, gam.prediction_intervals(XX, width=.95), color=c1, ls='--')\n",
        "\n",
        "    ax.scatter(X, y, facecolor='gray', edgecolors='none')\n",
        "    ax.set_title(f\"\"\"95% prediction interval with {t} \\n LL: {gam.statistics_['loglikelihood']}\"\"\");\n",
        "\n",
        "fig, axs = plt.subplots(4,2, figsize=(10, 20))\n",
        "axs = axs.flatten()\n",
        "titles = ['5_splines', '7_splines', '10_splines', '15_splines',\n",
        "'lam=.1', 'lam=.5', 'lam=5', 'lam=15']\n",
        "gs = [gam1, gam2, gam3, gam4, gam5, gam6, gam7, gam8]\n",
        "for ax, g, t in zip(axs, gs, titles):\n",
        "    plot_fit(g, X, y, ax, t)\n"
      ],
      "id": "15b585fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we've seen the `PyGAM` package applied to fitting our model to the data. In the formula specification we see `y ~ s(i)` where `i` denotes the index of the column variable in the X data. \n",
        "\n",
        "Over the range of of the x-axis we can see how the conditional expectation is more or less well fit to the data depending on how the penalities and complexity of the model is specified. \n",
        "\n",
        "## Optimising The Parameter Setting\n",
        "\n",
        "We can see from the model summary what is going on under the hood. For a given model specification the summary will report a number of model-fit statistics such as the log-likelihood and the AIC. \n"
      ],
      "id": "d7c398fa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## Naive Model manually specified splines\n",
        "gam_raw = LinearGAM(s(0)).fit(X, y)\n",
        "print(\"log_likelihood:\", gam_raw.statistics_['loglikelihood'])\n",
        "print(\"AIC:\", gam_raw.statistics_['AIC'])\n"
      ],
      "id": "5ae35ebe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The question then becomes what changes are induced in the model as we seek to optimise these model fit statistics. \n"
      ],
      "id": "0f06e565"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## model optimised\n",
        "gam = LinearGAM(s(0),  fit_intercept=False)\n",
        "gam.gridsearch(X, y)\n",
        "gam.summary()"
      ],
      "id": "c75437e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fortunately, this routine can be performed directly and results in the following differences between the naive and optimised model. \n",
        "\n",
        "### Plot the GAM fits\n"
      ],
      "id": "bb57bbd3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plot_fit(gam_raw, X, y, ax, \"Unoptimised Fit\", c1='orange', c2='green')\n",
        "plot_fit(gam, X, y, ax, \"Optimised Fit\")"
      ],
      "id": "38a72550",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is all well and good. We've seen an approach to modelling that can capture eccentric patterns in raw data. But how does it work and why should we care?\n",
        "\n",
        "If you're familiar with the nomenclature of machine learning, you should think of spline modelling as a variety of feature creation. It generates \"synthetic\" features over the range of the observed variable. These synthetic features are the splines in question. \n",
        "\n",
        "### Digression on the usage of \"Splines\" \n",
        "\n",
        "The history of the term \"spline\" is related to the history of draftmanship. Historically splines were thin strips of flexible wood or plastic that could be bent or shaped around a weight or \"knot\" points to express a traceable curve over the space of a \"numberline\". The elastic nature of the spline material allowed it to be bent around the knot points of curvature expressing a smooth or continuous bend. \n",
        "\n",
        "The mathematical technique apes these properties by defining a curve over in an analogous way. We specify \"knots\"  to carve up the support of the random variable $X$ into portions that require different weighting schemes to represent the outcome $y$ \n",
        "\n",
        "<img src=\"Spline.png\" alt=\"spline\" height=\"500\"/>\n",
        "\n",
        "### Extracting the Splines\n",
        "\n",
        "We can extract the spline features used in the `PyGAM` by invoking the following commands to first identify the knot points and create the b-spline basis appropriate for the variable $X$. \n"
      ],
      "id": "558de558"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "knot_edges=utils.gen_edge_knots(X,dtype='numerical')\n",
        "knots=np.linspace(knot_edges[0],knot_edges[-1],len(gam.coef_))\n",
        "\n",
        "splines = utils.b_spline_basis(X, edge_knots=knot_edges, sparse=False)\n",
        "\n",
        "splines_df = pd.DataFrame(splines, columns=[f'basis_{i}' for i in range(len(gam.coef_))])\n",
        "\n",
        "splines_df.head(10)"
      ],
      "id": "7138e9c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These spline features range the extent of the of covariate space $X$ defining \"partitions\" of the space. The model \"learns\" to capture the shape of the outcome variable $y$ by figuring out how to weight the different portion of this spline basis matrix i.e. the linear combination of this basis matrix with the derived coefficients is a model of our outcome variable. \n",
        "\n",
        "Note how each row is 0 everywhere except within the columns that represent a partition of $X$. Additionally each row sum to unity. These properties are important because they ensure that any weighted combination of this basis represents the outcome variable in a controlled and quite granular manner. The more splines we use the more control we have of the representation.  \n",
        "\n",
        "### Plotting the Weighted Spline"
      ],
      "id": "3e0e674b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ax = splines_df.dot(gam.coef_).plot(title='Weighted Splines', label='Weighted Combination of Spline Basis', figsize=(10, 6))\n",
        "ax.set_ylabel(\"Acceleration\")\n",
        "ax.set_xlabel(\"Time Steps\")\n",
        "ax.legend();"
      ],
      "id": "d0bbce37",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this manner we can see how the specification of a spline basis can help us model eccentric curves in an outcome space. \n",
        "\n",
        "Next we'll see how to more directly work with the specification of basis splines, passing these feature matrices into Bambi models. \n",
        "\n",
        "## Bayesian Splines with bambi\n",
        "\n",
        "Under the hood `Bambi` makes use of the patsy package formula syntax to specify spline basis terms.\n"
      ],
      "id": "d898c0ec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "knots_6 = np.linspace(0, np.max(X), 6+2)[1:-1]\n",
        "knots_10 = np.linspace(0, np.max(X), 10+2)[1:-1]\n",
        "knots_15 = np.linspace(0, np.max(X), 15+2)[1:-1]\n",
        "\n",
        "df = pd.DataFrame({'X': X.flatten(), 'y': y})\n",
        "formula1 = 'bs(X, degree=0, knots=knots_6)'\n",
        "formula2 = 'bs(X, degree=1, knots=knots_6, intercept=True)'\n",
        "formula3 = 'bs(X, degree=3, knots=knots_6, intercept=True)'\n",
        "formula4 = 'bs(X, degree=3, knots=knots_10, intercept=True)'\n",
        "formula5 = 'bs(X, degree=3, knots=knots_15, intercept=True)'\n",
        "model_spline1 = bmb.Model(f\"y ~ {formula1}\", df)\n",
        "model_spline2 = bmb.Model(f\"y ~ {formula2}\", df)\n",
        "model_spline3 = bmb.Model(f\"y ~ {formula3}\", df)\n",
        "model_spline4 = bmb.Model(f\"y ~ {formula4}\", df)\n",
        "model_spline5 = bmb.Model(f\"y ~ {formula5}\", df)\n",
        "model_spline5"
      ],
      "id": "b29bc05f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see here we are specifying a range of different degrees of spline basis. The different degrees corresspond to the smoothness of the overlapping splines. \n",
        "The `degree=0` splines mean we specify a piecewise constant basis i.e. 0 or 1 within each region of the partition. But we can add more degrees to see more flexible representations of the space. \n"
      ],
      "id": "86369877"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_spline5.build()\n",
        "model_spline5.graph()"
      ],
      "id": "163a15db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This will become clearer as we plot the various spline basis matrices below. \n",
        "\n",
        "### Plot the Spline Basis\n",
        "\n",
        "The below functions extract the basis specification from each model and plots the basis design for an increasingly complex series of spline basis matrices. \n"
      ],
      "id": "0c8b2cf0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_spline_basis(basis, X, ax, title=\"Spline Basis\"):\n",
        "    df = (\n",
        "        pd.DataFrame(basis)\n",
        "        .assign(X=X)\n",
        "        .melt(\"X\", var_name=\"basis_idx\", value_name=\"y\")\n",
        "    )\n",
        "\n",
        "\n",
        "    for idx in df.basis_idx.unique():\n",
        "        d = df[df.basis_idx == idx]\n",
        "        ax.plot(d[\"X\"], d[\"y\"])\n",
        "    \n",
        "    ax.set_title(title)\n",
        "    return ax\n",
        "\n",
        "def plot_knots(knots, ax):\n",
        "    for knot in knots:\n",
        "        ax.axvline(knot, color=\"0.1\", alpha=0.4)\n",
        "    return ax\n",
        "\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(5, 1, figsize=(9, 20))\n",
        "axs = axs.flatten()\n",
        "axs.flatten()\n",
        "B1 = model_spline1.response_component.design.common[formula1]\n",
        "plot_spline_basis(B1, df[\"X\"].values, ax=axs[0], title=\"Piecewise Constant Basis\")\n",
        "plot_knots(knots_6, axs[0]);\n",
        "\n",
        "B2 = model_spline2.response_component.design.common[formula2]\n",
        "ax = plot_spline_basis(B2, df[\"X\"].values, axs[1], \n",
        "title=\"Piecewise Linear Basis\")\n",
        "plot_knots(knots_6, axs[1]);\n",
        "\n",
        "B3 = model_spline3.response_component.design.common[formula3]\n",
        "ax = plot_spline_basis(B3, df[\"X\"].values, axs[2], \n",
        "title=\"Cubic Spline Basis (6 Knots)\")\n",
        "plot_knots(knots_6, axs[2]);\n",
        "\n",
        "B4 = model_spline4.response_component.design.common[formula4]\n",
        "ax = plot_spline_basis(B4, df[\"X\"].values, axs[3], \n",
        "title=\"Cubic Spline Basis (10 Knots)\")\n",
        "plot_knots(knots_10, axs[3]);\n",
        "\n",
        "\n",
        "B5 = model_spline5.response_component.design.common[formula5]\n",
        "ax = plot_spline_basis(B5, df[\"X\"].values, axs[4], \n",
        "title=\"Cubic Spline Basis (15 Knots)\")\n",
        "plot_knots(knots_15, axs[4]);\n"
      ],
      "id": "21e26419",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we've seen the nature of the modelling splines we'll fit each model to the data and plot how the weighted splines matrices are able to represent the raw data. \n",
        "\n",
        "### Fit the Individual Spline Models\n"
      ],
      "id": "6613a372"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "\n",
        "idata_spline1 = model_spline1.fit(random_seed=random_seed, idata_kwargs={\"log_likelihood\": True})\n",
        "\n",
        "idata_spline2 = model_spline2.fit(random_seed=random_seed, idata_kwargs={\"log_likelihood\": True})\n",
        "\n",
        "idata_spline3 = model_spline3.fit(random_seed=random_seed, idata_kwargs={\"log_likelihood\": True})\n",
        "\n",
        "idata_spline4 = model_spline4.fit(random_seed=random_seed, idata_kwargs={\"log_likelihood\": True})\n",
        "\n",
        "idata_spline5 = model_spline5.fit(random_seed=random_seed, idata_kwargs={\"log_likelihood\": True})"
      ],
      "id": "1df01098",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting the fits\n",
        "\n",
        "### Plot the Weighted Mean\n"
      ],
      "id": "9ab4ec67"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_weighted_splines(B, idata, formula, ax, knots):\n",
        "    posterior_stacked = az.extract(idata)\n",
        "    wp = posterior_stacked[formula].mean(\"sample\").values\n",
        "\n",
        "    plot_spline_basis(B * wp.T, df[\"X\"].values, ax)\n",
        "    ax.plot(df.X.values, np.dot(B, wp.T), color=\"black\", lw=3, label='Weighted Splines')\n",
        "    plot_knots(knots, ax);\n",
        "    ax.legend()\n",
        "\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(5, 1, figsize=(10, 20))\n",
        "axs = axs.flatten()\n",
        "axs.flatten()\n",
        "\n",
        "plot_weighted_splines(B1, idata_spline1, formula1, axs[0], knots_6)\n",
        "plot_weighted_splines(B2, idata_spline2, formula2, axs[1], knots_6)\n",
        "plot_weighted_splines(B3, idata_spline3, formula3, axs[2], knots_6)\n",
        "plot_weighted_splines(B4, idata_spline4, formula4, axs[3], knots_10)\n",
        "plot_weighted_splines(B5, idata_spline5, formula5, axs[4], knots_15)"
      ],
      "id": "ddb6ce6d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can see how the models with increasingly complex splines are more exactly able to fit the herky jerky trajectory of the outcome variable in each interval. The fewer the intervals, the less flexibility available to the model. \n",
        "\n",
        "### Compare Model Fits\n",
        "\n",
        "As before we can evaluate these model fits and compare them based on leave-one-out cross validation scores and information theoretic complexity measures. \n"
      ],
      "id": "e4036128"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axs = plt.subplots(2, 1, figsize=(9, 16))\n",
        "axs = axs.flatten()\n",
        "models_dict = {\"piecewise_constant\": idata_spline1, \"piecewise_linear\": idata_spline2, \"cubic_bspline\": idata_spline3, \"cubic_bspline_10\": idata_spline4, \n",
        "\"cubic_bspline_15\": idata_spline5}\n",
        "df_compare = az.compare(models_dict)\n",
        "az.plot_compare(df_compare, ax=axs[0])\n",
        "az.plot_compare(az.compare(models_dict, 'waic'), ax=axs[1])\n",
        "\n",
        "df_compare\n"
      ],
      "id": "2da21169",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we see that the extra complexity of using 15 splines leads to slightly worse performance measures than the less complex but seemingly adequate 10 splines.\n"
      ],
      "id": "e61e3d9a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "new_data = pd.DataFrame({\"X\": np.linspace(df.X.min(), df.X.max(), num=500)})\n",
        "    \n",
        "model_spline4.predict(idata_spline4, data=new_data, \n",
        "kind='pps', inplace=True)\n",
        "\n",
        "idata_spline4"
      ],
      "id": "55fdcca1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we plot the posterior predictive distribution of our observed variable and compare against the observed data. Additionally we plot the 89th and 50% HDI. \n"
      ],
      "id": "77d7bc78"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ax = az.plot_hdi(new_data['X'], idata_spline4['posterior_predictive']['y'], fill_kwargs={'alpha': 0.2, 'color':'firebrick'}, hdi_prob=0.89, figsize=(10, 8))\n",
        "\n",
        "az.plot_hdi(new_data['X'], idata_spline4['posterior_predictive']['y'], fill_kwargs={'alpha': 0.8, 'color':'firebrick'}, hdi_prob=0.5)\n",
        "\n",
        "y_mean = idata_spline4['posterior_predictive']['y'].mean(dim=('chain', 'draw'))\n",
        "\n",
        "ax.plot(new_data['X'], y_mean, label='Expected posterior predictive', color='k')\n",
        "ax.set_xlabel(\"Time Point\")\n",
        "ax.set_ylabel(\"Acceleration\")\n",
        "\n",
        "ax.scatter(df['X'], df['y'], label='Observed Datapoints')\n",
        "\n",
        "ax.legend()\n",
        "\n",
        "ax.set_title(\"Posterior Predictive Distribution \\n Based on 10 Knots\");"
      ],
      "id": "f5733e0e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This represents a good a clean model fit to the observed data using univariate spline smoothers. Next we'll see another alternative approach to model this outcome variable using approximate gaussian processes. \n",
        "\n",
        "## Gaussian processes\n",
        "\n",
        "The topic of gaussian processes is rich and detailed. Too rich to be fairly covered in this blog post, so we'll just say that we're using a method designed for function approximation that makes use of drawing samples from a multivariate normal distribution under a range of different covariance relationships. \n",
        "\n",
        "These relationships can be somewhat intuitively interrogated by defining different combinations of covariance relationships with priors over the parameters governing the covariance of a sequence of points. For example consider the following parameterisations. "
      ],
      "id": "bfdc5335"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lengthscale = 3\n",
        "sigma = 13\n",
        "cov = sigma**2 * pm.gp.cov.ExpQuad(1, lengthscale)\n",
        "\n",
        "X = np.linspace(0, 60, 200)[:, None]\n",
        "K = cov(X).eval()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(9, 7))\n",
        "ax.plot(\n",
        "    X,\n",
        "    pm.draw(\n",
        "        pm.MvNormal.dist(mu=np.zeros(len(K)), cov=K, shape=K.shape[0]), draws=10, random_seed=random_seed\n",
        "    ).T,\n",
        ")\n",
        "plt.title(f\"Samples from the GP prior \\n lengthscale: {3}, sigma: {13}\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.xlabel(\"X\");"
      ],
      "id": "3855b2d1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We've specified the range of X to reflect the support of the acceleration example and allowed the draws to be informed by a covariance function we have parameterised using the Exponentiated Quadratic kernel:\n",
        "\n",
        "$$k(x, x') = \\mathrm{exp}\\left[ -\\frac{(x - x')^2}{2 \\ell^2} \\right]$$\n",
        "\n",
        "The patterns exhibited show a good range of \"wiggliness\" that they should be flexible enough to capture the shape of the acceleration, if we can calibrate the posterior of parameters against the observed data. \n",
        "\n",
        "### Priors on Gaussian Priors\n",
        "\n",
        "Consider the following specification:\n"
      ],
      "id": "24e94645"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots(figsize=(9, 7))\n",
        "ax.hist(pm.draw(pm.InverseGamma.dist(mu=1, sigma=1), 1000), ec='black', bins=30);"
      ],
      "id": "40abdfa0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots(figsize=(9, 7))\n",
        "ax.hist(pm.draw(pm.Exponential.dist(lam=1), 1000), ec='black', bins=30);"
      ],
      "id": "2edafe9b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use these to specify priors on the Hilbert space approximation of gaussian priors available in the `Bambi` package. \n"
      ],
      "id": "81304bc0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prior_hsgp = {\n",
        "    \"sigma\": bmb.Prior(\"Exponential\", lam=1), # amplitude\n",
        "    \"ell\": bmb.Prior(\"InverseGamma\", mu=1, sigma=1) # lengthscale\n",
        "}\n",
        "\n",
        "# This is the dictionary we pass to Bambi\n",
        "priors = {\n",
        "    \"hsgp(X, m=10, c=1)\": prior_hsgp,\n",
        "    \"sigma\": bmb.Prior(\"HalfNormal\", sigma=4)\n",
        "}\n",
        "model_hsgp = bmb.Model(\"y ~ 0 + hsgp(X, m=10, c=1)\", df, priors=priors)\n",
        "model_hsgp\n"
      ],
      "id": "5d533b96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we've set the `m=10` to determine the number of basis vectors used in the Hilbert space approximation. The idea differs in detail from the spline based approximations we've seen, but it's perhaps useful to think of the process in the same vein. \n"
      ],
      "id": "9aa0fab1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "idata = model_hsgp.fit(inference_method=\"nuts_numpyro\",target_accept=0.95, random_seed=121195, \n",
        "idata_kwargs={\"log_likelihood\": True})\n",
        "print(idata.sample_stats[\"diverging\"].sum().to_numpy())"
      ],
      "id": "e4b18e19",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This model fits and the sampling seems to have worked well. \n"
      ],
      "id": "a524ae42"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.plot_trace(idata, backend_kwargs={\"layout\": \"constrained\"}, figsize=(9, 15));\n"
      ],
      "id": "7de14329",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The lengthscale and sigma parameters we have learned by calibrating our priors against the data. The degree to which these parameters are meaningful depend a little on how familar you are with covariance matrix kernels and their properties, so we won't dwell on the point here. \n"
      ],
      "id": "466a57f5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.summary(idata, var_names=['hsgp(X, m=10, c=1)_ell', 'hsgp(X, m=10, c=1)_sigma', 'y_sigma', 'hsgp(X, m=10, c=1)_weights'])\n"
      ],
      "id": "7b83a4d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But again we can sample from the posterior predictive distribution of the outcome variable \n"
      ],
      "id": "f523dc20"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_hsgp.predict(idata, data=new_data, \n",
        "kind='pps', inplace=True)\n",
        "\n",
        "idata"
      ],
      "id": "995ec22b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and plot the model fit to see if it can recover the observed data. \n"
      ],
      "id": "979e242e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ax = az.plot_hdi(new_data['X'], idata['posterior_predictive']['y'], fill_kwargs={'alpha': 0.2, 'color':'firebrick'}, figsize=(9, 8))\n",
        "\n",
        "az.plot_hdi(new_data['X'], idata['posterior_predictive']['y'], fill_kwargs={'alpha': 0.8, 'color':'firebrick'}, hdi_prob=0.5)\n",
        "\n",
        "y_mean = idata['posterior_predictive']['y'].mean(dim=('chain', 'draw'))\n",
        "\n",
        "ax.plot(new_data['X'], y_mean, label='Expected posterior predictive', color='k')\n",
        "\n",
        "ax.scatter(df['X'], df['y'], label='Observed Datapoints')\n",
        "\n",
        "ax.legend()\n",
        "\n",
        "ax.set_title(\"Posterior Predictive Distribution \\n Based on HSGP approximation\")\n"
      ],
      "id": "a4153690",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we can compare versus the spline models to see that by the aggregate performance measures our HSGP model seems to come out on top. \n"
      ],
      "id": "cc7c13c2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "models_dict = {\"piecewise_constant\": idata_spline1, \"piecewise_linear\": idata_spline2, \"cubic_bspline\": idata_spline3, \"cubic_bspline_10\": idata_spline4, \n",
        "\"cubic_bspline_15\": idata_spline5, 'hsgp': idata}\n",
        "df_compare = az.compare(models_dict)\n",
        "df_compare\n"
      ],
      "id": "4b3485e1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recap\n",
        "\n",
        "So far we've seen how we can use splines and gauassian processes to model highly eccentric functional relationships where the function could be approximated with univariate smoothing routine. Next we'll show how to use hierarchical modelling over spline fits to extract insight into the data generating process over a family of curves. In particular we'll focus on the development of insurance loss curves. \n",
        "\n",
        "## Insurance Loss Curves: Hierarchical Spline Models\n",
        "\n",
        "We draw on car insurance losses data set discussed in Mick Cooney's Stan case-study, but we simplify things for ourselves considerably by focusing on one type of loss and ensuring that each year under consideration has equal observations of the accruing losses. \n"
      ],
      "id": "646a333e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loss_df = pd.read_csv('ppauto_pos.csv')\n",
        "loss_df = loss_df[(loss_df['GRCODE'].isin([43, 353])) & (loss_df['DevelopmentYear'] < 1998)]\n",
        "\n",
        "loss_df = loss_df[['GRCODE', 'AccidentYear', 'DevelopmentYear', 'DevelopmentLag', 'EarnedPremDIR_B', 'CumPaidLoss_B']]\n",
        "\n",
        "\n",
        "loss_df.columns = ['grcode', 'acc_year', 'dev_year', 'dev_lag', 'premium', 'cum_loss']\n",
        "loss_df['lr'] = loss_df['cum_loss'] / loss_df['premium']\n",
        "loss_df = loss_df[(loss_df['acc_year'] <= 1992) & (loss_df['dev_lag'] <= 6)].reset_index(drop=True)\n",
        "\n",
        "loss_df['year_code'] = loss_df['acc_year'].astype(str) + '_' + loss_df['grcode'].astype(str)\n",
        "loss_df.sort_values(by=['year_code', 'acc_year', 'dev_lag'], inplace=True)\n",
        "loss_df['standardised_premium'] = (loss_df.assign(mean_premium = np.mean(loss_df['premium']))\n",
        ".assign(std_premium = np.std(loss_df['premium']))\n",
        ".apply(lambda x: (x['mean_premium'] - x['premium']) /x['std_premium'], axis=1)\n",
        ")\n",
        "\n",
        "loss_df.head(12)"
      ],
      "id": "be7f56c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot the Loss Curves\n",
        "Here we have plotted the developing loss curves from two different coded insurance products. \n"
      ],
      "id": "50417d77"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ax = (loss_df.pivot(index=['dev_lag'], columns=['grcode', 'acc_year'], values='lr')).plot(figsize=(10, 6))\n",
        "ax.set_title(\"Loss Ratios by Year\");\n"
      ],
      "id": "db8fa6c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to model these curves collectively as instances of draws from a distribution of loss curves. To do so we will specify a `PyMC` hierarchical (mixed) spline model. To do so we will have a spline basis for the global hyper parameters `beta_g` and the individual parameters for each curve. Here we define a convenience function to generate the basis splines. \n"
      ],
      "id": "3b2a86b1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from patsy import bs, dmatrix\n",
        "\n",
        "def make_basis_splines(num_knots=3, max_dev=7):\n",
        "    knot_list = np.linspace(0, max_dev, num_knots+2)[1:-1]\n",
        "    dev_periods = np.arange(1, max_dev, 1)\n",
        "\n",
        "    Bi = dmatrix(\n",
        "        \"bs(dev_periods, knots=knots, degree=3, include_intercept=True) - 1\",\n",
        "        {\"dev_periods\": dev_periods, \"knots\": knot_list},\n",
        "    )\n",
        "\n",
        "    Bg = dmatrix(\n",
        "        \"bs(dev_periods, knots=knots, degree=3, include_intercept=True) - 1\",\n",
        "        {\"dev_periods\": dev_periods, \"knots\": knot_list})\n",
        "\n",
        "\n",
        "    return Bi, Bg\n",
        "\n",
        "Bi, Bg = make_basis_splines()\n",
        "Bg"
      ],
      "id": "2bce5421",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we specify a model maker function to create the various pooled, unpooled and hierarhical (mixed) models of the insurance curve data. Note that even though we're specifying a hierarhical model we have not specified a hierarchy over the insurance codes, instead we have added this as a \"fixed\" effect feature into our regression model.\n"
      ],
      "id": "4f5ebfff"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "\n",
        "def make_model(loss_df, num_knots=3, max_dev=7, model_type='mixed'):\n",
        "    Bi, Bg = make_basis_splines(num_knots, max_dev)\n",
        "    observed = loss_df['lr'].values\n",
        "    uniques, unique_codes = pd.factorize(loss_df['year_code'])\n",
        "    coords= {'years': unique_codes, \n",
        "            'splines': list(range(Bi.shape[1])) ,\n",
        "            'measurement': list(range(6)), \n",
        "            'obs': uniques\n",
        "            }\n",
        "\n",
        "    with pm.Model(coords=coords) as sp_insur:\n",
        "        basis_g = pm.MutableData('Bg', np.asfortranarray(Bg))\n",
        "\n",
        "        tau = pm.HalfCauchy('tau', 1)\n",
        "        ## Global Hierarchical Spline Terms\n",
        "        beta_g = pm.Normal(\"beta_g\", mu=0, sigma=tau, \n",
        "        dims='splines')\n",
        "        mu_g = pm.Deterministic(\"mu_g\", pm.math.dot(basis_g, beta_g), dims='measurement')\n",
        "\n",
        "        ## Individual or Year Specific Spline Modifications\n",
        "        if model_type in ['mixed', 'unpooled']:\n",
        "            basis_i = pm.MutableData('Bi', np.asfortranarray(Bi))\n",
        "            beta = pm.Normal(\"beta\", mu=0, sigma=tau, dims=('splines', 'years'))\n",
        "            mui = pm.Deterministic(\"mui\", pm.math.dot(basis_i, beta), dims=('measurement', 'years'))\n",
        "        \n",
        "        ## Features\n",
        "        prem = pm.MutableData('prem', loss_df['standardised_premium'].values)\n",
        "        grcode = pm.MutableData('grcode', loss_df['grcode'] == 43)\n",
        "\n",
        "        beta_prem = pm.Normal('beta_prem', 0, 1)\n",
        "        beta_grcode = pm.Normal('beta_grcode', 0, 1)\n",
        "        mu_prem = beta_prem*prem\n",
        "        mu_grcode = beta_grcode*grcode\n",
        "\n",
        "        ## Likelihood\n",
        "        sigma = pm.TruncatedNormal(\"sigma\", 1, lower=0.05)\n",
        "        if model_type == 'mixed':\n",
        "            mu = pm.Deterministic('mu',  mu_grcode + mu_prem + (mu_g.T + mui.T).ravel(), dims='obs')\n",
        "            lr_likelihood = pm.Normal(\"lr\", mu, sigma, observed=observed, dims=('obs'))\n",
        "        elif model_type == 'pooled': \n",
        "             lr_likelihood = pm.Normal(\"lr\",  np.repeat(mu_g, len(unique_codes)), sigma, observed=observed, dims='obs')\n",
        "        elif model_type == 'unpooled':\n",
        "            lr_likelihood = pm.Normal(\"lr\",  mui.T.ravel(), sigma, observed=observed, dims=('obs'))\n",
        "\n",
        "\n",
        "        ## Sampling\n",
        "        idata_sp_insur = pm.sample(2000, return_inferencedata=True, target_accept=.99,\n",
        "        idata_kwargs={\"log_likelihood\": True})\n",
        "        idata_sp_insur = pm.sample_posterior_predictive(\n",
        "            idata_sp_insur,extend_inferencedata=True)\n",
        "\n",
        "    return idata_sp_insur, sp_insur\n",
        "\n",
        "\n",
        "idata_sp_insur_unpooled, sp_insur_unpooled = make_model(loss_df, model_type='unpooled')\n",
        "idata_sp_insur_pooled, sp_insur_pooled = make_model(loss_df, model_type='pooled')\n",
        "idata_sp_insur_mixed, sp_insur_mixed = make_model(loss_df, model_type='mixed')"
      ],
      "id": "af8329ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model structure can be seen more clearly in this graph\n"
      ],
      "id": "37db9303"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pm.model_to_graphviz(sp_insur_mixed)\n"
      ],
      "id": "b765487f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can extract the effect of the differences grcodes and examine the baseline and annual spline related coefficients. \n"
      ],
      "id": "bd54d7e0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "summary = az.summary(idata_sp_insur_mixed, var_names=['beta_grcode', 'beta_prem', 'beta_g', 'beta'])\n",
        "\n",
        "summary"
      ],
      "id": "bc6aebb5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again we can compare the performance metrics of the various models. \n"
      ],
      "id": "a3377184"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "compare_df = az.compare({'unpooled': idata_sp_insur_unpooled, \n",
        "            'pooled': idata_sp_insur_pooled, \n",
        "            'mixed': idata_sp_insur_mixed})\n",
        "\n",
        "compare_df"
      ],
      "id": "c7e56945",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot the Posterior Predictive Checks\n",
        "\n",
        "We can check how well the model can recapture the observed data.\n"
      ],
      "id": "7f13b3c3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_ppc_splines(idata):\n",
        "    fig, axs = plt.subplots(5, 2, figsize=(9, 30), sharey=True)\n",
        "    axs = axs.flatten()\n",
        "    dev_periods = np.arange(1, 7, 1)\n",
        "    uniques, unique_codes = pd.factorize(loss_df['year_code'])\n",
        "    for y, c in zip(unique_codes, range(10)):\n",
        "        az.plot_hdi(dev_periods, idata['posterior_predictive']['lr'].sel(obs=c), color='firebrick', ax=axs[c], fill_kwargs={'alpha': 0.2}, hdi_prob=.89)\n",
        "        az.plot_hdi(dev_periods, idata['posterior_predictive']['lr'].sel(obs=c), color='firebrick', ax=axs[c], hdi_prob=0.5)\n",
        "        axs[c].scatter(dev_periods, loss_df[(loss_df['year_code'] == y)]['lr'], color='k', label='Actual Loss Ratio')\n",
        "        axs[c].plot(dev_periods, loss_df[(loss_df['year_code'] == y)]['lr'], color='k')\n",
        "        axs[c].set_title(f\"Posterior Predictive Loss Ratio \\n 89% and 50% HDI: {y}\")\n",
        "        axs[c].legend()\n",
        "        axs[c].set_xlabel('Default Lag');\n",
        "\n",
        "plot_ppc_splines(idata_sp_insur_mixed)"
      ],
      "id": "0201ed92",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot the Hierarchical Components\n",
        "\n",
        "In the following plot we show similarly how to recapture the observed data, but additionally we decompose the structure of the model in each case and extract baseline forecasts which would be our guide to future loss-ratio development curves in lieu of any other information. \n"
      ],
      "id": "064a59bd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mu_g = idata_sp_insur_mixed.posterior.stack(draws=(\"chain\", \"draw\"))[\"mu_g\"]\n",
        "\n",
        "mu_i = idata_sp_insur_mixed.posterior.stack(draws=(\"chain\", \"draw\"))[\"mui\"]\n",
        "\n",
        "mu  = idata_sp_insur_mixed.posterior.stack(draws=(\"chain\", \"draw\"))['mu']\n",
        "\n",
        "beta_grcode = idata_sp_insur_mixed.posterior.stack(draws=(\"chain\", \"draw\"))['beta_grcode']\n",
        "\n",
        "dev_periods = np.arange(1, 7, 1)\n",
        "uniques, unique_codes = pd.factorize(loss_df['year_code'])\n",
        "\n",
        "mosaic = \"\"\"\n",
        "         AB\n",
        "         CD\n",
        "         EF\n",
        "         GH\n",
        "         IJ\n",
        "         KK\n",
        "\"\"\"\n",
        "fig, axs = plt.subplot_mosaic(mosaic, sharey=True, \n",
        "figsize=(9, 30))\n",
        "axs = [axs[k] for k in axs.keys()] \n",
        "\n",
        "mu_g_mean = mu_g.mean(dim='draws')\n",
        "for y, c in zip(unique_codes, range(10)):\n",
        "    group_effect = 0\n",
        "    if '43' in y: \n",
        "        group_effect = beta_grcode.mean().item()\n",
        "    mu_i_mean = mu_i.sel(years=y).mean(dim='draws')\n",
        "    axs[c].plot(dev_periods, group_effect + mu_g_mean.values + mu_i_mean.values, label='Combined + E(grp_effect)', color='purple', linewidth=3.5)\n",
        "    axs[c].plot(dev_periods, group_effect + mu_g_mean.values, label='E(Hierarchical Baseline)', color='red', linestyle='--')\n",
        "    axs[c].plot(dev_periods,  group_effect + mu_i_mean.values, label='E(Year Specific Adjustment term)', color='blue', linestyle='--')\n",
        "    axs[c].scatter(dev_periods, loss_df[(loss_df['year_code'] == y)]['lr'], color='k', label='Actual Loss Ratio')\n",
        "    axs[c].legend()\n",
        "    az.plot_hdi(dev_periods,mu.sel(obs=c).T  , ax=axs[c], color='firebrick', fill_kwargs={'alpha': 0.2})\n",
        "    az.plot_hdi(dev_periods, mu.sel(obs=c).T , ax=axs[c], color='firebrick', fill_kwargs={'alpha': 0.5}, hdi_prob=.50)\n",
        "    axs[c].set_title(f\"Components for Year {y}\")\n",
        "\n",
        "axs[10].plot(dev_periods, mu_g_mean.values, label='E(Hierarchical Baseline)', color='black')\n",
        "axs[10].plot(dev_periods, mu_g_mean.values + group_effect, label='E(Hierarchical Baseline) + E(grp_effect)', color='black', linestyle='--')\n",
        "az.plot_hdi(dev_periods, mu_g.T.values, color='slateblue', ax=axs[10], fill_kwargs={'alpha': 0.2})\n",
        "az.plot_hdi(dev_periods, mu_g.T.values + group_effect, color='magenta', ax=axs[10], fill_kwargs={'alpha': 0.2})\n",
        "az.plot_hdi(dev_periods, mu_g.T.values, color='slateblue', ax=axs[10], hdi_prob=.5)\n",
        "az.plot_hdi(dev_periods, mu_g.T.values  + group_effect, color='magenta', ax=axs[10], hdi_prob=.5)\n",
        "axs[10].set_title(\"Baseline Forecast Loss Ratio \\n with Uncertainty Intervals\")\n",
        "axs[10].legend();\n"
      ],
      "id": "03a532a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This plot is a bit clunky, because we're mixing expectations and posterior distributions over the parameters. The point is just to highlight the \"compositional\" structure of our model. A better way to interrogate the implications of the model is to \"push\" forward different data through the posterior predictive distribution and derive a kind of ceteris paribus rule for accrual of losses. We d\n"
      ],
      "id": "faf80560"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with sp_insur_mixed: \n",
        "    pm.set_data({'grcode': np.ones(len(loss_df)), \n",
        "    })\n",
        "    idata_43 = pm.sample_posterior_predictive(idata_sp_insur_mixed, var_names=['lr'], extend_inferencedata =True)\n",
        "\n",
        "with sp_insur_mixed: \n",
        "    pm.set_data({'grcode': np.zeros(len(loss_df))})\n",
        "    idata_353 = pm.sample_posterior_predictive(idata_sp_insur_mixed, var_names=['lr'], extend_inferencedata=True)\n",
        "\n",
        "idata_353"
      ],
      "id": "91cbfb4d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Even here though we want to average the curves over the specific years in the data and abstract a view of the model implications under different counterfactual settings. Here we define a helper function to effect this step. \n"
      ],
      "id": "bd736f53"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_posterior_predictive_curve(idata, prem=2, grcode=1):\n",
        "    weighted_splines = [np.dot(np.asfortranarray(Bi), az.extract(idata['posterior']['beta'])['beta'].values[:, :, i]) for i in range(1000)]\n",
        "\n",
        "    weighted_splines_1 = [np.dot(np.asfortranarray(Bg), az.extract(idata['posterior']['beta_g'])['beta_g'].values[:, i]) for i in range(1000)]\n",
        "\n",
        "    beta_grcode = az.extract(idata['posterior']['beta_grcode'])['beta_grcode']\n",
        "\n",
        "    beta_prem = az.extract(idata['posterior']['beta_prem'])['beta_prem']\n",
        "    df1 = pd.DataFrame([beta_prem.values[i]*prem + beta_grcode.values[i]*grcode for i in range(1000)]).T\n",
        "\n",
        "    ## Crucial step averaging over the years to get\n",
        "    ## a view of the development period\n",
        "    df = pd.concat([pd.DataFrame(weighted_splines_1[i].T + weighted_splines[i].T).mean() for i in range(1000)], axis=1)\n",
        "\n",
        "    df = df1.iloc[0] + df\n",
        "\n",
        "    return df\n",
        "\n",
        "pred_df_1 = get_posterior_predictive_curve(idata_43, prem=1, grcode=1)\n",
        "\n",
        "pred_df_0 = get_posterior_predictive_curve(idata_353, prem=1, grcode=0) \n",
        "\n",
        "fig, ax = plt.subplots(figsize=(9, 7), sharey=True)\n",
        "\n",
        "\n",
        "ax.plot(pred_df_1, color='firebrick', alpha=0.1)\n",
        "ax.plot(pred_df_0, color='slateblue', alpha=0.2);\n",
        "ax.plot(pred_df_0.mean(axis=1), linestyle='-', color='k', linewidth=4, label='grcode 353 prem 1')\n",
        "\n",
        "ax.plot(pred_df_1.mean(axis=1), linestyle='--', color='grey', linewidth=4, label='grcode 43 prem 1')\n",
        "\n",
        "ax.set_title(\"Posterior Samples of the Trajectories \\n Under different Counterfactual settings\")\n",
        "ax.set_ylabel(\"Loss Ratio\")\n",
        "ax.set_xlabel(\"Development Period\")\n",
        "ax.legend();"
      ],
      "id": "59415191",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "We've seen the application of splines as univariate smoothers to approximate wiggly curves of arbitrary shape. We've also tried gaussian process approximations of the same univariate functions. The suggested flexibility of both\n",
        "methods is a strength, but we need to be careful where splines have a tendency to over-fit to individual curves. As such we have tried to show that we incorporate spline basis modelling in a hierarchical bayesian model and recover very compelling posterior predictive checks and additionally derive predictiions from the mixed variant of the hierarhical model which helps us understand the implications of the data for generic forecasts. "
      ],
      "id": "bc35cc73"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "pymc_causal",
      "language": "python",
      "display_name": "pymc_causal"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}