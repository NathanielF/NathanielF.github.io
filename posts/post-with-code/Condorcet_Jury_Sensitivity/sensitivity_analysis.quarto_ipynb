{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"The Condorcet Jury Theorem and Democratic Rationality\"\n",
        "subtitle: \"Sensitivity Analysis of Failure Conditions\"\n",
        "categories: [\"theory\", \"simulation\", \"causal inference\", \"sensitivity analysis\"]\n",
        "keep-ipynb: true\n",
        "self-contained: true\n",
        "draft: true\n",
        "toc: true\n",
        "execute: \n",
        "  freeze: auto \n",
        "  execute: true\n",
        "  eval: true\n",
        "jupyter: applied-bayesian-regression-modeling-env\n",
        "image: 'evolving_dag.png'\n",
        "author:\n",
        "    - url: https://nathanielf.github.io/\n",
        "    - affiliation: PyMC dev\n",
        "citation: true\n",
        "---"
      ],
      "id": "e385426f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pymc as pm\n",
        "import matplotlib.pyplot as plt\n",
        "import arviz as az\n",
        "import seaborn as sns"
      ],
      "id": "f6554e45",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# ============================================================================\n",
        "# SIMULATE DATA\n",
        "# ============================================================================\n",
        "# Simulate some jury voting data\n",
        "n_cases = 50  # number of cases\n",
        "n_jurors = 15  # number of jurors\n",
        "\n",
        "def make_ground_truth(n_cases, n_jurors, blocks=False):\n",
        "    # True states (1 = guilty, 0 = not guilty)\n",
        "    true_states = np.random.binomial(1, 0.5, n_cases)\n",
        "\n",
        "    # Individual juror competencies (for simulation)\n",
        "    true_p = 0.7  # average competence\n",
        "    true_discrimination = 0.5  # how much competencies vary (sd in logit space)\n",
        "\n",
        "    # Simulate heterogeneous competencies\n",
        "    logit_p_jurors = np.random.normal(np.log(true_p / (1 - true_p)), \n",
        "                                    true_discrimination, \n",
        "                                    n_jurors)\n",
        "    \n",
        "    if blocks:\n",
        "        block_id = np.array([\n",
        "            0, 0, 0, 0, 0,\n",
        "            1, 1, 1, 1, 1, 1,\n",
        "            2, 2, 2, 2\n",
        "        ])\n",
        "        n_blocks = len(np.unique(block_id))\n",
        "\n",
        "        true_sigma_block = 1.2\n",
        "        block_effect = np.random.normal(\n",
        "            0.0,\n",
        "            true_sigma_block,\n",
        "            n_blocks\n",
        "        )\n",
        "        logit_p_jurors = (logit_p_jurors + block_effect[block_id])\n",
        "\n",
        "    p_jurors = 1 / (1 + np.exp(-logit_p_jurors))\n",
        "\n",
        "    # Simulate votes\n",
        "    votes = np.zeros((n_cases, n_jurors))\n",
        "    for i in range(n_cases):\n",
        "        for j in range(n_jurors):\n",
        "            if true_states[i] == 1:\n",
        "                votes[i, j] = np.random.binomial(1, p_jurors[j])\n",
        "            else:\n",
        "                votes[i, j] = np.random.binomial(1, 1 - p_jurors[j])\n",
        "\n",
        "    return votes, p_jurors\n",
        "\n",
        "votes, p_jurors = make_ground_truth(n_cases, n_jurors)\n",
        "\n",
        "print(f\"Data simulated: {n_cases} cases, {n_jurors} jurors\")\n",
        "print(f\"True average competence: {p_jurors.mean():.3f}\")\n",
        "print(f\"Majority vote accuracy: {(votes.mean(axis=1) > 0.5).mean():.3f}\")\n"
      ],
      "id": "16c3ff16",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define different prior specifications\n",
        "prior_specs = {\n",
        "    'weakly_informative': {'alpha': 3, 'beta': 2, 'desc': 'Weakly informative (centered at 0.6)'},\n",
        "    'strong_competence': {'alpha': 10, 'beta': 5, 'desc': 'Strong prior (p ~ 0.67)'},\n",
        "    'barely_competent': {'alpha': 6, 'beta': 5, 'desc': 'Skeptical prior (p ~ 0.55)'},\n",
        "    'incompetent': {'alpha': 5, 'beta': 10, 'desc': 'Incompetent prior (p ~ 0.33)'},\n",
        "}"
      ],
      "id": "f37670ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | output: false\n",
        "    \n",
        "def fit_condorcet_base_model(votes, spec):\n",
        "\n",
        "    with pm.Model() as model:\n",
        "        # SENSITIVITY PARAMETER: Prior on competence\n",
        "        # This is our first example of treating assumptions as parameters\n",
        "        p = pm.Beta('p', alpha=spec['alpha'], beta=spec['beta'])\n",
        "        \n",
        "        # True state of each case (latent)\n",
        "        true_state = pm.Bernoulli('true_state', p=0.5, shape=n_cases)\n",
        "        \n",
        "        # Likelihood\n",
        "        vote_prob = pm.Deterministic('vote_prob', pm.math.switch(\n",
        "            pm.math.eq(true_state[:, None], 1),\n",
        "            p,\n",
        "            1 - p\n",
        "        ))\n",
        "        \n",
        "        likelihood = pm.Bernoulli('votes', p=vote_prob, observed=votes)\n",
        "        \n",
        "        # Posterior predictive: majority vote accuracy for different jury sizes\n",
        "        jury_sizes_eval = [3, 7, 15]\n",
        "        for size in jury_sizes_eval:\n",
        "            # Simulate votes for a new case (truth = 1)\n",
        "            votes_sim = pm.Bernoulli(f'sim_votes_{size}', p=p, shape=size)\n",
        "            majority_correct = pm.Deterministic(\n",
        "                f'majority_correct_{size}',\n",
        "                pm.math.sum(votes_sim) > size / 2\n",
        "            )\n",
        "        \n",
        "        # Sample\n",
        "        idata = pm.sample_prior_predictive()\n",
        "        idata.extend(pm.sample(2000, tune=1000, random_seed=42,\n",
        "                                       target_accept=0.95, return_inferencedata=True))\n",
        "        idata.extend(pm.sample_posterior_predictive(idata))\n",
        "\n",
        "    return idata, model\n",
        "\n",
        "traces = {}\n",
        "\n",
        "for prior_name, spec in prior_specs.items():\n",
        "    print(f\"\\nFitting with {spec['desc']}...\")\n",
        "    idata, model = fit_condorcet_base_model(votes, spec)\n",
        "    traces[prior_name] = idata\n",
        "    traces[prior_name + '_model'] = model\n"
      ],
      "id": "bcb99b00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PRIOR SENSITIVITY RESULTS\")\n",
        "print(\"=\"*70)\n",
        "ests = {}\n",
        "for prior_name in prior_specs.keys():\n",
        "    for i in [3, 7, 15]:\n",
        "        p = traces[prior_name]['prior'][f'majority_correct_{i}'].mean().item()\n",
        "        if prior_name in ests:\n",
        "            ests[prior_name].append(p)\n",
        "        else: \n",
        "            ests[prior_name] = [p]\n",
        "    \n",
        "\n",
        "prior_estimates = pd.DataFrame(ests, index=['Correct % for Majority of 3', 'Correct % for Majority of 7', 'Correct % for Majority of 15'])\n",
        "prior_estimates\n"
      ],
      "id": "26dbda52",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PRIOR SENSITIVITY RESULTS\")\n",
        "print(\"=\"*70)\n",
        "ests = {}\n",
        "for prior_name in prior_specs.keys():\n",
        "    for i in [3, 7, 15]:\n",
        "        p = traces[prior_name]['posterior'][f'majority_correct_{i}'].mean().item()\n",
        "        if prior_name in ests:\n",
        "            ests[prior_name].append(p)\n",
        "        else: \n",
        "            ests[prior_name] = [p]\n",
        "    \n",
        "\n",
        "posterior_estimates = pd.DataFrame(ests, index=['Correct % for Majority of 3', 'Correct % for Majority of 7', 'Correct % for Majority of 15'])\n",
        "posterior_estimates"
      ],
      "id": "b7baab7a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
        "axs = axs.flatten()\n",
        "jitters = np.linspace(-0.4, 0.4, 3)\n",
        "\n",
        "for prior_name in prior_specs.keys():\n",
        "    axs[0].plot(prior_estimates.index, prior_estimates[prior_name], label=prior_name + ' prior', marker='o')\n",
        "    axs[1].plot(posterior_estimates.index, posterior_estimates[prior_name], label=prior_name + ' prior', marker='o')\n",
        "axs[0].legend()\n",
        "axs[1].legend()\n",
        "axs[0].set_title(\"Prior Estimates for Majority Accuracy\");\n",
        "axs[1].set_title(\"Posterior Estimates for Majority Accuracy\");\n"
      ],
      "id": "734bcea9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define different priors on discrimination parameter\n",
        "discrimination_priors = {\n",
        "    'weak_discrimination': {'sigma': 0.5, 'desc': 'Weak discrimination prior (σ ~ 0.25)'},\n",
        "    'moderate_discrimination': {'sigma': 1.0, 'desc': 'Moderate discrimination prior (σ ~ 0.5)'},\n",
        "    'strong_discrimination': {'sigma': 2.0, 'desc': 'Strong discrimination prior (σ ~ 2)'},\n",
        "}\n",
        "\n",
        "\n",
        "def fit_discrimination_binomial_model(votes, n_jurors, priors):\n",
        "    majority_votes = (votes.mean(axis=1) > 0.5).astype(int)\n",
        "    agreements_per_juror = np.array([(votes[:, j] == majority_votes).sum() for j in range(n_jurors)])\n",
        "    empirical_competence = agreements_per_juror / n_cases\n",
        "\n",
        "    with pm.Model() as sensitivity_model:\n",
        "        # Hyperpriors for the population distribution\n",
        "        mu_logit_p = pm.Normal('mu_logit_p', mu=0.6, sigma=0.5)\n",
        "        \n",
        "        # KEY SENSITIVITY PARAMETER: individual discrimination\n",
        "        sigma_logit_p = pm.HalfNormal('sigma_logit_p', sigma=spec['sigma'])\n",
        "        \n",
        "        # NON-CENTERED PARAMETERIZATION for better sampling\n",
        "        # Use: logit_p_juror = mu + sigma * z, where z ~ Normal(0, 1)\n",
        "        z_juror = pm.Normal('z_juror', mu=0, sigma=1, shape=n_jurors)\n",
        "        logit_p_juror = pm.Deterministic('logit_p_juror', \n",
        "                                        mu_logit_p + sigma_logit_p * z_juror)\n",
        "        p_juror = pm.Deterministic('p_juror', pm.math.invlogit(logit_p_juror))\n",
        "        \n",
        "        # Mean competence\n",
        "        mean_p = pm.Deterministic('mean_p', pm.math.invlogit(mu_logit_p))\n",
        "        \n",
        "        # Likelihood\n",
        "        pm.Binomial('agreements', \n",
        "                   n=n_cases, \n",
        "                   p=p_juror, \n",
        "                   observed=agreements_per_juror)\n",
        "        \n",
        "        # Sample with non-centered parameterization\n",
        "        idata = pm.sample_prior_predictive()\n",
        "        idata.extend(pm.sample(\n",
        "            1000,\n",
        "            tune=2000,\n",
        "            random_seed=42,\n",
        "            target_accept=0.95,\n",
        "            return_inferencedata=True,\n",
        "            idata_kwargs={\"log_likelihood\": True}\n",
        "        )\n",
        "        )\n",
        "        idata.extend(pm.sample_posterior_predictive(idata))\n",
        "\n",
        "    return idata, model\n",
        "\n",
        "\n",
        "traces_discrimination = {}\n",
        "\n",
        "for prior_name, spec in discrimination_priors.items():\n",
        "    print(f\"\\nFitting with {spec['desc']}...\")\n",
        "    idata, model = fit_discrimination_binomial_model(votes, n_jurors, spec)\n",
        "    traces_discrimination[prior_name] = idata\n",
        "    traces_discrimination[prior_name + '_model'] = model\n"
      ],
      "id": "b1cf1b5d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.summary(idata)\n"
      ],
      "id": "78e1c2cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "# ============================================================\n",
        "# CONSOLIDATED POSTERIOR PREDICTIVE WORKFLOW\n",
        "# ============================================================\n",
        "\n",
        "# Required inputs:\n",
        "#   idata        : InferenceData from collapsed (binomial) model\n",
        "#   n_cases      : number of cases\n",
        "#   n_jurors     : number of jurors\n",
        "#   jury_sizes   : list of jury sizes to evaluate (e.g. [3, 5, 7, 10, 15])\n",
        "\n",
        "jury_sizes = [3, 5,  7, 10, 15]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1. Generative expansion: truth -> votes\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def simulate_votes(p_juror, n_cases, truth=None):\n",
        "    \"\"\"\n",
        "    p_juror : (n_jurors,)\n",
        "    returns:\n",
        "        truth : (n_cases,)\n",
        "        votes : (n_cases, n_jurors)\n",
        "    \"\"\"\n",
        "    if truth is None:\n",
        "        truth = np.random.binomial(1, 0.5, size=n_cases)\n",
        "    votes = np.zeros((n_cases, n_jurors), dtype=int)\n",
        "\n",
        "    for i in range(n_cases):\n",
        "        for j in range(n_jurors):\n",
        "            prob = p_juror[j] if truth[i] == 1 else 1 - p_juror[j]\n",
        "            votes[i, j] = np.random.binomial(1, prob)\n",
        "\n",
        "    return truth, votes\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. Diagnostic functions\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def majority_accuracy(votes, truth):\n",
        "    majority = votes.mean(axis=1) > 0.5\n",
        "    return np.mean(majority == truth)\n",
        "\n",
        "\n",
        "def unanimity_rate(votes):\n",
        "    return np.mean(\n",
        "        (votes.sum(axis=1) == 0) |\n",
        "        (votes.sum(axis=1) == votes.shape[1])\n",
        "    )\n",
        "\n",
        "\n",
        "def juror_agreement_rates(votes, truth):\n",
        "    return np.mean(votes == truth[:, None], axis=0)\n",
        "\n",
        "\n",
        "def error_correlation(votes, truth):\n",
        "    errors = votes != truth[:, None]\n",
        "    return np.corrcoef(errors.T)\n",
        "\n",
        "\n",
        "def majority_accuracy_for_size(votes, truth, jury_size):\n",
        "    n_cases, n_jurors = votes.shape\n",
        "    correct = np.zeros(n_cases, dtype=int)\n",
        "\n",
        "    for i in range(n_cases):\n",
        "        jurors = np.random.choice(\n",
        "            n_jurors, size=jury_size, replace=False\n",
        "        )\n",
        "        sub_votes = votes[i, jurors]\n",
        "        majority = sub_votes.mean() > 0.5\n",
        "        correct[i] = (majority == truth[i])\n",
        "\n",
        "    return correct.mean()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4. Run posterior predictive simulations\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def run_post_fit_ppc(p_juror_samples, n_cases, truth=None, jury_sizes=[3, 5, 7, 10, 15]):\n",
        "    # Storage\n",
        "    n_samples = p_juror_samples.shape[1]\n",
        "    n_jurors = p_juror_samples.shape[0]\n",
        "    ppc_results = {\n",
        "        \"majority_accuracy_15\": np.zeros(n_samples),\n",
        "        \"unanimity_rate_15\": np.zeros(n_samples),\n",
        "        \"agreement_rates\": np.zeros((n_samples, n_jurors)),\n",
        "        \"error_corr\": np.zeros((n_samples, n_jurors, n_jurors)),\n",
        "        \"majority_accuracy_by_size\": {\n",
        "            k: np.zeros(n_samples) for k in jury_sizes\n",
        "        }\n",
        "    }\n",
        "    for s in range(n_samples):\n",
        "        truth_s, votes_s = simulate_votes(\n",
        "            p_juror_samples[:, s],\n",
        "            n_cases, \n",
        "            truth\n",
        "        )\n",
        "\n",
        "        # Full jury diagnostics\n",
        "        ppc_results[\"majority_accuracy_15\"][s] = (\n",
        "            majority_accuracy(votes_s, truth_s)\n",
        "        )\n",
        "        ppc_results[\"unanimity_rate_15\"][s] = (\n",
        "            unanimity_rate(votes_s)\n",
        "        )\n",
        "        ppc_results[\"agreement_rates\"][s] = (\n",
        "            juror_agreement_rates(votes_s, truth_s)\n",
        "        )\n",
        "        ppc_results[\"error_corr\"][s] = (\n",
        "            error_correlation(votes_s, truth_s)\n",
        "        )\n",
        "\n",
        "        # Sub-jury diagnostics\n",
        "        for k in jury_sizes:\n",
        "            ppc_results[\"majority_accuracy_by_size\"][k][s] = (\n",
        "                majority_accuracy_for_size(votes_s, truth_s, k)\n",
        "            )\n",
        "\n",
        "    return ppc_results\n",
        "\n",
        "def summarise_error_corr(ppc_results):\n",
        "    corr = ppc_results[\"error_corr\"]  # (samples, jurors, jurors)\n",
        "    n = corr.shape[1]\n",
        "\n",
        "    off_diag = []\n",
        "    for s in range(corr.shape[0]):\n",
        "        mat = corr[s]\n",
        "        off_diag.append(\n",
        "            mat[np.triu_indices(n, k=1)]\n",
        "        )\n",
        "\n",
        "    off_diag = np.concatenate(off_diag)\n",
        "\n",
        "    return {\n",
        "        \"mean_off_diag\": off_diag.mean(),\n",
        "        \"sd_off_diag\": off_diag.std(),\n",
        "        \"p95_abs_corr\": np.percentile(np.abs(off_diag), 95),\n",
        "    }\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5. Summaries (example outputs)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def summarise_post_fit_ppc(ppc_results):\n",
        "    print(\"\\n=== Majority accuracy (full jury) ===\")\n",
        "    a = np.percentile(\n",
        "        ppc_results[\"majority_accuracy_15\"], [5, 50, 95]\n",
        "    )\n",
        "    print(a)\n",
        "\n",
        "    print(\"\\n=== Unanimity rate (full jury) ===\")\n",
        "    b = np.percentile(\n",
        "        ppc_results[\"unanimity_rate_15\"], [5, 50, 95]\n",
        "    )\n",
        "    print(b)\n",
        "    summaries = []\n",
        "    percentiles = [5, 50, 95]\n",
        "    print(\"\\n=== Majority accuracy by jury size ===\")\n",
        "    for k in jury_sizes:\n",
        "        print(f\"Jury size {k}:\")\n",
        "        c = np.percentile(\n",
        "                ppc_results[\"majority_accuracy_by_size\"][k],\n",
        "                percentiles\n",
        "            )\n",
        "        print(c)\n",
        "        summaries.append(c)\n",
        "\n",
        "\n",
        "    print(\"\\n=== Mean juror agreement rates ===\")\n",
        "    d = ppc_results[\"agreement_rates\"].mean(axis=0)\n",
        "    print(d)\n",
        "    summaries.append(d)\n",
        "    summaries_df = pd.DataFrame(summaries[:-1]).T\n",
        "    majorities = [f'majority_accuracy_{i}' for i in jury_sizes]\n",
        "    columns = majorities\n",
        "    summaries_df.columns = columns\n",
        "    summaries_df.index = [f'percentile_{i}' for i in percentiles]\n",
        "    return summaries_df\n",
        "\n",
        "\n",
        "def compare_prior_posterior(idata):\n",
        "    p_juror_samples_prior = (idata.prior[\"p_juror\"].stack(sample=(\"chain\", \"draw\")).values)  \n",
        "    p_juror_samples_posterior = (idata.posterior[\"p_juror\"].stack(sample=(\"chain\", \"draw\")).values)  \n",
        "    n_jurors, n_samples = p_juror_samples_posterior.shape\n",
        "    ppc_result_prior = run_post_fit_ppc(p_juror_samples_prior, n_cases, true_states)\n",
        "    ppc_result_posterior = run_post_fit_ppc(p_juror_samples_posterior, n_cases, true_states)\n",
        "    summaries_prior = summarise_post_fit_ppc(ppc_result_prior)\n",
        "    summaries_posterior = summarise_post_fit_ppc(ppc_result_posterior)\n",
        "\n",
        "    summary = pd.concat({'prior': summaries_prior, 'posterior': summaries_posterior})\n",
        "    return summary, ppc_result_posterior\n"
      ],
      "id": "27da0510",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | code-fold: true\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def plot_differences(df):\n",
        "    # 1. Clean up column names to get numeric x-axis values\n",
        "    # Assuming your df is named 'df'\n",
        "    x_values = [3, 5, 7, 10, 15] # Extracted from your column headers\n",
        "    # If you have 'majority_accuracy_15' as mentioned in your text, add 15 to this list.\n",
        "\n",
        "    # 2. Extract the specific rows for plotting\n",
        "    prior_median = df.loc[('prior', 'percentile_50')]\n",
        "    prior_low = df.loc[('prior', 'percentile_5')]\n",
        "    prior_high = df.loc[('prior', 'percentile_95')]\n",
        "\n",
        "    post_median = df.loc[('posterior', 'percentile_50')]\n",
        "    post_low = df.loc[('posterior', 'percentile_5')]\n",
        "    post_high = df.loc[('posterior', 'percentile_95')]\n",
        "\n",
        "    # 3. Create the plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot Prior\n",
        "    plt.plot(x_values, prior_median, label='Prior Median', color='blue', marker='o')\n",
        "    plt.fill_between(x_values, prior_low, prior_high, color='blue', alpha=0.2, label='Prior (5th-95th)')\n",
        "\n",
        "    # Plot Posterior\n",
        "    plt.plot(x_values, post_median, label='Posterior Median', color='red', marker='o')\n",
        "    plt.fill_between(x_values, post_low, post_high, color='red', alpha=0.2, label='Posterior (5th-95th)')\n",
        "\n",
        "    # Formatting\n",
        "    plt.title('Majority Accuracy: Prior vs Posterior Distributions')\n",
        "    plt.xlabel('Number of Jurors (n) in Majority Calculation')\n",
        "    plt.ylabel('Majority Accuracy Score')\n",
        "    plt.xticks(x_values)\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_error_corr_heatmap(ppc_results, title=\"Error correlation\"):\n",
        "    mean_corr = ppc_results[\"error_corr\"].mean(axis=0)\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(\n",
        "        mean_corr,\n",
        "        vmin=-0.3,\n",
        "        vmax=0.3,\n",
        "        cmap=\"coolwarm\",\n",
        "        square=True,\n",
        "        cbar_kws={\"label\": \"Error correlation\"}\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "id": "2acd29cd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "summaries_weak_discrimination,  ppc_result_posterior_weak_discrimination = compare_prior_posterior(traces_discrimination['weak_discrimination'])\n",
        "\n",
        "summaries_weak_discrimination\n"
      ],
      "id": "6792a929",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_error_corr_heatmap(ppc_result_posterior_weak_discrimination)\n",
        "\n",
        "summarise_error_corr(ppc_result_posterior_weak_discrimination)\n"
      ],
      "id": "2fcd37ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_differences(summaries_weak_discrimination)"
      ],
      "id": "76da6e17",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "summaries_moderate_discrimination, ppc_result_posterior_moderate_discrimination = compare_prior_posterior(traces_discrimination['moderate_discrimination'])\n",
        "\n",
        "summaries_moderate_discrimination"
      ],
      "id": "ad21c41f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_error_corr_heatmap(ppc_result_posterior_moderate_discrimination)\n",
        "\n",
        "summarise_error_corr(ppc_result_posterior_moderate_discrimination)\n"
      ],
      "id": "026524af",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_differences(summaries_moderate_discrimination)"
      ],
      "id": "bbadf421",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "summaries_strong_discrimination, ppc_result_posterior_strong_discrimination = compare_prior_posterior(traces_discrimination['strong_discrimination'])\n",
        "\n",
        "summaries_strong_discrimination\n"
      ],
      "id": "17ebc5c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_error_corr_heatmap(ppc_result_posterior_strong_discrimination)\n",
        "\n",
        "summarise_error_corr(ppc_result_posterior_strong_discrimination)\n"
      ],
      "id": "eb082407",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_differences(summaries_strong_discrimination)"
      ],
      "id": "7649478d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Case Difficulty\n"
      ],
      "id": "f0d8d565"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def ppc_with_case_difficulty(\n",
        "    idata,\n",
        "    n_cases,\n",
        "    sigma_case,\n",
        "    rng=np.random.default_rng(123),\n",
        "    true_states = None\n",
        "):\n",
        "    \"\"\"\n",
        "    PPC generator with shared case-level shocks.\n",
        "    \"\"\"\n",
        "\n",
        "    logit_p = (idata.posterior['logit_p_juror']\n",
        "    .stack(sample=(\"chain\", \"draw\")).values)\n",
        "\n",
        "    n_jurors, n_samples = logit_p.shape\n",
        "    truth = np.zeros((n_samples, n_cases))\n",
        "    if true_states is None: \n",
        "        true_states = rng.binomial(1, 0.5, size=n_cases)\n",
        "    truth[:, ] = true_states\n",
        "    delta_case = rng.normal(0.0, sigma_case, size=(n_samples, n_cases))\n",
        "\n",
        "    votes = np.zeros((n_samples, n_cases, n_jurors), dtype=int)\n",
        "\n",
        "    for s in range(n_samples):\n",
        "        for i in range(n_cases):\n",
        "            sign = 1 if truth[s, i] == 1 else -1\n",
        "            logits = sign * logit_p[:, s] + delta_case[s, i]\n",
        "            p = 1 / (1 + np.exp(-logits))\n",
        "            votes[s, i] = rng.binomial(1, p)\n",
        "\n",
        "    return {\n",
        "        \"votes\": votes,\n",
        "        \"true_state\": truth,\n",
        "        \"delta_case\": delta_case,\n",
        "    }\n",
        "\n",
        "\n",
        "for sigma in [0.0, 0.2, 0.5, 1]:\n",
        "    ppc = ppc_with_case_difficulty(\n",
        "        traces_discrimination['weak_discrimination'],\n",
        "        n_cases=50,\n",
        "        sigma_case=sigma,\n",
        "        true_states=true_states\n",
        "    )\n",
        "\n",
        "    n_samples = ppc['votes'].shape[0]\n",
        "    corrs = []\n",
        "\n",
        "    for s in range(n_samples):\n",
        "        C = error_correlation(ppc[\"votes\"][s, :], ppc['true_state'][s])\n",
        "        corrs.append(C)\n",
        "\n",
        "    corrs = np.stack(corrs)\n",
        "    off_diag = corrs[:, ~np.eye(corrs.shape[1], dtype=bool)]\n",
        "\n",
        "    acc = [majority_accuracy(ppc[\"votes\"][i, :, :], ppc[\"true_state\"][i, :]) for i in range(n_samples)]\n",
        "\n",
        "    print(f\"\\nσ_case = {sigma}\")\n",
        "    print(\"Mean majority accuracy:\", np.mean(acc))\n",
        "    print(\"mean_corr\", np.nanmean(off_diag))\n",
        "    print(\"median_corr\", np.nanmedian(off_diag))\n",
        "    print(\"p95_abs_corr\", np.nanpercentile(np.abs(off_diag), 95))\n",
        "    print(\"nan_fraction\", np.isnan(off_diag).mean())\n"
      ],
      "id": "726331a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Model\n"
      ],
      "id": "b789a6b2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "votes, p_jurors = make_ground_truth(n_cases, n_jurors, blocks=True)\n",
        "majority_votes = (votes.mean(axis=1) > 0.5).astype(int)\n",
        "agreements_per_juror = np.array([(votes[:, j] == majority_votes).sum() for j in range(n_jurors)])\n",
        "\n",
        "block_id = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2])\n",
        "\n",
        "with pm.Model() as final_model:\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Juror skill (individual discrimination)\n",
        "    # --------------------------------------------------\n",
        "    mu_alpha = pm.Normal(\"mu_alpha\", mu=0.0, sigma=0.5)\n",
        "    sigma_alpha = pm.HalfNormal(\"sigma_alpha\", sigma=1.0)\n",
        "\n",
        "    z_juror = pm.Normal(\"z_juror\", 0.0, 1.0, shape=n_jurors)\n",
        "    alpha_j = pm.Deterministic(\n",
        "        \"alpha_j\",\n",
        "        mu_alpha + sigma_alpha * z_juror\n",
        "    )\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Block / faction effects\n",
        "    # --------------------------------------------------\n",
        "    n_blocks = len(np.unique(block_id))\n",
        "    sigma_block = pm.HalfNormal(\"sigma_block\", sigma=1.0)\n",
        "\n",
        "    block_effect = pm.Normal(\n",
        "        \"block_effect\",\n",
        "        mu=0.0,\n",
        "        sigma=sigma_block,\n",
        "        shape=n_blocks\n",
        "    )\n",
        "\n",
        "    beta_block_j = block_effect[block_id]\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Case difficulty (asymmetric, shared)\n",
        "    # --------------------------------------------------\n",
        "    mu_case = pm.Normal(\"mu_case\", mu=0.0, sigma=0.5)\n",
        "    sigma_case = pm.HalfNormal(\"sigma_case\", sigma=1.0)\n",
        "\n",
        "    # Expected difficulty effect (collapsed over cases)\n",
        "    delta_bar = pm.Normal(\n",
        "        \"delta_bar\",\n",
        "        mu=mu_case,\n",
        "        sigma=sigma_case / pm.math.sqrt(n_cases)\n",
        "    )\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Effective correctness probability\n",
        "    # --------------------------------------------------\n",
        "    logit_p_correct = alpha_j + beta_block_j + delta_bar\n",
        "    p_correct = pm.Deterministic(\n",
        "        \"p_correct\",\n",
        "        pm.math.sigmoid(logit_p_correct)\n",
        "    )\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Binomial likelihood (collapsed)\n",
        "    # --------------------------------------------------\n",
        "    pm.Binomial(\n",
        "        \"agreements\",\n",
        "        n=n_cases,\n",
        "        p=p_correct,\n",
        "        observed=agreements_per_juror\n",
        "    )\n",
        "\n",
        "    idata = pm.sample(\n",
        "        2000,\n",
        "        tune=2000,\n",
        "        target_accept=0.975,\n",
        "        return_inferencedata=True\n",
        "    )\n"
      ],
      "id": "e13912f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.summary(idata)\n"
      ],
      "id": "deb5844d",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "applied-bayesian-regression-modeling-env",
      "language": "python",
      "display_name": "applied-bayesian-regression-modeling-env",
      "path": "/Users/nathanielforde/Library/Jupyter/kernels/applied-bayesian-regression-modeling-env"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}