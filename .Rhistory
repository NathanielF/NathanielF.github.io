fig, axs = plt.subplots(1, 2, figsize=(10, 10))
axs = axs.flatten()
ax = axs[0]
ax1 = axs[1]
az.plot_forest(idata, var_names=['ksi'], combined=True, ax=ax, coords={'latent': ['SUP_P']});
az.plot_forest(idata, var_names=['ksi'], combined=True, ax=ax1, coords={'latent': ['LS']});
ax.set_yticklabels([]);
ax.set_xlabel("SUP_P");
ax1.set_yticklabels([]);
ax1.set_xlabel("LS");
ax.axvline(-2, color='red');
ax.set_title("Individual Parental Support Metric \n On Latent Factor SUP_P");
plt.show();
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pytensor import tensor as pt
import arviz as az
import networkx as nx
np.random.seed(150)
df_p = pd.read_csv('IIS.dat', sep='\s+')
df_p.head()
df = pd.read_csv('sem_data.csv')
drivers = ['se_acad_p1', 'se_acad_p2',
'se_acad_p3', 'se_social_p1', 'se_social_p2', 'se_social_p3',
'sup_friends_p1', 'sup_friends_p2', 'sup_friends_p3', 'sup_parents_p1',
'sup_parents_p2', 'sup_parents_p3', 'ls_p1', 'ls_p2', 'ls_p3']
coords = {'obs': list(range(len(df))),
'indicators': drivers,
'indicators_1': ['se_acad_p1','se_acad_p2','se_acad_p3'],
'indicators_2': ['se_social_p1','se_social_p2','se_social_p3'],
'indicators_3': ['sup_friends_p1','sup_friends_p2','sup_friends_p3'],
'indicators_4': [ 'sup_parents_p1','sup_parents_p2','sup_parents_p3'],
'indicators_5': ['ls_p1','ls_p2', 'ls_p3'],
'latent': ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS'],
'latent1': ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']
}
obs_idx = list(range(len(df)))
with pm.Model(coords=coords) as model:
Psi = pm.InverseGamma('Psi', 5, 10, dims='indicators')
lambdas_ = pm.Normal('lambdas_1', 1, 10, dims=('indicators_1'))
lambdas_1 = pm.Deterministic('lambdas1', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_1'))
lambdas_ = pm.Normal('lambdas_2', 1, 10, dims=('indicators_2'))
lambdas_2 = pm.Deterministic('lambdas2', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_2'))
lambdas_ = pm.Normal('lambdas_3', 1, 10, dims=('indicators_3'))
lambdas_3 = pm.Deterministic('lambdas3', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_3'))
lambdas_ = pm.Normal('lambdas_4', 1, 10, dims=('indicators_4'))
lambdas_4 = pm.Deterministic('lambdas4', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_4'))
lambdas_ = pm.Normal('lambdas_5', 1, 10, dims=('indicators_5'))
lambdas_5 = pm.Deterministic('lambdas5', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_5'))
tau = pm.Normal('tau', 3, 10, dims='indicators')
kappa = 0
sd_dist = pm.Exponential.dist(1.0, shape=5)
chol, _, _ = pm.LKJCholeskyCov('chol_cov', n=5, eta=2,
sd_dist=sd_dist, compute_corr=True)
cov = pm.Deterministic("cov", chol.dot(chol.T), dims=('latent', 'latent1'))
ksi = pm.MvNormal('ksi', kappa, chol=chol, dims=('obs', 'latent'))
m0 = tau[0] + ksi[obs_idx, 0]*lambdas_1[0]
m1 = tau[1] + ksi[obs_idx, 0]*lambdas_1[1]
m2 = tau[2] + ksi[obs_idx, 0]*lambdas_1[2]
m3 = tau[3] + ksi[obs_idx, 1]*lambdas_2[0]
m4 = tau[4] + ksi[obs_idx, 1]*lambdas_2[1]
m5 = tau[5] + ksi[obs_idx, 1]*lambdas_2[2]
m6 = tau[6] + ksi[obs_idx, 2]*lambdas_3[0]
m7 = tau[7] + ksi[obs_idx, 2]*lambdas_3[1]
m8 = tau[8] + ksi[obs_idx, 2]*lambdas_3[2]
m9 = tau[9] + ksi[obs_idx, 3]*lambdas_4[0]
m10 = tau[10] + ksi[obs_idx, 3]*lambdas_4[1]
m11 = tau[11] + ksi[obs_idx, 3]*lambdas_4[2]
m12 = tau[12] + ksi[obs_idx, 4]*lambdas_5[0]
m13 = tau[13] + ksi[obs_idx, 4]*lambdas_5[1]
m14 = tau[14] + ksi[obs_idx, 4]*lambdas_5[2]
mu = pm.Deterministic('mu', pm.math.stack([m0, m1, m2, m3, m4, m5, m6, m7,
m8, m9, m10, m11, m12, m13, m14]).T)
_  = pm.Normal('likelihood', mu, Psi, observed=df[drivers].values)
idata = pm.sample(nuts_sampler='numpyro', target_accept=.95, tune=1000,
idata_kwargs={"log_likelihood": True}, random_seed=100)
idata.extend(pm.sample_posterior_predictive(idata))
summary_df1 = az.summary(idata, var_names=['lambdas1', 'lambdas2', 'lambdas3', 'lambdas4', 'lambdas5', 'tau', 'Psi'])
cov_df = pd.DataFrame(az.extract(idata['posterior'])['cov'].mean(axis=2))
cov_df.index = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']
cov_df.columns = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']
correlation_df = pd.DataFrame(az.extract(idata['posterior'])['chol_cov_corr'].mean(axis=2))
correlation_df.index = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']
correlation_df.columns = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']
factor_loadings = pd.DataFrame(az.summary(idata, var_names=['lambdas1', 'lambdas2', 'lambdas3', 'lambdas4', 'lambdas5'])['mean']).reset_index()
factor_loadings['factor'] = factor_loadings['index'].str.split('[', expand=True)[0]
factor_loadings.columns =['factor_loading', 'factor_loading_weight', 'factor']
factor_loadings['factor_loading_weight_sq'] = factor_loadings['factor_loading_weight']**2
factor_loadings['sum_sq_loadings'] = factor_loadings.groupby('factor')['factor_loading_weight_sq'].transform(sum)
df = pd.read_csv('sem_data.csv')
drivers = ['se_acad_p1', 'se_acad_p2',
'se_acad_p3', 'se_social_p1', 'se_social_p2', 'se_social_p3',
'sup_friends_p1', 'sup_friends_p2', 'sup_friends_p3', 'sup_parents_p1',
'sup_parents_p2', 'sup_parents_p3', 'ls_p1', 'ls_p2', 'ls_p3']
coords = {'obs': list(range(len(df))),
'indicators': drivers,
'indicators_1': ['se_acad_p1','se_acad_p2','se_acad_p3'],
'indicators_2': ['se_social_p1','se_social_p2','se_social_p3'],
'indicators_3': ['sup_friends_p1','sup_friends_p2','sup_friends_p3'],
'indicators_4': [ 'sup_parents_p1','sup_parents_p2','sup_parents_p3'],
'indicators_5': ['ls_p1','ls_p2', 'ls_p3'],
'latent': ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS'],
'latent1': ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']
}
obs_idx = list(range(len(df)))
with pm.Model(coords=coords) as model:
Psi = pm.InverseGamma('Psi', 5, 10, dims='indicators')
lambdas_ = pm.Normal('lambdas_1', 1, 10, dims=('indicators_1'))
lambdas_1 = pm.Deterministic('lambdas1', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_1'))
lambdas_ = pm.Normal('lambdas_2', 1, 10, dims=('indicators_2'))
lambdas_2 = pm.Deterministic('lambdas2', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_2'))
lambdas_ = pm.Normal('lambdas_3', 1, 10, dims=('indicators_3'))
lambdas_3 = pm.Deterministic('lambdas3', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_3'))
lambdas_ = pm.Normal('lambdas_4', 1, 10, dims=('indicators_4'))
lambdas_4 = pm.Deterministic('lambdas4', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_4'))
lambdas_ = pm.Normal('lambdas_5', 1, 10, dims=('indicators_5'))
lambdas_5 = pm.Deterministic('lambdas5', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_5'))
tau = pm.Normal('tau', 3, 10, dims='indicators')
kappa = 0
sd_dist = pm.Exponential.dist(1.0, shape=5)
chol, _, _ = pm.LKJCholeskyCov('chol_cov', n=5, eta=2,
sd_dist=sd_dist, compute_corr=True)
cov = pm.Deterministic("cov", chol.dot(chol.T), dims=('latent', 'latent1'))
ksi = pm.MvNormal('ksi', kappa, chol=chol, dims=('obs', 'latent'))
m0 = tau[0] + ksi[obs_idx, 0]*lambdas_1[0]
m1 = tau[1] + ksi[obs_idx, 0]*lambdas_1[1]
m2 = tau[2] + ksi[obs_idx, 0]*lambdas_1[2]
m3 = tau[3] + ksi[obs_idx, 1]*lambdas_2[0]
m4 = tau[4] + ksi[obs_idx, 1]*lambdas_2[1]
m5 = tau[5] + ksi[obs_idx, 1]*lambdas_2[2]
m6 = tau[6] + ksi[obs_idx, 2]*lambdas_3[0]
m7 = tau[7] + ksi[obs_idx, 2]*lambdas_3[1]
m8 = tau[8] + ksi[obs_idx, 2]*lambdas_3[2]
m9 = tau[9] + ksi[obs_idx, 3]*lambdas_4[0]
m10 = tau[10] + ksi[obs_idx, 3]*lambdas_4[1]
m11 = tau[11] + ksi[obs_idx, 3]*lambdas_4[2]
m12 = tau[12] + ksi[obs_idx, 4]*lambdas_5[0]
m13 = tau[13] + ksi[obs_idx, 4]*lambdas_5[1]
m14 = tau[14] + ksi[obs_idx, 4]*lambdas_5[2]
mu = pm.Deterministic('mu', pm.math.stack([m0, m1, m2, m3, m4, m5, m6, m7,
m8, m9, m10, m11, m12, m13, m14]).T)
_  = pm.Normal('likelihood', mu, Psi, observed=df[drivers].values)
idata = pm.sample(nuts_sampler='numpyro', target_accept=.95, tune=1000,
idata_kwargs={"log_likelihood": True}, random_seed=100)
idata.extend(pm.sample_posterior_predictive(idata))
summary_df1 = az.summary(idata, var_names=['lambdas1', 'lambdas2', 'lambdas3', 'lambdas4', 'lambdas5', 'tau', 'Psi'])
cov_df = pd.DataFrame(az.extract(idata['posterior'])['cov'].mean(axis=2))
cov_df.index = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']
cov_df.columns = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']
correlation_df = pd.DataFrame(az.extract(idata['posterior'])['chol_cov_corr'].mean(axis=2))
correlation_df.index = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']
correlation_df.columns = ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P', 'LS']
factor_loadings = pd.DataFrame(az.summary(idata, var_names=['lambdas1', 'lambdas2', 'lambdas3', 'lambdas4', 'lambdas5'])['mean']).reset_index()
factor_loadings['factor'] = factor_loadings['index'].str.split('[', expand=True)[0]
factor_loadings.columns =['factor_loading', 'factor_loading_weight', 'factor']
factor_loadings['factor_loading_weight_sq'] = factor_loadings['factor_loading_weight']**2
factor_loadings['sum_sq_loadings'] = factor_loadings.groupby('factor')['factor_loading_weight_sq'].transform(sum)
fig, axs = plt.subplots(1, 2, figsize=(10, 10))
axs = axs.flatten()
ax = axs[0]
ax1 = axs[1]
az.plot_forest(idata, var_names=['ksi'], combined=True, ax=ax, coords={'latent': ['SUP_P']});
az.plot_forest(idata, var_names=['ksi'], combined=True, ax=ax1, coords={'latent': ['LS']});
ax.set_yticklabels([]);
ax.set_xlabel("SUP_P");
ax1.set_yticklabels([]);
ax1.set_xlabel("LS");
ax.axvline(-2, color='red');
ax.set_title("Individual Parental Support Metric \n On Latent Factor SUP_P");
ax1.set_title("Individual Life Satisfaction Metric \n On Latent Factor LS");
plt.show();
fig, axs = plt.subplots(1, 2, figsize=(10, 10))
axs = axs.flatten()
ax = axs[0]
ax1 = axs[1]
az.plot_forest(idata, var_names=['ksi'], combined=True, ax=ax, coords={'latent': ['SUP_P']});
az.plot_forest(idata, var_names=['ksi'], combined=True, ax=ax1, colors="C1", coords={'latent': ['LS']});
ax.set_yticklabels([]);
ax.set_xlabel("SUP_P");
ax1.set_yticklabels([]);
ax1.set_xlabel("LS");
ax.axvline(-2, color='red');
ax.set_title("Individual Parental Support Metric \n On Latent Factor SUP_P");
ax1.set_title("Individual Life Satisfaction Metric \n On Latent Factor LS");
plt.show();
fig, axs = plt.subplots(1, 2, figsize=(10, 10))
axs = axs.flatten()
ax = axs[0]
ax1 = axs[1]
az.plot_forest(idata, var_names=['ksi'], combined=True, ax=ax, coords={'latent': ['SUP_P']});
az.plot_forest(idata, var_names=['ksi'], combined=True, ax=ax1, colors="slateblue", coords={'latent': ['LS']});
ax.set_yticklabels([]);
ax.set_xlabel("SUP_P");
ax1.set_yticklabels([]);
ax1.set_xlabel("LS");
ax.axvline(-2, color='red');
ax.set_title("Individual Parental Support Metric \n On Latent Factor SUP_P");
ax1.set_title("Individual Life Satisfaction Metric \n On Latent Factor LS");
plt.show();
fig, axs = plt.subplots(1, 2, figsize=(10, 10))
axs = axs.flatten()
ax = axs[0]
ax1 = axs[1]
az.plot_forest(idata, var_names=['ksi'], combined=True, ax=ax, coords={'latent': ['SUP_P']});
az.plot_forest(idata, var_names=['ksi'], combined=True, ax=ax1, colors="slateblue", coords={'latent': ['LS']});
ax.set_yticklabels([]);
ax.set_xlabel("SUP_P");
ax1.set_yticklabels([]);
ax1.set_xlabel("LS");
ax.axvline(-2, color='red');
ax1.axvline(-2, color='red');
ax.set_title("Individual Parental Support Metric \n On Latent Factor SUP_P");
ax1.set_title("Individual Life Satisfaction Metric \n On Latent Factor LS");
plt.show();
57/24
47/10
def make_indirect_sem(priors):
coords = {'obs': list(range(len(df))),
'indicators': drivers,
'indicators_1': ['se_acad_p1','se_acad_p2','se_acad_p3'],
'indicators_2': ['se_social_p1','se_social_p2','se_social_p3'],
'indicators_3': ['sup_friends_p1','sup_friends_p2','sup_friends_p3'],
'indicators_4': [ 'sup_parents_p1','sup_parents_p2','sup_parents_p3'],
'indicators_5': ['ls_p1','ls_p2', 'ls_p3'],
'latent': ['SUP_F', 'SUP_P'],
'latent1': ['SUP_F', 'SUP_P'],
'latent_regression': ['SUP_F->SE_ACAD', 'SUP_P->SE_ACAD', 'SUP_F->SE_SOC', 'SUP_P->SE_SOC'],
'regression': ['SE_ACAD', 'SE_SOCIAL', 'SUP_F', 'SUP_P']
}
obs_idx = list(range(len(df)))
with pm.Model(coords=coords) as model:
Psi = pm.InverseGamma('Psi', 5, 10, dims='indicators')
lambdas_ = pm.Normal('lambdas_1',  priors['lambda'][0], priors['lambda'][1], dims=('indicators_1'))
lambdas_1 = pm.Deterministic('lambdas1', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_1'))
lambdas_ = pm.Normal('lambdas_2', priors['lambda'][0], priors['lambda'][1], dims=('indicators_2'))
lambdas_2 = pm.Deterministic('lambdas2', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_2'))
lambdas_ = pm.Normal('lambdas_3', priors['lambda'][0], priors['lambda'][1], dims=('indicators_3'))
lambdas_3 = pm.Deterministic('lambdas3', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_3'))
lambdas_ = pm.Normal('lambdas_4', priors['lambda'][0], priors['lambda'][1], dims=('indicators_4'))
lambdas_4 = pm.Deterministic('lambdas4', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_4'))
lambdas_ = pm.Normal('lambdas_5', priors['lambda'][0], priors['lambda'][1], dims=('indicators_5'))
lambdas_5 = pm.Deterministic('lambdas5', pt.set_subtensor(lambdas_[0], 1), dims=('indicators_5'))
tau = pm.Normal('tau', 3, 10, dims='indicators')
kappa = 0
sd_dist = pm.Exponential.dist(1.0, shape=2)
chol, _, _ = pm.LKJCholeskyCov('chol_cov', n=2, eta=priors['eta'],
sd_dist=sd_dist, compute_corr=True)
cov = pm.Deterministic("cov", chol.dot(chol.T), dims=('latent', 'latent1'))
ksi = pm.MvNormal('ksi', kappa, chol=chol, dims=('obs', 'latent'))
# Regression Components
beta_r = pm.Normal('beta_r', 0, 0.5, dims='latent_regression')
beta_r2 = pm.Normal('beta_r2', 0, 1, dims='regression')
resid_chol, _, _ = pm.LKJCholeskyCov('resid_chol', n=2, eta=priors['eta'],
sd_dist=sd_dist, compute_corr=True)
_ = pm.Deterministic("resid_cov", chol.dot(chol.T))
sigmas_resid = pm.MvNormal('sigmas_resid', kappa, chol=resid_chol)
# SE_ACAD ~ SUP_FRIENDS + SUP_PARENTS
regression_se_acad = pm.Normal('regr_se_acad', beta_r[0]*ksi[obs_idx, 0] + beta_r[1]*ksi[obs_idx, 1], sigmas_resid[0])
# SE_SOCIAL ~ SUP_FRIENDS + SUP_PARENTS
regression_se_social = pm.Normal('regr_se_social', beta_r[2]*ksi[obs_idx, 0] + beta_r[3]*ksi[obs_idx, 1], sigmas_resid[1])
# LS ~ SE_ACAD + SE_SOCIAL + SUP_FRIEND + SUP_PARENTS
regression = pm.Normal('regr', beta_r2[0]*regression_se_acad + beta_r2[1]*regression_se_social +
beta_r2[2]*ksi[obs_idx, 0] + beta_r2[3]*ksi[obs_idx, 1], 1)
m0 = tau[0] + regression_se_acad*lambdas_1[0]
m1 = tau[1] + regression_se_acad*lambdas_1[1]
m2 = tau[2] + regression_se_acad*lambdas_1[2]
m3 = tau[3] + regression_se_social*lambdas_2[0]
m4 = tau[4] + regression_se_social*lambdas_2[1]
m5 = tau[5] + regression_se_social*lambdas_2[2]
m6 = tau[6] + ksi[obs_idx, 0]*lambdas_3[0]
m7 = tau[7] + ksi[obs_idx, 0]*lambdas_3[1]
m8 = tau[8] + ksi[obs_idx, 0]*lambdas_3[2]
m9 = tau[9] + ksi[obs_idx, 1]*lambdas_4[0]
m10 = tau[10] + ksi[obs_idx, 1]*lambdas_4[1]
m11 = tau[11] + ksi[obs_idx, 1]*lambdas_4[2]
m12 = tau[12] + regression*lambdas_5[0]
m13 = tau[13] + regression*lambdas_5[1]
m14 = tau[14] + regression*lambdas_5[2]
mu = pm.Deterministic('mu', pm.math.stack([m0, m1, m2, m3, m4, m5, m6, m7,
m8, m9, m10, m11, m12, m13, m14]).T)
_  = pm.Normal('likelihood', mu, Psi, observed=df[drivers].values)
idata = pm.sample(10_000, chains=4, nuts_sampler='numpyro', target_accept=.99, tune=2000,
idata_kwargs={"log_likelihood": True}, random_seed=110)
idata.extend(pm.sample_posterior_predictive(idata))
return model, idata
model2, idata2 = make_indirect_sem(priors={'eta': 2, 'lambda': [1, 1]})
summary_df = az.summary(idata2, var_names=['beta_r', 'beta_r2'])
def calculate_effects(summary_df, var='SUP_P'):
#Indirect Paths
## VAR -> SE_SOC ->LS
indirect_parent_soc = summary_df.loc[f'beta_r[{var}->SE_SOC]']['mean']*summary_df.loc['beta_r2[SE_SOCIAL]']['mean']
## VAR -> SE_SOC ->LS
indirect_parent_acad = summary_df.loc[f'beta_r[{var}->SE_ACAD]']['mean']*summary_df.loc['beta_r2[SE_ACAD]']['mean']
## Total Indirect Effects
total_indirect = indirect_parent_soc + indirect_parent_acad
## Total Effects
total_effect = total_indirect + summary_df.loc[f'beta_r2[{var}]']['mean']
return pd.DataFrame([[indirect_parent_soc, indirect_parent_acad, total_indirect, total_effect]],
columns=[f'{var} -> SE_SOC ->LS', f'{var} -> SE_ACAD ->LS', f'Total Indirect Effects {var}', f'Total Effects {var}']
)
indirect_p = calculate_effects(summary_df, 'SUP_P')
indirect_f = calculate_effects(summary_df, 'SUP_F')
residuals_posterior_cov = get_posterior_resids(idata2, 500)
residuals_posterior_corr = get_posterior_resids(idata2, 500, 'corr')
